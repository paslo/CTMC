\documentclass[a4paper,reqno]{amsart}
\usepackage[british]{babel}
%\usepackage[garamond]{mathdesign}
\usepackage{mathptmx}
\usepackage{courier}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{nicefrac}
%\usepackage{pdfsync}

\renewcommand{\ttdefault}{cmtt}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
% - macros

\newcommand{\nats}{\mathbb{N}}
\newcommand{\natswith}{\nats_{0}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\realspos}{\reals_{>0}}
\newcommand{\realsnonneg}{\reals_{\geq 0}}

\newcommand{\states}{\mathcal{X}}

\newcommand{\paths}{\Omega}
\newcommand{\path}{\omega}

\newcommand{\power}{\mathcal{P}(\paths)}
\newcommand{\nonemptypower}{\power_{\emptyset}}
\newcommand{\events}{\mathcal{E}}
%\newcommand{\nonemptyevents}{\events^{\emptyset}}
\newcommand{\filter}[1][t]{\mathcal{F}_{#1}}
\newcommand{\eventst}[1][t]{\events_{#1}}

\newcommand{\processes}{\mathbb{P}}
\newcommand{\mprocesses}{\processes^{\mathrm{M}}}


\newcommand{\lt}{\underline{T}}
\newcommand{\lbound}{L}

\newcommand{\gambles}{\mathcal{L}}
\newcommand{\gamblesX}{\gambles(\states)} 

\newcommand{\ind}[1]{\mathbb{I}_{#1}}

\newcommand{\rateset}{\mathcal{Q}}
\newcommand{\lrate}{\underline{Q}}

\newcommand{\asa}{\Leftrightarrow}
\newcommand{\then}{\Rightarrow}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

\newcommand{\coloneqq}{:\!=}

\begin{document}
\title{Continuous-time Imprecise Markov Chains}
\author{Jasper De Bock\and Thomas Krak}
%\address{Ghent University, SYSTeMS Research Group, Technologiepark -- Zwijnaarde 914, 9052 Zwijnaarde, Belgium}
%\email{jasper.debock@ugent.be}

\begin{abstract}
These are some rough initial ideas and results. Notation, text and maths should all still be cleaned up; the only goal of this note is to get the ideas across.
\end{abstract}

\maketitle

\section{Preliminaries}

Consider some finite \emph{state space} $\states=\{1,\dots,m\}$. A cadlag function from $[0,+\infty)$ to $\states$ is called a \emph{path}, where cadlag means that it is right continuous for all $t\in[0,+\infty)$ and that the left limit exists for all $t\in(0,+\infty)$. Let $\paths$ be the set of all paths. For any path $\path\in\paths$ and any time point $t\in[0,+\infty)$, the value of $\path$ in $t$ is denoted by $\path(t)$.

A subset $E$ of $\paths$ is called an \emph{event}. The set of all events is denoted by $\power$ and we let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. For any $t\in[0,+\infty)$ and $x\in\states$, we define the elementary event
\begin{equation*}
(X_t=x)\coloneqq\{\path\in\paths\colon\path(t)=x\}.
\end{equation*}
Consider now any $t\in[0,+\infty)$. We then let $\events_{\geq t}$ be the algebra that is generated by all the elementary events $X_s=x$, for $s\geq t$ and $x\in\states$.\footnote{This is the smallest subset of $\power$ that contains all these elementary events and that is furthermore closed under complements, finite unions and hence also finite intersections.} Similarly, we let $\events_{\leq t}$ be the algebra that is generated by all the elementary events $X_s=x$, for $s\leq t$ and $x\in\states$. We also define $\filter\coloneqq\events_{\leq t}\setminus\{\emptyset\}$.

A full conditional finitely additive probability measure $P$ is a real-valued map from $\power\times\nonemptypower$ to $\reals$ that satisfies the following axioms. For all $A,B\in\power$ and all \mbox{$C,D\in\nonemptypower$}:
\vspace{5pt}

\begin{enumerate}[label=F\arabic*:]
\item
$P(C\vert C)=1$;
\item
$0\leq P(A\vert C)\leq 1$;
\item
$P(A\cup B\vert C)=P(A\vert C)+P(B\vert C)$ if $A\cap B=\emptyset$;
\item
$P(A\vert D)=P(A\vert C\cap D)P(C\vert D)$ if $A\subseteq C$.
\end{enumerate}
\vspace{5pt}

\noindent
For any $A\in\power$ and $C\in\nonemptypower$, we call $P(A\vert C)$ the probability of $A$ conditional on $B$. Also, for any $A\in\power$, we frequently use the shorthand notation $P(A)\coloneqq P(A\vert\paths)$ and then call $P(A)$ the probability of $A$.

\section{Stochastic processes}

A \emph{stochastic process} is the restriction of a full conditional finitely additive probability measure $P$ to some subset $\mathcal{C}$ of $\power\times\nonemptypower$ that includes at least
\begin{equation*}
\mathcal{C^*}\coloneqq\big\{
(A,C)
\colon
C\in\filter, A\in\events_{\geq t}, t\in[0,+\infty)
\big\}.
\end{equation*}
We denote the set of all such stochastic processes by $\processes$.

Let $\gamblesX$ be the set of all real-valued functions on $\states$. Consider any stochastic process $P\in\processes$ and any $t,s\in[0,+\infty)$ such that $t<s$. The transition matrix $T_t^s$ is then an $m\times m$ matrix that is defined by
\begin{equation*}
T_t^s(x_t,x_s)\coloneqq P(X_s=x_s\vert X_t=x_t)\text{ for all $x_s,x_t\in\states$}.
\end{equation*}
Obviously, this transition, matrix $T_t^s$ can also be regarded as a map from $\gamblesX$ to $\gamblesX$, defined for all $f\in\gamblesX$ by $T_t^s(f)\coloneqq T_t^sf$.

We say that a stochastic process $P\in\processes$ satisfies the \emph{Markov property} when, for any time sequence $0\leq t_1<t_2,\dots,t_{n}<t<s$ and set of states $x_{t_1},x_{t_2},\dots,x_{t_n},x_t,x_s\in\states$:
\begin{equation*}
P(X_s=x_s\vert X_t=x_t)=P(X_s=x_s\vert X_t=x_t,X_{t_1}=x_{t_1},X_{t_2}=x_{t_2}, \dots, X_{t_n}=x_{t_n}).
\end{equation*}
We denote the set of all stochastic processes that satisfy this Markov property by $\mprocesses$


\begin{lemma}\label{lemma:transitionmatrixfactorises}
Consider any $P\in\mprocesses$. Then
\begin{equation*}
T_t^s=\prod_{k=1}^n T_{t_{k-1}}^{t_k} \coloneqq T_{t_0}^{t_1}T_{t_1}^{t_2}\cdots T_{t_{n-1}}^{t_n}
\end{equation*}
for every sequence $0\leq t=t_0<t_1<t_2,\dots,t_{n}=s$.
\end{lemma}
\begin{proof}
This property is well known and therefore stated without proof.
\end{proof}

\section{The norms that are used in this document}

For any vector $f\in\gamblesX$, we let $\norm{f}\coloneqq\norm{f}_{\infty}\coloneqq\max\{\abs {f(x)}\colon x\in\states\}$ be the maximum norm. For any operator $A$ from $\gamblesX$ to $\gamblesX$ that is non-negatively homogeneous, meaning that
\begin{equation*}
A(\lambda f)=\lambda A(f)\text{ for all $f\in\gamblesX$ and all $\lambda\geq0$,}
\end{equation*}
we consider the induced operator norm
\begin{equation*}
\norm{A}\coloneqq\sup\{\norm{Af}\colon f\in\gamblesX,\norm{f}=1\}.
\end{equation*}
If $A$ is an $m\times m$ matrix, we have that
\begin{equation*}
\norm{A}
=
\max\{\sum_{y\in\states}\abs{A(x,y)}\colon x\in\states\}.
\end{equation*}


\noindent
These norms satisfy the following properties. For all $f,g\in\gamblesX$, all $A,B$ from $\gamblesX$ to $\gamblesX$ that are non-negatively homogeneous, all $\lambda\in\reals$ and all $x\in\states$, we have that
\vspace{5pt}

\begin{enumerate}[label=N\arabic*:]
\item
$\abs{f(x)}\leq\norm{f}$
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\norm{A}\geq0$
\item
$\norm{A}=0\asa A=0$
\item
$\norm{A+B}\leq\norm{A}+\norm{B}$
\item
$\norm{AB}\leq\norm{A}\norm{B}$
\item
$\norm{\lambda A}=\abs{\lambda}\norm{A}$
\item
$\norm{Af}\leq\norm{A}\norm{f}$
\item
$\norm{A}=1$ if $A$ is a stochastic matrix.
\end{enumerate}
\vspace{5pt}

\noindent
Finally, for any set $\mathcal{A}$ of $n\times n$ matrices, we define

\begin{equation*}
\norm{\mathcal{A}}\coloneqq\sup\{\norm{A}\colon A\in\mathcal{A}\}.
\end{equation*}

\begin{lemma}\label{lemma:differenceproductoftransition}
Consider two sequences $A_1,\dots,A_n$ and $B_1,\dots,B_n$ of stochastic matrixes such that, for all $i\in\{1,\dots,n\}$, $\norm{A_i-B_i}\leq c$. Then
\begin{equation*}
\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}\leq nc
\end{equation*}
\end{lemma}
\begin{proof}
We provide a proof by induction. For $n=1$, the result is trivially true. Assume that the result holds for $n=k-1$. The following derivation then shows that it also holds for $n=k$: 
\begin{align*}
\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}
&=
\norm{\prod_{i=1}^{n}A_i-\left(\prod_{i=1}^{n-1}A_i\right)B_n+\left(\prod_{i=1}^{n-1}A_i\right)B_n-\prod_{i=1}^{n}B_i}\\
&\leq
\left(\prod_{i=1}^{n-1}\norm{A_i}\right)\norm{A_n-B_n}+\norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\norm{B_n}\\
&\leq c + \norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\leq c+(n-1)c= nc.
\end{align*}
\end{proof}




\section{Sets of rate matrices and lower transition rate operators}

A real-valued $m\times m$ matrix $Q$ is said to be a rate matrix if

\vspace{5pt}
\begin{enumerate}[label=R\arabic*:]
\item
$\sum_{y\in\states}Q(x,y)=0$ for all $x\in\states$;
\item
$Q(x,y)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\noindent
We use $\mathcal{R}$ to denote the set of all rate matrices. Clearly, $\mathcal{R}$ is closed under finite sums and multiplication with non-negative scalars. For any set $\rateset\subseteq\mathcal{R}$ of rate matrices, we let
\begin{equation*}
\rateset_x\coloneqq\{Q(x,\cdot)\colon Q\in\rateset\}
\text{ for all $x\in\states$.}
\end{equation*}
We say that $\rateset$ has \emph{separately specified rows} if
\begin{equation*}
Q\in\rateset\asa(\forall x\in\states)~Q(x,\cdot)\in\rateset_x.
\end{equation*}
A set $\rateset\subseteq\mathcal{R}$ of rate matrices is said to be bounded if $\inf\{Q(x,x)\colon Q\in\rateset\}>-\infty$ for all $x\in\states$ or, equivalently, if $\norm{\rateset}<\infty$. 

For any bounded set $\rateset$ of rate matrices, the corresponding \emph{lower transition rate operator} $\lrate$ is a map from $\gamblesX$ to $\gamblesX$. For all $f\in\gamblesX$, $\lrate f$ is defined by
\begin{equation}\label{eq:deflowerbound}
(\lrate f)(x)\coloneqq\inf\{(Qf)(x)\colon Q\in\rateset\}\text{ for all $x\in\states$}.
\end{equation}
We will call a map $\lrate$ from $\gamblesX$ to $\gamblesX$ a \emph{coherent} lower transition rate operator if, for all $f,g\in\gamblesX$, $\lambda\geq0$ and $\mu\in\reals$:

\vspace{5pt}
\begin{enumerate}[label=LR\arabic*:]
\item
$\lrate(\mu)=0$;
\item
$\lrate(f+g)\geq\lrate(f)+\lrate(g)$;
\item
$\lrate(\lambda f)=\lambda\lrate(f)$;
\item
$\lrate(\ind{y})(x)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}

\begin{lemma}\label{lemma:lrateiscoherent}
For any bounded set $\rateset$ of rate matrices, the corresponding lower transition rate operator $\lrate$ is coherent.
\end{lemma}
\begin{proof}
This is immediate.
\end{proof}

For any coherent lower transition rate operator $\lrate$, we use $\rateset_{\lrate}$ to denote the set of all rate matrices $Q$ that dominate $\lrate$, in the sense that
\begin{equation*}
Qf\geq\lrate f\text{ for all $f\in\gamblesX$.}
\end{equation*}

\begin{lemma}
Consider a coherent lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices. Then $\rateset_{\lrate}$ is closed, bounded and convex, and has separately specified rows. Also, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $\lrate f=Qf$. Furthermore, $\norm{\lrate}<\infty$.
\end{lemma}
\begin{proof}
*** This is not trivial, but I am convinced it is true (we would have to adapt the proof for a similar well-known result for coherent lower previsions) ***
\end{proof}

\noindent
If $\lrate$ is the coherent lower transition rate operator that corresponds to some bounded set $\rateset$ of rate matrices, then clearly: $\rateset\subseteq\rateset_{\lrate}$.

\section{Lower transition operators}

We will call a map $\lt$ from $\gamblesX$ to $\gamblesX$ a \emph{coherent lower transition operator} if, for all $f,g\in\gamblesX$ and $\lambda\geq0$:

\vspace{5pt}
\begin{enumerate}[label=C\arabic*:]
\item
$\lt f\geq\min f$
\item
$\lt(f+g)\geq\lt(f)+\lt(g)$;
\item
$\lt(\lambda f)=\lambda\lt(f)$.
\end{enumerate}
\vspace{5pt}


Consider now any $t,s\in[0,+\infty)$ such that $t<s$ and let $\lrate$ be an arbitrary coherent lower transition rate operator . The corresponding lower transition operator $\lbound_t^s$ is then a map from $\gamblesX$ to $\gamblesX$, defined by
\begin{equation}\label{eq:lowerbound}
\lbound_t^s\coloneqq\lim_{\sigma(u)\to0}\prod_{k=1}^n(I+\Delta_k\lrate),
\end{equation}
where the limit is taken with respect to the set $\mathcal{U}_{[t,s]}$ of all sequences $t=t_0<t_1<\dots<t_n=s$ and where, for any $u\in\mathcal{U}_{[t,s]}$, we first define $\Delta_i\coloneqq t_i-t_{i-1}$ for all $i\in\{1,\dots,n\}$ and then let $\sigma(u)\coloneqq\max\{\Delta_i\colon i\in\{1,\dots,n\}\}$. The following result establishes that the limit in Equation~\eqref{eq:lowerbound} exists, and that it is a coherent lower transition operator.

\begin{theorem}\label{theo:convergencelowerbound}
For any $t,s\in[0,+\infty)$ such that $t<s$ and any coherent lower transition rate operator $\lrate$, there is a coherent lower transition operator $\lbound_t^s$ such that 
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \prod_{k=1}^n(I+\Delta_k\lrate)}<\epsilon.
\end{equation*}
\end{theorem}
\begin{proof}
See the rough sketch in Section~\ref{sec:proofsketch}.
\end{proof}

The derivative of $\lbound_t^s$ with respect to $t$ furthermore satisfies differential equations in the style of Damjan.

\begin{proposition}
Consider any $t,s\in[0,+\infty)$ such that $t<s$, let $\lrate$ be an arbitrary coherent lower transition rate operator and let $\lbound_t^s$ be the corresponding lower transition operator. Then $\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s$ and $\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate$, meaning that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\in(-\delta,\delta))~
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation*}
and
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\in(-\delta,\delta))~
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon.
\end{equation*}
\end{proposition}
\begin{proof}
*** Not 100\% sure this is true... but something along these lines must definitely hold ***
\end{proof}

\section{The Markovian (but possibly time-inhomogeneous) case}

For any bounded set of rate matrices $\rateset$, we consider the set $\mprocesses_{\rateset}$ of all $P\in\mprocesses$ such that
\begin{equation}\label{eq:conditionforMarkov}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall t\in[0,+\infty))\,
(\forall\Delta\in(0,\delta))\,
(\exists Q\in\rateset)~
\Big\lVert\frac{T_t^{t+\Delta}-I}{\Delta}-Q\Big\rVert<\epsilon.
\end{equation}
%\vspace{5pt}

\begin{theorem}
Consider any $t,s\in[0,+\infty)$ such that $t<s$ and let $\rateset$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then for any $P\in\mprocesses_\rateset$ and $f\in\gamblesX$:
\begin{equation*}
\lbound_t^sf(x)\leq T_t^sf(x).
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\mprocesses_\rateset$ and any $f\in\gamblesX$ and $x\in\states$. Assume \emph{ex absurdo} that $\lbound_t^sf(x)>T_t^sf(x)$. We prove that this leads to a contradiction.  Let $C\coloneqq t-s$ and choose $\epsilon>0$ small enough such that
\begin{equation}\label{eq:chooseepsilon}
\epsilon(1+C\norm{f})<\lbound_t^sf(x)-T_t^sf(x).
\end{equation}
Since $P\in\mprocesses_\rateset$, it follows from Equation~\eqref{eq:conditionforMarkov} that there is some $\delta>0$ such that
\begin{equation}\label{eq:1conditionforMarkov}
(\forall \tau\in[0,+\infty))\,
(\forall\Delta\in(0,\delta))\,
(\exists Q\in\rateset)~
\Big\lVert\frac{T_\tau^{\tau+\Delta}-I}{\Delta}-Q\Big\rVert<\epsilon.
\end{equation}
Furthermore, because of Equation~\eqref{eq:deflowerbound} and Theorem~\ref{theo:convergencelowerbound}, there is some $\delta'>0$ such that
\begin{equation}\label{eq:deltaprimeformula}
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta')~\abs{\lbound_t^sf(x) - \left(\left(\prod_{k=1}^n(I+\Delta_k\lrate)\right)(f)\right)(x)}<\epsilon.
\end{equation}
Now choose $n>\max\{\nicefrac{C}{\delta},\nicefrac{C}{\delta'},C\norm{\rateset}\}$. Then for $\Delta\coloneqq\nicefrac{C}{n}$, we find that $\Delta<\delta$, $\Delta<\delta'$ and $\Delta\norm{\rateset}<1$. 

For all $k\in\{0,1,\dots,n\}$, define $t_k\coloneqq t+k\Delta$. Since $\Delta<\delta$, it follows from Equation~\eqref{eq:1conditionforMarkov} that, for all $i\in\{1,\dots,n\}$, there is some $Q_i\in\rateset$ such that 
\begin{equation*}
\norm{T_{t_{i-1}}^{t_i}-(I+\Delta Q_i)}
=\norm{\frac{T_{t_{i-1}}^{t_i}-I}{\Delta}-Q_i}\Delta
<\epsilon\Delta.
\end{equation*}
Furthermore, for all $i\in\{1,\dots,n\}$, since $\abs{\Delta Q_i(x,y)}\leq\norm{\Delta Q_i}=\Delta\norm{Q_i}\leq\Delta\norm{\rateset}<1$ for all $x,y\in\states$, we have that $I+\Delta Q_i$ is a stochastic matrix.
Therefore, due to Lemmas~\ref{lemma:transitionmatrixfactorises} and~\ref{lemma:differenceproductoftransition}, we find that
\begin{equation*}
\norm{T_t^s-\prod_{i=1}^n(I+\Delta Q_i)}
=\norm{\prod_{i=1}^n T_{t_{i-1}}^{t_i}-\prod_{i=1}^n(I+\Delta Q_i)}
\leq\epsilon\Delta n=\epsilon C,
\end{equation*}
which implies that
\begin{align}
\abs{T_t^sf(x)-\left(\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)\right)(x)}
&\leq
\norm{T_t^sf-\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)}\notag\\
&\leq
\norm{T_t^s-\prod_{i=1}^n(I+\Delta Q_i)}\norm{f}
\leq\epsilon C\norm{f}.\label{eq:firstpartofbound}
\end{align}
\noindent
Equation~\eqref{eq:deflowerbound} implies that
\begin{equation}
\left(\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)\right)(x)
\geq
\left((I+\Delta\lrate)^n(f)\right)(x)\label{eq:middlepartofbound}
\end{equation}
and, since $\Delta<\delta'$, it follows from Equation~\eqref{eq:deltaprimeformula} that
\begin{equation}\label{eq:lastpartofbound}
\abs{\lbound_t^sf(x) - \left((I+\Delta\lrate)^n(f)\right)(x)}<\epsilon.
\end{equation}
\noindent
By combining Equations~\eqref{eq:firstpartofbound}, \eqref{eq:middlepartofbound} and~\eqref{eq:lastpartofbound}, we find that
\begin{align*}
T_t^sf(x)
&\geq
\left(\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)\right)(x)-\epsilon C\norm{f}\\
&\geq
\left((I+\Delta\lrate)^n(f)\right)(x)-\epsilon C\norm{f}
>\lbound_t^sf(x)-\epsilon-\epsilon C\norm{f},
\end{align*}
which provides the required contradiction; see Equation~\eqref{eq:chooseepsilon}.
\end{proof}

\begin{theorem}
Consider any $t,s\in[0,+\infty)$ such that $t<s$ and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$, there is some $P\in\mprocesses_{\rateset}$ such that
\begin{equation*}
\lbound_t^sf=T_t^sf.
\end{equation*}
\end{theorem}
\begin{proof}
*** The time-inhomogenous Markov chain for which $Q_v\coloneqq\lrate\lbound_v^sf$ establishes the equality. ***
\end{proof}

\section{Lemma's which I think will turn out to be useful}

\begin{lemma}\label{lemma:smallerpartition}
Consider a rate matrix $Q$. Then for all $\epsilon>0$, there is some $\delta>0$ such that for all $\Delta_1,\dots,\Delta_n>0$ with $\Delta\coloneqq\sum_{j=1}^n\Delta_j<\delta$, it holds that
\begin{equation*}
\Delta<\delta\then\left\lVert\left(\prod_{j=1}^n[I+\Delta_j Q]\right)-(I+\Delta Q)\right\rVert<\epsilon\Delta
\end{equation*}


\end{lemma}
\begin{proof}
Fix $\epsilon>0$. Since $\frac{d}{dt}e^{t\lVert Q\rVert}\vert_{t=0}=\lVert Q\rVert$, there is some $\delta>0$ such that, for all $0<t<\delta$, $\big\vert e^{t\lVert Q\rVert}- (1+t\lVert Q\rVert)\big\vert<\epsilon t$. Consider now any $\Delta_1,\dots,\Delta_n>0$ such that $\Delta\coloneqq\sum_{j=1}^n\Delta_j<\delta$. Then
\begin{multline*}
\left\lVert\left(\prod_{j=1}^n[I+\Delta_j Q]\right)-(I+\Delta Q)\right\rVert\\
\begin{aligned}
&=
\left\lVert\left(I+Q\sum_{j=1}^n\Delta_j+Q^2\sum_{1\leq j<j'\leq n}\Delta_j\Delta_{j'}+ \dots +Q^n\prod_{j=1}^n\Delta_j\right)-(I+\Delta Q)\right\rVert\\
&=
\left\lVert Q^2\sum_{1\leq j<j'\leq n}\Delta_j\Delta_{j'}+ \dots +Q^n\prod_{j=1}^n\Delta_j \right\rVert\\
&\leq
{\lVert Q\rVert}^2\sum_{1\leq j<j'\leq n}\Delta_j\Delta_{j'}+ \dots +{\lVert Q\rVert}^n\prod_{j=1}^n\Delta_j\\
&=\left(\prod_{j=1}^n[1+\Delta_j\lVert Q\rVert]\right)-(1+\Delta\lVert Q\rVert)\\
&\leq \left(1+\frac{\Delta\lVert Q\rVert}{n}\right)^n-(1+\Delta\lVert Q\rVert)
\leq e^{\Delta\lVert Q\rVert}-(1+\Delta\lVert Q\rVert)
<\epsilon\Delta.
\end{aligned}
\end{multline*}
\end{proof}

\section{Rough sketch of the proof of Theorem~\ref{theo:convergencelowerbound}}\label{sec:proofsketch}

This section gives a rough sketch of the proof of Theorem~\ref{theo:convergencelowerbound}. There are still many claims in here, which need to be proved, but the general structure of the proof is completely there. I won't work on this anymore (at least for now). Feel free to use this structure to come up with a solid formal proof. 


We start with a bunch of lemma's and propositions.

\begin{lemma}\label{lemma:normQsmallenough}
If $0<\Delta\leq\nicefrac{1}{\norm{\lrate}}$, then $I+\Delta\lrate$ is a coherent lower transition operator.
\end{lemma}
\begin{proof}
Just check each of the three defining properties. C2 and C3 are trivial. C1 requires a bit more work.
\end{proof}

\begin{lemma}\label{lemma:compositioncoherence}
If $\lt_1,\lt_2,\dots,\lt_n$ are coherent lower transition operators, then  $\lt_1\lt_2\cdots\lt_n$ is also a coherent lower transition operator.
\end{lemma}
\begin{proof}
Simply check each of the properties.
\end{proof}

\begin{lemma}\label{lemma:productiscoherent}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$. Then
\begin{equation*}
\prod_{k=i}^n(I+\Delta_i\lrate)
\end{equation*}
is a coherent lower transition operator.
\end{lemma}
\begin{proof}
Trivial consequence of Lemma~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}.
\end{proof}

\begin{lemma}\label{lemma:normofcoherenttrans}
For any coherent lower transition operator $\lt$, we have that $\norm{\lt}\leq 1$.
\end{lemma}
\begin{proof}
This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttrans}
For any coherent lower transition operator $\lt$ and any two non-negatively homogeneous operators $A$, $B$, we have that $\norm{\lt A-\lt B}\leq \norm{A-B}$.
\end{lemma}
\begin{proof}
This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttransrate}
Consider any two non-negatively homogeneous operators $A$, $B$. It then holds that $\norm{\lrate A-\lrate B}\leq 2\norm{\lrate}\norm{A-B}$.
\end{lemma}
\begin{proof}
This can be shown to follow from the definition of the norm and the properties of $\lrate$.
\end{proof}


\begin{lemma}\label{lemma:justtheindicator}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
&=\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)+\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{(I+\Delta_n\lrate)-I}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&=\Delta_n\norm{\lrate}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttrans}. By repeating this argument over and over again (actually, by induction), we find that
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
\leq \Delta_n\norm{\lrate} +\Delta_{n-1}\norm{\lrate}+\cdots
+\Delta_1\norm{\lrate}
=\Delta\norm{\lrate}.
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:justthelinearpart}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1\norm{\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I},
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttransrate}. Due to Lemma~\ref{lemma:justtheindicator}, this implies that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right).
\end{align*}
By continuing in this way (applying induction) we find that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq
\norm{\prod_{i=n}^n(I+\Delta_i\lrate)-(I+\sum_{i=n}^n\Delta_i\lrate)}
+2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=2\norm{\lrate}^2\sum_{k=1}^n\Delta_k\sum_{i=k+1}^n\Delta_i\\
&\leq2\norm{\lrate}^2\frac{1}{2}\left(\sum_{k=1}^n\Delta_k\right)^2=\Delta^2\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:differencebetweennested}
For any $k\in\{1,\dots,n\}$, consider a sequence of $\Delta_{k,i}>0$, $i=1,\dots,n_k$ and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{n,k}$. Let $\Delta\coloneqq\sum_{k=1}^n\Delta_k$ and let $\alpha\coloneqq\max\{\Delta_k\colon k\in\{1,\dots,n\}\}$. If $\alpha\leq\nicefrac{1}{\norm{\lrate}}$, then
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&=\norm{\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&=\norm{
\left(
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
},
\end{align*}

\noindent
which, because of Lemma~\ref{lemma:productiscoherent}, \ref{lemma:normofcoherenttrans} and~\ref{lemma:differencenormofcoherenttrans}, implies that

\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
(I+\Delta_n\lrate)
}\\
&\leq
\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
+
\Delta_n^2\norm{\lrate}^2,
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:justthelinearpart}.

By continuing in this way (applying induction), we find that
\begin{align*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
&\leq
\Delta_1^2\norm{\lrate}^2+\cdot+\Delta_k^2\norm{\lrate}^2+\cdot
+
\Delta_n^2\norm{\lrate}^2\\
&\leq
\alpha\Delta_1\norm{\lrate}^2+\cdot+\alpha\Delta_k\norm{\lrate}^2+\cdot
+
\alpha\Delta_n\norm{\lrate}^2\\
&=
\alpha\Delta\norm{\lrate}^2
\end{align*}


\end{proof}


For any $u\in\mathcal{U}_{[t,s]}$, we now let
\begin{equation*}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate).
\end{equation*}

\begin{proposition}\label{prop:differencebetweenu}
Consider any $u,u^*\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\alpha$ and $\sigma(u^*)<\alpha$, with $0<\alpha\leq\nicefrac{1}{\norm{\lrate}}$. Let $\Delta\coloneqq t-s$. Then $\norm{\Phi_u-\Phi_{u^*}}\leq 2\alpha\Delta\norm{\lrate}^2$
\end{proposition}
\begin{proof}
Consider any $u'\in\mathcal{U}_{[t,s]}$ that is finer than $u$ and $u^*$, meaning that the timepoints it consists of contain the timepoints in $u$ and the timepoints in $u^*$. For example, let $u'$ be the ordered union of the timepoints in $u$ and $u^*$.

This implies that, for all $k\in\{1,\dots,n\}$, there is some sequence $\Delta_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta_k=\sum_{i=1}^{n_k}\Delta_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_u}\leq\alpha\Delta\norm{\lrate}^2$. 

Similarly, for all $k\in\{1,\dots,n^*\}$, there is some sequence $\Delta^*_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta^*_k=\sum_{i=1}^{n_k}\Delta^*_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^{n^*}\left(\prod_{i=1}^{n^*_k}(I+\Delta^*_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_{u^*}}\leq\alpha\Delta\norm{\lrate}^2$.

Hence, we find that
\begin{equation*}
\norm{\Phi_{u}-\Phi_{u^*}}
=
\norm{\Phi_{u}-\Phi_{u'}+\Phi_{u'}-\Phi_{u^*}}
\leq
\norm{\Phi_{u}-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u^*}}
\leq2\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{proof}

\begin{corollary}\label{corol:cauchy}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ is a \emph{cauchy sequence}, meaning that
\begin{equation*}
(\forall \epsilon>0)(\exists N\in\nats)(\forall n,m\geq N)
\norm{\Phi_{u_n}-\Phi_{u_m}}<\epsilon.
\end{equation*}
\end{corollary}
\begin{proof}
This follows almost directly from Proposition~\ref{prop:differencebetweenu}.
\end{proof}

\begin{lemma}\label{lemma:completemetricspace}
The set of all coherent lower transition operators is a complete metric space.
\end{lemma}
\begin{proof}
Since a coherent lower transition operator is just a finite vector of coherent lower previsions, this result should follow fairly easily for the (known) fact that the set of all coherent lower previsions (on a given fixed space) is a complete metric space.
\end{proof}

\begin{corollary}\label{corol:limitexistsandiscoherent}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a coherent lower transition operator.
\end{corollary}
\begin{proof}
Since $\lim_{n\to\infty}\sigma(u_n)=0$, and because of Lemma~\ref{lemma:productiscoherent}, there is some index $i$ such that the sequence $\Phi_{u_i},\Phi_{u_{i+1}},\dots,\Phi_{u_n},\dots$ consists of coherent lower transition operators. Due to Corollary~\ref{corol:cauchy}, this sequence is cauchy and therefore, because of Lemma~\ref{lemma:completemetricspace}, this sequence has a limit that is also a coherent lower transition operator. Since the limit starting from $i$ and the limit starting from $1$ are identical (initial elements do not influence the limit), we find that the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ has a limit, and that this limit is a coherent lower transition operator.
\end{proof}

Proving Theorem~\ref{theo:convergencelowerbound} is now easy. Just consider any sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$. Due to Corollary~\ref{corol:limitexistsandiscoherent}, the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a coherent lower transition operator, which we denote by $\lbound_t^s$. 

Consider now any $\epsilon>0$ and let $\Delta\coloneqq t-s$ and
\begin{equation*}
\delta\coloneqq\min\left\{\frac{\epsilon}{4\Delta\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
\end{equation*}

Since $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to $\lbound_t^s$, there is some $N\in\nats$ such that
\begin{equation*}
(\forall n\geq N)~\norm{\lbound_t^s - \Phi_{u_n}}<\frac{\epsilon}{2}.
\end{equation*}
Therefore, since $\lim_{n\to\infty}\sigma(u_n)=0$, there is some $N^*\geq N$ such that
\begin{equation*}
\sigma(u_{N^*})<\delta\text{ and }\norm{\lbound_t^s - \Phi_{u_{N^*}}}<\frac{\epsilon}{2}
\end{equation*}

Consider now any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then

\begin{equation*}
\norm{\lbound_t^s - \Phi_u}\leq\norm{\lbound_t^s-\Phi_{u_{N^*}}}
+\norm{\Phi_{u_{N^*}}-\Phi_u}
<\frac{\epsilon}{2}+2\delta\Delta\norm{\lrate}^2\leq\epsilon,
\end{equation*}
where the strict inequality follows from Proposition~\ref{prop:differencebetweenu}.
In summary, we have shown that there is some coherent lower transition operator $\lbound_t^s$ such that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon,
\end{equation*}
thereby proving Theorem~\ref{theo:convergencelowerbound}.

\newpage
\section{The Non-Markovian Case with Nice Notation}

\subsection{Operators and Norms}
For any $t,s \in \realsnonneg$ such that $t < s$, let $\mathcal{U}_{[t,s]}$ denote the set of all finite sequences $t=t_0 < t_1 < \cdots < t_n=s$ as before. For any $u\in\mathcal{U}_{[t,s]}$, we will now define
\begin{equation*}
\states^u\coloneqq \prod_{i=0}^n\states
\end{equation*}
to be the joint state space at times $t_0,\ldots,t_n$. Let $\gambles(\states^u)$ denote the set of gambles on $(X_{t_0},\ldots,X_{t_n})$. For any $u\in\mathcal{U}_{[t,t']}$ and $v\in\mathcal{U}_{[s,s']}$, denote $\states^{u\cup v}\coloneqq\states^u\times\states^v$.

For a given $u\in\mathcal{U}_{[t,s]}$ and $\omega\in\Omega$, define the $u$-discretized path $\omega^u\coloneqq \omega(t_0),\ldots,\omega(t_n)$. Denote the entire set of $u$-discretized paths as $\Omega_{\mathcal{U}_{[t,s]}}$. For any $\omega^u\in\Omega_{\mathcal{U}_{[t,s]}}$ and $f\in\gambles(\states^u)$, we introduce the shorthand notation
\begin{equation*}
f(\omega^u) \equiv f(\omega(t_0),\ldots,\omega(t_n))\,.
\end{equation*}

Now, for any $t,s\in\realsnonneg$ such that $t<s$, and any $u\in\mathcal{U}_{[0,t]}$, let $A_u^{\{s\}}$ be a non-negatively homogeneous operator from $\gambles(\states^{u\cup \{s\}})$ to $\gambles(\states^u)$.
As before, define the norm for any vector $f\in\gambles(\states^u)$ as
\begin{equation*}
\norm{f}\coloneqq\norm{f}_\infty\coloneqq\max\left\{\bigl\vert f(x_{t_0},\ldots,x_{t_n})\bigr\vert\,:\,(x_{t_0},\ldots,x_{t_n})\in\states^u\right\}\,.
\end{equation*}
For a non-negatively homogeneous operator $A_u^{\{s\}}$, we then define its norm as
\begin{equation*}
\norm{A_u^{\{s\}}} \coloneqq \sup\left\{ \norm{A_u^{\{s\}}f}\,:\,f\in\gambles(\states^{u\cup \{s\}}),\norm{f}=1 \right\}\,.
\end{equation*}
This norm satisfies the following properties. For all $t,s,s'\in\realsnonneg$ such that $t<s<s'$, all $u\in\mathcal{U}_{[0,t]}$, all $f,g\in\gambles(\states^u)$ and $h\in\gambles(\states^{u\cup\{s\}})$, all $A_u^{\{s\}},B_u^{\{s\}}$ from $\gambles(\states^{u\cup \{s\}})$ to $\gambles(\states^u)$ and $C_{u\cup \{s\}}^{\{s'\}}$ from $\gambles(\states^{u\cup \{s\}\cup \{s'\}})$ to $\gambles(\states^{u\cup \{s\}})$ that are non-negatively homogeneous, all $\lambda\in\mathbb{R}$ and all $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$, we have that
\vspace{5pt}

\begin{enumerate}[label=N\arabic*:]
\item
$\abs{f(\omega^u)}\leq\norm{f}$
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\norm{A_u^{\{s\}}}\geq0$
\item
$\norm{A_u^{\{s\}}}=0\asa A_u^{\{s\}}=0$
\item
$\norm{A_u^{\{s\}}+B_u^{\{s\}}}\leq\norm{A_u^{\{s\}}}+\norm{B_u^{\{s\}}}$
\item
$\norm{A_u^{\{s\}}C_{u\cup \{s\}}^{\{s'\}}}\leq\norm{A_u^{\{s\}}}\norm{C_{u\cup \{s\}}^{\{s'\}}}$
\item
$\norm{\lambda A_u^{\{s\}}}=\abs{\lambda}\norm{A_u^{\{s\}}}$
\item
$\norm{A_u^{\{s\}}h}\leq\norm{A_u^{\{s\}}}\norm{h}$
\item
$\norm{A_u^{\{s\}}}=1$ if $A_u^{\{s\}}$ is stochastic.
\end{enumerate}
\vspace{5pt}
%Consider now any $v\in\mathcal{U}_{[s,s']}$ such that $v=s_0,\ldots,s_n$, and any sequence of non-negatively homogeneous operators $A_u^{\{s_0\}},A_{u\cup\{s_0\}}^{\{s_1\}},\ldots,A_{u\cup\{s_0,\ldots,s_{n-1}\}}^{\{s_n\}}$. We introduce the (non-negatively homogeneous) operator $A_u^v$, defined as
%\begin{equation*}
%A_u^v \coloneqq A_u^{\{s_0\}}A_{u\cup\{s_0\}}^{\{s_1\}}\cdots A_{u\cup\{s_0,\ldots,s_{n-1}\}}^{\{s_n\}}\,.
%\end{equation*}

%\begin{equation*}
%A_u^vf(\omega) \equiv \left[A_u^vf\right]\bigl(\omega(t_0),\ldots,\omega(t_n)\bigr)\,.
%\end{equation*}

%As a special case of an operator from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$, we will define for all $f\in\gambles(\states^{u\cup v})$ the operator
%\begin{equation*}
%T_u^vf \coloneqq \mathbb{E}_{X_{s_0},\ldots,X_{s_{n^*}}}\bigl[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_{n^*}})\,\vert\,X_{t_0},\ldots,X_{t_n}\bigr]\,,
%\end{equation*}
%where $u\equiv t_0,\ldots,t_n$ and $v\equiv s_0,\ldots,s_{n^*}$. We now introduce the shorthand notation $X^u=X_{t_0},\ldots,X_{t_n}$, so that we can write
%\begin{equation*}
%T_u^vf = \mathbb{E}_{X^v}\bigl[f(X^u,X^v)\,\vert\,X^u\bigr]\,.
%\end{equation*}
%Because it holds trivially that $\gambles(\states^v)\subset\gambles(\states^{u\cup v})$, we can simply use the same notation for any $f\in\gambles(\states^v)$, so that
%\begin{equation*}
%T_u^vf = \mathbb{E}_{X^v}\bigl[f(X^v)\,\vert\,X^u\bigr]\,.
%\end{equation*}
%Finally, we will write an operator with a specific history as $A_u^v(\omega)$, defined for all $f\in\gambles(\states^{u\cup v})$ as
%\begin{equation*}
%A_u^v(\omega)f \coloneqq A_u^vf(\omega).
%\end{equation*}
%Thus, $A_u^v(\omega)$ is a map from $\gambles(\states^{u\cup v})$ to $\mathbb{R}$.

As a special case of an operator from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, we will define for all $f\in\gambles(\states^{u\cup\{s\}})$ the operator
\begin{equation*}
T_u^{\{s\}}f \coloneqq \mathbb{E}_{X_{s}}\bigl[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0},\ldots,X_{t_n}\bigr]\,.
\end{equation*}
Because it holds trivially that $\gambles(\states^{\{s\}})\subset \gambles(\states^{u\cup\{s\}})$, we can simply use the same notation for any $g\in\gambles(\states^{\{s\}})$, so that
\begin{equation*}
T_u^{\{s\}}g \equiv \mathbb{E}_{X_s}\bigl[g(X_s)\,\vert\,X_{t_0},\ldots,X_{t_n}\bigr]\,.
\end{equation*}

We have the following result for transition operators.
\begin{lemma}\label{lemma:nonmarkov_transition_decompose}
Consider any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, and any $P\in\mathbb{P}$ with transition operator $T_u^{\{s\}}$. Then, for all $v\in\mathcal{U}_{[t,s]}$ such that $v= \tau_0,\ldots,\tau_m$, it holds that
\begin{align*}
T_u^{\{s\}} &= T_{u\cup\{\tau_0\}}^{\{\tau_1\}}T_{u\cup\{\tau_0,\tau_1\}}^{\{\tau_2\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} \\
 &= \prod_{i=1}^{m} T_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\,.
\end{align*}
\end{lemma}
\begin{proof}
This follows directly from the definition and composition properties of expectation.
\end{proof}

Moving on, we will write an operator with a specific history as $A_u^{\{s\}}(\omega^u)$, defined for all $f\in\gambles(\states^{u\cup\{s\}})$ as
\begin{equation*}
A_u^{\{s\}}(\omega^u)f \coloneqq \left[A_u^{\{s\}}f\right](\omega^u)\,.
\end{equation*}
Thus, $A_u^{\{s\}}(\omega^u)$ is a map from $\gambles(\states^{u\cup\{s\}})$ to $\mathbb{R}$. For example, we have for any $g\in\gambles(\states^{\{s\}})$ that
\begin{equation*}
T_u^{\{s\}}(\omega^u)g = \mathbb{E}_{X_s}\bigl[g(X_s)\,\vert\,X_{t_0}=\omega(t_0),\ldots,X_{t_n}=\omega(t_n)\bigr]\,.
\end{equation*}

More generally, we will use a similar notation to denote ``partially fixed'' histories. For example, we will write $A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})$ for the map from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{t_n\}})$. 

Furthermore, we note that if $A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})$ is a linear operator with a partially fixed history, it can be interpreted as a matrix when applied to functions $f\in\gambles(\states^{\{s\}})$. Thus, for a given set of matrices $\mathcal{M}$ and given $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$, we will write
\begin{align*}
A_u^{\{s\}}(x_{t_0},&\ldots,x_{t_{n-1}})\in\mathcal{M} \\
&\Leftrightarrow \\
(\exists M\in\mathcal{M})(\forall x_{t_n}\in\states)(\forall x_s\in\states): &\quad A_u^{\{s\}}(x_{t_0},\ldots,x_{t_n},x_s) = M(x_{t_n},x_s)\,.
\end{align*}
Similarly, we will define addition and subtraction operators between matrices and linear operators with partially fixed histories. Specifically, for any matrix $M$ and linear operator $A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{t_n\}})$, we will define a new operator $\left(M + A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\right)$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{t_n\}})$, defined for all $f\in\gambles(\states^{u\cup\{s\}})$ as
\begin{align*}
\left[\left(M + A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\right)f\right](x_{t_n}) &\coloneqq \sum_{x_s\in\states} \left(M + A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\right)(x_{t_n},x_s) f(x_{t_0},\ldots,x_{t_n},x_s) \\
 &= \sum_{x_s\in\states} \left(M(x_{t_n},x_s) + A_u^{\{s\}}(x_{t_0},\ldots,x_{t_n},x_s)\right)f(x_{t_0},\ldots,x_{t_n},x_s)\,.
\end{align*}
We then extend this operator to an operator $\left(M + A_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_n})$ with fully fixed history, and thus a map from $\gambles(\states^{u\cup\{s\}})$ to $\mathbb{R}$, defined for all $f\in\gambles(\states^{u\cup\{s\}})$ as
\begin{equation*}
\left(M + A_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_n})f \coloneqq \left[\left(M + A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\right)f\right](x_{t_n})\,.
\end{equation*}
This finally leads us to consider the corresponding map $\left(M + A_u^{\{s\}}\right)$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, defined as
\begin{equation*}
\left[\left(M + A_u^{\{s\}}\right)f\right](x_{t_0},\ldots,x_{t_n}) \coloneqq \left(M + A_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_n})f\,.
\end{equation*}

We end this section with the following result.
\begin{lemma}\label{lemma:nonmarkov_fixedhistory_bound_to_global_bound}
Consider two operators $A_u^{\{s\}}$ and $B_u^{\{s\}}$ such that for all $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$ it holds that $\norm{A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}}) - B_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})} < c$. Then,
\begin{equation*}
\norm{A_u^{\{s\}} - B_u^{\{s\}}} \leq c\,.
\end{equation*}
\end{lemma}
\begin{proof}
From the definition of the norm, we have
\begin{align*}
&\quad\norm{A_u^{\{s\}} - B_u^{\{s\}}} \\
&= \sup\left\{\norm{\left(A_u^{\{s\}} - B_u^{\{s\}}\right)f}\,:\,f\in\gambles(\states^{u\cup\{s\}}),\norm{f}=1\right\} \\
&= \sup\left\{\max\left\{\left\vert \left[\left(A_u^{\{s\}} - B_u^{\{s\}}\right)f\right](x_{t_0},\ldots,x_{t_n}) \right\vert\,:\,(x_{t_0},\ldots,x_{t_n})\in\states^u\right\}\,:\,f\in\gambles(\states^{u\cup\{s\}}),\norm{f}=1\right\} \\
&= \sup\left\{\max\left\{\left\vert \left[\left(A_u^{\{s\}} - B_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) \right\vert\,:\,(x_{t_0},\ldots,x_{t_n})\in\states^u\right\}\,:\,f\in\gambles(\states^{u\cup\{s\}}),\norm{f}=1\right\} \\
&\leq \sup\left\{\max\left\{\norm{ \left(A_u^{\{s\}} - B_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_{n-1}})f} \,:\,(x_{t_0},\ldots,x_{t_n})\in\states^u\right\}\,:\,f\in\gambles(\states^{u\cup\{s\}}),\norm{f}=1\right\} \\
&= \sup\left\{\max\left\{\norm{ \left(A_u^{\{s\}} - B_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_{n-1}})f} \,:\,(x_{t_0},\ldots,x_{t_{n-1}})\in\states^u\right\}\,:\,f\in\gambles(\states^{u\cup\{s\}}),\norm{f}=1\right\} \\
&\leq \sup\left\{\max\left\{\norm{ \left(A_u^{\{s\}} - B_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_{n-1}})} \,:\,(x_{t_0},\ldots,x_{t_{n-1}})\in\states^u\right\}\,:\,f\in\gambles(\states^{u\cup\{s\}}),\norm{f}=1\right\} \\
&= \max\left\{\norm{ \left(A_u^{\{s\}} - B_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_{n-1}})} \,:\,(x_{t_0},\ldots,x_{t_{n-1}})\in\states^u\right\} \\
&= \max\left\{\norm{ A_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}}) - B_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})} \,:\,(x_{t_0},\ldots,x_{t_{n-1}})\in\states^u\right\} \\
&< \max\left\{c \,:\,(x_{t_0},\ldots,x_{t_{n-1}})\in\states^u\right\} \\
&= c\,,
\end{align*}
where the last inequality follows from the assumed bound on all fixed-history norms.
\end{proof}

%Note that there is also another interpretation for such an operator. Let
%\begin{equation*}
%\gambles(\states^{u\cup\{s\}})\downarrow(x_{t_0},\ldots,x_{t_{n-1}}) \coloneqq \left\{ f(x_{t_0},\ldots,x_{t_{n-1}},X_{t_n},X_s)\,:\, f\in\gambles(\states^{u\cup\{s\}}) \right\}\,.
%\end{equation*}

%\subsection{Transition Operators}

%Consider any stochastic process $P\in\mathbb{P}$ with transition operator $T_u^{\{s\}}$ such that $u\equiv t_0,\ldots,t_n$. Then, for all $w\in\mathcal{U}_{[t_n,s]}$ such that $w\equiv \tau_0,\ldots,\tau_m$, it holds that
%\begin{align*}
%T_u^v &= T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\tau_1}T_{u\cup\{\tau_0,\tau_1\}}^{\{\tau_2\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}T_{u\cup w}^v \\
% &= T_u^{\{\tau_0\}}\left[\prod_{i=0}^{m-1} T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}}\right]T_{u\cup w}^v\,.
%\end{align*}
%Because $t_n=\tau_0$ we have $u\cup\{\tau_0\}= u$, whence $T_u^{\tau_0}=I$. Thus, the above simplifies as
%\begin{equation*}
%T_u^v = \left[\prod_{i=0}^{m-1} T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}}\right]T_{u\cup w}^v
%\end{equation*}
%For notational convenience, we will introduce the term $\tau_{m+1}\coloneqq v \equiv s_0,\ldots,s_{n^*}$, so that we can write
%\begin{equation*}
%T_u^v = \prod_{i=0}^{m} T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}}\,.
%\end{equation*}
%The below establishes that this factorization is actually valid.
%\begin{lemma}
%Consider any $P\in\mathbb{P}$ with transition operator $T_u^v$. Then, for all $w\in\mathcal{U}_{[t_n,s_0]}$, it holds that
%\begin{equation*}
%T_u^v = \prod_{i=0}^{m} T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}}\,,
%\end{equation*}
%where we have written $\tau_{m+1}\equiv s_0,\ldots,s_{^*}$.
%\end{lemma}
%\begin{proof}
%By definition, we have that
%\begin{equation*}
%T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}} = \mathbb{E}_{X_{\tau_{i+1}}}\bigl[ [\cdot](X^u,X_{\tau_0},\ldots,X_{\tau_i},X_{\tau_{i+1}})\,\vert\,X^u,X_{\tau_0},\ldots,X_{\tau_i}\bigr]\,.
%\end{equation*}
%Hence, we have
%\begin{align*}
%&\quad T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}}T_{u\cup\{\tau_0,\ldots,\tau_{i+1}\}}^{\{\tau_{i+2}\}} \\
%&= \mathbb{E}_{X_{\tau_{i+1}}}\Bigl[\mathbb{E}_{X_{\tau_{i+2}}}\bigl[[\cdot](X^u,X_{\tau_0},\ldots,X_{\tau_{i+2}})\,\vert\,X^u,X_{\tau_0},\ldots,X_{\tau_i},X_{\tau_{i+1}}\bigr]\,\big\vert\,X^u,X_{\tau_0},\ldots,X_{\tau_i}\Bigr] \\
% &= \mathbb{E}_{X_{\tau_{i+2}}}\bigl[[\cdot](X^u,X_{\tau_0},\ldots,X_{\tau_i},X_{\tau_{i+2}})\,\vert\,X^u,X_{\tau_0},\ldots,X_{\tau_i}\bigr] \\
%&= T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+2}\}}\,.
%\end{align*}
%Applying this argument to the full factorization in the lemma gives
%\begin{align*}
%\prod_{i=0}^{m} T_{u\cup\{\tau_0,\ldots,\tau_i\}}^{\{\tau_{i+1}\}} &= \mathbb{E}_{X_{\tau_1}}\Bigl[ \cdots \mathbb{E}_{X^v}\bigl[[\cdot](X^u,X^w,X^v)\,\vert\,X^u,X^w\bigr]\cdots \,\big\vert\, X^u\Bigr] \\
% &= \mathbb{E}_{X^v}\bigl[[\cdot](X^u,X^v)\,\vert\,X^u\bigr] \\
% &= T_u^v\,.
%\end{align*}
%\end{proof}

\subsection{Stochastic Operators}

We will now look at some properties of stochastic operators. We will say that an operator $A_u^{\{s\}}$ is stochastic iff $A_u^{\{s\}}(\omega^u)$ is stochastic for all $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$. For a given $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$, an operator $A_u^{\{s\}}(\omega^u)$ is said to be stochastic iff, 
\begin{enumerate}
\item $A_u^{\{s\}}(\omega^u)$ is a linear operator from $\gambles(\states^{u\cup \{s\}})$ to $\mathbb{R}$, given by 
\begin{equation*}
A_u^{\{s\}}(\omega^u)f = \sum_{x_s\in\states}A_u^{\{s\}}(\omega^u,x_s)f(\omega^u,x_s)\,.
\end{equation*}
\item $A_u^{\{s\}}(\omega^u, x_s) \geq 0 $ for all $x_s\in\states$.
\item $\sum_{x_s\in\states}A_u^{\{s\}}(\omega^u,x_s) = 1$.
\end{enumerate}
We next have the following result.

\begin{lemma}\label{lemma:nonmarkov_operator_bound_to_product_bound}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and any two sequences $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[t,s]}$ such that $v=\tau_0,\ldots,\tau_m$. Consider then two sequences of stochastic operators $A_{u\cup\{\tau_0\}}^{\{\tau_1\}},\ldots,A_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}$ and $B_{u\cup\{\tau_0\}}^{\{\tau_1\}},\ldots,B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}$ such that for all $i\in\{1,\ldots,m\}$, it holds that $\norm{A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}}< c$. Then,
\begin{equation*}
\norm{\prod_{i=1}^m A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \prod_{i=1}^m B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}} < mc.
\end{equation*}
\end{lemma}
\begin{proof}
We use induction on $m$. For $m=1$, the result trivially holds. Assume that it also holds for $k=m-1$. We now show that it also holds for $m$:
\begin{align*}
&\quad \norm{\prod_{i=1}^m A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \prod_{i=1}^m B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}} \\
&= \left\Vert \prod_{i=1}^m A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \left(\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right)B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}  \right.\\
&\quad\quad\quad\left.+ \left(\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right)B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} - \prod_{i=1}^m B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right\Vert \\
&= \left\Vert \left(\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right)\left(A_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} - B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}\right) \right. \\
&\quad\quad\quad\left.+ \left(\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \prod_{i=1}^{m-1} B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right)B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} \right\Vert \\
&\leq \left\Vert \left(\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right)\left(A_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} - B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}\right) \right\Vert \\
&\quad\quad\quad+ \left\Vert\left(\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \prod_{i=1}^{m-1} B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right)B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} \right\Vert \\
&\leq \left(\prod_{i=1}^{m-1}\left\Vert A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right\Vert \right)\left\Vert A_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} - B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}\right\Vert \\
&\quad\quad\quad+ \left\Vert\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \prod_{i=1}^{m-1} B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right\Vert\left\Vert B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} \right\Vert \\
&= \left\Vert A_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} - B_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}\right\Vert + \left\Vert\prod_{i=1}^{m-1} A_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}} - \prod_{i=1}^{m-1} B_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\right\Vert \\
&< c + (m-1)c \\
&= mc\,.
\end{align*}
\end{proof}
%\begin{lemma}
%Consider any two sequences $u\in\mathcal{U}_{[a,b]}$, $v\in\mathcal{U}_{[b,c]}$, and the induced sequence $u_1\in\mathcal{U}_{[t_0,t_1]},u_2\in\mathcal{U}_{[t_1,t_2]},\ldots,u_n\in\mathcal{U}_{[t_{n-1},t_n]},u_{n+1}= v$. Consider then two sequences of stochastic operators $A_{u_1}^{u_2},\ldots,A_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}$ and $B_{u_1}^{u_2},\ldots,B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}$ such that for all $i\in\{1,\ldots,n\}$, $\norm{A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}}\leq c$. Then,
%\begin{equation*}
%\norm{\prod_{i=1}^{n}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}} \leq nc\,.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%We use induction on $n$. For $n=1$, the result trivially holds. Now, assume that it holds for $k=n-1$. We show that it also holds for $n$:
%\begin{align*}
%&\quad \norm{\prod_{i=1}^{n}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}} \\
% &= \norm{\prod_{i=1}^{n}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \left[\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}}\right]B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} + \left[\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}}\right]B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} - \prod_{i=1}^{n}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}} \\
% &= \norm{\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}}\Biggl[A_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} - B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}\Biggr] + \left[\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n-1}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}\right]B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}} \\
% &\leq \norm{\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}}\Biggl[A_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} - B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}\Biggr]} + \norm{\left[\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n-1}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}\right]B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}} \\
% &\leq \left(\prod_{i=1}^{n-1}\norm{A_{u_1\cup\cdots\cup u_i}^{u_{i+1}}}\right)\norm{A_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} - B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}} + \norm{\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n-1}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}}\cdot\norm{B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}} \\
% &= 1\cdot\norm{A_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} - B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}} + \norm{\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n-1}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}}\cdot 1\,,
%\end{align*}
%where the last equality follows from stochasticity. By assumption, we now have
%\begin{align*}
%\norm{A_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}} - B_{u_1\cup\cdots\cup u_{n}}^{u_{n+1}}} + \norm{\prod_{i=1}^{n-1}A_{u_1\cup\cdots\cup u_i}^{u_{i+1}} - \prod_{i=1}^{n-1}B_{u_1\cup\cdots\cup u_i}^{u_{i+1}}} \leq c + (n-1)\cdot c = nc.
%\end{align*}
%\end{proof}

\subsection{Rate Operators}

We start the section on rate operators by establishing the link between rate operators and rate matrices. We will call an operator $Q_u^{\{s\}}$ a rate operator iff, for all $\omega^u$,
\begin{enumerate}
\item The operator $Q_u^{\{s\}}(\omega^u)$ is a linear operator from $\gambles(\states^{u\cup\{s\}})$ to $\mathbb{R}$, given by
\begin{equation*}
Q_u^{\{s\}}(\omega^u)f = \sum_{x_s\in\states} Q_u^{\{s\}}(\omega^u,x_s)f(\omega^u,x_s)\,.
\end{equation*}
\item $Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_n},x_s) \geq 0$ for all $x_{t_n}\neq x_s$.
\item $\sum_{x_s\in\states} Q_u^{\{s\}}(\omega^u,x_s) = 0$.
\end{enumerate}


%We finally introduce the linear operator $I_u^{\{s\}}$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, defined for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$ and all $f\in\gambles(\states^{u\cup\{s\}})$ as
%\begin{align*}
%\left[I_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) &\coloneqq \sum_{x_s\in\states} \mathbb{I}_{x_{t_n}}(x_s)f(x_{t_0},\ldots,x_{t_n},x_s) \\
%&= f(x_{t_0},\ldots,x_{t_n},x_{t_n})\,.
%\end{align*}
%\begin{lemma}
%Consider any $f\in\gambles(\states^{\{s\}})$, and let $I$ denote the identity matrix. Then, for all $t,s\in\realsnonneg$ such that $t<s$, all $u\in\mathcal{U}_{[0,t]}$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^{u}$:
%\begin{equation*}
%\left[I_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) = \left[If\right](x_{t_n})\,.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%Note that $I$ is a linear operator from $\gamblesX$ to $\gamblesX$, defined for all $f\in\gamblesX$ and $x\in\states$ as
%\begin{align*}
%\left[If\right](x) &= \sum_{y\in\states} \mathbb{I}_{x}(y)f(y) \\
% &= f(x)\,.
%\end{align*}
%Furthermore, we have for any $f\in\gambles(\states^{\{s\}})\subset\gambles(\states^{u\cup\{s\}})$ that
%\begin{align*}
%\left[I_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) &= f(x_{t_0},\ldots,x_{t_n},x_{t_n}) \\
% &= f(x_{t_n}) \\
% &= \left[If\right](x_{t_n})\,.
%\end{align*}
%\end{proof}
%Consider next the operator $I_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})$. Then, for all $f\in\gambles(\states^{u\cup\{s\}})$,
%\begin{equation*}
%\left[I_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) = f(x_{t_0},\ldots,x_{t_n},x_{t_n})\,.
%\end{equation*}

\noindent We now have the following result for rate operators.

\begin{lemma}\label{lemma:nonmarkov_fixed_history_rate_stochastic}
Consider a bounded set of rate matrices $\mathcal{Q}$ and a rate operator $Q_u^{\{s\}}$ such that for all $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$, it holds that $Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\in\mathcal{Q}$. Let $I$ denote the identity matrix. Then, for all $\Delta\in\realspos$ such that $\Delta\norm{\mathcal{Q}}<1$, the operator
\begin{equation*}
\left(I + \Delta Q_u^{\{s\}}\right)(x_{t_0},\ldots,x_{t_{n}}) = \left(I + \Delta Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\right)(x_{t_n})
\end{equation*}
is stochastic for all $(x_{t_0},\ldots,x_{t_{n}})\in\states^{u}$.

%Hence, we have that
%\begin{equation*}
%\left(I + \Delta Q_u^{\{s_0\}}(x_{t_0},\ldots,x_{t_{n-1}})\right)(x_{t_n})
%\end{equation*}
%is stochastic in $X_{s_0}$.
\end{lemma}
\begin{proof}
Simply check the conditions for stochasticity of a fixed-history operator.
\end{proof}

%For a given linear operator $Q_u^{\{s\}}$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$ and given $\Delta\in\realspos$, we now introduce an induced operator from $\gambles\left(\states^{u\cup\{s\}}\right)$ to $\gambles(\states^u)$. For all $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$ and $f\in\gambles\left(\states^{u\cup\{s\}}\right)$, let this operator be given by
%\begin{equation*}
%\left[\left(I + \Delta Q_u^{\{s\}}\right)f\right](\omega^u) \coloneqq \sum_{x_s\in\states} \left(I + \Delta Q_u^{\{s\}}\bigl(\omega(t_0),\ldots,\omega(t_{n-1})\bigr)\right)(\omega(t_n), x_s)f(\omega^u, x_s)\,. 
%\end{equation*} 
%{\bf ATTN:} The notation here is really iffy, since $Q_u^{\{s\}}$ is map from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{u})$, whereas $I$ ostensibly denotes the identity matrix/function/map, which it does not seem valid to add to a map between spaces of different dimension.

\begin{corollary}\label{lemma:nonmarkov_rate_stochastic}
Consider a bounded set of rate matrices $\mathcal{Q}$ and a history-dependent operator $Q_u^{\{s\}}$ such that for all $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$, it holds that $Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\in\mathcal{Q}$. Let $I$ denote the identity matrix. Then, for all $\Delta\in\realspos$ such that $\Delta\norm{\mathcal{Q}}<1$, the operator
\begin{equation*}
\left(I + \Delta Q_u^{\{s\}}\right)
\end{equation*}
is stochastic.
\end{corollary}
\begin{proof}
This is immediate from Lemma \ref{lemma:nonmarkov_fixed_history_rate_stochastic} and the definition of stochasticity.
\end{proof}

We finally need the following result.
\begin{lemma}\label{lemma:nonmarkov_rateproduct_bounded_by_lrate}
Consider any bounded set of rate matrices $\mathcal{Q}$ with lower rate matrix $\lrate$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[t,s]}$ such that $u=t_0,\ldots,t_n$ and $v=t_{n+0},\ldots,t_{n+m}$, and any sequence of rate operators $Q_{\{t_0,\ldots,t_{n+0}\}}^{\{t_{n+1}\}},\ldots,Q_{\{t_0,\ldots,t_{n+m-1}\}}^{\{t_{n+m}\}}$ such that for all $i\in\{1,\ldots,m\}$, all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and all $(x_{t_{n+1}},\ldots,x_{t_{n+m}})\in\states^{\{t_{n+1},\ldots,t_{n+m}\}}$ it holds that 
\begin{equation*}
Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}})\in\mathcal{Q}\,.
\end{equation*}
Then, for all $\Delta\in\realspos$ such that $\Delta\norm{\mathcal{Q}}<1$, all $f\in\gambles(\states^{\{s\}})$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
\begin{align*}
\left[\left(\prod_{i=1}^m \left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}} \right)\right)f\right](x_{t_0},\ldots,x_{t_n}) &\geq \left[\left(I + \Delta \lrate\right)^m f\right](x_{t_0},\ldots,x_{t_n})\\
 &= \left[\left(I + \Delta \lrate\right)^m f\right](x_{t_n})\,.
\end{align*}
\end{lemma}
\begin{proof}
Start by noting that for all $i\in\{1,\ldots,m\}$, the terms $\left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)$ are stochastic operators. Hence,
\begin{align*}
&\quad \left[\left(\prod_{i=1}^m \left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}} \right)\right)f\right](x_{t_0},\ldots,x_{t_n}) \\
&= \mathbb{E}_{X_{t_{n+1}}}\left[\cdots\mathbb{E}_{X_s}\left[f(X_s)\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{t_{n+1}},\ldots,X_{t_{n+m-1}}\right]\cdots\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
%&= \sum_{x_{t_{n+1}}} P\left(X_{t_{n+1}}=x_{t_{n+1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right) \sum_{x_{t_{n+2}}}\cdots\sum_{x_s}f(x_s)P\left(X_s=x_s\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{t_{n+1}}=x_{t_{n+1}},\ldots,X_{t_{n+m-1}}=x_{t_{n+m-1}}\right) \\
&= \sum_{x_{t_{n+1}}}\cdots\sum_{x_{t_{n+m-1}}} P\left(X_{t_{n+1}}=x_{t_{n+1}},\ldots,X_{t_{n+m-1}}=x_{t_{n+m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{t_0},\ldots,x_{t_{n+m-1}})\,,
\end{align*}
where
\begin{equation*}
g(x_{t_0},\ldots,x_{t_{n+m-1}}) \coloneqq \sum_{x_s}f(x_s)P\left(X_s=x_s\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_{n+m-1}}=x_{t_{n+m-1}}\right)\,.
\end{equation*}
Note that also,
\begin{align*}
g(x_{t_0},\ldots,x_{t_{n+m-1}}) &= \left[\left(I+\Delta Q_{\{t_0,\ldots,t_{n+m-1}\}}^{\{t_{n+m}\}}\right)f\right](x_{t_0},\ldots,x_{t_{n+m-1}}) \\
 &= \left[\left(I+\Delta Q_{\{t_0,\ldots,t_{n+m-1}\}}^{\{t_{n+m}\}}\right)(x_{t_0},\ldots,x_{t_{n+m-2}})f\right](x_{t_{n+m-1}})
\end{align*}
Since by assumption $Q_{\{t_0,\ldots,t_{n+m-1}\}}^{\{t_{n+m}\}}(x_{t_0},\ldots,x_{t_{n+m-2}})\in\mathcal{Q}$, we have that
\begin{align*}
g(x_{t_0},\ldots,x_{t_{n+m-1}}) &= \left[\left(I+\Delta Q_{\{t_0,\ldots,t_{n+m-1}\}}^{\{t_{n+m}\}}\right)(x_{t_0},\ldots,x_{t_{n+m-2}})f\right](x_{t_{n+m-1}})\\
 &\geq \left[\left(I + \Delta\lrate\right)f\right](x_{t_{n+m-1}})\,.
\end{align*}
Define a new function
\begin{equation*}
g'(x_{t_{n+m-1}}) \coloneqq \left[\left(I + \Delta\lrate\right)f\right](x_{t_{n+m-1}})\,,
\end{equation*}
and note that for all $(x_{t_0},\ldots,x_{t_n})$ and $(x_{t_{n+1}},\ldots,x_{t_{n+m-1}})$, it holds that $g(x_{t_0},\ldots,x_{t_{n+m-1}}) \geq g'(x_{t_{n+m-1}})$.
Because
\begin{equation*}
\sum_{x_{t_{n+1}}}\cdots\sum_{x_{t_{n+m-1}}} P\left(X_{t_{n+1}}=x_{t_{n+1}},\ldots,X_{t_{n+m-1}}=x_{t_{n+m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{t_0},\ldots,x_{t_{n+m-1}})
\end{equation*}
is a convex combination of values $g(x_{t_0},\ldots,x_{t_{n+m-1}})$ over all combinations $(x_{t_{n+1}},\ldots,x_{t_{n+m-1}})$, we have that
\begin{align*}
&\quad \left[\left(\prod_{i=1}^m \left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}} \right)\right)f\right](x_{t_0},\ldots,x_{t_n}) \\
&= \sum_{x_{t_{n+1}}}\cdots\sum_{x_{t_{n+m-1}}} P\left(X_{t_{n+1}}=x_{t_{n+1}},\ldots,X_{t_{n+m-1}}=x_{t_{n+m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{t_0},\ldots,x_{t_{n+m-1}}) \\
&\geq \sum_{x_{t_{n+1}}}\cdots\sum_{x_{t_{n+m-1}}} P\left(X_{t_{n+1}}=x_{t_{n+1}},\ldots,X_{t_{n+m-1}}=x_{t_{n+m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g'(x_{t_{n+m-1}}) \\
&= \left[\left(\prod_{i=1}^{m-1} \left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}} \right)\right)g'\right](x_{t_0},\ldots,x_{t_n}) \\
&= \left[\left(\prod_{i=1}^{m-1} \left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}} \right)\right)\bigl(\left(I + \Delta\lrate\right)f\bigr)\right](x_{t_0},\ldots,x_{t_n})\,.
\end{align*}
Repeatedly applying this argument, i.e. using induction on $m$, then yields the full lemma.
\end{proof}

\subsection{Imprecise Non-Markovian Stochastic Processes.} We are now finally ready to characterize the set of non-Markovian stochastic processes corresponding to a given set of rate matrices $\mathcal{Q}$, and show how to compute bounds on the expectation.

For any bounded set of rate matrices $\mathcal{Q}$, we consider the set $\mathbb{P}_\mathcal{Q}$ of all $P\in\mathbb{P}$ such that
\newline
\begin{align*}
(\forall \epsilon\in\realspos)&(\exists \delta\in\realspos): \\
&(\forall t\in\realsnonneg)(\forall \Delta\in(0,\delta))(\forall u\in\mathcal{U}_{[0,t]})(\forall (x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}): \\
&\quad\quad\quad\quad(\exists Q\in\mathcal{Q})\norm{\frac{T_u^{\{t+\Delta\}}\bigl(x_{t_0},\ldots,x_{t_{n-1}}\bigr) - I}{\Delta} - Q} < \epsilon\,.
\end{align*}

\noindent We now have the following result for the lower bound on the expectation under this set.

\begin{theorem}\label{theorem:nonmarkov_single_var_lower_bounded}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\mathbb{P}_{\mathcal{Q}}$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $f\in\gambles(\states^{\{s\}})$:
\begin{equation*}
\left[L_{t_n}^s f\right](x_{t_n}) \leq \left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n})\,.
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\mathbb{P}_{\mathcal{Q}}$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $f\in\gambles(\states^{\{s\}})$. Assume \emph{ex absurdo} that $\left[L_{t_n}^sf\right](x_{t_n}) > \left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n})$. We prove that this leads to a contradiction. Let $C\coloneqq s-t$ and choose $\epsilon\in\realspos$ small enough such that
\begin{equation}
\epsilon(1 + C\norm{f}) < \left[L_{t_n}^sf\right](x_{t_n}) - \left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n})\,.
\end{equation}
Since $P\in\mathbb{P}_\mathcal{Q}$, we have that there is some $\delta\in\realspos$ such that
\begin{align}
(\forall \tau\in\realsnonneg)&(\forall\Delta\in(0,\delta))(\forall u^*\in\mathcal{U}_{[0,\tau]})(\forall (x_{t_0^*},\ldots,x_{t_{n-1}^*})\in\states^{\{t_0^*,\ldots,t_{n-1}^*\}}): \\
&(\exists Q\in\mathcal{Q}) \norm{\frac{T_{u^*}^{\{\tau+\Delta\}}(x_{t_0^*},\ldots,x_{t_{n-1}^*}) - I}{\Delta} - Q} < \epsilon\,.
\end{align}
Furthermore, because of Theorem 5, there is some $\delta'\in\realspos$ such that
\begin{equation}
(\forall v\in\mathcal{U}_{[t,s]} : \sigma(v) < \delta') \left\vert \left[L_{t_n}^s f\right](x_{t_n}) - \left[\left(\prod_{k=1}^m(I + \Delta_k\lrate)\right)f\right](x_{t_n})\right\vert < \epsilon\,.
\end{equation}
Now, choose $m > \max\{\nicefrac{C}{\delta}, \nicefrac{C}{\delta'},C\norm{\mathcal{Q}} \}$. Then for $\Delta\coloneqq\nicefrac{C}{m}$, we find that $\Delta<\delta$, $\Delta<\delta'$, and $\Delta\norm{\mathcal{Q}}<1$.

For all $k\in\{0,1,\ldots,m\}$, define $t_{n+k}\coloneqq t+k\Delta$. Since $\Delta<\delta$, it follows from Equations 17 and 18 that for all $(x_{t_{n+1}},\ldots,x_{t_{n+m}})\in\states^{\{t_{n+1},\ldots,t_{n+m}\}}$ and all $i\in\{1,\ldots,m\}$, there is some
\begin{equation*}
Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}})\in\mathcal{Q}\,,
\end{equation*}
such that
\begin{align*}
&\quad \norm{T_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}}) - \left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}})\right)} \\
 &= \norm{\frac{T_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}}) - I}{\Delta} - Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}})}\Delta \\
 &<\epsilon\Delta\,.
\end{align*}
Furthermore, since $\Delta\norm{\mathcal{Q}}<1$, we have by Corollary \ref{lemma:nonmarkov_rate_stochastic} that
\begin{equation*}
\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)
\end{equation*}
is a stochastic operator for all $i\in\{1,\ldots,m\}$. Therefore, we find by Lemmas \ref{lemma:nonmarkov_transition_decompose}, \ref{lemma:nonmarkov_fixedhistory_bound_to_global_bound}, and \ref{lemma:nonmarkov_operator_bound_to_product_bound} that
\begin{align*}
&\quad \norm{T_u^{\{s\}} - \prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)} \\
&= \norm{\prod_{i=1}^m T_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}} - \prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)} \\
&\leq \epsilon\Delta m \\
&= \epsilon C.
\end{align*}
This implies that
\begin{align*}
&\quad\left\vert \left[T_{u}^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) - \left[\left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n})\right\vert \\
%&= \left\vert T_{u}^{\{s\}}(x_{t_0},\ldots,x_{t_{n}})f - \left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)(x_{t_0},\ldots,x_{t_{n}})f\right\vert \\
&= \left\vert \left[T_{u}^{\{s\}}f\right](x_{t_0},\ldots,x_{t_{n}}) - \left[\left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)f\right](x_{t_0},\ldots,x_{t_{n}})\right\vert \\
&\leq \norm{T_{u}^{\{s\}}f - \left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)f} \\
&\leq \norm{T_{u}^{\{s\}} - \left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)}\norm{f} \\
&\leq \epsilon C\norm{f}\,,
\end{align*}
and hence,
\begin{equation}
\left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) \geq \left[\left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) - \epsilon C\norm{f}\,.
\end{equation}
Furthermore, since $\Delta<\delta'$, it follows from Equation 19 that
\begin{equation*}
\Bigl\vert \left[L_{t_n}^sf\right](x_{t_n}) - \left[\left(I + \Delta\lrate\right)^mf\right](x_{t_n})\Bigr\vert < \epsilon\,,
\end{equation*}
and hence
\begin{equation}
\left[\left(I+\Delta\lrate\right)^mf\right](x_{t_n}) > \left[L_{t_n}^sf\right](x_{t_n}) - \epsilon\,.
\end{equation}
Finally, by Lemma \ref{lemma:nonmarkov_rateproduct_bounded_by_lrate}, we have
\begin{align}
&\quad \left[\left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right) \right)(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) \\
&= \left[\left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right) \right)f\right](x_{t_0},\ldots,x_{t_n}) \\
&\geq \left[\left(I+\Delta\lrate\right)^m\right](x_{t_n})\,.
\end{align}
Combining Equations 20 and 22-24 then shows
\begin{align*}
\left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) &\geq \left[\left(\prod_{i=1}^m\left(I + \Delta Q_{u\cup\{t_{n+0},\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) - \epsilon C\norm{f} \\
&\geq \left[\left(I+\Delta\lrate\right)^m\right](x_{t_n}) - \epsilon C\norm{f}\,,
\end{align*}
so that by Equation 21,
\begin{equation*}
\left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n}) > \left[L_{t_n}^sf\right](x_{t_n}) - \epsilon -\epsilon C\norm{f}\,,
\end{equation*}
which implies
\begin{equation*}
\epsilon(1 + C\norm{f}) > \left[L_{t_n}^sf\right](x_{t_n}) - \left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f\right](x_{t_n})\,.
\end{equation*}
This provides the required contradiction; see Equation 16.
\end{proof}

\begin{theorem}\label{theorem:nonmarkov_single_variable_lower_envelope}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $f\in\gambles(\states^{\{s\}})$, there is some $P\in\mathbb{P}_\mathcal{Q}$ such that, for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
\begin{equation*}
\left[L_{t_n}^{s}f\right](x_{t_n}) = \left[T_{u}^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{equation*}
\end{theorem}
\begin{proof}
*** The time-inhomogeneous Markov chain for which $Q_{\tau}\coloneqq \lrate L_{\tau}^sf$ establishes the equality. ***
\end{proof}

\section{The case for $f\in\gambles(\states^{u\cup\{s\}})$ }

Note that all the main results of the previous section were for the case $f\in\gambles(\states^{\{s\}})$. We now generalize this to the full domain of the operators, that is, to the case of functions $f\in\gambles(\states^{u\cup\{s\}})$.

This requires us to both introduce new lower-transition and lower-rate operators, as well as reformulate some of the results from the previous section. We start with introducing the new operators.

Let $L_u^{\{s\}}$ be a map from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, defined for all $f\in\gambles(\states^{u\cup\{s\}})$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$ as
\begin{equation*}
\left[L_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) \coloneqq \left[L_{t_n}^s f(x_{t_0},\ldots,x_{t_{n}},X_s)\right](x_{t_n})\,,
\end{equation*}
where $L_{t_n}^s$ is the normal lower transition operator given by Equation~2.

Let $\lrate_{u}^{\{s\}}$ be a map from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, defined for all $f\in\gambles(\states^{u\cup\{s\}})$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$ as
\begin{equation*}
\left[\lrate_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) \coloneqq \left[\lrate f(x_{t_0},\ldots,x_{t_{n}},X_s)\right](x_{t_n})\,.
\end{equation*}

We now first need the following result
\begin{lemma}
Consider a bounded set of rate matrices $\mathcal{Q}$ with lower transition rate operator $\lrate$, with $\lrate_u^{\{s\}}$ the extension of $\lrate$ to $\gambles(\states^{u\cup\{s\}})$. Consider a rate operator $Q_u^{\{s\}}$ such that for all $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$, it holds that $Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\in\mathcal{Q}$. Then, for all $f\in\gambles(\states^{u\cup\{s\}})$, and $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
\begin{equation*}
\left[Q_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) \geq \left[\lrate_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{equation*}
\end{lemma}
\begin{proof}
By the definition of rate operators, we have
\begin{align*}
\left[Q_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) &= \sum_{x_s\in\states} Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_n}, x_s) f(x_{t_0},\ldots,x_{t_n}, x_s)\,,
\end{align*}
and since $Q_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})\in\mathcal{Q}$, there must be some $Q\in\mathcal{Q}$ such that
\begin{align*}
\left[Q_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) &= \sum_{x_s\in\states} Q(x_{t_n}, x_s) f(x_{t_0},\ldots,x_{t_n}, x_s) \\
 &= \left[Q f(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
\end{align*}
Now, since by definition
\begin{align*}
\left[\lrate_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) &= \left[\lrate f(x_{t_0},\ldots,x_{t_{n}},X_s)\right](x_{t_n}) \\
 &= \inf\Bigl\{\left[Q' f(x_{t_0},\ldots,x_{t_{n}},X_s)\right](x_{t_n})\,:\,Q'\in\mathcal{Q}\Bigr\}\,,
\end{align*}
we find
\begin{align*}
\left[\lrate_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) &= \left[\lrate f(x_{t_0},\ldots,x_{t_{n}},X_s)\right](x_{t_n}) \\
 &\leq \left[Q f(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n}) \\
 &= \left[Q_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{align*}
\end{proof}

%\begin{lemma}
%Consider any bounded set of rate matrices $\mathcal{Q}$ with lower rate matrix $\lrate$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[t,s]}$ such that $u=t_0,\ldots,t_n$ and $v=t_{n+0},\ldots,t_{n+m}$, and any sequence of rate operators $Q_{\{t_0,\ldots,t_{n+0}\}}^{\{t_{n+1}\}},\ldots,Q_{\{t_0,\ldots,t_{n+m-1}\}}^{\{t_{n+m}\}}$ such that for all $i\in\{1,\ldots,m\}$, all $(x_{t_0},\ldots,x_{t_n})\in\states^u$ and all $(x_{t_{n+1}},\ldots,x_{t_{n+m}})\in\states^{\{t_{n+1},\ldots,t_{n+m}\}}$ it holds that
%\begin{equation*}
%Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}(x_{t_0},\ldots,x_{t_{n+i-2}})\in\mathcal{Q}\,.
%\end{equation*}
%Then, for all $\Delta\in\realspos$ such that $\Delta\norm{\mathcal{Q}}<1$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\left[\left(\prod_{i=1}^m\left(I + \Delta Q_{\{t_0,\ldots,t_{n+i-1}\}}^{\{t_{n+i}\}}\right)\right)f\right](x_{t_0},\ldots,x_{t_n}) \geq \left[\left(I + \Delta\lrate_{u}^{\{s\}}\right)^m f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%{\bf TODO:} Write proof (is this needed?)
%\end{proof}

\begin{theorem}\label{theorem:nonmarkov_historic_variable_lower_bounded}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\mathbb{P}_\mathcal{Q}$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $f\in\gambles(\states^{u\cup\{s\}})$:
\begin{equation*}
\left[L_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) \leq \left[T_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{equation*}
\end{theorem}
\begin{proof}
By definition, we have
\begin{equation*}
\left[T_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) = \left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})f(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,,
\end{equation*}
and also,
\begin{equation*}
\left[L_u^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) = \left[L_{t_n}^s f(x_{t_0},\ldots,x_{t_{n}},X_s)\right](x_{t_n})\,.
\end{equation*}
Now, define a function $g\in\gambles(\states^{\{s\}})$ as
\begin{equation*}
g(x_s) \coloneqq f(x_{t_0},\ldots,x_{t_n},x_s)\,.
\end{equation*}
Substituting into the above, it remains to show that
\begin{equation*}
\left[L_{t_n}^s g\right](x_{t_n}) \leq \left[T_u^{\{s\}}(x_{t_0},\ldots,x_{t_{n-1}})g\right](x_{t_n})\,,
\end{equation*}
which was proved in Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
\end{proof}

\begin{theorem}\label{theorem:nonmarkov_historic_variable_lower_envelope}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$ and all $f\in\gambles(\states^{u\cup\{s\}})$, there is some $P\in\mathbb{P}_\mathcal{Q}$ such that, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
\begin{equation*}
\left[L_{u}^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n}) = \left[T_{u}^{\{s\}}f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{equation*}
\end{theorem}
\begin{proof}
*** The non-Markovian stochastic process for which 
\begin{equation*}
Q_{u\cup\{\tau\}}(x_{t_0},\ldots,x_{t_{n}})\coloneqq \lrate L_{\tau}^sf(x_{t_0},\ldots,x_{t_n},X_s)
\end{equation*}
establishes the equality. ***
\end{proof}


\section{The Multi-Variable Non-Markovian Case}

We now generalize the results from the previous sections to the lower bound of expectations on functions $f\in\gambles(\states^{u\cup v})$, where $v\in\mathcal{U}_{[s,s']}$.

We again start with some notation. For any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, any $u\in\mathcal{U}_{[0,t]}$ and any $v\in\mathcal{U}_{[s,s']}$, let $A_u^v$ be a non-negatively homogeneous operator from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$. Define the norm as before, i.e.,
\begin{equation*}
\norm{A_u^v} \coloneqq \sup\left\{\norm{A_u^v}\,:\,f\in\gambles(\states^{u\cup v}), \norm{f}=1\right\}\,.
\end{equation*}
This norm satisfies all properties N6-N12 from the previous section. The default notation for sequences of time points will be $u=t_0,\ldots,t_n$ and $v=\tau_0,\ldots,\tau_m$, where we will assume that $t_n\leq \tau_0$.

As a special case of such an operator, define for all $f\in\gambles(\states^{u\cup v})$ the operator
\begin{equation*}
T_u^vf \coloneqq \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
This also satisfies for all $g\in\gambles(\states^v)$ the equality
\begin{equation*}
T_u^vg = \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[g(X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}

\begin{proposition}\label{proposition:nonmarkov_multi_variable_decompose}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[s,s']}$, and any $P\in\mathbb{P}$ that has, for all $i\in\{1,\ldots,m\}$, transition operators
\begin{equation*}
T_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\,.
\end{equation*}
Then, the expectation operator $T_u^v$ corresponding to $P$ satisfies, for all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
T_u^v f = T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\,.
\end{equation*}
\end{proposition}
\begin{proof}
%We will use backward induction on $m$. Note that
%\begin{equation*}
%T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} g = \mathbb{E}_{X_{\tau_m}}\left[g(X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]
%\end{equation*}
This is immediate from the decomposition properties of expectation, i.e.,
\begin{align*}
&\quad T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f \\
 &= \mathbb{E}_{X_{\tau_0}}\left[\cdots\mathbb{E}_{X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\cdots\,\vert\,X_{t_0},\ldots,X_{t_n}\right] \\
 &= \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right] \\
 &= T_u^v f\,.
\end{align*}
\end{proof}

Now, recall the extended operator $L_u^{\{s\}}$ from the previous section, and define a new operator $L_u^v$ from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$, as
\begin{equation*}
L_u^v \coloneqq L_u^{\{\tau_0\}}L_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}.
\end{equation*}
We have the following result.


\begin{theorem}\label{theorem:nonmarkov_multi_variable_lower_bounded}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\mathbb{P}_\mathcal{Q}$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[s,s']}$, any $f\in\gambles(\states^{u\cup v})$ and any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, it holds that
\begin{equation*}
\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) \leq \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{equation*}
\end{theorem}
\begin{proof}
Start by considering the decomposition from Proposition~\ref{proposition:nonmarkov_multi_variable_decompose}. We have, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, that
\begin{align*}
&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
&= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
&= \mathbb{E}_{X_{\tau_0}}\left[\cdots\mathbb{E}_{X_{\tau_m}}\left[f(x_{t_0},\ldots,x_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\cdots\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
&= \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_{m-1}}}\left[\mathbb{E}_{X_{\tau_m}}\left[f(x_{t_0},\ldots,x_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
&= \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,,
\end{align*}
where
\begin{equation*}
g(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}}) \coloneqq \left[T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,.
\end{equation*}
Define a new function $g'(\cdot)$ as
\begin{equation*}
g'(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}}) \coloneqq \left[L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,,
\end{equation*}
and note that by Theorem~\ref{theorem:nonmarkov_historic_variable_lower_bounded} we have that for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$ and $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$:
\begin{equation}\label{equation:nonmarkov_multiple_variable_theorem_eq}
g'(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}}) \leq g(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,.
\end{equation}
Now, because
\begin{equation*}
\sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})
\end{equation*}
is a convex combination of values $g(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})$ over all combinations $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, we have by Equation~\ref{equation:nonmarkov_multiple_variable_theorem_eq} that
\begin{align*}
&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
&= \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}}) \\
&\geq \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g'(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}}) \\
&= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-2}\}}^{\{\tau_{m-1}\}}L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{align*}
Repeatedly applying this argument, i.e. using backward induction on $m$, finally reveals
\begin{align*}
&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
 &= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
 &\geq \left[L_u^{\{\tau_0\}}L_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
 &= \left[L_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{align*}
\end{proof}

\begin{theorem}\label{theorem:nonmarkov_multi_variable_lower_envelope}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$, there is some $P\in\mathbb{P}_\mathcal{Q}$ such that, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
\begin{equation*}
\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) = \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
\end{equation*}
\end{theorem}
\begin{proof}
*** The non-Markovian stochastic process for which, for all $i\in\{0,\ldots,{m-1}\}$ and all $(x_{\tau_0},\ldots,x_{\tau_i})\in\states^{\{\tau_0,\ldots,\tau_i\}}$,
\begin{equation*}
Q_{u\cup\{\tau_0,\ldots,\tau_i,\mu\}}(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_i}) \coloneqq \lrate L_\mu^{\tau_{i+1}} \left[L_{u\cup\{\tau_0,\ldots,\tau_{i+1}\}}^{\{\tau_{i+2}\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_i})
\end{equation*}
establishes the equality. ***
\end{proof}

%\begin{theorem}\label{theorem:nonmarkov_multi_historic_variable_lower_bounded}
%For all $f\in\gambles(\states^{u\cup v})$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, it holds that
%\begin{equation*}
%\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) \leq \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Pretty much a direct consequence of Theorem~\ref{theorem:nonmarkov_multi_variable_lower_bounded}, similar to Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} $\rightarrow$ Theorem~\ref{theorem:nonmarkov_historic_variable_lower_bounded}.

%{\bf TODO:} Prove this properly.
%\end{proof}

In the sequel, for a given $f\in\gambles(\states^{u\cup v})$, we will refer to the transition operator corresponding to the distribution $P\in\mathbb{P}_\mathcal{Q}$ that satisfies Theorem~\ref{theorem:nonmarkov_multi_variable_lower_envelope} as $\lt_u^v$.

\section{Tractability Aspects}

\begin{proposition}
For any $f\in\gambles(\states^{u\cup v})$ and given the corresponding $\lt_u^v$, computing $\left[\lt_u^v f\right](x_{t_0},\ldots,x_{t_n})$ is not more difficult than computing $\left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})$ for an arbitrary $P\in\mathbb{P}_{\mathcal{Q}}$. Specifically, this takes a number of operations that is exponential in $m$.
\end{proposition}
\begin{proof}
The first claim is immediate from the fact that $\lt_u^v$ corresponds to a $P\in\mathbb{P}_\mathcal{Q}$. The difficulty of the computation follows from the fact that it is an expectation on $m$ variables, which requires summing over all values $(x_{\tau_0},\ldots,x_{\tau_m})\in\states^v$.
\end{proof}

\begin{proposition}
Identifying the $\lt_u^v$ corresponding to a given $f\in\gambles(\states^{u\cup v})$ is intractable.
\end{proposition}
\begin{proof}
The problem is essentially that for all combinations $(x_{\tau_0},\ldots,x_{\tau_m})\in\states^v$, you need to perform an optimization to find the corresponding $\lrate$. Clearly, the number of optimizations required is exponential in $m$.
\end{proof}

\begin{proposition}
There is a subclass of gambles $\mathcal{C}(\states^{u\cup v})\subset\gambles(\states^{u\cup v})$, so that for $f\in\mathcal{C}(\states^{u\cup v})$, identifying the corresponding $\lt_u^v$ is tractable, i.e., the required number of times that $\lrate$ needs to be computed is linear in $m$.
\end{proposition}
\begin{proof}
As a trivial example, take $\mathcal{C}(\states^{u\cup v}) = \gambles(\states^{\{s'\}})$. {\bf Claim: } This class can be made a lot bigger.
\end{proof}


%\noindent {\bf TODO:} Define $T_u^v\coloneqq T_u^{\{\tau_1\}}\cdots T_{u\cup\{\tau_1,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}$ from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$
%\newline
%{\bf TODO:} Define $L_{t_n}^v\coloneqq L_{t_n}^{\tau_1}\cdots L_{\tau_{m-1}}^{\tau_m}$ or something like this (how does this work?)
%\newline
%{\bf TODO:} Generalize to $T_u^vf \geq L_{t_n}^vf$ for all $f\in\gambles(\states^v)$.
%\newline
%{\bf TODO:} Generalize to $T_u^vf \geq L_{t_n}^vf$ for all $f\in\gambles(\states^{u\cup v})$

\bibliographystyle{plain} 
%\bibliography{general}


\end{document}


\section{The Non-Markovian Case}

\subsection{Notation and some initial problems}

We first need some notation. For any $\omega\in\Omega$ and $u\in\mathcal{U}_{[0,t]}$, define the $u$-discretized path $\omega^u\coloneqq \omega(t_1),\ldots,\omega(t_n)$. Let the complete set of all such discretized paths be written $\Omega_{\mathcal{U}_{[0,t]}}$. Denote sequence concatenation as $\omega^u\oplus x\coloneqq \omega(t_1),\ldots,\omega(t_n),x$.

Now, note that in the Markovian case, the transition operator $T_t^s$ is a conditional expectation operator on a given function $f\in\gamblesX$,
\begin{align*}
T_t^s f &\equiv \mathbb{E}[f(X_s)\,\vert\,X_t] \\
 &= \sum_{y\in\states} f(y)\cdot P(X_s=y\,\vert\,X_t)\,,
\end{align*}
which clearly is then itself a function of $X_t$, so that
\begin{align*}
[T_t^s f](x) \equiv \mathbb{E}[f(X_s)\,\vert\,X_t=x]\,.
\end{align*}
In other words, as mentioned in Section 2, the operator $T_t^s$ is a map from $\gamblesX$ to $\gamblesX$.

For the non-Markovian case, we will need to introduce the notion of a \emph{history-dependent} transition operator, say $T_t^s(\omega^u)$, which we define as
\begin{align*}
T_t^s(\omega^u) f &\coloneqq \mathbb{E}[f(X_s)\,\vert\,X_t,\omega^u] \\
 &= \sum_{y\in\states} f(y)\cdot P(X_s=y\,\vert\,X_t, \omega^u) \\
 &= \sum_{y\in\states} f(y)\cdot P(X_s=y\,\vert\,X_t, X_{t_1}=\omega(t_1),\ldots,X_{t_n}=\omega(t_n))\,.
\end{align*}
Clearly, this is again a function only of $X_t$, so that $[T_t^s(\omega^u) f](x)$ is properly defined. Observe that $T_t^s(\omega^u)$ is still a map from $\gamblesX$ to $\gamblesX$.

However, if we want to factorize the operator $T_t^s(\omega^u)$, say on some time point $\tau$, where $t\leq \tau\leq s$, we will also need the notion of a \emph{variable history}. Denote with $\omega^u\otimes X_t$ the collection
\begin{equation*}
\omega^u\otimes X_t \coloneqq \{\omega^u\oplus x_t\,:\,x_t\in\states\}\,.
\end{equation*}
Define now a \emph{variable history}-dependent operator $T_\tau^s(\omega^u\otimes X_t)$ as
\begin{align*}
T_\tau^s(\omega^u\otimes X_t)f \coloneqq \mathbb{E}[f(X_s)\,\vert\,X_\tau,\omega^u,X_t]\,,
\end{align*}
which is to say that
\begin{align*}
\bigl[T_\tau^s(\omega^u\otimes X_t)f\bigr] \cong \bigl\{T_\tau^s(\omega^u\oplus x_t)f \,:\, x_t\in\states\bigr\}\,.
\end{align*}
Note that this is a function in both $X_\tau$ and $X_t$, so that $T_\tau^s(\omega^u\otimes X_t)$ is a map from $\gamblesX$ to $\gambles(\states\times\states)$. 

This is problematic. To see why, note that from the rules of expectation, we have
\begin{align*}
\mathbb{E}_{X_\tau}\bigl[ \mathbb{E}_{X_s}[f(X_s)\,\vert\,X_\tau,\omega^u,X_t] \,\vert\,X_t,\omega^u\bigr] &= \mathbb{E}_{X_\tau}\left[ \sum_{x_s\in\states} f(x_s)\cdot P(X_s=x_s\,\vert\,X_\tau,\omega^u,X_t) \,\bigg\vert\,X_t,\omega^u\right] \\
 = \sum_{x_\tau\in\states} P(X_\tau=x_\tau\,\vert\,X_t,\omega^u)&\cdot \sum_{x_s\in\states} f(x_s)\cdot P(X_s=x_s\,\vert\,X_\tau=x_\tau,\omega^u,X_t) \\
 &= \sum_{x_\tau\in\states}\sum_{x_s\in\states}f(x_s)\cdot P(X_s=x_s, X_\tau=x_\tau\,\vert\,X_t,\omega^u) \\
 &= \sum_{x_s\in\states} f(x_s)\cdot P(X_s=x_s\,\vert\,X_t,\omega^u) \\
 &= \mathbb{E}[f(X_s)\,\vert\,X_t,\omega^u]\,.
\end{align*}
What this says is that we can `split up' the computation of an expectation at time $s$ given the state at time $t$, by nesting expectations conditional on the possible states at time $\tau$. The problem now occurs when we try to do something similar in operator notation. Clearly, we would want the following to be properly defined:
\begin{equation*}
T_t^\tau(\omega^u)T_\tau^s(\omega^u\otimes X_t) f\,,
\end{equation*}
which it is not, since the leftmost operator is a map from $\gamblesX$ to $\gamblesX$, while the term $[T_\tau^s(\omega^u\otimes X_t) f]$ yields an element in $\gambles(\states\times\states)$.

One way around this is to observe that the derivation with expectation operators only worked because the notation and conventions allows one to nest them like this.

In other words, we can choose to simply go with the intuitive interpretation of the notation, and say that since
\begin{equation*}
T_\tau^s(\omega^u\otimes X_t) \equiv \mathbb{E}_{X_s}[\,\cdot\,\vert\,X_\tau,\omega^u,X_t]\,,
\end{equation*}
we will read
\begin{equation*}
T_t^\tau(\omega^u)T_\tau^s(\omega^u\otimes X_t) \coloneqq \mathbb{E}_{X_\tau}\bigl[ \mathbb{E}_{X_s}[\,\cdot\,\vert\, X_\tau,\omega^u,X_t] \,\big\vert\,X_t,\omega^u\bigr]\,.
\end{equation*}
The downside to this approach is that it is no longer clear what a given operator does, and that one can only read this from context. That is, under this interpretation, we would move away from saying that $T_t^s(\omega^u)$ is strictly a map from $\gamblesX$ to $\gamblesX$.
The upshot is that it is now relatively easy to provide a decomposition for $T_t^s(\omega^u)$.
\begin{lemma}
Consider any history dependent transition matrix $T_t^s(\omega^u)$ and sequence $u^*\in\mathcal{U}_{(t,s]}$ such that $t=\!:t_0<t_1,\ldots,t_n=s$. Then,
\begin{equation*}
T_t^s(\omega^u) = \prod_{i=1}^{n} T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j})
\end{equation*}
\end{lemma}
\begin{proof}
This is mostly directly from definition. Since
\begin{equation*}
T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j}) \equiv \mathbb{E}_{X_{t_i}}\bigl[\,\cdot\,\vert\,\omega^u\otimes_{j=0}^{i-1}X_{t_j}\bigr]\,,
\end{equation*}
we can interpret
\begin{align*}
\prod_{i=1}^{n} T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j}) &\equiv \mathbb{E}_{X_{t_1}}\bigl[\cdots\mathbb{E}_{X_{t_n}}[\,\cdot\,\vert\,\omega^u\otimes_{j=0}^{n-1}X_{t_j}]\cdots\,\vert\,\omega^u\otimes X_{t_0}\bigr] \\
 &= \mathbb{E}_{X_{t_n}}\bigl[\,\cdot\,\vert\,\omega^u\otimes X_{t_0}\bigr] \\
 &= T_t^s(\omega^u)\,.
\end{align*}
\end{proof}

Another point is that, for the purposes of this work, we are also interested in rate matrices $Q$. These are, again, maps from $\gamblesX$ to $\gamblesX$. For the non-Markovian case, however, we will also need history-dependent rate matrices $Q(\omega^u)$, as well as variable history dependent rate matrices $Q(\omega^u\otimes X)$.

We will again choose to go with the intuitive interpretation that for rate `matrices' $Q_1(\omega^u)$ and $Q_2(\omega^u\otimes X_1)$,
\begin{align*}
\bigl[Q_1(\omega^u)Q_2(\omega^u\otimes X_1)f\bigr](x) &\coloneqq Q_1(\omega^u)_{(x,\cdot)}\bigl[Q_2(\omega^u\oplus x)f\bigr] \\
 &= \sum_{y\in\states} Q_1(\omega^u)_{(x,y)}\bigl[Q_2(\omega^u\oplus x)f\bigr](y) \\
 &= \sum_{y\in\states} Q_1(\omega^u)_{(x,y)} Q_2(\omega^u\oplus x)_{y,\cdot}f \\
 &= \sum_{y\in\states} Q_1(\omega^u)_{(x,y)} \sum_{z\in\states}Q_2(\omega^u\oplus x)_{y,z}f(z)\,.
\end{align*}
In the general case, for a sequence of variable history dependent matrices $A_1(\omega^u\otimes X_0),\ldots,A_n(\omega^u\otimes_{j=1}^nX_j)$, where the $X_0$ is only for convenience so that $\omega^u\otimes X_0\equiv\omega^u$, we will interpret
\begin{align*}
\left[\left[\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]\,\cdot\,\right] \equiv \sum_{x_1}\cdots\sum_{x_{n+1}}\prod_{i=1}^{n}A_i(\omega^u\oplus_{j=0}^{i-1}x_j)_{(x_i,x_{i+1})}[\,\cdot\,]
\end{align*}
%Similarly, for rate `matrices' $Q_1(\omega^u),Q_2(\omega^u\otimes X_1)$, and $Q_3(\omega^u\otimes X_1\otimes X_2)$, we have
%\begin{align*}
%\bigl[Q_1(\omega^u)Q_2(\omega^u\otimes X_1)Q_3(\omega^u\otimes X_1\otimes X_2)f\bigr](x) &= Q_1(\omega^u)_{(x,\cdot)}\big[Q_2(\omega^u\oplus x)Q_3(\omega^u\oplus x\otimes X_2)f\big] \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\big[Q_2(\omega^u\oplus x)Q_3(\omega^u\oplus x\otimes X_2)f\big](y) \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&Q_2(\omega^u\oplus x)_{(y,\cdot)}\big[Q_3(\omega^u\oplus x\oplus y)f\big] \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\sum_{z\in\states}Q_2(\omega^u\oplus x)_{(y,z)}\big[Q_3(\omega^u\oplus x\oplus y)f\big](z) \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\sum_{z\in\states}Q_2(\omega^u\oplus x)_{(y,z)}Q_3(\omega^u\oplus x\oplus y)_{(z,\cdot)}f \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\sum_{z\in\states}Q_2(\omega^u\oplus x)_{(y,z)}\sum_{w\in\states}Q_3(\omega^u\oplus x\oplus y)_{(z,w)}f(w)\,.
%\end{align*}
From now on, we will drop the scare-quotes around `matrix', assuming that it is clear from context what is meant. We will also sometimes abbreviate `variable history dependent matrix' as `\emph{v.h.d. matrix}'.

We will finally need some conventions for summation of operators. For a given matrix $A$ and v.h.d. matrix $B(\omega^u\otimes X)$, we define
\begin{equation*}
\bigl[A + B(\omega^u\otimes X)\bigr] \cong \bigl\{A + B(\omega^u\oplus x)\,:\,x\in\states\bigr\}\,.
\end{equation*}
Furthermore, for two v.h.d. matrices $A(\omega^u\otimes X)$ and $B(\omega^u\otimes X)$, we will say
\begin{equation*}
\bigl[A(\omega^u\otimes X) + B(\omega^u\otimes X)\bigr] \cong \bigl\{A(\omega^u\oplus x) + B(\omega^u\oplus x)\,:\,x\in\states\bigr\}
\end{equation*}

\subsection{History-dependent norms and their properties} 

Having moved away from saying that an operator $A(\omega^u)$ is strictly a map from $\gamblesX$ to $\gamblesX$, we now run into some problems regarding the definition of its norm.

For a fixed (i.e., non-variable) history dependent operator $A(\omega^u)$, we will simply define the norm as before, that is
\begin{equation*}
\norm{A(\omega^u)} \coloneqq \sup\{\norm{A(\omega^u)f}\,:\,f\in\gamblesX, \norm{f}=1\}\,.
\end{equation*}

For variable history-dependent operators, note that they in some sense denote a collection of operators; this is already made clear by the congruence relation
\begin{equation*}
A(\omega^u\otimes X) \cong \bigl\{A(\omega^u\oplus x)\,:\,x\in\states\bigr\}\,.
\end{equation*}
It would thus appear to make sense to define their norm in the way that we previously defined the norm for sets of operators. Hence, we define
\begin{equation*}
\norm{A(\omega^u\otimes X)} \coloneqq \max\bigl\{\norm{A(\omega^u\oplus x)}\,:\,x\in\states\bigr\}\,.
\end{equation*}
We now have the following result.
\begin{proposition}
Consider two history dependent matrices $A_1(\omega^u)$ and $A_2(\omega^u\otimes X_1)$. Then,
\begin{equation*}
\norm{A_1(\omega^u)A_2(\omega^u\otimes X)} \leq \norm{A_1(\omega^u)}\cdot\norm{A_2(\omega^u\otimes X_1)}\,.
\end{equation*}
\end{proposition}
\begin{proof}
Note that $[A_1(\omega^u)A_2(\omega^u\otimes X)]$ is non-variable history dependent. Hence,
\begin{align*}
&\quad \norm{A_1(\omega^u)A_2(\omega^u\otimes X)} \\
&= \sup\bigl\{\norm{A_1(\omega^u)A_2(\omega^u\otimes X)f}\,:\,f\in\gamblesX, \norm{f}=1\bigr\} \\
 &= \sup\left\{\max\left\{\left\vert \bigl[A_1(\omega^u)A_2(\omega^u\otimes X)f\bigr](x)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert A_1(\omega^u)_{(x,\cdot)}\bigl[A_2(\omega^u\oplus x)f\bigr]\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert\sum_{y\in\states}A_1(\omega^u)_{(x,y)}\bigl[A_2(\omega^u\oplus x)f\bigr](y)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\bigl[A_2(\omega^u\oplus x)f\bigr](y)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot\left\vert\bigl[A_2(\omega^u\oplus x)f\bigr](y)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot \max\left\{\left\vert\bigl[A_2(\omega^u\oplus x)f\bigr](z)\right\vert\,:\,z\in\states\right\}\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &= \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot \norm{A_2(\omega^u\oplus x)f}\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus x)f}\cdot \sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert \,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\}\cdot \sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert \,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\}\cdot \max\left\{ \sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert \,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\}\cdot \norm{A_1(\omega^u)}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\} \,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \max\left\{\sup\left\{\norm{A_2(\omega^u\oplus z)f} \,:\,f\in\gamblesX, \norm{f}=1\right\}\,:\,z\in\states\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \max\left\{\norm{A_2(\omega^u\oplus z)}\,:\,z\in\states\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \norm{A_2(\omega^u\otimes X)}\,.
% &= \sup\left\{\max\left\{\left\vert\sum_{y\in\states}A_1(\omega^u)_{(x,y)}\sum_{z\in\states}A_2(\omega^u\oplus x)_{(y,z)}f(z)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\sum_{z\in\states}A_2(\omega^u\oplus x)_{(y,z)}f(z)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &= \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot\left\vert\sum_{z\in\states}A_2(\omega^u\oplus x)_{(y,z)}f(z)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot\max\left\{\left\vert\sum_{z\in\states}A_2(\omega^u\oplus x)_{(w,z)}f(z)\right\vert\,:\,w\in\states\right\}\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
\end{align*}
\end{proof}
We now extent this result to arbitrary sequences of operators.
\begin{proposition}
Consider a sequence of history dependent matrices $A_1(\omega^u\otimes X_0),A_2(\omega^u\otimes X_0\otimes X_1),\ldots,A_n(\omega^u\otimes_{j=0}^{n-1}X_j)$, where we have written $\omega^u\otimes X_0\equiv \omega^u$ for notational convenience. Then,
\begin{equation*}
\norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)} \leq \prod_{i=1}^{n} \norm{A_i(\omega^u\otimes_{j=0}^{i-1}X_j)}\,.
\end{equation*}
\end{proposition}
\begin{proof}
Again note that the product of the elements in the sequence is non-variable history dependent. Thus,
\begin{align*}
&\quad \norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)} \\
 &= \sup\left\{\norm{\left[\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]f}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert\left[\left[\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]f\right](x_1)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert\left[A_1(\omega^u)\left[\prod_{i=2}^n A_i(\omega^u\otimes_{j=1}^{i-1}X_j)\right]f\right](x_1)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert A_1(\omega^u)_{(x_1,\cdot)}\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right]\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert \sum_{x_2\in\states}A_1(\omega^u)_{(x_1,x_2)}\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](x_2)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](x_2)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\cdot\left\vert\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](x_2)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\cdot\max\left\{\left\vert\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](y)\right\vert\,:\,y\in\states\right\}\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\cdot\norm{\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f}\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\norm{\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f}\,:\,x_1\in\states\right\}\cdot\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot\sup\left\{\max\left\{\norm{\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f}\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot\norm{A_2(\omega^u\otimes X_1)\prod_{i=3}^n A_i(\omega^u\otimes_{j=1}^{i-1}X_j)}\,.
\end{align*}
The induction step is now trivial since it uses the same derivation as above. For example:
\begin{align*}
&\quad \norm{A_2(\omega^u\otimes X_1)\prod_{i=3}^n A_i(\omega^u\otimes_{j=1}^{i-1}X_j)} \\
 &= \max\left\{ \norm{A_2(\omega^u\oplus x_1)\prod_{i=3}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)} \,:\,x_1\in\states\right\} \\
 &\leq \max\left\{ \norm{A_2(\omega^u\oplus x_1)}\cdot\max\left\{\norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)}\,:\,x_2\in\states\right\} \,:\,x_1\in\states\right\} \\
 &\leq \max\left\{ \norm{A_2(\omega^u\otimes X_1)}\cdot\max\left\{\norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)}\,:\,x_2\in\states\right\} \,:\,x_1\in\states\right\} \\
 &= \norm{A_2(\omega^u\otimes X_1)}\cdot \max\left\{ \max\left\{\norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)}\,:\,x_2\in\states\right\} \,:\,x_1\in\states\right\} \\
 &= \norm{A_2(\omega^u\otimes X_1)}\cdot \max\left\{ \norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)} \,:\,x_1,x_2\in\states\right\}\,.
\end{align*}
It should be clear that repeatedly applying this step yields the proposition.
\end{proof}

\begin{lemma}
Consider a v.h.d. matrix $C(\omega^u\otimes_{j=0}^n X_j)$ and two sequences of v.h.d. matrices $A_1(\omega^u\otimes X_0),\ldots, A_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ and $B_1(\omega^u\otimes X_0),\ldots, B_n(\omega^u\otimes_{j=0}^{n-1}X_j)$. Then,
\begin{align*}
&\quad \norm{\left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]C(\omega^u\otimes_{j=0}^n X_j)} \\
 &\quad\quad\quad\leq \norm{\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)}\norm{C(\omega^u\otimes_{j=0}^n X_j)}\,.
\end{align*}
\end{lemma}
\begin{proof}
Note that the result is a non-v.h.d. matrix. Hence, expanding the definition of the norm:
\begin{align*}
&\quad \norm{\left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]C(\omega^u\otimes_{j=0}^n X_j)} \\
&= \max\left\{ \sum_{x_{n+2}\in\states}\left\vert \left[\left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]C(\omega^u\otimes_{j=0}^n X_j)\right]_{(x_1,x_{n+2})}\right\vert \,:\,x_1\in\states\right\} \\
&= \max\left\{\sum_{x_{n+2}}\left\vert \sum_{x_2}\cdots\sum_{x_{n+1}}\left(\prod_{i=1}^{n} A_i(\omega^u\oplus_{j=0}^{i-1})_{(x_i,x_{i+1})} - \prod_{i=1}^{n} B_i(\omega^u\oplus_{j=0}^{i-1})_{(x_i,x_{i+1})}\right)C(\omega^u\oplus_{j=0}^nx_j)_{(x_{n+1},x_{n+2})}\right\vert\,:\,x_1\in\states\right\}
\end{align*}
{\bf TODO} Finish this. But, to be fair, I'm honestly not sure if this lemma even holds...
%For brevity, define the map $D$ as
%\begin{equation*}
%D \coloneqq \left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]\,.
%\end{equation*}
%We then again start by expanding the definition of the norm:
%\begin{align*}
%&\quad \norm{ DC(\omega^u\otimes_{j=0}^n X_j)} \\
% &= \sup\left\{ \norm{\bigl[DC(\omega^u\otimes_{j=0}^n X_j)\bigr]f} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{ \left\vert\left[\bigl[DC(\omega^u\otimes_{j=0}^n X_j)\bigr]f\right](x)\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{ \left\vert
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\left[C(\omega^u\oplus x\oplus_{j=2}^nx_j)f\right](x_{n+1})
%\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &\leq \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\left[C(\omega^u\oplus x\oplus_{j=2}^nx_j)f\right](x_{n+1})
%\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert\cdot\left\vert\left[C(\omega^u\oplus x\oplus_{j=2}^nx_j)f\right](x_{n+1})
%\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &\leq \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert\cdot\norm{C(\omega^u\oplus x\oplus_{j=2}^nx_j)f} \,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &\leq \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert\cdot\max\left\{\norm{C(\omega^u\oplus x'\oplus_{j=2}^nx_j')f}\,:\,x',x_2',\ldots,x_n'\in\states\right\} \,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{\norm{C(\omega^u\oplus x'\oplus_{j=2}^nx_j')f}\,:\,x',x_2',\ldots,x_n'\in\states\right\}\cdot\max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert \,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert \,:\,x\in\states\right\}\cdot\norm{C(\omega^u\otimes_{j=0}^nX_j)}
%\end{align*}
\end{proof}

\subsection{Stochastic History-Dependent Matrices}

We now look at some properties of stochastic v.h.d. matrices. We will say that a v.h.d. matrix $A(\omega^u\otimes X)$ is a stochastic v.h.d. matrix iff $A(\omega^u\oplus x)$ is stochastic for all $x\in\states$.

\begin{lemma}
Consider a bounded set of rate matrices $\mathcal{Q}$ and a v.h.d. rate matrix $Q(\omega^u\otimes X)$ such that $Q(\omega^u\oplus x)\in\mathcal{Q}$ for all $x\in\states$. Then, for all $\Delta>0$ such that $\Delta\norm{\mathcal{Q}}<1$, we have that $\bigl(I + \Delta Q(\omega^u\otimes X)\bigr)$ is a stochastic v.h.d. matrix.
\end{lemma}
\begin{proof}
Simply note that the lemma holds for any $Q\in\mathcal{Q}$, and that
\begin{equation*}
\bigl(I+\Delta Q(\omega^u\otimes X)\bigr)\cong \left\{\bigl(I+\Delta Q(\omega^u\oplus x)\bigr)\,:\,x\in\states\right\}\,.
\end{equation*}
\end{proof}

\begin{lemma}
Consider two sequences of stochastic variable-history dependent matrices $A_1(\omega^u\otimes X_0),\ldots, A_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ and $B_1(\omega^u\otimes X_0),\ldots, B_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ such that for all $i\in\{1,\ldots,n\}$ and all sequences $(x_0\equiv\emptyset),x_1,\ldots,x_{n-1}$, it holds that
\begin{equation*}
\norm{A_i(\omega^u\oplus_{j=0}^{i-1}x_j) - B_i(\omega^u\oplus_{j=0}^{i-1}x_j)}<c\,.
\end{equation*}
Then,
\begin{equation*}
\norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^n B_i(\omega^u\otimes_{j=0}^{i-1}X_j)} < nc\,.
\end{equation*}
\end{lemma}
\begin{proof}
We use induction on $n$. For $n=1$, the result trivially holds. Now, assume that it holds for $k=n-1$. We show that it also holds for $n$:
\begin{align*}
&\quad \norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^n B_i(\omega^u\otimes_{j=0}^{i-1}X_j)} \\
 &= \left\lVert\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)B_n(\omega^u\otimes_{j=0}^{n-1}X_j) \right. \\
 &\quad\quad\quad \left.+ \prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)B_n(\omega^u\otimes_{j=0}^{n-1}X_j) - \prod_{i=1}^n B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right\lVert \\
 &= \left\lVert\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigl[A_n(\omega^u\otimes_{j=0}^{n-1}X_j) - B_n(\omega^u\otimes_{j=0}^{n-1}X_j)\bigr] \right. \\
 &\quad\quad\quad \left.+ \left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)\right\lVert \\
 &\leq \norm{\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigl[A_n(\omega^u\otimes_{j=0}^{n-1}X_j) - B_n(\omega^u\otimes_{j=0}^{n-1}X_j)\bigr] } \\
 &\quad\quad\quad + \norm{\left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} \\
 &\leq \left(\prod_{i=1}^{n-1} \norm{A_i(\omega^u\otimes_{j=0}^{i-1}X_j)}\right)\norm{A_n(\omega^u\otimes_{j=0}^{n-1}X_j) - B_n(\omega^u\otimes_{j=0}^{n-1}X_j) } \\
 &\quad\quad\quad + \norm{\left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} \\
 &\leq 1\cdot c + \norm{\left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} 
\end{align*}
{\bf TODO} The last step requires us to solve Lemma 26. Assuming that holds, we can continue as
\begin{align*}
&\leq c + \norm{\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)}\norm{B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} \\
&\leq c + (n-1)c\cdot 1 \\
&= nc\,. 
\end{align*}
\end{proof}

We finally need the following result.
\begin{lemma}
Consider any bounded set of rate matrices $\mathcal{Q}$ with lower transition rate operator $\lrate$ and any sequence $Q_1(\omega^u\otimes X_0),Q_2(\omega^u\otimes X_0\otimes X_1),\ldots,Q_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ such that for all $i\in\{1,\ldots,n\}$ and all sequences $x_1,\ldots,x_n$, it holds that $Q_i(\omega^u\oplus_{j=0}^{i-1}x_j)\in\mathcal{Q}$.
Then, for any $\Delta>0$ such that $\Delta\norm{\mathcal{Q}}<1$ and all $f\in\gamblesX$ it holds that
\begin{equation*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) \geq \bigl((I+\Delta\lrate)^n f\bigr)(x)\,.
\end{equation*}
\end{lemma}
\begin{proof}
First, note that for all $i\in\{1,\ldots,n\}$, the terms $\bigl(I+\Delta Q_i(\omega^u\otimes_{j=0}^{i-1})\bigr)$ are stochastic variable history dependent matrices. Hence,
\begin{align*}
\bigl(I+\Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr) &\equiv \mathbb{E}_{X_{i+1}}\left[\,\cdot\,\vert\,X_i,\omega^u,\otimes_{j=0}^{i-1}X_j\right] \\
 &= \sum_{x_{i+1}\in\states} [\cdot](x_{i+1}) P(X_{i+1}=x_{i+1}\,\vert\,X_i,\omega^u,\otimes_{j=0}^{i-1}X_j)\,.
\end{align*}
We therefore have
\begin{align*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) = \mathbb{E}_{X_2}&\left[\cdots\mathbb{E}_{X_{n+1}}\left[f(X_{n+1})\,\vert\,X_n,\omega^u,\otimes_{j=2}^{n-1}X_j,X_1=x\right]\cdots\,\vert\,X_1=x,\omega^u\right] \\
 = \sum_{x_2\in\states}P(X_2=x_2\,\vert\,X_1=x,\omega^u)\sum_{x_3\in\states}\cdots&\sum_{x_{n+1}\in\states} f(x_{n+1})\cdot P(X_{n+1}=x_{n+1}\,\vert\,X_n=x_n,\omega^u,\oplus_{j=2}^{n-1}x_j, X_1=x) \\
 = \sum_{x_2\in\states}\cdots\sum_{x_{n}\in\states} P(X_2=x_2,\ldots,X_{n}=x_n\,\vert\,X_1=x,\omega^u)&\sum_{x_{n+1}\in\states} f(x_{n+1})\cdot P(X_{n+1}=x_{n+1}\,\vert\,X_n=x_n,\omega^u,\oplus_{j=2}^{n-1}x_j, X_1=x)\,.
\end{align*}
Introduce a function $g(\cdot)$ as
\begin{equation*}
g(x,x_2,\ldots,x_n) \coloneqq \sum_{x_{n+1}\in\states} f(x_{n+1})\cdot P(X_{n+1}=x_{n+1}\,\vert\,X_n=x_n,\omega^u,\oplus_{j=2}^{n-1}x_j, X_1=x)\,.
\end{equation*}
Note that also
\begin{align*}
g(x,x_2,\ldots,x_n) \equiv \left[\bigl(I + \Delta Q_n(\omega^u\oplus (X_1=x)\oplus_{j=2}^{n-1}x_j)\bigr)f\right](x_n)\,,
\end{align*}
and since $Q_n(\omega^u\oplus (X_1=x)\oplus_{j=2}^{n-1}x_j)\in\mathcal{Q}$, we have
\begin{equation*}
g(x,x_2,\ldots,x_n) \geq \bigl[(I + \Delta\lrate)f\bigr](x_n)\,.
\end{equation*}
Define a function $g'(\cdot)$ as
\begin{equation*}
g'(x_n) \coloneqq \bigl[(I + \Delta\lrate)f\bigr](x_n).
\end{equation*}

Now, we have that since
\begin{align*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) &= \sum_{x_2\in\states}\cdots\sum_{x_{n}\in\states} P(X_2=x_2,\ldots,X_{n}=x_n\,\vert\,X_1=x,\omega^u)\cdot g(x,x_2,\ldots,x_n)
\end{align*}
is a convex combination of values $g(x,x_2,\ldots,x_n)$ over all sequences $x_2,\ldots,x_n$, that
\begin{align*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) &\geq \sum_{x_2\in\states}\cdots\sum_{x_{n}\in\states} P(X_2=x_2,\ldots,X_{n}=x_n\,\vert\,X_1=x,\omega^u)\cdot g'(x_n) \\
 &= \left[\left[\prod_{i=1}^{n-1} \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]g'\right](x) \\
 &= \left[\left[\prod_{i=1}^{n-1} \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]\bigl[(I + \Delta\lrate)f\bigr]\right](x)\,.
\end{align*}
Repeatedly applying this argument then yields the lemma.
\end{proof}

\subsection{Imprecise Non-Markovian Stochastic Processes}

We are now finally ready to show our main result for imprecise non-Markovian stochastic processes.

For any bounded set of rate matrices $\mathcal{Q}$, we consider the set $\mathbb{P}_\mathcal{Q}$ of all $P\in\mathbb{P}$ such that
\begin{equation*}
(\forall\epsilon>0)(\exists\delta>0)(\forall t\in[0,+\infty))(\forall\Delta\in(0,\delta))(\forall\omega^u\in\Omega_{\mathcal{U}_{[0,t]}})(\exists Q\in\mathcal{Q})\norm{\frac{T_t^{t+\Delta}(\omega^u)-I}{\Delta}-Q}<\epsilon\,.
\end{equation*}
The following result now holds.
\begin{theorem}
Consider any $t,s\in[0,+\infty)$ such that $t<s$ and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then for any $P\in\mathbb{P}_\mathcal{Q}$, $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$ and $f\in\gamblesX$:
\begin{equation*}
\bigl[L_t^sf\bigr](x) \leq \bigl[T_t^s(\omega^u)f\bigr](x)\,.
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\mathbb{P}_\mathcal{Q}$ and any $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$, $f\in\gamblesX$, and $x\in\states$. Assume \emph{ex absurdo} that $\bigl[L_t^sf\bigr](x) > \bigl[T_t^s(\omega^u)f\bigr](x)$. We prove that this leads to a contradiction. Let $C\coloneqq s-t$ and choose $\epsilon>0$ small enough such that
\begin{equation}
\epsilon(1+C\norm{f}) < \bigl[L_t^s\bigr](x) - \bigl[T_t^s(\omega^u)f\bigr](x)\,.
\end{equation}
Since $P\in\mathbb{P}_\mathcal{Q}$, we have that there is some $\delta>0$ such that
\begin{equation}
(\forall \tau\in[0,+\infty))(\forall\Delta\in(0,\delta))(\forall\omega^u\in\Omega_{\mathcal{U}_{[0,\tau]}})(\exists Q\in\mathcal{Q})\norm{\frac{T_\tau^{\tau+\Delta}(\omega^u)-I}{\Delta}-Q}<\epsilon\,.
\end{equation}
Furthermore, because of Theorem 5, there is some $\delta'>0$ such that
\begin{equation}
(\forall u\in\mathcal{U}_{t,s}\,:\,\sigma(u)<\delta')\left\vert \bigl[L_t^sf\bigr](x) - \left[\left[\prod_{k=1}^n\bigl(I+\Delta_k\lrate\bigr)\right]f\right](x)\right\vert < \epsilon\,.
\end{equation}
Now choose $n>\max\{\nicefrac{C}{\delta},\nicefrac{C}{\delta'},C\norm{\mathcal{Q}}\}$. Then for $\Delta\coloneqq\nicefrac{C}{n}$, we find that $\Delta<\delta$, $\Delta<\delta'$, and $\Delta\norm{\mathcal{Q}}<1$.

For all $k\in\{0,1,\ldots,n\}$, define $t_k\coloneqq t+k\Delta$. Since $\Delta<\delta$, it follows from Equation 11 that for all sequences $(x_{t_0}\equiv\emptyset),x_{t_1},\ldots,x_{t_n}$ and all $i\in\{1,\ldots,n\}$, there is some $Q_i(\omega^u\oplus_{j=0}^{i-1}x_{t_j})\in\mathcal{Q}$ such that
\begin{align*}
&\quad \norm{T_{t_{i-1}}^{t_i}(\omega^u\oplus_{j=0}^{i-1}x_{t_j}) - \bigl(I+\Delta Q_i(\omega^u\oplus_{j=0}^{i-1}x_{t_j})\bigr)} \\
 &= \norm{\frac{T_{t_{i-1}}^{t_i}(\omega^u\oplus_{j=0}^{i-1}x_{t_j}) - I}{\Delta} - Q_i(\omega^u\oplus_{j=0}^{i-1}x_{t_j})}\Delta \\
 &< \epsilon\Delta\,.
\end{align*}
Furthermore, since $\Delta\norm{\mathcal{Q}}<1$, we have by Lemma 27 that $\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr)$ is a variable history dependent stochastic matrix for all $i\in\{1,\ldots,n\}$. Therefore, we find by Lemmas 23 and 28 that
\begin{align*}
&\quad \norm{T_t^s(\omega^u) - \prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr)} \\
 &= \norm{\prod_{i=1}^n T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j}) - \prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr)} \\
 &\leq \epsilon\Delta n = \epsilon C\,.
\end{align*}
This implies that
\begin{align*}
&\quad\left\vert \bigl[T_t^s(\omega^u)f\bigr](x) - \left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) \right\vert \\
 &\leq \norm{T_t^s(\omega^u)f - \left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f} \\
 &\leq \norm{T_t^s(\omega^u) - \left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]}\norm{f} \\
 &\leq \epsilon C\norm{f}\,,
\end{align*}
and hence,
\begin{equation}
\bigl[T_t^s(\omega^u)f\bigr](x) \geq \left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) - \epsilon C\norm{f}\,.
\end{equation}
Furthermore, since $\Delta<\delta'$, it follows from Equation 12 that
\begin{equation*}
\left\vert \bigl[L_t^sf\bigr](x) - \left[\bigl(I + \Delta\lrate\bigr)^n f\right](x)\right\vert < \epsilon\,,
\end{equation*}
and hence
\begin{equation}
\left[\bigl(I + \Delta\lrate\bigr)^n f\right](x) > \bigl[L_t^sf\bigr](x) - \epsilon\,.
\end{equation}
Finally, by Lemma 29, we have
\begin{equation}
\left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) \geq \left[\bigl(I + \Delta\lrate\bigr)^n f\right](x)\,.
\end{equation}
Combining equations 13, 14, and 15, yields
\begin{align*}
\bigl[T_t^s(\omega^u)f\bigr](x) &\geq \left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) - \epsilon C\norm{f} \\
 &\geq \left[\bigl(I + \Delta\lrate\bigr)^n f\right](x) - \epsilon C\norm{f} > \bigl[L_t^sf\bigr](x) - \epsilon - \epsilon C\norm{f}\,,
\end{align*}
whence
\begin{equation*}
\epsilon(1+C\norm{f})> \bigl[L_t^sf\bigr](x) - \bigl[T_t^s(\omega^u)f\bigr](x)\,.
\end{equation*}
This provides the required contradiction; see Equation 10.
\end{proof}

\newpage
\section{Alternative suggestions for notation}

Let $A^s_{t_0,\dots,t_n}$ be a non-negatively homogeneous operator from gambles on $(X_{t_0},\dots,X_{t_n},X_s)$ to gambles on $(X_{t_0},\dots,X_{t_n})$
and let
\begin{equation*}
\norm{A^s_{t_0,\dots,t_n}}\coloneqq\sup\Big\{\norm{A^s_{t_0,\dots,t_n}f}, \text{$f$ a gamble on $(X_{t_0},\dots,X_{t_n},X_s)$}, \norm{f}=1\Big\}.
\end{equation*}
As a special case, let $T^s_{t_0,\dots,t_n}$ be defined for all $f$ on $X_{t_0},\dots,X_{t_n},X_s$ by
\begin{equation*}
T^s_{t_0,\dots,t_n}f\coloneqq E(f(X_{t_0},\dots,X_{t_n},X_s)\vert X_{t_0},\dots,X_{t_n})
\end{equation*}
$T^s_t$ is now an operator that maps gambles on $(X_t,X_s)$ to gambles on $X_t$, whereas before, it was a map from gambles on $X_s$ to gambles on $X_t$. However, this is not a problem, because any gamble on $(X_t,X_s)$ is trivially a gamble on $X_s$ as well.

What is nice about these definitions is that

\begin{align*}
\norm{A^{t_n}_{t_0}}
=\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-1}}^{t_n}
}
\leq
\norm{A_{t_0}^{t_1}}
	\norm{A_{t_0,t_1}^{t_2}}
	\cdots
	\norm{A_{t_0,\dots,t_{n-1}}^{t_n}}
\end{align*}

Furthermore:

\begin{align*}
&\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-1}}^{t_n}
	-
	B_{t_0}^{t_1}
	B_{t_0,t_1}^{t_2}
	\cdots
	B_{t_0,\dots,t_{n-1}}^{t_n}
}\\
&\leq
\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-1}}^{t_n}
	-
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	B_{t_0,\dots,t_{n-1}}^{t_n}}\\
	&~~~~~~+
	\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	B_{t_0,\dots,t_{n-1}}^{t_n}
	-
	B_{t_0}^{t_1}
	B_{t_0,t_1}^{t_2}
	\cdots
	B_{t_0,\dots,t_{n-1}}^{t_n}
}\\
&=
\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	(A_{t_0,\dots,t_{n-1}}^{t_n}-
	B_{t_0,\dots,t_{n-1}}^{t_n})
	}\\
	&~~~~~~+
	\norm{
	(A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	-
	B_{t_0}^{t_1}
	B_{t_0,t_1}^{t_2}
	\cdots
	B_{t_0,\dots,t_{n-2}}^{t_{n-1}})
	B_{t_0,\dots,t_{n-1}}^{t_n}
}\\
&\leq
\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}}
	\norm{A_{t_0,\dots,t_{n-1}}^{t_n}-
	B_{t_0,\dots,t_{n-1}}^{t_n}
	}\\
	&~~~~~~+
	\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	-
	B_{t_0}^{t_1}
	B_{t_0,t_1}^{t_2}
	\cdots
	B_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	}
	\norm{
	B_{t_0,\dots,t_{n-1}}^{t_n}
}.
\end{align*}

If $A$ and $B$ are stochastic, this implies that

\begin{align*}
&\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-1}}^{t_n}
	-
	B_{t_0}^{t_1}
	B_{t_0,t_1}^{t_2}
	\cdots
	B_{t_0,\dots,t_{n-1}}^{t_n}
}\\
&\leq
\norm{A_{t_0,\dots,t_{n-1}}^{t_n}-
	B_{t_0,\dots,t_{n-1}}^{t_n}
	}+
	\norm{
	A_{t_0}^{t_1}
	A_{t_0,t_1}^{t_2}
	\cdots
	A_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	-
	B_{t_0}^{t_1}
	B_{t_0,t_1}^{t_2}
	\cdots
	B_{t_0,\dots,t_{n-2}}^{t_{n-1}}
	}
\end{align*}

