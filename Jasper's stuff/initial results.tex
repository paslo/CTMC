\documentclass[a4paper,reqno]{amsart}
\usepackage[british]{babel}
%\usepackage[garamond]{mathdesign}
\usepackage{mathptmx}
\usepackage{courier}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{nicefrac}
%\usepackage{pdfsync}
\renewcommand{\ttdefault}{cmtt}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}
% - macros

\newcommand{\nats}{\mathbb{N}}
\newcommand{\natswith}{\nats_{0}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\states}{\mathcal{X}}

\newcommand{\paths}{\Omega}
\newcommand{\path}{\omega}

\newcommand{\power}{\mathcal{P}(\paths)}
\newcommand{\nonemptypower}{\power_{\emptyset}}
\newcommand{\events}{\mathcal{E}}
%\newcommand{\nonemptyevents}{\events^{\emptyset}}
\newcommand{\filter}[1][t]{\mathcal{F}_{#1}}
\newcommand{\eventst}[1][t]{\events_{#1}}

\newcommand{\processes}{\mathbb{P}}
\newcommand{\mprocesses}{\processes^{\mathrm{M}}}


\newcommand{\lt}{\underline{T}}
\newcommand{\lbound}{L}

\newcommand{\gambles}{\mathcal{L}}
\newcommand{\gamblesX}{\gambles(\states)} 

\newcommand{\ind}[1]{\mathbb{I}_{#1}}

\newcommand{\rateset}{\mathcal{Q}}
\newcommand{\lrate}{\underline{Q}}

\newcommand{\asa}{\Leftrightarrow}
\newcommand{\then}{\Rightarrow}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

\newcommand{\coloneqq}{:\!=}

\begin{document}
\title{Continuous-time Imprecise Markov Chains}
\author{Jasper De Bock\and Thomas Krak}
%\address{Ghent University, SYSTeMS Research Group, Technologiepark -- Zwijnaarde 914, 9052 Zwijnaarde, Belgium}
%\email{jasper.debock@ugent.be}

\begin{abstract}
These are some rough initial ideas and results. Notation, text and maths should all still be cleaned up; the only goal of this note is to get the ideas across.
\end{abstract}

\maketitle

\section{Preliminaries}

Consider some finite \emph{state space} $\states=\{1,\dots,m\}$. A cadlag function from $[0,+\infty)$ to $\states$ is called a \emph{path}, where cadlag means that it is right continuous for all $t\in[0,+\infty)$ and that the left limit exists for all $t\in(0,+\infty)$. Let $\paths$ be the set of all paths. For any path $\path\in\paths$ and any time point $t\in[0,+\infty)$, the value of $\path$ in $t$ is denoted by $\path(t)$.

A subset $E$ of $\paths$ is called an \emph{event}. The set of all events is denoted by $\power$ and we let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. For any $t\in[0,+\infty)$ and $x\in\states$, we define the elementary event
\begin{equation*}
(X_t=x)\coloneqq\{\path\in\paths\colon\path(t)=x\}.
\end{equation*}
Consider now any $t\in[0,+\infty)$. We then let $\events_{\geq t}$ be the algebra that is generated by all the elementary events $X_s=x$, for $s\geq t$ and $x\in\states$.\footnote{This is the smallest subset of $\power$ that contains all these elementary events and that is furthermore closed under complements, finite unions and hence also finite intersections.} Similarly, we let $\events_{\leq t}$ be the algebra that is generated by all the elementary events $X_s=x$, for $s\leq t$ and $x\in\states$. We also define $\filter\coloneqq\events_{\leq t}\setminus\{\emptyset\}$.

A full conditional finitely additive probability measure $P$ is a real-valued map from $\power\times\nonemptypower$ to $\reals$ that satisfies the following axioms. For all $A,B\in\power$ and all \mbox{$C,D\in\nonemptypower$}:
\vspace{5pt}

\begin{enumerate}[label=F\arabic*:]
\item
$P(C\vert C)=1$;
\item
$0\leq P(A\vert C)\leq 1$;
\item
$P(A\cup B\vert C)=P(A\vert C)+P(B\vert C)$ if $A\cap B=\emptyset$;
\item
$P(A\vert D)=P(A\vert C\cap D)P(C\vert D)$ if $A\subseteq C$.
\end{enumerate}
\vspace{5pt}

\noindent
For any $A\in\power$ and $C\in\nonemptypower$, we call $P(A\vert C)$ the probability of $A$ conditional on $B$. Also, for any $A\in\power$, we frequently use the shorthand notation $P(A)\coloneqq P(A\vert\paths)$ and then call $P(A)$ the probability of $A$.

\section{Stochastic processes}

A \emph{stochastic process} is the restriction of a full conditional finitely additive probability measure $P$ to some subset $\mathcal{C}$ of $\power\times\nonemptypower$ that includes at least
\begin{equation*}
\mathcal{C^*}\coloneqq\big\{
(A,C)
\colon
C\in\filter, A\in\events_{\geq t}, t\in[0,+\infty)
\big\}.
\end{equation*}
We denote the set of all such stochastic processes by $\processes$.

Let $\gamblesX$ be the set of all real-valued functions on $\states$. Consider any stochastic process $P\in\processes$ and any $t,s\in[0,+\infty)$ such that $t<s$. The transition matrix $T_t^s$ is then an $m\times m$ matrix that is defined by
\begin{equation*}
T_t^s(x_t,x_s)\coloneqq P(X_s=x_s\vert X_t=x_t)\text{ for all $x_s,x_t\in\states$}.
\end{equation*}
Obviously, this transition, matrix $T_t^s$ can also be regarded as a map from $\gamblesX$ to $\gamblesX$, defined for all $f\in\gamblesX$ by $T_t^s(f)\coloneqq T_t^sf$.

We say that a stochastic process $P\in\processes$ satisfies the \emph{Markov property} when, for any time sequence $0\leq t_1<t_2,\dots,t_{n}<t<s$ and set of states $x_{t_1},x_{t_2},\dots,x_{t_n},x_t,x_s\in\states$:
\begin{equation*}
P(X_s=x_s\vert X_t=x_t)=P(X_s=x_s\vert X_t=x_t,X_{t_1}=x_{t_1},X_{t_2}=x_{t_2}, \dots, X_{t_n}=x_{t_n}).
\end{equation*}
We denote the set of all stochastic processes that satisfy this Markov property by $\mprocesses$


\begin{lemma}\label{lemma:transitionmatrixfactorises}
Consider any $P\in\mprocesses$. Then
\begin{equation*}
T_t^s=\prod_{k=1}^n T_{t_{k-1}}^{t_k} \coloneqq T_{t_0}^{t_1}T_{t_1}^{t_2}\cdots T_{t_{n-1}}^{t_n}
\end{equation*}
for every sequence $0\leq t=t_0<t_1<t_2,\dots,t_{n}=s$.
\end{lemma}
\begin{proof}
This property is well known and therefore stated without proof.
\end{proof}

\section{The norms that are used in this document}

For any vector $f\in\gamblesX$, we let $\norm{f}\coloneqq\norm{f}_{\infty}\coloneqq\max\{\abs {f(x)}\colon x\in\states\}$ be the maximum norm. For any operator $A$ from $\gamblesX$ to $\gamblesX$ that is non-negatively homogeneous, meaning that
\begin{equation*}
A(\lambda f)=\lambda A(f)\text{ for all $f\in\gamblesX$ and all $\lambda\geq0$,}
\end{equation*}
we consider the induced operator norm
\begin{equation*}
\norm{A}\coloneqq\sup\{\norm{Af}\colon f\in\gamblesX,\norm{f}=1\}.
\end{equation*}
If $A$ is an $m\times m$ matrix, we have that
\begin{equation*}
\norm{A}
=
\max\{\sum_{y\in\states}\abs{A(x,y)}\colon x\in\states\}.
\end{equation*}


\noindent
These norms satisfy the following properties. For all $f,g\in\gamblesX$, all $A,B$ from $\gamblesX$ to $\gamblesX$ that are non-negatively homogeneous, all $\lambda\in\reals$ and all $x\in\states$, we have that
\vspace{5pt}

\begin{enumerate}[label=N\arabic*:]
\item
$\abs{f(x)}\leq\norm{f}$
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\norm{A}\geq0$
\item
$\norm{A}=0\asa A=0$
\item
$\norm{A+B}\leq\norm{A}+\norm{B}$
\item
$\norm{AB}\leq\norm{A}\norm{B}$
\item
$\norm{\lambda A}=\abs{\lambda}\norm{A}$
\item
$\norm{Af}\leq\norm{A}\norm{f}$
\item
$\norm{A}=1$ if $A$ is a stochastic matrix.
\end{enumerate}
\vspace{5pt}

\noindent
Finally, for any set $\mathcal{A}$ of $n\times n$ matrices, we define

\begin{equation*}
\norm{\mathcal{A}}\coloneqq\sup\{\norm{A}\colon A\in\mathcal{A}\}.
\end{equation*}

\begin{lemma}\label{lemma:differenceproductoftransition}
Consider two sequences $A_1,\dots,A_n$ and $B_1,\dots,B_n$ of stochastic matrixes such that, for all $i\in\{1,\dots,n\}$, $\norm{A_i-B_i}\leq c$. Then
\begin{equation*}
\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}\leq nc
\end{equation*}
\end{lemma}
\begin{proof}
We provide a proof by induction. For $n=1$, the result is trivially true. Assume that the result holds for $n=k-1$. The following derivation then shows that it also holds for $n=k$: 
\begin{align*}
\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}
&=
\norm{\prod_{i=1}^{n}A_i-\left(\prod_{i=1}^{n-1}A_i\right)B_n+\left(\prod_{i=1}^{n-1}A_i\right)B_n-\prod_{i=1}^{n}B_i}\\
&\leq
\left(\prod_{i=1}^{n-1}\norm{A_i}\right)\norm{A_n-B_n}+\norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\norm{B_n}\\
&\leq c + \norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\leq c+(n-1)c= nc.
\end{align*}
\end{proof}




\section{Sets of rate matrices and lower transition rate operators}

A real-valued $m\times m$ matrix $Q$ is said to be a rate matrix if

\vspace{5pt}
\begin{enumerate}[label=R\arabic*:]
\item
$\sum_{y\in\states}Q(x,y)=0$ for all $x\in\states$;
\item
$Q(x,y)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\noindent
We use $\mathcal{R}$ to denote the set of all rate matrices. Clearly, $\mathcal{R}$ is closed under finite sums and multiplication with non-negative scalars. For any set $\rateset\subseteq\mathcal{R}$ of rate matrices, we let
\begin{equation*}
\rateset_x\coloneqq\{Q(x,\cdot)\colon Q\in\rateset\}
\text{ for all $x\in\states$.}
\end{equation*}
We say that $\rateset$ has \emph{separately specified rows} if
\begin{equation*}
Q\in\rateset\asa(\forall x\in\states)~Q(x,\cdot)\in\rateset_x.
\end{equation*}
A set $\rateset\subseteq\mathcal{R}$ of rate matrices is said to be bounded if $\inf\{Q(x,x)\colon Q\in\rateset\}>-\infty$ for all $x\in\states$ or, equivalently, if $\norm{\rateset}<\infty$. 

For any bounded set $\rateset$ of rate matrices, the corresponding \emph{lower transition rate operator} $\lrate$ is a map from $\gamblesX$ to $\gamblesX$. For all $f\in\gamblesX$, $\lrate f$ is defined by
\begin{equation}\label{eq:deflowerbound}
(\lrate f)(x)\coloneqq\inf\{(Qf)(x)\colon Q\in\rateset\}\text{ for all $x\in\states$}.
\end{equation}
We will call a map $\lrate$ from $\gamblesX$ to $\gamblesX$ a \emph{coherent} lower transition rate operator if, for all $f,g\in\gamblesX$, $\lambda\geq0$ and $\mu\in\reals$:

\vspace{5pt}
\begin{enumerate}[label=LR\arabic*:]
\item
$\lrate(\mu)=0$;
\item
$\lrate(f+g)\geq\lrate(f)+\lrate(g)$;
\item
$\lrate(\lambda f)=\lambda\lrate(f)$;
\item
$\lrate(\ind{y})(x)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}

\begin{lemma}\label{lemma:lrateiscoherent}
For any bounded set $\rateset$ of rate matrices, the corresponding lower transition rate operator $\lrate$ is coherent.
\end{lemma}
\begin{proof}
This is immediate.
\end{proof}

For any coherent lower transition rate operator $\lrate$, we use $\rateset_{\lrate}$ to denote the set of all rate matrices $Q$ that dominate $\lrate$, in the sense that
\begin{equation*}
Qf\geq\lrate f\text{ for all $f\in\gamblesX$.}
\end{equation*}

\begin{lemma}
Consider a coherent lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices. Then $\rateset_{\lrate}$ is closed, bounded and convex, and has separately specified rows. Also, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $\lrate f=Qf$. Furthermore, $\norm{\lrate}<\infty$.
\end{lemma}
\begin{proof}
*** This is not trivial, but I am convinced it is true (we would have to adapt the proof for a similar well-known result for coherent lower previsions) ***
\end{proof}

\noindent
If $\lrate$ is the coherent lower transition rate operator that corresponds to some bounded set $\rateset$ of rate matrices, then clearly: $\rateset\subseteq\rateset_{\lrate}$.

\section{Lower transition operators}

We will call a map $\lt$ from $\gamblesX$ to $\gamblesX$ a \emph{coherent lower transition operator} if, for all $f,g\in\gamblesX$ and $\lambda\geq0$:

\vspace{5pt}
\begin{enumerate}[label=C\arabic*:]
\item
$\lt f\geq\min f$
\item
$\lt(f+g)\geq\lt(f)+\lt(g)$;
\item
$\lt(\lambda f)=\lambda\lt(f)$.
\end{enumerate}
\vspace{5pt}


Consider now any $t,s\in[0,+\infty)$ such that $t<s$ and let $\lrate$ be an arbitrary coherent lower transition rate operator . The corresponding lower transition operator $\lbound_t^s$ is then a map from $\gamblesX$ to $\gamblesX$, defined by
\begin{equation}\label{eq:lowerbound}
\lbound_t^s\coloneqq\lim_{\sigma(u)\to0}\prod_{k=1}^n(I+\Delta_k\lrate),
\end{equation}
where the limit is taken with respect to the set $\mathcal{U}_{[t,s]}$ of all sequences $t=t_0<t_1<\dots<t_n=s$ and where, for any $u\in\mathcal{U}_{[t,s]}$, we first define $\Delta_i\coloneqq t_i-t_{i-1}$ for all $i\in\{1,\dots,n\}$ and then let $\sigma(u)\coloneqq\max\{\Delta_i\colon i\in\{1,\dots,n\}\}$. The following result establishes that the limit in Equation~\eqref{eq:lowerbound} exists, and that it is a coherent lower transition operator.

\begin{theorem}\label{theo:convergencelowerbound}
For any $t,s\in[0,+\infty)$ such that $t<s$ and any coherent lower transition rate operator $\lrate$, there is a coherent lower transition operator $\lbound_t^s$ such that 
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \prod_{k=1}^n(I+\Delta_k\lrate)}<\epsilon.
\end{equation*}
\end{theorem}
\begin{proof}
See the rough sketch in Section~\ref{sec:proofsketch}.
\end{proof}

The derivative of $\lbound_t^s$ with respect to $t$ furthermore satisfies differential equations in the style of Damjan.

\begin{proposition}
Consider any $t,s\in[0,+\infty)$ such that $t<s$, let $\lrate$ be an arbitrary coherent lower transition rate operator and let $\lbound_t^s$ be the corresponding lower transition operator. Then $\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s$ and $\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate$, meaning that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\in(-\delta,\delta))~
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation*}
and
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\in(-\delta,\delta))~
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon.
\end{equation*}
\end{proposition}
\begin{proof}
*** Not 100\% sure this is true... but something along these lines must definitely hold ***
\end{proof}

\section{The Markovian (but possibly time-inhomogeneous) case}

For any bounded set of rate matrices $\rateset$, we consider the set $\mprocesses_{\rateset}$ of all $P\in\mprocesses$ such that
\begin{equation}\label{eq:conditionforMarkov}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall t\in[0,+\infty))\,
(\forall\Delta\in(0,\delta))\,
(\exists Q\in\rateset)~
\Big\lVert\frac{T_t^{t+\Delta}-I}{\Delta}-Q\Big\rVert<\epsilon.
\end{equation}
%\vspace{5pt}

\begin{theorem}
Consider any $t,s\in[0,+\infty)$ such that $t<s$ and let $\rateset$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then for any $P\in\mprocesses_\rateset$ and $f\in\gamblesX$:
\begin{equation*}
\lbound_t^sf(x)\leq T_t^sf(x).
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\mprocesses_\rateset$ and any $f\in\gamblesX$ and $x\in\states$. Assume \emph{ex absurdo} that $\lbound_t^sf(x)>T_t^sf(x)$. We prove that this leads to a contradiction.  Let $C\coloneqq t-s$ and choose $\epsilon>0$ small enough such that
\begin{equation}\label{eq:chooseepsilon}
\epsilon(1+C\norm{f})<\lbound_t^sf(x)-T_t^sf(x).
\end{equation}
Since $P\in\mprocesses_\rateset$, it follows from Equation~\eqref{eq:conditionforMarkov} that there is some $\delta>0$ such that
\begin{equation}\label{eq:1conditionforMarkov}
(\forall \tau\in[0,+\infty))\,
(\forall\Delta\in(0,\delta))\,
(\exists Q\in\rateset)~
\Big\lVert\frac{T_\tau^{\tau+\Delta}-I}{\Delta}-Q\Big\rVert<\epsilon.
\end{equation}
Furthermore, because of Equation~\eqref{eq:deflowerbound} and Theorem~\ref{theo:convergencelowerbound}, there is some $\delta'>0$ such that
\begin{equation}\label{eq:deltaprimeformula}
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta')~\abs{\lbound_t^sf(x) - \left(\left(\prod_{k=1}^n(I+\Delta_k\lrate)\right)(f)\right)(x)}<\epsilon.
\end{equation}
Now choose $n>\max\{\nicefrac{C}{\delta},\nicefrac{C}{\delta'},C\norm{\rateset}\}$. Then for $\Delta\coloneqq\nicefrac{C}{n}$, we find that $\Delta<\delta$, $\Delta<\delta'$ and $\Delta\norm{\rateset}<1$. 

For all $k\in\{0,1,\dots,n\}$, define $t_k\coloneqq t+k\Delta$. Since $\Delta<\delta$, it follows from Equation~\eqref{eq:1conditionforMarkov} that, for all $i\in\{1,\dots,n\}$, there is some $Q_i\in\rateset$ such that 
\begin{equation*}
\norm{T_{t_{i-1}}^{t_i}-(I+\Delta Q_i)}
=\norm{\frac{T_{t_{i-1}}^{t_i}-I}{\Delta}-Q_i}\Delta
<\epsilon\Delta.
\end{equation*}
Furthermore, for all $i\in\{1,\dots,n\}$, since $\abs{\Delta Q_i(x,y)}\leq\norm{\Delta Q_i}=\Delta\norm{Q_i}\leq\Delta\norm{\rateset}<1$ for all $x,y\in\states$, we have that $I+\Delta Q_i$ is a stochastic matrix.
Therefore, due to Lemmas~\ref{lemma:transitionmatrixfactorises} and~\ref{lemma:differenceproductoftransition}, we find that
\begin{equation*}
\norm{T_t^s-\prod_{i=1}^n(I+\Delta Q_i)}
=\norm{\prod_{i=1}^n T_{t_{i-1}}^{t_i}-\prod_{i=1}^n(I+\Delta Q_i)}
\leq\epsilon\Delta n=\epsilon C,
\end{equation*}
which implies that
\begin{align}
\abs{T_t^sf(x)-\left(\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)\right)(x)}
&\leq
\norm{T_t^sf-\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)}\notag\\
&\leq
\norm{T_t^s-\prod_{i=1}^n(I+\Delta Q_i)}\norm{f}
\leq\epsilon C\norm{f}.\label{eq:firstpartofbound}
\end{align}
\noindent
Equation~\eqref{eq:deflowerbound} implies that
\begin{equation}
\left(\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)\right)(x)
\geq
\left((I+\Delta\lrate)^n(f)\right)(x)\label{eq:middlepartofbound}
\end{equation}
and, since $\Delta<\delta'$, it follows from Equation~\eqref{eq:deltaprimeformula} that
\begin{equation}\label{eq:lastpartofbound}
\abs{\lbound_t^sf(x) - \left((I+\Delta\lrate)^n(f)\right)(x)}<\epsilon.
\end{equation}
\noindent
By combining Equations~\eqref{eq:firstpartofbound}, \eqref{eq:middlepartofbound} and~\eqref{eq:lastpartofbound}, we find that
\begin{align*}
T_t^sf(x)
&\geq
\left(\left(\prod_{i=1}^n(I+\Delta Q_i)\right)(f)\right)(x)-\epsilon C\norm{f}\\
&\geq
\left((I+\Delta\lrate)^n(f)\right)(x)-\epsilon C\norm{f}
>\lbound_t^sf(x)-\epsilon-\epsilon C\norm{f},
\end{align*}
which provides the required contradiction; see Equation~\eqref{eq:chooseepsilon}.
\end{proof}

\begin{theorem}
Consider any $t,s\in[0,+\infty)$ such that $t<s$ and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$, there is some $P\in\mprocesses_{\rateset}$ such that
\begin{equation*}
\lbound_t^sf=T_t^sf.
\end{equation*}
\end{theorem}
\begin{proof}
*** The time-inhomogenous Markov chain for which $Q_v\coloneqq\lrate\lbound_v^sf$ establishes the equality. ***
\end{proof}

\section{Lemma's which I think will turn out to be useful}

\begin{lemma}\label{lemma:smallerpartition}
Consider a rate matrix $Q$. Then for all $\epsilon>0$, there is some $\delta>0$ such that for all $\Delta_1,\dots,\Delta_n>0$ with $\Delta\coloneqq\sum_{j=1}^n\Delta_j<\delta$, it holds that
\begin{equation*}
\Delta<\delta\then\left\lVert\left(\prod_{j=1}^n[I+\Delta_j Q]\right)-(I+\Delta Q)\right\rVert<\epsilon\Delta
\end{equation*}


\end{lemma}
\begin{proof}
Fix $\epsilon>0$. Since $\frac{d}{dt}e^{t\lVert Q\rVert}\vert_{t=0}=\lVert Q\rVert$, there is some $\delta>0$ such that, for all $0<t<\delta$, $\big\vert e^{t\lVert Q\rVert}- (1+t\lVert Q\rVert)\big\vert<\epsilon t$. Consider now any $\Delta_1,\dots,\Delta_n>0$ such that $\Delta\coloneqq\sum_{j=1}^n\Delta_j<\delta$. Then
\begin{multline*}
\left\lVert\left(\prod_{j=1}^n[I+\Delta_j Q]\right)-(I+\Delta Q)\right\rVert\\
\begin{aligned}
&=
\left\lVert\left(I+Q\sum_{j=1}^n\Delta_j+Q^2\sum_{1\leq j<j'\leq n}\Delta_j\Delta_{j'}+ \dots +Q^n\prod_{j=1}^n\Delta_j\right)-(I+\Delta Q)\right\rVert\\
&=
\left\lVert Q^2\sum_{1\leq j<j'\leq n}\Delta_j\Delta_{j'}+ \dots +Q^n\prod_{j=1}^n\Delta_j \right\rVert\\
&\leq
{\lVert Q\rVert}^2\sum_{1\leq j<j'\leq n}\Delta_j\Delta_{j'}+ \dots +{\lVert Q\rVert}^n\prod_{j=1}^n\Delta_j\\
&=\left(\prod_{j=1}^n[1+\Delta_j\lVert Q\rVert]\right)-(1+\Delta\lVert Q\rVert)\\
&\leq \left(1+\frac{\Delta\lVert Q\rVert}{n}\right)^n-(1+\Delta\lVert Q\rVert)
\leq e^{\Delta\lVert Q\rVert}-(1+\Delta\lVert Q\rVert)
<\epsilon\Delta.
\end{aligned}
\end{multline*}
\end{proof}

\section{Rough sketch of the proof of Theorem~\ref{theo:convergencelowerbound}}\label{sec:proofsketch}

This section gives a rough sketch of the proof of Theorem~\ref{theo:convergencelowerbound}. There are still many claims in here, which need to be proved, but the general structure of the proof is completely there. I won't work on this anymore (at least for now). Feel free to use this structure to come up with a solid formal proof. 


We start with a bunch of lemma's and propositions.

\begin{lemma}\label{lemma:normQsmallenough}
If $0<\Delta\leq\nicefrac{1}{\norm{\lrate}}$, then $I+\Delta\lrate$ is a coherent lower transition operator.
\end{lemma}
\begin{proof}
Just check each of the three defining properties. C2 and C3 are trivial. C1 requires a bit more work.
\end{proof}

\begin{lemma}\label{lemma:compositioncoherence}
If $\lt_1,\lt_2,\dots,\lt_n$ are coherent lower transition operators, then  $\lt_1\lt_2\cdots\lt_n$ is also a coherent lower transition operator.
\end{lemma}
\begin{proof}
Simply check each of the properties.
\end{proof}

\begin{lemma}\label{lemma:productiscoherent}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$. Then
\begin{equation*}
\prod_{k=i}^n(I+\Delta_i\lrate)
\end{equation*}
is a coherent lower transition operator.
\end{lemma}
\begin{proof}
Trivial consequence of Lemma~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}.
\end{proof}

\begin{lemma}\label{lemma:normofcoherenttrans}
For any coherent lower transition operator $\lt$, we have that $\norm{\lt}\leq 1$.
\end{lemma}
\begin{proof}
This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttrans}
For any coherent lower transition operator $\lt$ and any two non-negatively homogeneous operators $A$, $B$, we have that $\norm{\lt A-\lt B}\leq \norm{A-B}$.
\end{lemma}
\begin{proof}
This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttransrate}
Consider any two non-negatively homogeneous operators $A$, $B$. It then holds that $\norm{\lrate A-\lrate B}\leq 2\norm{\lrate}\norm{A-B}$.
\end{lemma}
\begin{proof}
This can be shown to follow from the definition of the norm and the properties of $\lrate$.
\end{proof}


\begin{lemma}\label{lemma:justtheindicator}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
&=\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)+\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{(I+\Delta_n\lrate)-I}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&=\Delta_n\norm{\lrate}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttrans}. By repeating this argument over and over again (actually, by induction), we find that
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
\leq \Delta_n\norm{\lrate} +\Delta_{n-1}\norm{\lrate}+\cdots
+\Delta_1\norm{\lrate}
=\Delta\norm{\lrate}.
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:justthelinearpart}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1\norm{\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I},
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttransrate}. Due to Lemma~\ref{lemma:justtheindicator}, this implies that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right).
\end{align*}
By continuing in this way (applying induction) we find that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq
\norm{\prod_{i=n}^n(I+\Delta_i\lrate)-(I+\sum_{i=n}^n\Delta_i\lrate)}
+2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=2\norm{\lrate}^2\sum_{k=1}^n\Delta_k\sum_{i=k+1}^n\Delta_i\\
&\leq2\norm{\lrate}^2\frac{1}{2}\left(\sum_{k=1}^n\Delta_k\right)^2=\Delta^2\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:differencebetweennested}
For any $k\in\{1,\dots,n\}$, consider a sequence of $\Delta_{k,i}>0$, $i=1,\dots,n_k$ and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{n,k}$. Let $\Delta\coloneqq\sum_{k=1}^n\Delta_k$ and let $\alpha\coloneqq\max\{\Delta_k\colon k\in\{1,\dots,n\}\}$. If $\alpha\leq\nicefrac{1}{\norm{\lrate}}$, then
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&=\norm{\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&=\norm{
\left(
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
},
\end{align*}

\noindent
which, because of Lemma~\ref{lemma:productiscoherent}, \ref{lemma:normofcoherenttrans} and~\ref{lemma:differencenormofcoherenttrans}, implies that

\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
(I+\Delta_n\lrate)
}\\
&\leq
\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
+
\Delta_n^2\norm{\lrate}^2,
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:justthelinearpart}.

By continuing in this way (applying induction), we find that
\begin{align*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
&\leq
\Delta_1^2\norm{\lrate}^2+\cdot+\Delta_k^2\norm{\lrate}^2+\cdot
+
\Delta_n^2\norm{\lrate}^2\\
&\leq
\alpha\Delta_1\norm{\lrate}^2+\cdot+\alpha\Delta_k\norm{\lrate}^2+\cdot
+
\alpha\Delta_n\norm{\lrate}^2\\
&=
\alpha\Delta\norm{\lrate}^2
\end{align*}


\end{proof}


For any $u\in\mathcal{U}_{[t,s]}$, we now let
\begin{equation*}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate).
\end{equation*}

\begin{proposition}\label{prop:differencebetweenu}
Consider any $u,u^*\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\alpha$ and $\sigma(u^*)<\alpha$, with $0<\alpha\leq\nicefrac{1}{\norm{\lrate}}$. Let $\Delta\coloneqq t-s$. Then $\norm{\Phi_u-\Phi_{u^*}}\leq 2\alpha\Delta\norm{\lrate}^2$
\end{proposition}
\begin{proof}
Consider any $u'\in\mathcal{U}_{[t,s]}$ that is finer than $u$ and $u^*$, meaning that the timepoints it consists of contain the timepoints in $u$ and the timepoints in $u^*$. For example, let $u'$ be the ordered union of the timepoints in $u$ and $u^*$.

This implies that, for all $k\in\{1,\dots,n\}$, there is some sequence $\Delta_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta_k=\sum_{i=1}^{n_k}\Delta_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_u}\leq\alpha\Delta\norm{\lrate}^2$. 

Similarly, for all $k\in\{1,\dots,n^*\}$, there is some sequence $\Delta^*_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta^*_k=\sum_{i=1}^{n_k}\Delta^*_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^{n^*}\left(\prod_{i=1}^{n^*_k}(I+\Delta^*_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_{u^*}}\leq\alpha\Delta\norm{\lrate}^2$.

Hence, we find that
\begin{equation*}
\norm{\Phi_{u}-\Phi_{u^*}}
=
\norm{\Phi_{u}-\Phi_{u'}+\Phi_{u'}-\Phi_{u^*}}
\leq
\norm{\Phi_{u}-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u^*}}
\leq2\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{proof}

\begin{corollary}\label{corol:cauchy}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ is a \emph{cauchy sequence}, meaning that
\begin{equation*}
(\forall \epsilon>0)(\exists N\in\nats)(\forall n,m\geq N)
\norm{\Phi_{u_n}-\Phi_{u_m}}<\epsilon.
\end{equation*}
\end{corollary}
\begin{proof}
This follows almost directly from Proposition~\ref{prop:differencebetweenu}.
\end{proof}

\begin{lemma}\label{lemma:completemetricspace}
The set of all coherent lower transition operators is a complete metric space.
\end{lemma}
\begin{proof}
Since a coherent lower transition operator is just a finite vector of coherent lower previsions, this result should follow fairly easily for the (known) fact that the set of all coherent lower previsions (on a given fixed space) is a complete metric space.
\end{proof}

\begin{corollary}\label{corol:limitexistsandiscoherent}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a coherent lower transition operator.
\end{corollary}
\begin{proof}
Since $\lim_{n\to\infty}\sigma(u_n)=0$, and because of Lemma~\ref{lemma:productiscoherent}, there is some index $i$ such that the sequence $\Phi_{u_i},\Phi_{u_{i+1}},\dots,\Phi_{u_n},\dots$ consists of coherent lower transition operators. Due to Corollary~\ref{corol:cauchy}, this sequence is cauchy and therefore, because of Lemma~\ref{lemma:completemetricspace}, this sequence has a limit that is also a coherent lower transition operator. Since the limit starting from $i$ and the limit starting from $1$ are identical (initial elements do not influence the limit), we find that the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ has a limit, and that this limit is a coherent lower transition operator.
\end{proof}

Proving Theorem~\ref{theo:convergencelowerbound} is now easy. Just consider any sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$. Due to Corollary~\ref{corol:limitexistsandiscoherent}, the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a coherent lower transition operator, which we denote by $\lbound_t^s$. 

Consider now any $\epsilon>0$ and let $\Delta\coloneqq t-s$ and
\begin{equation*}
\delta\coloneqq\min\left\{\frac{\epsilon}{4\Delta\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
\end{equation*}

Since $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to $\lbound_t^s$, there is some $N\in\nats$ such that
\begin{equation*}
(\forall n\geq N)~\norm{\lbound_t^s - \Phi_{u_n}}<\frac{\epsilon}{2}.
\end{equation*}
Therefore, since $\lim_{n\to\infty}\sigma(u_n)=0$, there is some $N^*\geq N$ such that
\begin{equation*}
\sigma(u_{N^*})<\delta\text{ and }\norm{\lbound_t^s - \Phi_{u_{N^*}}}<\frac{\epsilon}{2}
\end{equation*}

Consider now any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then

\begin{equation*}
\norm{\lbound_t^s - \Phi_u}\leq\norm{\lbound_t^s-\Phi_{u_{N^*}}}
+\norm{\Phi_{u_{N^*}}-\Phi_u}
<\frac{\epsilon}{2}+2\delta\Delta\norm{\lrate}^2\leq\epsilon,
\end{equation*}
where the strict inequality follows from Proposition~\ref{prop:differencebetweenu}.
In summary, we have shown that there is some coherent lower transition operator $\lbound_t^s$ such that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon,
\end{equation*}
thereby proving Theorem~\ref{theo:convergencelowerbound}.

\newpage
\section{The Non-Markovian Case}

\subsection{Notation and some initial problems}

We first need some notation. For any $\omega\in\Omega$ and $u\in\mathcal{U}_{[0,t]}$, define the $u$-discretized path $\omega^u\coloneqq \omega(t_1),\ldots,\omega(t_n)$. Let the complete set of all such discretized paths be written $\Omega_{\mathcal{U}_{[0,t]}}$. Denote sequence concatenation as $\omega^u\oplus x\coloneqq \omega(t_1),\ldots,\omega(t_n),x$.

Now, note that in the Markovian case, the transition operator $T_t^s$ is a conditional expectation operator on a given function $f\in\gamblesX$,
\begin{align*}
T_t^s f &\equiv \mathbb{E}[f(X_s)\,\vert\,X_t] \\
 &= \sum_{y\in\states} f(y)\cdot P(X_s=y\,\vert\,X_t)\,,
\end{align*}
which clearly is then itself a function of $X_t$, so that
\begin{align*}
[T_t^s f](x) \equiv \mathbb{E}[f(X_s)\,\vert\,X_t=x]\,.
\end{align*}
In other words, as mentioned in Section 2, the operator $T_t^s$ is a map from $\gamblesX$ to $\gamblesX$.

For the non-Markovian case, we will need to introduce the notion of a \emph{history-dependent} transition operator, say $T_t^s(\omega^u)$, which we define as
\begin{align*}
T_t^s(\omega^u) f &\coloneqq \mathbb{E}[f(X_s)\,\vert\,X_t,\omega^u] \\
 &= \sum_{y\in\states} f(y)\cdot P(X_s=y\,\vert\,X_t, \omega^u) \\
 &= \sum_{y\in\states} f(y)\cdot P(X_s=y\,\vert\,X_t, X_{t_1}=\omega(t_1),\ldots,X_{t_n}=\omega(t_n))\,.
\end{align*}
Clearly, this is again a function only of $X_t$, so that $[T_t^s(\omega^u) f](x)$ is properly defined. Observe that $T_t^s(\omega^u)$ is still a map from $\gamblesX$ to $\gamblesX$.

However, if we want to factorize the operator $T_t^s(\omega^u)$, say on some time point $\tau$, where $t\leq \tau\leq s$, we will also need the notion of a \emph{variable history}. Denote with $\omega^u\otimes X_t$ the collection
\begin{equation*}
\omega^u\otimes X_t \coloneqq \{\omega^u\oplus x_t\,:\,x_t\in\states\}\,.
\end{equation*}
Define now a \emph{variable history}-dependent operator $T_\tau^s(\omega^u\otimes X_t)$ as
\begin{align*}
T_\tau^s(\omega^u\otimes X_t)f \coloneqq \mathbb{E}[f(X_s)\,\vert\,X_\tau,\omega^u,X_t]\,,
\end{align*}
which is to say that
\begin{align*}
\bigl[T_\tau^s(\omega^u\otimes X_t)f\bigr] \cong \bigl\{T_\tau^s(\omega^u\oplus x_t)f \,:\, x_t\in\states\bigr\}\,.
\end{align*}
Note that this is a function in both $X_\tau$ and $X_t$, so that $T_\tau^s(\omega^u\otimes X_t)$ is a map from $\gamblesX$ to $\gambles(\states\times\states)$. 

This is problematic. To see why, note that from the rules of expectation, we have
\begin{align*}
\mathbb{E}_{X_\tau}\bigl[ \mathbb{E}_{X_s}[f(X_s)\,\vert\,X_\tau,\omega^u,X_t] \,\vert\,X_t,\omega^u\bigr] &= \mathbb{E}_{X_\tau}\left[ \sum_{x_s\in\states} f(x_s)\cdot P(X_s=x_s\,\vert\,X_\tau,\omega^u,X_t) \,\bigg\vert\,X_t,\omega^u\right] \\
 = \sum_{x_\tau\in\states} P(X_\tau=x_\tau\,\vert\,X_t,\omega^u)&\cdot \sum_{x_s\in\states} f(x_s)\cdot P(X_s=x_s\,\vert\,X_\tau=x_\tau,\omega^u,X_t) \\
 &= \sum_{x_\tau\in\states}\sum_{x_s\in\states}f(x_s)\cdot P(X_s=x_s, X_\tau=x_\tau\,\vert\,X_t,\omega^u) \\
 &= \sum_{x_s\in\states} f(x_s)\cdot P(X_s=x_s\,\vert\,X_t,\omega^u) \\
 &= \mathbb{E}[f(X_s)\,\vert\,X_t,\omega^u]\,.
\end{align*}
What this says is that we can `split up' the computation of an expectation at time $s$ given the state at time $t$, by nesting expectations conditional on the possible states at time $\tau$. The problem now occurs when we try to do something similar in operator notation. Clearly, we would want the following to be properly defined:
\begin{equation*}
T_t^\tau(\omega^u)T_\tau^s(\omega^u\otimes X_t) f\,,
\end{equation*}
which it is not, since the leftmost operator is a map from $\gamblesX$ to $\gamblesX$, while the term $[T_\tau^s(\omega^u\otimes X_t) f]$ yields an element in $\gambles(\states\times\states)$.

One way around this is to observe that the derivation with expectation operators only worked because the notation and conventions allows one to nest them like this.

In other words, we can choose to simply go with the intuitive interpretation of the notation, and say that since
\begin{equation*}
T_\tau^s(\omega^u\otimes X_t) \equiv \mathbb{E}_{X_s}[\,\cdot\,\vert\,X_\tau,\omega^u,X_t]\,,
\end{equation*}
we will read
\begin{equation*}
T_t^\tau(\omega^u)T_\tau^s(\omega^u\otimes X_t) \coloneqq \mathbb{E}_{X_\tau}\bigl[ \mathbb{E}_{X_s}[\,\cdot\,\vert\, X_\tau,\omega^u,X_t] \,\big\vert\,X_t,\omega^u\bigr]\,.
\end{equation*}
The downside to this approach is that it is no longer clear what a given operator does, and that one can only read this from context. That is, under this interpretation, we would move away from saying that $T_t^s(\omega^u)$ is strictly a map from $\gamblesX$ to $\gamblesX$.
The upshot is that it is now relatively easy to provide a decomposition for $T_t^s(\omega^u)$.
\begin{lemma}
Consider any history dependent transition matrix $T_t^s(\omega^u)$ and sequence $u^*\in\mathcal{U}_{(t,s]}$ such that $t=\!:t_0<t_1,\ldots,t_n=s$. Then,
\begin{equation*}
T_t^s(\omega^u) = \prod_{i=1}^{n} T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j})
\end{equation*}
\end{lemma}
\begin{proof}
This is mostly directly from definition. Since
\begin{equation*}
T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j}) \equiv \mathbb{E}_{X_{t_i}}\bigl[\,\cdot\,\vert\,\omega^u\otimes_{j=0}^{i-1}X_{t_j}\bigr]\,,
\end{equation*}
we can interpret
\begin{align*}
\prod_{i=1}^{n} T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j}) &\equiv \mathbb{E}_{X_{t_1}}\bigl[\cdots\mathbb{E}_{X_{t_n}}[\,\cdot\,\vert\,\omega^u\otimes_{j=0}^{n-1}X_{t_j}]\cdots\,\vert\,\omega^u\otimes X_{t_0}\bigr] \\
 &= \mathbb{E}_{X_{t_n}}\bigl[\,\cdot\,\vert\,\omega^u\otimes X_{t_0}\bigr] \\
 &= T_t^s(\omega^u)\,.
\end{align*}
\end{proof}

Another point is that, for the purposes of this work, we are also interested in rate matrices $Q$. These are, again, maps from $\gamblesX$ to $\gamblesX$. For the non-Markovian case, however, we will also need history-dependent rate matrices $Q(\omega^u)$, as well as variable history dependent rate matrices $Q(\omega^u\otimes X)$.

We will again choose to go with the intuitive interpretation that for rate `matrices' $Q_1(\omega^u)$ and $Q_2(\omega^u\otimes X_1)$,
\begin{align*}
\bigl[Q_1(\omega^u)Q_2(\omega^u\otimes X_1)f\bigr](x) &\coloneqq Q_1(\omega^u)_{(x,\cdot)}\bigl[Q_2(\omega^u\oplus x)f\bigr] \\
 &= \sum_{y\in\states} Q_1(\omega^u)_{(x,y)}\bigl[Q_2(\omega^u\oplus x)f\bigr](y) \\
 &= \sum_{y\in\states} Q_1(\omega^u)_{(x,y)} Q_2(\omega^u\oplus x)_{y,\cdot}f \\
 &= \sum_{y\in\states} Q_1(\omega^u)_{(x,y)} \sum_{z\in\states}Q_2(\omega^u\oplus x)_{y,z}f(z)\,.
\end{align*}
In the general case, for a sequence of variable history dependent matrices $A_1(\omega^u\otimes X_0),\ldots,A_n(\omega^u\otimes_{j=1}^nX_j)$, where the $X_0$ is only for convenience so that $\omega^u\otimes X_0\equiv\omega^u$, we will interpret
\begin{align*}
\left[\left[\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]\,\cdot\,\right] \equiv \sum_{x_1}\cdots\sum_{x_{n+1}}\prod_{i=1}^{n}A_i(\omega^u\oplus_{j=0}^{i-1}x_j)_{(x_i,x_{i+1})}[\,\cdot\,]
\end{align*}
%Similarly, for rate `matrices' $Q_1(\omega^u),Q_2(\omega^u\otimes X_1)$, and $Q_3(\omega^u\otimes X_1\otimes X_2)$, we have
%\begin{align*}
%\bigl[Q_1(\omega^u)Q_2(\omega^u\otimes X_1)Q_3(\omega^u\otimes X_1\otimes X_2)f\bigr](x) &= Q_1(\omega^u)_{(x,\cdot)}\big[Q_2(\omega^u\oplus x)Q_3(\omega^u\oplus x\otimes X_2)f\big] \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\big[Q_2(\omega^u\oplus x)Q_3(\omega^u\oplus x\otimes X_2)f\big](y) \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&Q_2(\omega^u\oplus x)_{(y,\cdot)}\big[Q_3(\omega^u\oplus x\oplus y)f\big] \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\sum_{z\in\states}Q_2(\omega^u\oplus x)_{(y,z)}\big[Q_3(\omega^u\oplus x\oplus y)f\big](z) \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\sum_{z\in\states}Q_2(\omega^u\oplus x)_{(y,z)}Q_3(\omega^u\oplus x\oplus y)_{(z,\cdot)}f \\
%= \sum_{y\in\states}Q_1(\omega^u)_{(x,y)}&\sum_{z\in\states}Q_2(\omega^u\oplus x)_{(y,z)}\sum_{w\in\states}Q_3(\omega^u\oplus x\oplus y)_{(z,w)}f(w)\,.
%\end{align*}
From now on, we will drop the scare-quotes around `matrix', assuming that it is clear from context what is meant. We will also sometimes abbreviate `variable history dependent matrix' as `\emph{v.h.d. matrix}'.

We will finally need some conventions for summation of operators. For a given matrix $A$ and v.h.d. matrix $B(\omega^u\otimes X)$, we define
\begin{equation*}
\bigl[A + B(\omega^u\otimes X)\bigr] \cong \bigl\{A + B(\omega^u\oplus x)\,:\,x\in\states\bigr\}\,.
\end{equation*}
Furthermore, for two v.h.d. matrices $A(\omega^u\otimes X)$ and $B(\omega^u\otimes X)$, we will say
\begin{equation*}
\bigl[A(\omega^u\otimes X) + B(\omega^u\otimes X)\bigr] \cong \bigl\{A(\omega^u\oplus x) + B(\omega^u\oplus x)\,:\,x\in\states\bigr\}
\end{equation*}

\subsection{History-dependent norms and their properties} 

Having moved away from saying that an operator $A(\omega^u)$ is strictly a map from $\gamblesX$ to $\gamblesX$, we now run into some problems regarding the definition of its norm.

For a fixed (i.e., non-variable) history dependent operator $A(\omega^u)$, we will simply define the norm as before, that is
\begin{equation*}
\norm{A(\omega^u)} \coloneqq \sup\{\norm{A(\omega^u)f}\,:\,f\in\gamblesX, \norm{f}=1\}\,.
\end{equation*}

For variable history-dependent operators, note that they in some sense denote a collection of operators; this is already made clear by the congruence relation
\begin{equation*}
A(\omega^u\otimes X) \cong \bigl\{A(\omega^u\oplus x)\,:\,x\in\states\bigr\}\,.
\end{equation*}
It would thus appear to make sense to define their norm in the way that we previously defined the norm for sets of operators. Hence, we define
\begin{equation*}
\norm{A(\omega^u\otimes X)} \coloneqq \max\bigl\{\norm{A(\omega^u\oplus x)}\,:\,x\in\states\bigr\}\,.
\end{equation*}
We now have the following result.
\begin{proposition}
Consider two history dependent matrices $A_1(\omega^u)$ and $A_2(\omega^u\otimes X_1)$. Then,
\begin{equation*}
\norm{A_1(\omega^u)A_2(\omega^u\otimes X)} \leq \norm{A_1(\omega^u)}\cdot\norm{A_2(\omega^u\otimes X_1)}\,.
\end{equation*}
\end{proposition}
\begin{proof}
Note that $[A_1(\omega^u)A_2(\omega^u\otimes X)]$ is non-variable history dependent. Hence,
\begin{align*}
&\quad \norm{A_1(\omega^u)A_2(\omega^u\otimes X)} \\
&= \sup\bigl\{\norm{A_1(\omega^u)A_2(\omega^u\otimes X)f}\,:\,f\in\gamblesX, \norm{f}=1\bigr\} \\
 &= \sup\left\{\max\left\{\left\vert \bigl[A_1(\omega^u)A_2(\omega^u\otimes X)f\bigr](x)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert A_1(\omega^u)_{(x,\cdot)}\bigl[A_2(\omega^u\oplus x)f\bigr]\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert\sum_{y\in\states}A_1(\omega^u)_{(x,y)}\bigl[A_2(\omega^u\oplus x)f\bigr](y)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\bigl[A_2(\omega^u\oplus x)f\bigr](y)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot\left\vert\bigl[A_2(\omega^u\oplus x)f\bigr](y)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot \max\left\{\left\vert\bigl[A_2(\omega^u\oplus x)f\bigr](z)\right\vert\,:\,z\in\states\right\}\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &= \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot \norm{A_2(\omega^u\oplus x)f}\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus x)f}\cdot \sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert \,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\}\cdot \sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert \,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\}\cdot \max\left\{ \sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert \,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\}\cdot \norm{A_1(\omega^u)}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \sup\left\{\max\left\{\norm{A_2(\omega^u\oplus z)f}\,:\,z\in\states\right\} \,:\,f\in\gamblesX, \norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \max\left\{\sup\left\{\norm{A_2(\omega^u\oplus z)f} \,:\,f\in\gamblesX, \norm{f}=1\right\}\,:\,z\in\states\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \max\left\{\norm{A_2(\omega^u\oplus z)}\,:\,z\in\states\right\} \\
 &= \norm{A_1(\omega^u)}\cdot \norm{A_2(\omega^u\otimes X)}\,.
% &= \sup\left\{\max\left\{\left\vert\sum_{y\in\states}A_1(\omega^u)_{(x,y)}\sum_{z\in\states}A_2(\omega^u\oplus x)_{(y,z)}f(z)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\sum_{z\in\states}A_2(\omega^u\oplus x)_{(y,z)}f(z)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &= \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot\left\vert\sum_{z\in\states}A_2(\omega^u\oplus x)_{(y,z)}f(z)\right\vert\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
% &\leq \sup\left\{\max\left\{\sum_{y\in\states}\left\vert A_1(\omega^u)_{(x,y)}\right\vert\cdot\max\left\{\left\vert\sum_{z\in\states}A_2(\omega^u\oplus x)_{(w,z)}f(z)\right\vert\,:\,w\in\states\right\}\,:\,x\in\states\right\}\,:\,f\in\gamblesX, \norm{f}=1\right\} \\
\end{align*}
\end{proof}
We now extent this result to arbitrary sequences of operators.
\begin{proposition}
Consider a sequence of history dependent matrices $A_1(\omega^u\otimes X_0),A_2(\omega^u\otimes X_0\otimes X_1),\ldots,A_n(\omega^u\otimes_{j=0}^{n-1}X_j)$, where we have written $\omega^u\otimes X_0\equiv \omega^u$ for notational convenience. Then,
\begin{equation*}
\norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)} \leq \prod_{i=1}^{n} \norm{A_i(\omega^u\otimes_{j=0}^{i-1}X_j)}\,.
\end{equation*}
\end{proposition}
\begin{proof}
Again note that the product of the elements in the sequence is non-variable history dependent. Thus,
\begin{align*}
&\quad \norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)} \\
 &= \sup\left\{\norm{\left[\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]f}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert\left[\left[\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]f\right](x_1)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert\left[A_1(\omega^u)\left[\prod_{i=2}^n A_i(\omega^u\otimes_{j=1}^{i-1}X_j)\right]f\right](x_1)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert A_1(\omega^u)_{(x_1,\cdot)}\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right]\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{\left\vert \sum_{x_2\in\states}A_1(\omega^u)_{(x_1,x_2)}\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](x_2)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](x_2)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\cdot\left\vert\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](x_2)\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\cdot\max\left\{\left\vert\left[\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f\right](y)\right\vert\,:\,y\in\states\right\}\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \sup\left\{\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\cdot\norm{\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f}\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &\leq \sup\left\{\max\left\{\norm{\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f}\,:\,x_1\in\states\right\}\cdot\max\left\{ \sum_{x_2\in\states}\left\vert A_1(\omega^u)_{(x_1,x_2)}\right\vert\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot\sup\left\{\max\left\{\norm{\left[\prod_{i=2}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)\right]f}\,:\,x_1\in\states\right\}\,:\,f\in\gamblesX,\norm{f}=1\right\} \\
 &= \norm{A_1(\omega^u)}\cdot\norm{A_2(\omega^u\otimes X_1)\prod_{i=3}^n A_i(\omega^u\otimes_{j=1}^{i-1}X_j)}\,.
\end{align*}
The induction step is now trivial since it uses the same derivation as above. For example:
\begin{align*}
&\quad \norm{A_2(\omega^u\otimes X_1)\prod_{i=3}^n A_i(\omega^u\otimes_{j=1}^{i-1}X_j)} \\
 &= \max\left\{ \norm{A_2(\omega^u\oplus x_1)\prod_{i=3}^n A_i(\omega^u\oplus x_1\otimes_{j=2}^{i-1}X_j)} \,:\,x_1\in\states\right\} \\
 &\leq \max\left\{ \norm{A_2(\omega^u\oplus x_1)}\cdot\max\left\{\norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)}\,:\,x_2\in\states\right\} \,:\,x_1\in\states\right\} \\
 &\leq \max\left\{ \norm{A_2(\omega^u\otimes X_1)}\cdot\max\left\{\norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)}\,:\,x_2\in\states\right\} \,:\,x_1\in\states\right\} \\
 &= \norm{A_2(\omega^u\otimes X_1)}\cdot \max\left\{ \max\left\{\norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)}\,:\,x_2\in\states\right\} \,:\,x_1\in\states\right\} \\
 &= \norm{A_2(\omega^u\otimes X_1)}\cdot \max\left\{ \norm{A_3(\omega^u\oplus x_1\oplus x_2)\prod_{i=4}^n A_i(\omega^u\oplus x_1\oplus x_2\otimes_{j=3}^{i-1}X_j)} \,:\,x_1,x_2\in\states\right\}\,.
\end{align*}
It should be clear that repeatedly applying this step yields the proposition.
\end{proof}

\begin{lemma}
Consider a v.h.d. matrix $C(\omega^u\otimes_{j=0}^n X_j)$ and two sequences of v.h.d. matrices $A_1(\omega^u\otimes X_0),\ldots, A_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ and $B_1(\omega^u\otimes X_0),\ldots, B_n(\omega^u\otimes_{j=0}^{n-1}X_j)$. Then,
\begin{align*}
&\quad \norm{\left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]C(\omega^u\otimes_{j=0}^n X_j)} \\
 &\quad\quad\quad\leq \norm{\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)}\norm{C(\omega^u\otimes_{j=0}^n X_j)}\,.
\end{align*}
\end{lemma}
\begin{proof}
Note that the result is a non-v.h.d. matrix. Hence, expanding the definition of the norm:
\begin{align*}
&\quad \norm{\left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]C(\omega^u\otimes_{j=0}^n X_j)} \\
&= \max\left\{ \sum_{x_{n+2}\in\states}\left\vert \left[\left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]C(\omega^u\otimes_{j=0}^n X_j)\right]_{(x_1,x_{n+2})}\right\vert \,:\,x_1\in\states\right\} \\
&= \max\left\{\sum_{x_{n+2}}\left\vert \sum_{x_2}\cdots\sum_{x_{n+1}}\left(\prod_{i=1}^{n} A_i(\omega^u\oplus_{j=0}^{i-1})_{(x_i,x_{i+1})} - \prod_{i=1}^{n} B_i(\omega^u\oplus_{j=0}^{i-1})_{(x_i,x_{i+1})}\right)C(\omega^u\oplus_{j=0}^nx_j)_{(x_{n+1},x_{n+2})}\right\vert\,:\,x_1\in\states\right\}
\end{align*}
{\bf TODO} Finish this. But, to be fair, I'm honestly not sure if this lemma even holds...
%For brevity, define the map $D$ as
%\begin{equation*}
%D \coloneqq \left[\prod_{i=1}^nA(\omega^u\otimes_{j=0}^{i-1} X_j) - \prod_{i=1}^nB(\omega^u\otimes_{j=0}^{i-1} X_j)\right]\,.
%\end{equation*}
%We then again start by expanding the definition of the norm:
%\begin{align*}
%&\quad \norm{ DC(\omega^u\otimes_{j=0}^n X_j)} \\
% &= \sup\left\{ \norm{\bigl[DC(\omega^u\otimes_{j=0}^n X_j)\bigr]f} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{ \left\vert\left[\bigl[DC(\omega^u\otimes_{j=0}^n X_j)\bigr]f\right](x)\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{ \left\vert
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\left[C(\omega^u\oplus x\oplus_{j=2}^nx_j)f\right](x_{n+1})
%\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &\leq \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\left[C(\omega^u\oplus x\oplus_{j=2}^nx_j)f\right](x_{n+1})
%\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert\cdot\left\vert\left[C(\omega^u\oplus x\oplus_{j=2}^nx_j)f\right](x_{n+1})
%\right\vert\,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &\leq \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert\cdot\norm{C(\omega^u\oplus x\oplus_{j=2}^nx_j)f} \,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &\leq \sup\left\{ \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert\cdot\max\left\{\norm{C(\omega^u\oplus x'\oplus_{j=2}^nx_j')f}\,:\,x',x_2',\ldots,x_n'\in\states\right\} \,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \sup\left\{ \max\left\{\norm{C(\omega^u\oplus x'\oplus_{j=2}^nx_j')f}\,:\,x',x_2',\ldots,x_n'\in\states\right\}\cdot\max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert \,:\,x\in\states\right\} \,:\,f\in\gamblesX,\norm{f}=1\right\} \\
% &= \max\left\{ 
%\sum_{x_2\in\states}\cdots\sum_{x_n\in\states}\sum_{x_{n+1}\in\states} \left\vert D(x\oplus_{j=2}^{n-1}x_j)_{(x_n,x_{n+1})}\right\vert \,:\,x\in\states\right\}\cdot\norm{C(\omega^u\otimes_{j=0}^nX_j)}
%\end{align*}
\end{proof}

\subsection{Stochastic History-Dependent Matrices}

We now look at some properties of stochastic v.h.d. matrices. We will say that a v.h.d. matrix $A(\omega^u\otimes X)$ is a stochastic v.h.d. matrix iff $A(\omega^u\oplus x)$ is stochastic for all $x\in\states$.

\begin{lemma}
Consider a bounded set of rate matrices $\mathcal{Q}$ and a v.h.d. rate matrix $Q(\omega^u\otimes X)$ such that $Q(\omega^u\oplus x)\in\mathcal{Q}$ for all $x\in\states$. Then, for all $\Delta>0$ such that $\Delta\norm{\mathcal{Q}}<1$, we have that $\bigl(I + \Delta Q(\omega^u\otimes X)\bigr)$ is a stochastic v.h.d. matrix.
\end{lemma}
\begin{proof}
Simply note that the lemma holds for any $Q\in\mathcal{Q}$, and that
\begin{equation*}
\bigl(I+\Delta Q(\omega^u\otimes X)\bigr)\cong \left\{\bigl(I+\Delta Q(\omega^u\oplus x)\bigr)\,:\,x\in\states\right\}\,.
\end{equation*}
\end{proof}

\begin{lemma}
Consider two sequences of stochastic variable-history dependent matrices $A_1(\omega^u\otimes X_0),\ldots, A_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ and $B_1(\omega^u\otimes X_0),\ldots, B_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ such that for all $i\in\{1,\ldots,n\}$ and all sequences $(x_0\equiv\emptyset),x_1,\ldots,x_{n-1}$, it holds that
\begin{equation*}
\norm{A_i(\omega^u\oplus_{j=0}^{i-1}x_j) - B_i(\omega^u\oplus_{j=0}^{i-1}x_j)}<c\,.
\end{equation*}
Then,
\begin{equation*}
\norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^n B_i(\omega^u\otimes_{j=0}^{i-1}X_j)} < nc\,.
\end{equation*}
\end{lemma}
\begin{proof}
We use induction on $n$. For $n=1$, the result trivially holds. Now, assume that it holds for $k=n-1$. We show that it also holds for $n$:
\begin{align*}
&\quad \norm{\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^n B_i(\omega^u\otimes_{j=0}^{i-1}X_j)} \\
 &= \left\lVert\prod_{i=1}^n A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)B_n(\omega^u\otimes_{j=0}^{n-1}X_j) \right. \\
 &\quad\quad\quad \left.+ \prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)B_n(\omega^u\otimes_{j=0}^{n-1}X_j) - \prod_{i=1}^n B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right\lVert \\
 &= \left\lVert\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigl[A_n(\omega^u\otimes_{j=0}^{n-1}X_j) - B_n(\omega^u\otimes_{j=0}^{n-1}X_j)\bigr] \right. \\
 &\quad\quad\quad \left.+ \left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)\right\lVert \\
 &\leq \norm{\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigl[A_n(\omega^u\otimes_{j=0}^{n-1}X_j) - B_n(\omega^u\otimes_{j=0}^{n-1}X_j)\bigr] } \\
 &\quad\quad\quad + \norm{\left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} \\
 &\leq \left(\prod_{i=1}^{n-1} \norm{A_i(\omega^u\otimes_{j=0}^{i-1}X_j)}\right)\norm{A_n(\omega^u\otimes_{j=0}^{n-1}X_j) - B_n(\omega^u\otimes_{j=0}^{n-1}X_j) } \\
 &\quad\quad\quad + \norm{\left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} \\
 &\leq 1\cdot c + \norm{\left[\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)\right]B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} 
\end{align*}
{\bf TODO} The last step requires us to solve Lemma 26. Assuming that holds, we can continue as
\begin{align*}
&\leq c + \norm{\prod_{i=1}^{n-1} A_i(\omega^u\otimes_{j=0}^{i-1}X_j) - \prod_{i=1}^{n-1} B_i(\omega^u\otimes_{j=0}^{i-1}X_j)}\norm{B_n(\omega^u\otimes_{j=0}^{n-1}X_j)} \\
&\leq c + (n-1)c\cdot 1 \\
&= nc\,. 
\end{align*}
\end{proof}

We finally need the following result.
\begin{lemma}
Consider any bounded set of rate matrices $\mathcal{Q}$ with lower transition rate operator $\lrate$ and any sequence $Q_1(\omega^u\otimes X_0),Q_2(\omega^u\otimes X_0\otimes X_1),\ldots,Q_n(\omega^u\otimes_{j=0}^{n-1}X_j)$ such that for all $i\in\{1,\ldots,n\}$ and all sequences $x_1,\ldots,x_n$, it holds that $Q_i(\omega^u\oplus_{j=0}^{i-1}x_j)\in\mathcal{Q}$.
Then, for any $\Delta>0$ such that $\Delta\norm{\mathcal{Q}}<1$ and all $f\in\gamblesX$ it holds that
\begin{equation*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) \geq \bigl((I+\Delta\lrate)^n f\bigr)(x)\,.
\end{equation*}
\end{lemma}
\begin{proof}
First, note that for all $i\in\{1,\ldots,n\}$, the terms $\bigl(I+\Delta Q_i(\omega^u\otimes_{j=0}^{i-1})\bigr)$ are stochastic variable history dependent matrices. Hence,
\begin{align*}
\bigl(I+\Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr) &\equiv \mathbb{E}_{X_{i+1}}\left[\,\cdot\,\vert\,X_i,\omega^u,\otimes_{j=0}^{i-1}X_j\right] \\
 &= \sum_{x_{i+1}\in\states} [\cdot](x_{i+1}) P(X_{i+1}=x_{i+1}\,\vert\,X_i,\omega^u,\otimes_{j=0}^{i-1}X_j)\,.
\end{align*}
We therefore have
\begin{align*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) = \mathbb{E}_{X_2}&\left[\cdots\mathbb{E}_{X_{n+1}}\left[f(X_{n+1})\,\vert\,X_n,\omega^u,\otimes_{j=2}^{n-1}X_j,X_1=x\right]\cdots\,\vert\,X_1=x,\omega^u\right] \\
 = \sum_{x_2\in\states}P(X_2=x_2\,\vert\,X_1=x,\omega^u)\sum_{x_3\in\states}\cdots&\sum_{x_{n+1}\in\states} f(x_{n+1})\cdot P(X_{n+1}=x_{n+1}\,\vert\,X_n=x_n,\omega^u,\oplus_{j=2}^{n-1}x_j, X_1=x) \\
 = \sum_{x_2\in\states}\cdots\sum_{x_{n}\in\states} P(X_2=x_2,\ldots,X_{n}=x_n\,\vert\,X_1=x,\omega^u)&\sum_{x_{n+1}\in\states} f(x_{n+1})\cdot P(X_{n+1}=x_{n+1}\,\vert\,X_n=x_n,\omega^u,\oplus_{j=2}^{n-1}x_j, X_1=x)\,.
\end{align*}
Introduce a function $g(\cdot)$ as
\begin{equation*}
g(x,x_2,\ldots,x_n) \coloneqq \sum_{x_{n+1}\in\states} f(x_{n+1})\cdot P(X_{n+1}=x_{n+1}\,\vert\,X_n=x_n,\omega^u,\oplus_{j=2}^{n-1}x_j, X_1=x)\,.
\end{equation*}
Note that also
\begin{align*}
g(x,x_2,\ldots,x_n) \equiv \left[\bigl(I + \Delta Q_n(\omega^u\oplus (X_1=x)\oplus_{j=2}^{n-1}x_j)\bigr)f\right](x_n)\,,
\end{align*}
and since $Q_n(\omega^u\oplus (X_1=x)\oplus_{j=2}^{n-1}x_j)\in\mathcal{Q}$, we have
\begin{equation*}
g(x,x_2,\ldots,x_n) \geq \bigl[(I + \Delta\lrate)f\bigr](x_n)\,.
\end{equation*}
Define a function $g'(\cdot)$ as
\begin{equation*}
g'(x_n) \coloneqq \bigl[(I + \Delta\lrate)f\bigr](x_n).
\end{equation*}

Now, we have that since
\begin{align*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) &= \sum_{x_2\in\states}\cdots\sum_{x_{n}\in\states} P(X_2=x_2,\ldots,X_{n}=x_n\,\vert\,X_1=x,\omega^u)\cdot g(x,x_2,\ldots,x_n)
\end{align*}
is a convex combination of values $g(x,x_2,\ldots,x_n)$ over all sequences $x_2,\ldots,x_n$, that
\begin{align*}
\left[\left[\prod_{i=1}^n \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]f\right](x) &\geq \sum_{x_2\in\states}\cdots\sum_{x_{n}\in\states} P(X_2=x_2,\ldots,X_{n}=x_n\,\vert\,X_1=x,\omega^u)\cdot g'(x_n) \\
 &= \left[\left[\prod_{i=1}^{n-1} \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]g'\right](x) \\
 &= \left[\left[\prod_{i=1}^{n-1} \bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_j)\bigr)\right]\bigl[(I + \Delta\lrate)f\bigr]\right](x)\,.
\end{align*}
Repeatedly applying this argument then yields the lemma.
\end{proof}

\subsection{Imprecise Non-Markovian Stochastic Processes}

We are now finally ready to show our main result for imprecise non-Markovian stochastic processes.

For any bounded set of rate matrices $\mathcal{Q}$, we consider the set $\mathbb{P}_\mathcal{Q}$ of all $P\in\mathbb{P}$ such that
\begin{equation*}
(\forall\epsilon>0)(\exists\delta>0)(\forall t\in[0,+\infty))(\forall\Delta\in(0,\delta))(\forall\omega^u\in\Omega_{\mathcal{U}_{[0,t]}})(\exists Q\in\mathcal{Q})\norm{\frac{T_t^{t+\Delta}(\omega^u)-I}{\Delta}-Q}<\epsilon\,.
\end{equation*}
The following result now holds.
\begin{theorem}
Consider any $t,s\in[0,+\infty)$ such that $t<s$ and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then for any $P\in\mathbb{P}_\mathcal{Q}$, $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$ and $f\in\gamblesX$:
\begin{equation*}
\bigl[L_t^sf\bigr](x) \leq \bigl[T_t^s(\omega^u)f\bigr](x)\,.
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\mathbb{P}_\mathcal{Q}$ and any $\omega^u\in\Omega_{\mathcal{U}_{[0,t]}}$, $f\in\gamblesX$, and $x\in\states$. Assume \emph{ex absurdo} that $\bigl[L_t^sf\bigr](x) > \bigl[T_t^s(\omega^u)f\bigr](x)$. We prove that this leads to a contradiction. Let $C\coloneqq s-t$ and choose $\epsilon>0$ small enough such that
\begin{equation}
\epsilon(1+C\norm{f}) < \bigl[L_t^s\bigr](x) - \bigl[T_t^s(\omega^u)f\bigr](x)\,.
\end{equation}
Since $P\in\mathbb{P}_\mathcal{Q}$, we have that there is some $\delta>0$ such that
\begin{equation}
(\forall \tau\in[0,+\infty))(\forall\Delta\in(0,\delta))(\forall\omega^u\in\Omega_{\mathcal{U}_{[0,\tau]}})(\exists Q\in\mathcal{Q})\norm{\frac{T_\tau^{\tau+\Delta}(\omega^u)-I}{\Delta}-Q}<\epsilon\,.
\end{equation}
Furthermore, because of Theorem 5, there is some $\delta'>0$ such that
\begin{equation}
(\forall u\in\mathcal{U}_{t,s}\,:\,\sigma(u)<\delta')\left\vert \bigl[L_t^sf\bigr](x) - \left[\left[\prod_{k=1}^n\bigl(I+\Delta_k\lrate\bigr)\right]f\right](x)\right\vert < \epsilon\,.
\end{equation}
Now choose $n>\max\{\nicefrac{C}{\delta},\nicefrac{C}{\delta'},C\norm{\mathcal{Q}}\}$. Then for $\Delta\coloneqq\nicefrac{C}{n}$, we find that $\Delta<\delta$, $\Delta<\delta'$, and $\Delta\norm{\mathcal{Q}}<1$.

For all $k\in\{0,1,\ldots,n\}$, define $t_k\coloneqq t+k\Delta$. Since $\Delta<\delta$, it follows from Equation 11 that for all sequences $(x_{t_0}\equiv\emptyset),x_{t_1},\ldots,x_{t_n}$ and all $i\in\{1,\ldots,n\}$, there is some $Q_i(\omega^u\oplus_{j=0}^{i-1}x_{t_j})\in\mathcal{Q}$ such that
\begin{align*}
&\quad \norm{T_{t_{i-1}}^{t_i}(\omega^u\oplus_{j=0}^{i-1}x_{t_j}) - \bigl(I+\Delta Q_i(\omega^u\oplus_{j=0}^{i-1}x_{t_j})\bigr)} \\
 &= \norm{\frac{T_{t_{i-1}}^{t_i}(\omega^u\oplus_{j=0}^{i-1}x_{t_j}) - I}{\Delta} - Q_i(\omega^u\oplus_{j=0}^{i-1}x_{t_j})}\Delta \\
 &< \epsilon\Delta\,.
\end{align*}
Furthermore, since $\Delta\norm{\mathcal{Q}}<1$, we have by Lemma 27 that $\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr)$ is a variable history dependent stochastic matrix for all $i\in\{1,\ldots,n\}$. Therefore, we find by Lemmas 23 and 28 that
\begin{align*}
&\quad \norm{T_t^s(\omega^u) - \prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr)} \\
 &= \norm{\prod_{i=1}^n T_{t_{i-1}}^{t_i}(\omega^u\otimes_{j=0}^{i-1}X_{t_j}) - \prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr)} \\
 &\leq \epsilon\Delta n = \epsilon C\,.
\end{align*}
This implies that
\begin{align*}
&\quad\left\vert \bigl[T_t^s(\omega^u)f\bigr](x) - \left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) \right\vert \\
 &\leq \norm{T_t^s(\omega^u)f - \left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f} \\
 &\leq \norm{T_t^s(\omega^u) - \left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]}\norm{f} \\
 &\leq \epsilon C\norm{f}\,,
\end{align*}
and hence,
\begin{equation}
\bigl[T_t^s(\omega^u)f\bigr](x) \geq \left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) - \epsilon C\norm{f}\,.
\end{equation}
Furthermore, since $\Delta<\delta'$, it follows from Equation 12 that
\begin{equation*}
\left\vert \bigl[L_t^sf\bigr](x) - \left[\bigl(I + \Delta\lrate\bigr)^n f\right](x)\right\vert < \epsilon\,,
\end{equation*}
and hence
\begin{equation}
\left[\bigl(I + \Delta\lrate\bigr)^n f\right](x) > \bigl[L_t^sf\bigr](x) - \epsilon\,.
\end{equation}
Finally, by Lemma 29, we have
\begin{equation}
\left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) \geq \left[\bigl(I + \Delta\lrate\bigr)^n f\right](x)\,.
\end{equation}
Combining equations 13, 14, and 15, yields
\begin{align*}
\bigl[T_t^s(\omega^u)f\bigr](x) &\geq \left[\left[\prod_{i=1}^n\bigl(I + \Delta Q_i(\omega^u\otimes_{j=0}^{i-1}X_{t_j})\bigr) \right]f\right](x) - \epsilon C\norm{f} \\
 &\geq \left[\bigl(I + \Delta\lrate\bigr)^n f\right](x) - \epsilon C\norm{f} > \bigl[L_t^sf\bigr](x) - \epsilon - \epsilon C\norm{f}\,,
\end{align*}
whence
\begin{equation*}
\epsilon(1+C\norm{f})> \bigl[L_t^sf\bigr](x) - \bigl[T_t^s(\omega^u)f\bigr](x)\,.
\end{equation*}
This provides the required contradiction; see Equation 10.
\end{proof}

\bibliographystyle{plain} 
%\bibliography{general}


\end{document}
