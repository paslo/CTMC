\documentclass[10pt]{paper}
%\documentclass[a4paper,reqno]{amsart}
\usepackage[british]{babel}
%\usepackage[garamond]{mathdesign}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{courier}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{nicefrac}
\usepackage{bm}
%\usepackage{pdfsync}
%\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{exmp}{Example}%[section]
 
\renewcommand{\ttdefault}{cmtt}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}

% - macros

\newcommand{\nats}{\mathbb{N}}
\newcommand{\natswith}{\nats_{0}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\realspos}{\reals_{>0}}
\newcommand{\realsnonneg}{\reals_{\geq 0}}

\newcommand{\states}{\mathcal{X}}

\newcommand{\paths}{\Omega}
\newcommand{\path}{\omega}

\newcommand{\power}{\mathcal{P}(\paths)}
\newcommand{\nonemptypower}{\power_{\emptyset}}
\newcommand{\events}{\mathcal{E}}
%\newcommand{\nonemptyevents}{\events^{\emptyset}}
\newcommand{\filter}[1][t]{\mathcal{F}_{#1}}
\newcommand{\eventst}[1][t]{\events_{#1}}

\newcommand{\processes}{\mathbb{P}}
\newcommand{\mprocesses}{\processes^{\mathrm{M}}}

\newcommand{\hmprocesses}{\processes^{\mathrm{HM}}}

\newcommand{\wprocesses}{\processes^{\mathrm{W}}}
\newcommand{\wmprocesses}{\processes^{\mathrm{WM}}}

\newcommand{\whmprocesses}{\processes^{\mathrm{WHM}}}


\newcommand{\lt}{\underline{T}}
\newcommand{\lbound}{L}

\newcommand{\gambles}{\mathcal{L}}
\newcommand{\gamblesX}{\gambles(\states)} 

\newcommand{\ind}[1]{\mathbb{I}_{#1}}

\newcommand{\rateset}{\mathcal{Q}}
\newcommand{\lrate}{\underline{Q}}

\newcommand{\asa}{\Leftrightarrow}
\newcommand{\then}{\Rightarrow}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

\newcommand{\coloneqq}{:\!=}

\newcommand{\opinset}{\,\,\widetilde{\in}\,\,}

\newcommand{\argmin}{\arg\min}

\newcommand{\exampleend}{\hfill$\Diamond$}

\title{Imprecise Continuous-Time Markov chains}

%\author[1]{Thomas E. Krak\thanks{t.e.krak@uu.nl}}
%\author[2]{Jasper de Bock\thanks{jasper.debock@ugent.be}}
%\affil[1]{Universiteit Utrecht}
%\affil[2]{Ghent University}

\author{Thomas Krak \and Jasper de Bock}

\begin{document}

%\author{{\bf Thomas E. Krak} \\ Utrecht}
%\address{Utrecht University}
%\curraddr{}
%\email{t.e.krak@uu.nl}
%\thanks{}

%\author{{\bf Jasper de Bock} \\ Ghent}
%\address{Ghent University}

%\author{
	%{\bf Thomas E. Krak} \quad\quad {\bf Jasper de Bock} \\
%	Utrecht University \quad Ghent University \\
	%Department of Information and Computing Sciences \\
	%Princetonplein 5, De Uithof \\
	%3584 CC Utrecht \\
	%The Netherlands \\
%	\texttt{\quad\quad t.e.krak@uu.nl} \quad\quad \texttt{jasper.debock@ugent.be}
%\and
	%{\bf Jasper de Bock} \\
%	Ghent University \\
	%SYSTeMS Research Group \\
	%Technologiepark -- Zwijnaarde 914 \\
	%9052 Zwijnaarde \\ 
	%Belgium \\
%	\texttt{jasper.debock@ugent.be}
%}
\date{}
\maketitle

\begin{abstract}
Lorem ipsum.
\end{abstract}

\section{Introduction}\label{sec:introduction}

This paper is organized as follows. In Section~\ref{sec:prelim} we introduce notation and basic definitions used throughout this work. Section~\ref{sec:lower_operator} contains our definition for the lower transition operator, along with existence proofs and the connection to previous work from the literature. 

In Sections~\ref{sec:imp_markov} and~\ref{sec:imp_non_markov} we show that this lower transition operator correctly gives the lower envelope of expectation functionals with respect to sets of Markov and non-Markov processes, respectively. In Section~\ref{sec:decomp}, we extend these results to expectation functionals on multiple time points, and show that certain decomposition properties hold when the lower envelope is taken with respect to sets of non-Markov models. 

Section~\ref{sec:tractability} contains some notes on tractability aspects of  computing such lower expectations, and we describe different function classes with varying degrees of tractability. We finally close with some conclusions and an outlook to further work in Section~\ref{sec:conclusions}.

\section{Preliminaries}\label{sec:prelim}

Consider some finite \emph{state space} $\states=\{1,\dots,m\}$.

*** introduce state space, notation for naturals, reals, ... ***

\subsection{Stochastic matrices and rate matrices}
\begin{definition}[Rate Matrix]\label{def:rate_matrix}
A real-valued $m\times m$ matrix $Q$ is said to be a \emph{rate matrix} if

\vspace{5pt}
\begin{enumerate}[label=R\arabic*:]
\item
$\sum_{y\in\states}Q(x,y)=0$ for all $x\in\states$;
\item
$Q(x,y)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\noindent
We use $\mathcal{R}$ to denote the set of all rate matrices. 
\end{definition}

Clearly, $\mathcal{R}$ is closed under finite sums and multiplication with non-negative scalars. 

\begin{definition}[Stochastic Matrix]\label{def:stoch_matrix}
A real-valued $m\times m$ matrix $A$ is said to be \emph{stochastic} if
\vspace{5pt}
\begin{enumerate}[label=S\arabic*:]
\item
$\sum_{y\in\states}A(x,y)=1$ for all $x\in\states$;
\item
$A(x,y)\geq0$ for all $x,y\in\states$.
\end{enumerate}
\vspace{5pt}
\noindent
\end{definition}

\begin{proposition}\label{prop:stochastic_from_rate_matrix}
Consider any $m\times m$ rate matrix $Q\in\mathcal{R}$, and any $0\leq \Delta \leq \nicefrac{1}{\norm{Q}}$. Let $I$ denote the $m\times m$ identity matrix. Then, the matrix $[I+\Delta Q]$ is stochastic.
\end{proposition}
\begin{proof}
Let $A=[I+\Delta Q]$. We will verify the properties from Definition~\ref{def:stoch_matrix}.

We start with property S1. Consider any $x\in\states$. Then
\begin{equation*}
\sum_{y\in\states} A(x,y) = \sum_{y\in\states} [I + \Delta Q](x,y) = \sum_{y\in\states}I(x,y) + \Delta \sum_{y\in\states}Q(x,y) = 1\,,
\end{equation*}
where we used property R1 from Definition~\ref{def:rate_matrix}.

For property S2, note that $0\leq \Delta \leq \nicefrac{1}{\norm{Q}}$. Whence, for all $x\in\states$, we have $-1\leq \Delta Q(x,x) \leq 0$, so that $[I+\Delta Q](x,x) \geq 0$. Furthermore, for all $x,y\in\states$ s.t. $x\neq y$, we have $0\leq \Delta Q(x,y) \leq 1$, so that $[I+\Delta Q](x,y)\geq 0$.
\end{proof}

\begin{proposition}\label{prop:rate_from_stochastic_matrix}
Consider any stochastic $m\times m$ matrix $A$, and any $\Delta\in\realspos$. Let $I$ denote the $m\times m$ identity matrix. Then, $[\nicefrac{1}{\Delta}\cdot(A-I)]$ is a rate matrix.
\end{proposition}
\begin{proof}
This follows from a similar argument as the proof of Proposition~\ref{prop:stochastic_from_rate_matrix}; simply verify the properties from Definition~\ref{def:rate_matrix}.
\end{proof}

***

Obviously, a transition matrix $T_t^s$ can also be regarded as a map from $\gamblesX$ to $\gamblesX$, defined for all $f\in\gamblesX$ by $T_t^s(f)\coloneqq T_t^sf$. Note that for all $x_t\in\states$, we have that
\begin{align*}
\left[T_t^sf\right](x_t) &= \sum_{x_s\in\states}f(x_s)P(X_s=x_s\,\vert\,X_t=x_t)
= \mathbb{E}_{X_s}\left[f(X_s)\,\vert\,X_t=x_t\right]\,.
\end{align*}

\subsection{Functions, Operators, and Norms}\label{sec:func_oper_norm}
Let $\gamblesX$ denote the set of all real-valued functions on $\states$. Because $\states$ is finite, we can also interpret any function $f\in\gamblesX$ as a vector in $\reals^m$. Hence, we will in the sequel use the terms `function' and `vector' interchangeably when referring to elements of $\gamblesX$.

For any vector $f\in\gamblesX$, we let
\begin{equation*}
\norm{f}\coloneqq\norm{f}_{\infty}\coloneqq\max\{\abs {f(x)}\colon x\in\states\}
\end{equation*}
be the maximum norm. For any operator $A$ from $\gamblesX$ to $\gamblesX$ that is non-negatively homogeneous, meaning that
\begin{equation*}
A(\lambda f)=\lambda A(f)\text{ for all $f\in\gamblesX$ and all $\lambda\geq0$,}
\end{equation*}
we consider the induced operator norm
\begin{equation*}
\norm{A}\coloneqq\sup\left\{\norm{Af}\colon f\in\gamblesX,\norm{f}=1\right\}.
\end{equation*}
If $A$ is an $m\times m$ matrix, we have that
\begin{equation*}
\norm{A}
=
\max\left\{\sum_{y\in\states}\abs{A(x,y)}\colon x\in\states\right\}.
\end{equation*}

\noindent
These norms satisfy the following properties. 

\begin{proposition}
For all $f,g\in\gamblesX$, all $A,B$ from $\gamblesX$ to $\gamblesX$ that are non-negatively homogeneous, all $\lambda\in\reals$ and all $x\in\states$, we have that
\vspace{5pt}

\begin{enumerate}[label=N\arabic*:,ref=N\arabic*]
\item
$\abs{f(x)}\leq\norm{f}$
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\norm{A}\geq0$
\item
$\norm{A}=0\asa A=0$
\item
$\norm{A+B}\leq\norm{A}+\norm{B}$
\item
$\norm{AB}\leq\norm{A}\norm{B}$
\item\label{N:homogeneous}
$\norm{\lambda A}=\abs{\lambda}\norm{A}$
\item\label{N:normAf}
$\norm{Af}\leq\norm{A}\norm{f}$
\item
$\norm{A}=1$ if $A$ is a stochastic matrix.
\end{enumerate}
\vspace{5pt}
\end{proposition}

\noindent
Finally, for any set $\mathcal{A}$ of square matrices, we define

\begin{equation*}
\norm{\mathcal{A}}\coloneqq\sup\{\norm{A}\colon A\in\mathcal{A}\}.
\end{equation*}

\subsubsection{wat notatie voor meerdere variabelen}\label{sec:multivar_notation}

*** dit kan opzich wel korter denk ik, maar kunnen we bij oppoetsen straks nog wel even checken ***

We will also find it convenient to have notation for functions defined on the state space at multiple time points.
To this end, consider any $t,s\in\realsnonneg$ such that $t<s$, and any $u\in\mathcal{U}_{[t,s]}$. We will now define
\begin{equation*}
\states^u\coloneqq \prod_{i=0}^n\states
\end{equation*}
to be the joint state space at times $t_0,\ldots,t_n$. Let $\gambles(\states^u)$ denote the set of functions on $(X_{t_0},\ldots,X_{t_n})$.

For any $t,t',s\in\realsnonneg$ such that $t<t'<s$ and any $u\in\mathcal{U}_{[t,t']}$, let $\states^{\{s\}}$ denote the state space at time $s$, and let
\begin{equation*}
\states^{u\cup\{s\}} \coloneqq \states^u\times\states^{\{s\}}
\end{equation*}
denote the joint state space at times $t_0,\ldots,t_n,s$. Similarly, for any $t,t',s,s'\in\realsnonneg$ such that $t<t'<s<s'$, any $u\in\mathcal{U}_{[t,t']}$ and $v\in\mathcal{U}_{[s,s']}$, let
\begin{equation*}
\states^{u\cup v}\coloneqq\states^u\times\states^v\,.
\end{equation*}
Let the sets of functions $\gambles(\states^{u\cup\{s\}})$ and $\gambles(\states^{u\cup v})$ be defined in the obvious manner.

For any function $f\in\gambles(\states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}})$, let the norm $\norm{f}$ be defined as
\begin{equation*}
\norm{f} \coloneqq \max\left\{ \abs{f(x_{t_0},\ldots,x_{t_n})}\,:\,(x_{t_0},\ldots,x_{t_n})\in \states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}}\right\}\,.
\end{equation*}

For the sake of brevity, we will also write a joint state assignment as
\begin{equation*}
\left(X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right)\equiv (X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})\,.
\end{equation*}

For a function $f\in\gambles(\states^{u\cup\{s\}})$, we will write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ specific to the joint state assignment $(x_{t_0},\ldots,x_{t_n})$. Thus, $f(x_{t_0},\ldots,x_{t_n},X_s)$ corresponds to a $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$.

Finally, we will want to use operators as defined in Section~\ref{sec:func_oper_norm} for functions defined on (multiple) explicit time points. It is clear that an operator $A$ defined on $\gamblesX$ can be applied to any element of a set of functions $\gambles(\states^{\{s\}})$ that depend on the state at just a single point in time. It remains to determine how to cope with functions defined on multiple time points. 

To avoid introducing overly complex notation, we will stipulate the following convention. If $f\in\gambles(\states^u)$ is a function defined on an ordered sequence $u$ of time points $t_0<t_1<\cdots<t_n$, and if $A$ is a non-negatively homogeneous operator from $\gamblesX$ to $\gamblesX$, we allow $A$ to be applied to $f$ by applying it to the restriction of $f$ to the state $\states^{\{t_n\}}$ at the \emph{latest} time point $t_n$ in $u$. Because such a restriction is dependent on the specific state assignment $(x_{t_0},\ldots,x_{t_{n-1}})$, the result is a function $[Af]\in\gambles(\states^{\{t_0,\ldots,t_{n-1}\}})$. In other words, we then have
\begin{equation*}
\left[Af\right](x_{t_0},\ldots,x_{t_{n-1}}) \equiv \left[A f(x_{t_0},\ldots,x_{t_{n-1}},X_{t_n})\right](x_{t_{n-1}})\,.
\end{equation*}

% \subsection{Outer derivatives}

% \begin{definition}[Continuity]
% A time-dependent stochastic matrix $A_t$ is said to be continuous in $t^*$ if each of its components is, or equivalently, if
% \begin{equation*}
% \lim_{t\to t^*}\norm{A_{t^*}-A_t}=0.
% \end{equation*}
% \end{definition}

% \begin{definition}
% Let $A_t$ be a time-dependent stochastic matrix. The right outer derivative of $A_t$ in $t_*$ is then 
% \end{definition}


\section{Continuous-Time Stochastic Processes}

*** add nice introtext here ***

\subsection{Full conditional probabilities}

A cadlag function from $\realsnonneg$ to $\states$ is called a \emph{path}, where cadlag means that it is right continuous for all $t\in\realsnonneg$ and that the left limit exists for all $t\in\realspos$. Let $\paths$ be the set of all paths. For any path $\path\in\paths$ and any time point $t\in\realsnonneg$, the value of $\path$ in $t$ is denoted by $\path(t)$.

A subset $E$ of $\paths$ is called an \emph{event}. The set of all events is denoted by $\power$ and we let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. For any $t\in\realsnonneg$ and $x\in\states$, we define the elementary event
\begin{equation*}
(X_t=x)\coloneqq\{\path\in\paths\colon\path(t)=x\}.
\end{equation*}
%We use $\events^{\mathrm{f}}$ to denote the set of all these elementary events. 
Similarly, for any finite sequence of time points $0\leq t_1<\dots<t_n$ and any $x_{i}\in\states$, $i\in\{1,\dots,n\}$, we define the event
\begin{align*}
\left(X_{t_1}=x_{1}, \dots, X_{t_n}=x_{n}\right)
\coloneqq%&
%\bigcap_{i\in\{0,\dots,n\}}(X_{t_i}=x_{t_i})\\
%=&
\left\{\path\in\paths\colon(\forall i\in\{1,\dots,n\})~\path(t_i)=x_{i}\right\}.%,
\end{align*}
%which we will sometimes also denote by $(X_{t_i}=x_{t_i}, i\in\{0,\dots,n\})$.
%We use $\events^{\mathrm{s}}$ to denote the set of all these events.

Consider now any $t\in\realsnonneg$. We then let $\mathcal{A}_{>t}$ be the algebra that is generated by all the elementary events $X_s=x$, for $s> t$ and $x\in\states$,\footnote{This is the smallest subset of $\power$ that contains all these elementary events and that is furthermore closed under complements, finite unions and hence also finite intersections.} and we let $\mathcal{F}_{\leq t}$ be the set of all events $\left(X_{t_1}=x_{1}, \dots, X_{t_n}=x_{n}\right)$, with $0\leq t_1\leq\dots\leq t_n\leq t$ and $x_1$.%, and we define $\filter\coloneqq\mathcal{C}_{\leq t}\setminus\{\emptyset\}$.

\begin{definition}[Full conditional probability]\label{def:cond_prob}
A full conditional probability $P$ is a real-valued map from $\power\times\nonemptypower$ to $\reals$ that satisfies the following axioms. For all $A,B\in\power$ and all \mbox{$C,D\in\nonemptypower$}:
\vspace{5pt}

\begin{enumerate}[label=F\arabic*:]
\item
$P(C\vert C)=1$;
\item
$0\leq P(A\vert C)\leq 1$;
\item
$P(A\cup B\vert C)=P(A\vert C)+P(B\vert C)$ if $A\cap B=\emptyset$;
\item
$P(A\vert C)=P(A\vert D)P(D\vert C)$ if $A\subseteq D\subseteq C$.
\end{enumerate}
\vspace{5pt}

\noindent
For any $A\in\power$ and $C\in\nonemptypower$, we call $P(A\vert C)$ the probability of $A$ conditional on $C$. Also, for any $A\in\power$, we use the shorthand notation $P(A)\coloneqq P(A\vert\paths)$ and then call $P(A)$ the probability of $A$.
\end{definition}

\begin{definition}[Coherent conditional probability]\label{def:coherence}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is said to be a coherent conditional probability if, for all $n\in\mathbb{N}$ and every choice of $(A_i,C_i)\in\mathcal{C}$ and $\lambda_i\in\reals$, $i\in\{1,\dots,n\}$,
\begin{equation*}
\sup\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(P(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation*}
with $C_0\coloneqq\cup_{i=1}^nC_i$.
\end{definition}

\begin{theorem}\label{theo:coherentextendable}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it can be extended to a full conditional probability.
\end{theorem}
\begin{proof}
*** REFERENCE ***
\end{proof}

\begin{corollary}
Let $P$ be a real-valued map from $\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it is a full conditional probability.
\end{corollary}
\begin{proof}
Trivial consequence of Theorem~\ref{theo:coherentextendable}.
\end{proof}

\subsection{Finitely additive stochastic processes}


\begin{definition}[Stochastic Process]\label{def:stoch_process}
A \emph{stochastic process} is the restriction of a full conditional probability to
\begin{equation*}
\mathcal{C}^\mathrm{SP}\coloneqq\big\{
(A,C)
\colon
A\in\mathcal{A}_{> t},~C\in\filter,~t\in\realsnonneg\big\}\subset\power\times\nonemptypower.
\end{equation*}
We denote the set of all such stochastic processes by $\processes$.
\end{definition}

\begin{corollary}
Let $P$ be a real-valued map from $\mathcal{C}^\mathrm{SP}$ to $\reals$. Then $P$ is a stochastic process if and only if it is a coherent conditional probability.
\end{corollary}
\begin{proof}
Trivial consequence of Theorem~\ref{theo:coherentextendable}.
\end{proof}

*** explain that $\sigma$-additivity is not required ***

*** discuss that a unique $\sigma$-additive extension always exists, but that for our purposes, it is not necessary to consider it ***

\subsection{Well-behaved stochastic processes}

*** explain that stochastic process can behave in rather extreme ways. For example, the transition probabilities can jump instantaneously. In order to avoid this behavior, we restrict our attention to a subclass of stochastic processes, which we call well-behaved. ***

\begin{definition}[Well-behaved stochastic process]
\label{def:well-behaved}
A stochastic process $P\in\processes$ is said to be well-behaved if, for any time sequence $0\leq t_0<\dots<t_{n}<t$ and any set of states $x_{0},\dots,x_{n},x,y\in\states$:
\begin{equation*}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\abs{P(X_{t+\Delta}=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})-\delta_{xy}}<+\infty
\end{equation*}
and
\begin{equation*}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\abs{P(X_{t}=y\vert X_{t-\Delta}=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})-\delta_{xy}}<+\infty
\end{equation*}
The set of all well-behaved Markov processes is denoted by $\wprocesses$.
\end{definition}

***

\begin{definition}[Transition matrix]\label{def:trans_matrix}
Consider a stochastic process $P\in\processes$. Then, for any $0\leq t\leq s$, the \emph{transition matrix} $T_t^s$ is a $m\times m$ matrix that is defined by
\begin{equation*}
T_t^s(x_t, x_s) \coloneqq P(X_s=x_s\,\vert X_t=x_t)\quad\text{for all $x_s,x_t\in\states$}\,.
\end{equation*}
We denote this family of matrices by $\mathcal{T}_P$.%, and call it the \emph{system of transition matrices} that corresponds to $P$.
\end{definition}

\begin{proposition}\label{prop:stochasticprocess:simpleproperties}
Let $P\in\processes$ be a stochastic process. Then for any $0\leq t\leq s$, $T_t^s$ is stochastic and $T_t^t=I$ and, if $P$ is well-behaved, then also
\begin{equation}\label{eq:wellbehavedtransitionmatrix}
\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_t^{t+\Delta}-I}<+\infty
\text{~~~~and~~~~}
\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta}^t-I}<+\infty.
\end{equation}
\end{proposition}
\begin{proof}
*** trivial ***
\end{proof}

***


\begin{equation*}
T^s_{t,\,x_u}(x,y)
\coloneqq
P(X_s=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})
\end{equation*}


\begin{equation*}
\partial_{+}{T_{t,\,x_u}^t}
\coloneqq
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-T^t_{t,\,x_u})
=
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)
\end{equation*}

\begin{equation*}
\partial_{-}{T_{t,\,x_u}^t}
\coloneqq
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-T^t_{t,\,x_u})
=
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-I)
\end{equation*}

If these partial derivatives exist, then because of Proposition~\ref{prop:rate_from_stochastic_matrix}, they are guaranteed to belong to $\mathcal{R}$. If they both exists and coincide, we write $\partial{T_{t,\,x_u}^t}$ to denote their common value.



If these partial derivatives do not exist, then more generally, we can consider the following outer partial derivatives:

\begin{equation}
\label{eq:rightouterderivative}
\overline{\partial}_{+}
{T^t_{t,\,x_u}}
\coloneqq
\left\{
Q\in\mathcal{R}
\colon
(\exists \Delta_i\to0^+,\,i\in\nats)
~
%\lim_{i\to+\infty}\Delta_i=0
%\text{~~and~}
\lim_{i\to+\infty}
\frac{1}{\Delta_i}
(T^{t+\Delta_i}_{t,\,x_u}-I)
=Q
\right\}
\end{equation}

\begin{equation*}
\overline{\partial}_{-}
{T^t_{t,\,x_u}}
\coloneqq
\left\{
Q\in\mathcal{R}
\colon
(\exists \Delta_i\to0^+,\,i\in\nats)
~
%\lim_{i\to+\infty}\Delta_i=0
%\text{~~and~}
\lim_{i\to+\infty}
\frac{1}{\Delta_i}
(T^{t}_{t-\Delta_i,\,x_u}-I)
=Q
\right\}
\end{equation*}
and let
\begin{equation*}
\overline{\partial}
{T^t_{t,\,x_u}}
\coloneqq
\overline{\partial}_{+}
{T^t_{t,\,x_u}}
\cup
\overline{\partial}_{-}
{T^t_{t,\,x_u}}
\end{equation*}

\begin{proposition}\label{prop:boundednon-emptyandclosed}
For any $P\in\wprocesses$, $\overline{\partial}_{+}
{T^t_{t,\,x_u}}$, $\overline{\partial}_{-}
{T^t_{t,\,x_u}}$ and $\overline{\partial}
{T^t_{t,\,x_u}}$ are bounded, non-empty and closed subsets of $\mathcal{R}$.
\end{proposition}
\begin{proof}
We only give the proof for $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. The proof for $\smash{\overline{\partial}_{-}
{T^t_{t,\,x_u}}}$ is completely analogous. The proof for $\smash{\overline{\partial}
{T^t_{t,\,x_u}}}$ then follows trivially because a union of two bounded, non-empty and closed sets is always bounded, non-empty and closed itself.

We start by establishing the boundedness of $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Since $P$ is well-behaved, it follows from Definition~\ref{def:well-behaved} that there is some $B>0$ and $\delta>0$ such that
\begin{equation}\label{eq:boundedbyB}
(\forall 0<\Delta<\delta)
~
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)}\leq B.
\end{equation}
Consider now any $Q\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Because of Equation~\eqref{eq:rightouterderivative}, $Q$ is the limit of a sequence of matrices $Q_k$, $k\in\nats$, defined by
\begin{equation}\label{eq:sequenceofQsinproof}
Q_k\coloneqq\frac{1}{\Delta_k}
(T^{t+\Delta_k}_{t,\,x_u}-I)
\text{~~for all $k\in\nats$}.
\end{equation}
Because of Equation~\eqref{eq:boundedbyB}, the norms $\norm{Q_k}$ of these matrices are eventually (for large enough $k$) bounded above by $B$. Hence, it follows that $\norm{Q}\leq B$. Since this is true for any $Q\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, we find that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is bounded.


In order to prove that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is non-empty, we consider any sequence $\Delta_k\to0^+$, $k\in\nats$. The corresponding sequence of matrices $Q_k$, $k\in\nats$, as defined by Equation~\eqref{eq:sequenceofQsinproof}, is then bounded because $P$ is well-behaved---see Definition~\ref{def:well-behaved}---and therefore, it follows from the Bolzano-Weierstrass theorem that it has a convergent subsequence $Q_{k_i}$, $i\in\nats$, of which we denote the limit by $Q^*$. Hence, we have found a sequence $\Delta_{k_i}\to0^+$, $i\in\nats$, such that $Q_{k_i}\to Q^*$.
Since we know from Lemma~\ref{prop:rate_from_stochastic_matrix} that each of the matrices $Q_{k_i}$, $i\in\nats$, is a rate matrix, the limit $Q^*$ is also a rate matrix, which therefore clearly belongs to $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$.

We end by showing that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is closed, or equivalently, that for any converging sequence $Q^*_k$, $k\in\nats$, of rate matrices in $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, the limit point $Q^*\coloneqq\lim_{k\to+\infty}Q^*_k$ is again an element of $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Since the rate matrices $Q^*_k$ belong to the bounded set $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, their limit $Q^*$ is a (real-valued) rate matrix. For any $k\in\nats$, it now follows from Definition~\eqref{eq:rightouterderivative} that there is some $0<\Delta_k<\nicefrac{1}{k}$ such that $\norm{Q_k-Q^*_k}\leq\nicefrac{1}{k}$, with $Q_k$ defined as in Equation~\eqref{eq:sequenceofQsinproof}.
Since
\begin{equation*}
0\leq\lim_{k\to+\infty}\norm{Q^*-Q_k}\leq\lim_{k\to+\infty}\norm{Q^*-Q^*_k}+\lim_{k\to+\infty}\norm{Q^*_k-Q_k}=0,
\end{equation*}
we find that the sequence $Q_k$, $k\in\nats$, converges to $Q^*$: $\lim_{k\to+\infty}Q_k=Q^*$. Hence, because $\lim_{k\to+\infty}\Delta_k=0$, we conclude that $Q^*\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$.
\end{proof}


\begin{proposition}\label{prop:outerderivativebehaveslikelimit}
For all $\epsilon>0$, there is some $\delta>0$ such that, for all $0<\Delta<\delta$:
\begin{equation}
\label{eq:outerderivativebehaveslikelimit1}
(\exists Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}<\epsilon
\end{equation}
and
\begin{equation}
\label{eq:outerderivativebehaveslikelimit2}
(\exists Q\in\overline{\partial}_{-}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-I)-Q}<\epsilon
\end{equation}
\end{proposition}
\begin{proof}
Fix any $\epsilon>0$.
Assume \emph{ex absurdo} that
\begin{equation*}
(\forall\delta>0)(\exists0<\Delta<\delta)
(\forall Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}\geq\epsilon.
\end{equation*}
Clearly, this implies the existence of a sequence $\Delta_k\to0^+$, $k\in\nats$, such that
\begin{equation}\label{eq:boundednonelement}
\norm{Q_k-Q}\geq\epsilon
\text{~~for all $k\in\nats$ and all $Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}}$},
\end{equation}
with $Q_k$ defined as in Equation~\eqref{eq:sequenceofQsinproof}. As we know from the proof of Proposition~\ref{prop:boundednon-emptyandclosed}, the sequence $Q_k$, $k\in\nats$, has a convergent subsequence $Q_{k_i}$, $i\in\nats$, of which the limit $Q^*$ belongs to $\overline{\partial}_{+}
{T^t_{t,\,x_u}}$. Hence, since Equation~\eqref{eq:boundednonelement} implies that $\norm{Q^*-Q}\geq\epsilon$ for all $Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}}$, we find that $0=\norm{Q^*-Q^*}\geq\epsilon$. From this contradiction, it follows that there must be some $\delta_1>0$ such that Equation~\eqref{eq:outerderivativebehaveslikelimit1} holds for all $0<\Delta<\delta_1$. Similarly, using a completely analogous argument, we infer that there must be some $\delta_2>0$ such that Equation~\eqref{eq:outerderivativebehaveslikelimit2} holds for all $0<\Delta<\delta_1$. Now let $\delta\coloneqq\min\{\delta_1,\delta_2\}$.
\end{proof}

\begin{corollary}\label{corol:outersingleton}
Consider any $P\in\wprocesses$. Then $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is a singleton if and only if $\smash{\partial_{+}
{T^t_{t,\,x_u}}}$ exists and, in that case, $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}=\{\partial_{+}
{T^t_{t,\,x_u}}\}$. Analogous results hold for $\smash{\overline{\partial}_{-}
{T^t_{t,\,x_u}}}$ and $\smash{\partial_{-}
{T^t_{t,\,x_u}}}$, and for $\smash{\overline{\partial}
{T^t_{t,\,x_u}}}$ and $\smash{\partial
{T^t_{t,\,x_u}}}$.
\end{corollary}
\begin{proof}
This follows trivially from Proposition~\ref{prop:outerderivativebehaveslikelimit}.
\end{proof}


%In order to re-use the lower transition operator $L_t^s$ from Definition~\ref{def:low_trans}, we will finally have to introduce some conventions. Note that $L_t^s$ is a map from $\gamblesX$ to $\gamblesX$, so it is straightforward to apply to functions $g\in\gambles(\states^{\{s\}})$. This is because these functions only depend on a single variable. However, $L_t^s$ is not yet properly defined for, e.g., functions $f\in\gambles(\states^{u\cup\{s\}})$. To this end, let first 
%\begin{equation*}
%f(x_{t_0},\ldots,x_{t_n},X_{s})\in\gambles(\states^{\{s\}})
%\end{equation*}
%denote the restriction of any $f\in\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{s\}})$, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$. Similarly, for all $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$, let
%\begin{equation*} f(x_{t_0},\ldots,x_{t_{n-1}},X_{t_n},X_s)\in\gambles(\states^{\{t_n,s\}})\,,
%\end{equation*}
%and so forth.

\section{Continuous-Time Markov Chains}

*** tralala ***



%\begin{proposition}\label{prop:transitionmatrixhasprocess}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and any stochastic $m\times m$ matrix $A$. Then, there is at least one stochastic process $P\in\mprocesses$ that satisfies the Markov property, and that has a corresponding transition matrix $T_t^s$ such that
%\begin{equation*}
%T_t^s = A\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%This is immediate from Definition~\ref{def:trans_matrix} and the fact that $\mprocesses$ contains all Markov processes, according to Definition~\ref{def:markov_property}.\newline
%{\bf ATTN:} Not sure how to give an in-depth argument for this, but I don't see how it could be false.
%\end{proof}



% \begin{lemma}\label{lemma:transitionmatrixfactorises}
% Consider any $P\in\processes$. Then
% \begin{equation*}
% T_t^s=\prod_{k=1}^n T_{t_{k-1}}^{t_k} \coloneqq T_{t_0}^{t_1}T_{t_1}^{t_2}\cdots T_{t_{n-1}}^{t_n}
% \end{equation*}
% for every sequence $0\leq t=t_0<t_1<t_2,\dots,t_{n}=s$.
% \end{lemma}
% \begin{proof}
% This property is well known and therefore stated without proof. {\bf TODO:} Ref.
% \end{proof}



%\begin{proposition}\label{prop:transitionmatrix_factorized_has_process}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, any sequence of time points $u\in\mathcal{U}_{[t,s]}$, and any sequence of stochastic matrices $A_1,\ldots,A_n$. Then, there is at least one stochastic process $P\in\mprocesses$ such that, for all $i\in\{1,\ldots,n\}$, $P$ has corresponding transition matrix $T_{t_{i-1}}^{t_i}$ on the interval $[t_{i-1},t_i]$, and
%\begin{equation*}
%T_{t_{i-1}}^{t_i} = A_i\,.
%\end{equation*}
%Furthermore, $P$ has a corresponding transition matrix $T_t^s$ on the interval $[t,s]$ such that
%\begin{equation*}
%T_t^s = \prod_{i=1}^n A_i\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%Should be true. Final statement is trivial assuming the first is true. First statement should be true, given that $\mprocesses$ contains \emph{all} Markov processes, and a finite sequence of well-defined stochastic matrices should be non-degenerate.
%
%{\bf ATTN:} Does need a proof though.
%\end{proof}

%\begin{lemma}\label{lemma:linear_factorization_has_process}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, any bounded set of rate matrices $\mathcal{Q}\subset\mathcal{R}$, any sequence of time points $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\nicefrac{1}{\norm{\mathcal{Q}}}$, and any sequence of rate matrices $Q_1,\ldots,Q_n$ such that $Q_i\in\mathcal{Q}$ for all $i\in\{1,\ldots,n\}$. 
%
%Consider the induced sequence of matrices $[I + \Delta_1 Q_1],\ldots,[I+\Delta_n Q_n]$. Then,
%\begin{enumerate}
%\item The matrix $[I+\Delta_i Q_i]$ is stochastic for all $i\in\{1,\ldots,n\}$.
%\item There is at least one stochastic process $P\in\mprocesses$ such that $T_{t_{i-1}}^{t_i}=[I+\Delta_i Q_i]$ for all $i\in\{1,\ldots,n\}$, and $T_t^s = \prod_{i=1}^n[I+\Delta_i Q_i]$.
%\end{enumerate}
%\end{lemma}
%\begin{proof}
%The first claim is immediate from Proposition~\ref{prop:stochastic_from_rate_matrix}. The second claim is immediate from Proposition~\ref{prop:transitionmatrix_factorized_has_process}.
%\end{proof}
%
%\begin{lemma}\label{lemma:exponential_matrix_has_process}
%Consider any rate matrix $Q$, any $t,s\in\realsnonneg$ such that $t<s$, and any sequence of sequences of time points $u_1,u_2,\ldots,u_n,\ldots$ such that $u_i\in\mathcal{U}_{[t,s]}$ and $\lim_{n\rightarrow\infty}\sigma(u_n)= 0$. For each $u_i$, define the stochastic matrix
%\begin{equation*}
%\Phi_i \coloneqq \prod_{k=1}^{n_i} [I+\Delta^i_k Q]\,.
%\end{equation*}
%Then, the corresponding sequence $\Phi_1,\Phi_2,\ldots,\Phi_n,\ldots$ converges, with solution
%\begin{equation*}
%\lim_{n\rightarrow\infty}\Phi_n = \exp\{(s-t)Q\}\,,
%\end{equation*}
%where the right hand side denotes the matrix exponential.
%
%Furthermore, there is at least one stochastic process $P\in\mprocesses$ such that, for all $\tau,\tau'\in[t,s]$ such that $\tau<\tau'$, this process has corresponding transition matrix
%\begin{equation*}
%T_\tau^{\tau'} = \exp\{(\tau'-\tau)Q\}\,.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%The claim of convergence is well-known. The last claim is then immediate from Lemma~\ref{lemma:linear_factorization_has_process}.
%\end{proof}
%
%\begin{corollary}\label{corollary:exponential_process_approximates_everywhere_and_has_derivative}
%Consider any rate matrix $Q$ and any $t,s\in\realsnonneg$ such that $t<s$. Then, there is at least one stochastic process $P\in\mprocesses$ such that, for all $\tau\in[t,s]$, this process has corresponding transition matrix $T_t^\tau$, and
%\begin{equation*}
%(\forall \epsilon\in\realspos)(\exists \delta\in\realspos)(\forall u\in\mathcal{U}_{[t,\tau]}\,:\,\sigma(u)<\delta) \norm{T_t^\tau - \prod_{k=1}^n[I+\Delta_i Q]} < \epsilon\,.
%\end{equation*}
%Furthermore, this process' transition matrix satisfies
%\begin{equation*}
%(\forall \epsilon>0)(\exists \delta>0)(\forall \Delta\in(0,\delta))(\forall \tau\in[t,s))\norm{\frac{T_\tau^{\tau+\Delta} - I}{\Delta} - Q} < \epsilon\,.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%This is immediate from Lemma~\ref{lemma:exponential_matrix_has_process}.
%\end{proof}

\begin{definition}[Markov chain, Markov property]\label{def:markov_property}
A stochastic process $P\in\processes$ satisfies the \emph{Markov property} if, for any time sequence $0\leq t_0<\dots<t_{n}<t<s$ and any set of states $x_{0},\dots,x_{n},x,y\in\states$:
\begin{equation*}
P(X_s=y\vert X_{t}=x)=P(X_s=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n}).
\end{equation*}
A stochastic process that satisfies this property is called a \emph{Markov chain}. We denote the set of all Markov chains by $\mprocesses$ and use $\wmprocesses$ to refer to the subset that only contains the well-behaved Markov chains.
\end{definition}

\subsection{Transition matrix systems}

We know from Proposition~\ref{prop:stochasticprocess:simpleproperties} that the transition matrices of a stochastic process---and therefore also, in particular, of a Markov chain---satisfy a number of simple properties. For the specific case of a Markov chain $P\in\mprocesses$, the family of transition matrices $\mathcal{T}_P$ also satisfies and additional property---see Equation~\eqref{eq:transmatrixproduct}. Whenever this is the case, we will call such a family of stochastic matrices a transition matrix system.

\begin{definition}[Transition matrix system]
A \emph{transition matrix system} $\mathcal{T}$ is a family of stochastic matrices $T_t^s$, defined for all $0\leq t\leq s$, such that
\begin{equation}\label{eq:transmatrixproduct}
T_t^s=T_t^r T_r^s
\text{~~for all $0\leq t\leq r\leq s$}
\end{equation}
and $T_t^t=I$ for all $t\geq0$. A transition matrix system that also satisfies Equation~\eqref{eq:wellbehavedtransitionmatrix} is called a \emph{well-behaved transition matrix system}.
\end{definition}

\begin{proposition}\label{prop:Markovhassystem}
Consider a Markov chain $P\in\mprocesses$ and let $\mathcal{T}_P$ be the corresponding family of transition matrices. Then $\mathcal{T}_P$ is a transition matrix system and, if $P$ is well behaved, then $\mathcal{T}_P$ is also well-behaved.
\end{proposition}
\begin{proof}
*** fairly easy to prove ***
\end{proof}

At this point, we already know that every (well-behaved) Markov chain has a corresponding (well-behaved) transition matrix system. Our next result establishes that the converse is true as well: every (well-behaved) transition matrix system has a corresponding (well-behaved) Markov chain, and for a given initial distribution, this Markov chain is even unique.

\begin{theorem}\label{theo:uniqueMarkovchain}*** only to be touched by Jasper *** 
 Let $p$ be an arbitrary mass function on $\states$ and let $\mathcal{T}$ be a transition matrix system. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}$ and $P(X_0)=p(X_0)$ and, if $\mathcal{T}$ is well-behaved, then $P$ is also well-behaved.
\end{theorem}
\begin{proof}
*** Not trivial; I have a coherence-based proof that I'll add here someday ***
\end{proof}

Hence, Markov chains---and well-behaved Markov chains in particular---are completely characterised by their transition matrices and their initial distribution. 
We now focus on a number of special cases.

\subsection{Homogeneous Markov chains}

\begin{definition}[Homogeneous Markov chain]\label{def:homogeneousMarkov}
A Markov chain $P\in\mprocesses$ is called \emph{homogeneous} if its transition matrices $T_t^s$ do not depend on the absolute value of $t$ and $s$, but only on the time-difference $s-t$:
\begin{equation}\label{eq:homogeneousMarkov}
T_t^s=T_{t+\Delta}^{s+\Delta}
\text{~~for all $0\leq t\leq s$ and $\Delta\geq0$.}
\end{equation}
We denote the set of all homogeneous Markov chains by $\hmprocesses$ and use $\whmprocesses$ to refer to the subset that consists of the well-behaved homogeneous Markov chains.
\end{definition}


\begin{definition}\label{def:systemfromQ}For any rate matrix $Q\in\mathcal{R}$, we use $\mathcal{T}_Q$ to denote the family of stochastic matrices that is defined by
\begin{equation*}
T_t^s=e^{Q(s-t)}
\text{~~for all $0\leq t\leq s$.}
\end{equation*}
\end{definition}

\begin{proposition}
\label{prop:systemQ}
For any $Q\in\mathcal{R}$, $\mathcal{T}_Q$ is a well-behaved transition matrix system.
\end{proposition}
\begin{proof}
*** trivial ***
\end{proof}

\begin{corollary}
 Consider any rate matrix $Q\in\mathcal{R}$ and let $p$ be an arbitrary mass function on $\states$. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$ and, furthermore, this unique Markov chain is well-behaved and homogeneous.
\end{corollary}
\begin{proof}
Since we know from Proposition~\ref{prop:systemQ} that $\mathcal{T}_Q$ is a well-behaved transition matrix system, it follows from Theorem~\ref{theo:uniqueMarkovchain} that there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$, and that this Markov chain is furthermore well-behaved. Since it---trivially---follows from Definition~\ref{def:systemfromQ} that $\mathcal{T}_Q$ satisfies Equation~\eqref{eq:homogeneousMarkov}, Definition~\ref{def:homogeneousMarkov} implies that $P$ is homogeneous.
\end{proof}



%As our next result shows, the converse is true as well: every well-behaved transition matrix system of a well-behaved homogeneous Markov chain can be uniquely characterised by a single transition rate matrix.

\begin{theorem}\label{theo:homogeneoushasQ}
For any well-behaved homogeneous Markov chain $P\in\whmprocesses$, there is a unique rate matrix $Q\in\mathcal{R}$ such that $\mathcal{T}_P=\mathcal{T}_Q$.
\end{theorem}
\begin{proof}
Because of Proposition~\ref{prop:boundednon-emptyandclosed}, we know that $\overline{\partial}_{+}
{T^0_{0}}$ is a non-empty bounded set of rate matrices, which implies that there is some real $B>0$ such that $\norm{Q'}\leq B$ for all $Q'\in\overline{\partial}_{+}
{T^0_{0}}$. Let $Q$ be any of element of $\overline{\partial}_{+}
{T^0_{0}}$.


Fix any $c\geq0$, $\epsilon>0$ and $\delta>0$. 
It then follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and~\ref{N:homogeneous} that there is some $\delta^*>0$ such that
\begin{equation}
\label{eq:homogeneoushasQ1}
(\forall 0<\Delta^*<\delta^*)
~
(\exists Q^*\in\overline{\partial}_{+}
{T^0_{0}})
~
\norm{T_0^{\Delta^*}-(I+\Delta^*Q^*)}<\Delta^*\epsilon.
\end{equation}
Furthermore, because of Equation~\eqref{eq:rightouterderivative} and~\ref{N:homogeneous}, there is some $0<\Delta<\min\{\delta,\delta^*\}$ such that
\begin{equation}
\label{eq:homogeneoushasQ2}
\norm{T^{\Delta}_{0}-(I+\Delta Q)}<\Delta\epsilon.
\end{equation}
If we now define $n\coloneqq\lfloor\nicefrac{c}{\Delta}\rfloor$ and $d\coloneqq c-n\Delta$, then $n\Delta\leq c<(n+1)\Delta$ and therefore also $0\leq d<\Delta$. Because of Proposition~\ref{prop:Markovhassystem}, Equation~\eqref{eq:transmatrixproduct} and Definition~\ref{def:homogeneousMarkov}, we know that
\begin{equation*}
T_0^c=\left(
\prod_{j=1}^{n}
T_{(j-1)\Delta}^{j\Delta}
\right)
T_{n\Delta}^c
=\left(T_0^{\Delta}\right)^{n}
T_0^{d}
\end{equation*}
and therefore, it follows from *** SOME LEMMA WITH A GENERAL VERSION OF THE RECURSIVELY PROVEN NORM INEQUALITY WE USE A LOT IN THIS PAPER *** that
\begin{equation}
\label{eq:homogeneoushasQ3}
\norm{
	e^{Qc}-T_0^c
}
=
\norm{
\left(T_0^{\Delta}\right)^{n}
T_0^{d}
-
\left(
e^{Q\Delta}
\right)^{n}
e^{Qd}
}
\leq
n\norm{T_0^{\Delta}-e^{Q\Delta}}
+\norm{T_0^{d}-e^{Qd}}.
\end{equation}
From Equation~\eqref{eq:homogeneoushasQ2} and Lemma~\ref{lemma:linearpartofexponential}, we infer that
\begin{equation}
\label{eq:homogeneoushasQ4}
\norm{T_0^{\Delta}-e^{Q\Delta}}
\leq
\norm{T_0^{\Delta}-(I+\Delta Q)}
+
\norm{(I+\Delta Q)-e^{Q\Delta}}
\leq
\Delta\epsilon
+
\Delta^2\norm{Q}^2.
\end{equation}
Since $d<\Delta<\delta^*$, we infer from Equation~\eqref{eq:homogeneoushasQ1} that there is some $Q^*\in\overline{\partial}_{+}
{T^0_{0}}$ such that $\norm{T_0^{d}-(I+d Q^*)}<d\epsilon$. Hence, also using Lemma~\ref{lemma:linearpartofexponential}, we find that
\begin{align}
\norm{T_0^{d}-e^{Qd}}
&\leq
\norm{T_0^{d}-(I+d Q^*)}
+
\norm{(I+d Q^*)-(I+d Q)}
+
\norm{(I+d Q)-e^{Qd}}\notag\\
&\leq
d\epsilon+d\norm{Q^*-Q}
+d^2\norm{Q}^2
\leq
d\epsilon+d\norm{Q^*}
+d\norm{Q}
+d^2\norm{Q}^2.\label{eq:homogeneoushasQ5}
\end{align}
By combining Equations~\eqref{eq:homogeneoushasQ3}, \eqref{eq:homogeneoushasQ4} and~\eqref{eq:homogeneoushasQ5}, it follows that
\begin{equation*}
\norm{
	e^{Qc}-T_0^c
}
\leq
n\Delta\epsilon
+
n\Delta^2\norm{Q}^2
+
d\epsilon
+d\norm{Q^*}
+d\norm{Q}
+d^2\norm{Q}^2.
\end{equation*}
Taking into account that $\norm{Q}\leq B$, $\norm{Q^*}\leq B$, $n\Delta\leq c$ and $d<\Delta<\delta$, this implies that
\begin{equation*}
\norm{
	e^{Qc}-T_0^c
}
\leq
c\epsilon
+
c\delta B^2
+
\delta\epsilon
+2\delta B
+\delta^2 B^2.
\end{equation*}
Since this is true for any $\epsilon>0$ and $\delta>0$, it follows that $\norm{e^{Qc}-T_0^c}\leq0$, which implies that $T_0^c=e^{Qc}$. Since this is true for all $c\geq0$, it follows from Definition~\ref{def:homogeneousMarkov} that
\begin{equation}\label{eq:homogeneoushasQ6}
T_t^s=T_0^{s-t}=e^{Q(s-t)}
\text{~~for all $0\leq t\leq s$,}
\end{equation}
or equivalently, that $\mathcal{T}_P=\mathcal{T}_Q$.

Finally, we prove that $Q$ is unique. Assume \emph{ex absurdo} that this is not the case, or equivalently, that there are rate matrices $Q_1$ and $Q_2$, with $Q_1\neq Q_2$, such that $\mathcal{T}_P=\mathcal{T}_{Q_1}$ and $\mathcal{T}_P=\mathcal{T}_{Q_2}$. It then follows from Definition~\ref{def:systemfromQ} that $\partial_{+}{T^0_{0}}=Q_1$ and $\partial_{+}{T^0_{0}}=Q_2$, which implies that $Q_1=Q_2$. From this contradiction, it follows $Q$ is indeed unique.
\end{proof}

\begin{lemma}\label{lemma:linearpartofexponential}
Consider any $Q\in\mathcal{R}$ and any $\Delta\geq0$. Then
\begin{equation*}
\norm{e^{Q\Delta}-(I+\Delta Q)}\leq
\Delta^2\norm{Q}^2.
\end{equation*}
\end{lemma}
\begin{proof}
*** this follows by combining Lemma~\ref{lemma:justthelinearpart} with the limit expression for the exponential ***
\end{proof}

Hence, any well-behaved homogenous Markov chain $P\in\whmprocesses$ is completely characterised by its initial distribution and a rate matrix $Q\in\mathcal{R}$. We will denote this rate matrix by $Q_P$.


\subsection{Non-homogeneous Markov chains}

*** I would explicitely discuss the special cases of continous $Q_t$ and piecewice constant $Q_t$ here, as examples, and refer to some other papers for more general cases, since we don't need them.

% \begin{definition}[Markov Process]\label{def:markov_process}
% A stochastic process $P\in\mprocesses$ is said to be a \emph{continuous-time Markov process} if the transition matrix corresponding to $P$ satisfies
% \begin{equation*}
% (\forall \epsilon\in\realspos)(\exists \delta\in\realspos)(\forall \Delta\in(0,\delta))(\forall t\in\realsnonneg)(\exists Q\in\mathcal{R}) \norm{\frac{T_t^{t+\Delta} - I}{\Delta} - Q} < \epsilon\,.
% \end{equation*}
% \end{definition}

% \begin{definition}[Characterizing Rate Matrix]\label{def:markov_process_char_matrix}
% Consider any continuous-time Markov process $P$, and let $Q_t$ be a function that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$. We say that $Q_t$ \emph{characterizes} $P$ if it satisfies
% \begin{equation*}
% (\forall \epsilon\in\realspos)(\exists \delta\in\realspos)(\forall \Delta\in(0,\delta))(\forall t\in\realsnonneg)\norm{\frac{T_t^{t+\Delta} - I}{\Delta} - Q_t} < \epsilon\,.
% \end{equation*}
% \end{definition}

\begin{theorem}\label{theorem:continuous_rate_matrix_has_process}
For any continuous map $Q_t$ from $\reals_{\geq0}$ to $\mathcal{R}$, there is a unique transition matrix system $\mathcal{T}$ such that
\begin{equation*}
test
\end{equation*}
\end{theorem}
\begin{proof}
*** I guess its best to provide a reference here, although I think it should also be possible to prove this ourselves. ***
\end{proof}

\begin{theorem}\label{theorem:continuous_rate_matrix_has_process}
Consider any right-continuous function $Q_t$ that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$, where by right-continuity we mean that for all $t\in\realsnonneg$,
\begin{equation*}
\lim_{s\downarrow t} Q_s = Q_t\,.
\end{equation*}
Then, there exists a stochastic process $P\in\mprocesses$ such that $Q_t$ characterizes $P$.
\end{theorem}
\begin{proof}
{\bf TODO.}
\end{proof}

"Exact methods for the transient analysis of nonhomogeneous continuous time Markov chains"


"an empirical transition matrix for non-homogeneous markov chains based on censored observations"



% \subsubsection{*** non-markov version (?) ***}
% TODO

%\subsection{*** Ik denk dat dat brol is ***}
%
%\begin{lemma}\label{lemma:differenceproductoftransition}
%Consider two sequences $A_1,\dots,A_n$ and $B_1,\dots,B_n$ of stochastic matrixes such that, for all $i\in\{1,\dots,n\}$, $\norm{A_i-B_i}\leq c$. Then
%\begin{equation*}
%\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}\leq nc
%\end{equation*}
%\end{lemma}
%\begin{proof}
%We provide a proof by induction. For $n=1$, the result is trivially true. Assume that the result holds for $n=k-1$. The following derivation then shows that it also holds for $n=k$: 
%\begin{align*}
%\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}
%&=
%\norm{\prod_{i=1}^{n}A_i-\left(\prod_{i=1}^{n-1}A_i\right)B_n+\left(\prod_{i=1}^{n-1}A_i\right)B_n-\prod_{i=1}^{n}B_i}\\
%&\leq
%\left(\prod_{i=1}^{n-1}\norm{A_i}\right)\norm{A_n-B_n}+\norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\norm{B_n}\\
%&\leq c + \norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\leq c+(n-1)c= nc.
%\end{align*}
%\end{proof}

%\section{The Lower Transition Operator}\label{sec:lower_operator}

\subsection{Stuff that needs to find a place}




*** Throughout this work, we will make extensive use of the notion of sequences of time points. For any $t,s\in\realsnonneg$ such that $t<s$, let $u$ denote any finite sequence of time points $t_0,t_1,\ldots,t_n$ such that $t=t_0 < t_1 <\ldots < t_n = s$. We use $\mathcal{U}_{[t,s]}$ to denote the set of all such sequences. 
Given any $u\in\mathcal{U}_{[t,s]}$, we define for all $i\in\{1,\ldots,n\}$ the terms $\Delta_i\coloneqq t_i-t_{i-1}$.  Finally, define a function $\sigma(u)$, as
\begin{equation*}
\sigma(u) \coloneqq \max\bigl\{\Delta_i\,:\,i\in\{1,\ldots,n\}\bigr\}\,.
\end{equation*}
***

*** deze moeten we ergens kwijt, ik heb hem maar even hier neergezet. beetje onhandig alleen aangezien we boundedness van $\rateset$ hier nog niet gedefinieerd hebben ***

\begin{lemma}\label{lemma:bound_precise_linear_approx}
Let $Q_\tau$ be any piecewise-constant function that gives for each time point $\tau\in\realsnonneg$ a rate matrix in some non-empty bounded set of rate matrices $\rateset$. By Theorem~\ref{theorem:continuous_rate_matrix_has_process}, there is then some Markov process $P\in\mprocesses_\rateset$ that is characterized by $Q_\tau$. Consider any $t\in\realsnonneg$ and any $\Delta\in\realsnonneg$ such that $Q_\tau$ is constant on the interval $[t,t+\Delta]$. Then, the transition matrix $T_t^{t+\Delta}$ corresponding to $P$ satisfies
\begin{equation*}
\norm{\left[I + \Delta Q_t\right] - T_t^{t+\Delta}} \leq \Delta^2\norm{\rateset}^2\,.
\end{equation*}
\end{lemma}
\begin{proof}
{\bf TODO}
\end{proof}

\section{Imprecise Continuous-Time Markov chains}
\label{sec:iCTMC}

*** starten van een set van Q's (bounded), en dan drie manieren beschouwen om imprecies te maken:

Consider any set $\rateset\subseteq\mathcal{R}$ of rate matrices. Then $\rateset$ is said to be \emph{non-empty} if $\rateset\neq\emptyset$ and $\rateset$ is said to be \emph{bounded} if $\norm{\rateset}<+\infty$. The following proposition provides a simple alternative characterisation of boundedness.

\begin{proposition}\label{prop:alternativedefforbounded}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ is bounded if and only if
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset\right\}>-\infty\text{~~for all $x\in\states$.}
\end{equation*}
\end{proposition}
\begin{proof}
*** moet dit nog invullen ***
\end{proof}

\subsection{*** onder van homogene ***}


*** uitleggen dat het dat NIET is ***

\begin{align*}
\whmprocesses_\rateset
\coloneqq&
\left\{
P\in\whmprocesses
\colon
\overline{\partial}
{T^t_{t,\,x_u}}\subseteq\rateset
\right\}\\
=&
\left\{
P\in\whmprocesses
\colon
\overline{\partial}
{T^t_{t}}\subseteq\rateset
\right\}\\
=&
\left\{
P\in\whmprocesses
\colon
Q_P\in\rateset
\right\}
\end{align*}

\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{WHM}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\whmprocesses_\rateset\right\}\,.
\end{equation*}


\subsection{*** onder van inhomogene Markov ***}

\begin{definition}[Set of Markov Processes]\label{def:markov_process_set_new}
For any bounded set of rate matrices $\rateset$, we define the set $\wmprocesses_{\rateset}$ of all well-behaved continuous-time Markov processes \emph{consistent} with $\rateset$. Formally, we let $\wmprocesses_{\rateset}$ be the set of all $P\in\wmprocesses$ such that
%\begin{align*}\label{eq:conditionforMarkov_new}
%(\forall\epsilon\in\realspos)&\,
%(\exists\delta\in\realspos)\,
%(\forall t\in\realsnonneg)\,
%(\forall\Delta\in(0,\delta))\,
%(\exists Q\in\rateset)\,:\\
% &\,(\forall f\in\gamblesX)(\forall x\in\states)~
%\left\lvert\frac{\mathbb{E}_{X_{t+\Delta}}[f(X_{t+\Delta})\,\vert\,X_t=x]-f(x)}{\Delta}-\left[Qf\right](x)\right\rvert<\epsilon\cdot\norm{f}\,.
%\end{align*}
\begin{align*}
\wmprocesses_\rateset
\coloneqq&
\left\{
P\in\wmprocesses
\colon
\overline{\partial}
{T^t_{t,\,x_u}}\subseteq\rateset
\right\}\\
=&
\left\{
P\in\wmprocesses
\colon
\overline{\partial}
{T^t_{t}}\subseteq\rateset
\right\}
\end{align*}
\end{definition}

\begin{definition}[Lower Expectation for Set of Markov Processes]\label{def:lower_markov} Consider any bounded set of rate matrices $\rateset$, and the set of corresponding well-behaved continuous-time Markov processes $\wmprocesses_\rateset$. Then, the \emph{lower expectation with respect to $\wmprocesses_\rateset$} is defined as
%\begin{equation*}
%\underline{\mathbb{E}}^\mathrm{M}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\mprocesses_\rateset\right\}\,.
%\end{equation*}
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{WM}[\cdot\,\vert\,\cdot] \coloneqq \inf\left\{\mathbb{E}[\cdot\,\vert\,\cdot]\,:\,P\in\wmprocesses_\rateset\right\}\,.
\end{equation*}
\end{definition}

\subsection{*** onder van niet-Markov}

\begin{definition}[Set of Non-Markov Processes]\label{def:set_non_markov_process}
For any bounded set of rate matrices $\rateset$, we consider the set $\wprocesses_\rateset$ of all stochastic processes \emph{consistent} with $\rateset$. Formally, we let $\wprocesses_\rateset$ be the set of all $P\in\wprocesses$ such that
%\begin{align*}
%&(\forall\epsilon\in\realspos)\,(\exists\delta\in\realspos)\,: \\
% &(\forall t\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall u\in\mathcal{U}_{[0,t]})\,(\forall(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}})\,(\exists Q\in\rateset)\,: \\
% &(\forall f\in\gambles(\states^{u\cup\{t+\Delta\}}))\,(\forall x_{t_n}\in\states^{\{t_n\}}): \\
% &\abs{\frac{\mathbb{E}_{X_{t+\Delta}}[f(x_{t_0},\ldots,x_{t_n},X_{t+\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] - f(x_{t_0},\ldots,x_{t_n},x_{t_n})}{\Delta} - \left[Q f(x_{t_0},\ldots,x_{t_{n}},X_{t+\Delta})\right](x_{t_n})} \\ 
% &\quad\quad < \epsilon\cdot\norm{f}\,.
%\end{align*}
\begin{equation}
\label{def:nonmarkovsetQ}
\wprocesses_\rateset
\coloneqq
\left\{
P\in\wprocesses
\colon
\overline{\partial}
{T^t_{t,\,x_u}}\subseteq\rateset
\right\}
\end{equation}
\end{definition}

\begin{definition}[Lower Expectation for Set of Non-Markov Processes]\label{def:lower_non_markov} Consider any bounded set of rate matrices $\rateset$, and the set of corresponding well-behaved continuous-time non-Markov processes $\wprocesses_\rateset$. Then, the \emph{lower expectation with respect to $\wprocesses_\rateset$} is defined as
%\begin{equation*}
%\underline{\mathbb{E}}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\processes_\rateset\right\}\,.
%\end{equation*}
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{W}[\cdot\,\vert\,\cdot] \coloneqq \inf\left\{\mathbb{E}[\cdot\,\vert\,\cdot]\,:\,P\in\wprocesses_\rateset\right\}\,.
\end{equation*}
\end{definition}

\begin{proposition}\label{prop:markov_set_subset_of_nonmarkov_set}
Consider any bounded set of rate matrices $\rateset$, and the corresponding sets $\wmprocesses_\rateset$ and $\wprocesses_\rateset$ of well-behaved continuous-time Markov processes and non-Markov processes, respectively. Then,
\begin{equation*}
\wmprocesses_\rateset \subseteq \wprocesses_\rateset\,.
\end{equation*}
\end{proposition}
\begin{proof}
This is immediate from Definitions \ref{def:markov_process_set_new} and \ref{def:set_non_markov_process}.
\end{proof}

\begin{proposition}\label{prop:lower_exp_markov_bounded_by_nonmarkov}
Consider any $t,s\in\realsnonneg$ such that $t<s$, any bounded set of rate matrices $\rateset$, any $f\in\gamblesX$, and any $x_t\in\states$. Then,
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{WM}[f(X_s)\,\vert\,X_t=x_t] \geq \underline{\mathbb{E}}_\rateset^\mathrm{W}[f(X_s)\,\vert\,X_t=x_t]\,.
\end{equation*}
\end{proposition}
\begin{proof}
This is immediate from Definitions \ref{def:lower_markov} and \ref{def:lower_non_markov}, and Proposition \ref{prop:markov_set_subset_of_nonmarkov_set}.
\end{proof}

\begin{lemma}
Consider any $P\in\wprocesses_\rateset$, any $0<t<s$ and any $x_u$. Then for all $\epsilon>0$ and $\delta>0$, there is some $v\in\mathcal{U}_{[t,s]}$ such that $\sigma(v)<\delta$ and, for all $i\in\{0,\dots,n-1\}$:
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{t_{i+1}}_{t_i,\,x_u}-(I+\Delta_{i+1}Q)
}<\Delta_{i+1}\epsilon
\end{equation*}
\end{lemma}
\begin{proof}
Fix any $\epsilon>0$ and $\delta>0$.
For any $r\in[t,s]$, it follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and Definition~\eqref{def:nonmarkovsetQ} that there is some $0<\delta_r<\delta$ such that, for all $0<\Delta<\delta_r$:
\begin{equation}\label{eq:epsilonboundsforlemma}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{r+\Delta}_{r,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{r}_{r-\Delta,\,x_u}-I)-Q}<\epsilon.
\end{equation}
Let $U_r\coloneqq(r-\delta_r,r+\delta_r)$. Then the set $C\coloneqq\{U_r\colon r\in[t,s]\}$ is an open cover of $[t,s]$. By the Heine-Borel theorem, $C$ contains a finite subcover $C^*$ of $[t,s]$. Without loss of generality, we can take this subcover to be minimal, in the sense that if we remove any of its elements, it is no longer a cover. Let $m$ be the cardinality of $C^*$ and let $r_1<r_2<\dots<r_m$ be the ordered sequence of the midpoints of the intervals in $C^*$.

We will now prove that
\begin{equation}\label{eq:orderingofbounds}
r_i-\delta_{r_i}<r_j-\delta_{r_j}
\text{~~and~~}
r_i+\delta_{r_i}<r_j+\delta_{r_j}
\text{~~for all $1\leq i<j\leq m$.}
\end{equation}
Assume \emph{ex absurdo} that this statement is not true. Then this implies that there are $1\leq i<j\leq m$ such that either $r_i-\delta_{r_i}\geq r_j-\delta_{r_j}$ or $r_i+\delta_{r_i}\geq r_j+\delta_{r_j}$. If $r_i-\delta_{r_i}\geq r_j-\delta_{r_j}$, then since $i<j$ implies that $r_i<r_j$, it follows that $\delta_{r_j}\geq\delta_{r_i}+r_j-r_i>\delta_{r_i}$ and therefore, that $r_j+\delta_{r_j}>r_i+\delta_{r_i}$. 
Hence, we find that $U_{r_i}\subseteq U_{r_j}$. Since $C^*$ was taken to be a minimal cover, this is a contradiction.
Similarly, if $r_i+\delta_{r_i}\geq r_j+\delta_{r_j}$, then since $i<j$ implies that $r_i<r_j$, it follows that $\delta_{r_i}\geq\delta_{r_j}+r_j-r_i>\delta_{r_j}$ and therefore, that $r_i-\delta_{r_i}<r_j-\delta_{r_i}$. Hence, we find that $U_{r_j}\subseteq U_{r_i}$. Since $C^*$ was taken to be a minimal cover, this is again a contradiction. From these two contradictions, it follows that Equation~\eqref{eq:orderingofbounds} is indeed true.

Next, we prove that
\begin{equation}\label{eq:overlapasyouwantit}
r_{k+1}-\delta_{r_{k+1}}<r_k+\delta_{r_k}
\text{~~for all $k\in\{1,\dots,m-1\}$.}
\end{equation}
Assume \emph{ex absurdo} that this statement is not true or, equivalently, that there is some $k\in\{1,\dots,m-1\}$ such that $r_k+\delta_{r_k}\leq r_{k+1}-\delta_{r_{k+1}}$. For all $i\in\{k+1,\dots, m\}$, it then follows from Equation~\eqref{eq:orderingofbounds} that $r_k+\delta_{r_k}\leq r_i-\delta_{r_i}$, which implies that $r_k+\delta_{r_k}\notin U_{r_i}$. Similarly, for all $i\in\{1,\dots,k\}$, it follows from Equation~\eqref{eq:orderingofbounds} that $r_i+\delta_{r_i}\leq r_k+\delta_{r_k}$, which again implies that $r_k+\delta_{r_k}\notin U_{r_i}$. 
Hence, for all $i\in\{1,\dots,m\}$, we have found that $r_k+\delta_{r_k}\notin U_{r_i}$. 
Since $C^*$ is a cover of $[t,s]$, this implies that $r_k+\delta_{r_k}\notin[t,s]$, which, since $r_k\in[t,s]$, implies that $r_k+\delta_{r_k}>s$. Hence, since we know from Equation~\eqref{eq:orderingofbounds} that $r_k-\delta_{r_k}<r_m-\delta_{r_m}$, it follows that $U_{r_m}\cap[t,s]\subseteq U_{r_k}\cap[t,s]$. This contradicts the fact that $C^*$ was taken to be a minimal cover.

For all $k\in\{1,\dots,m-1\}$, we now define $q_k\coloneqq\nicefrac{1}{2}(r_k+\delta_{r_k}+r_{k+1}-\delta_{r_{k+1}})$.
Using Equation~\eqref{eq:orderingofbounds}, it then follows that
\begin{equation*}
q_k<\frac{r_{k+1}+\delta_{r_{k+1}}+r_{k+1}-\delta_{r_{k+1}}}{2}=r_{k+1}
\text{~~and~~}
q_k>\frac{r_{k}+\delta_{r_{k}}+r_{k}-\delta_{r_{k}}}{2}=r_{k},
\end{equation*}
and Equation~\eqref{eq:overlapasyouwantit} trivially implies that $r_{k+1}-\delta_{r_{k+1}}<q_k<r_k+\delta_{r_k}$. Hence,
\begin{equation*}
r_k<q_k<r_k+\delta_{r_k}
\text{~~and~~}
r_{k+1}-\delta_{r_{k+1}}<q_k<r_{k+1}.
\end{equation*}
Because of Equation~\eqref{eq:epsilonboundsforlemma}, and with $\Delta^*_k\coloneqq q_k-r_k$ and $\Delta^{**}_k\coloneqq r_{k+1}-q_k$, this implies that
\begin{equation}\label{eq:epsilonboundsforqk}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta^*_k}
(T^{q_k}_{r_k,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta^{**}_k}
(T^{r_{k+1}}_{q_k,\,x_u}-I)-Q}<\epsilon.
\end{equation}

For all $k\in\{1,\dots,m-1\}$, we now let $s_{2k}\coloneqq q_k$ and, for all $k\in\{1,\dots,m\}$, we let $s_{2k-1}\coloneqq r_k$. For the resulting sequence $s_1<s_2<\dots<s_{2m-2}<s_{2m-1}$, it then follows from Equation~\eqref{eq:epsilonboundsforqk} and~\ref{N:homogeneous} that, for all $i\in\{1,\dots,2m-2\}$:
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{s_{i+1}}_{s_i,\,x_u}-(I+\Delta'_{i+1}Q)
}<\Delta'_{i+1}\epsilon,
\end{equation*}
with $\Delta'_{i+1}\coloneqq s_{i+1}-s_i<\delta$. 

If $s_1\neq t$, we also define $s_0\coloneqq t$ and $\Delta'_1\coloneqq s_1-s_0=r_1-t>0$. Since $C^*$ is a minimal cover, and because of Equation~\eqref{eq:orderingofbounds}, it follows that $r_1-\delta_{r_1}<t<r_1$, which, because of Equation~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous}, implies that
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{s_{1}}_{s_0,\,x_u}-(I+\Delta'_{1}Q)
}<\Delta'_{1}\epsilon.
\end{equation*}

If $s_{2m-1}\neq s$, we also define $s_{2m}\coloneqq s$ and $\Delta'_{2m}\coloneqq s_{2m}-s_{2m-1}=s-r_m>0$. Since $C^*$ is a minimal cover, and because of Equation~\eqref{eq:orderingofbounds}, it follows that $r_m<s<r_m+\delta_{r_m}$, which, because of Equation~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous}, implies that
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{s_{2m}}_{s_{2m-1},\,x_u}-(I+\Delta'_{2m}Q)
}<\Delta'_{2m}\epsilon.
\end{equation*}

We now consider four cases.
If $s_1=t$ and $s_{2m-1}=s$, the result follows by letting $n\coloneqq 2m-2$ and defining $t_i\coloneqq s_{i+1}$ for all $i\in\{0,\dots,n\}$. 
If $s_1=t$ and $s_{2m-1}\neq s$, the result follows by letting $n\coloneqq 2m-1$ and defining $t_i\coloneqq s_{i+1}$ for all $i\in\{0,\dots,n\}$.
If $s_1\neq t$ and $s_{2m-1}=s$, the result follows by letting $n\coloneqq 2m-1$ and defining $t_i\coloneqq s_{i}$ for all $i\in\{0,\dots,n\}$.
If $s_1\neq t$ and $s_{2m-1}\neq s$, the result follows by letting $n\coloneqq 2m$ and defining $t_i\coloneqq s_{i}$ for all $i\in\{0,\dots,n\}$.
\end{proof}

\begin{theorem}
*** only to be touched by Jasper *** Consider a non-empty convex set of rate matrices $\rateset\subseteq\mathcal{R}$ (perhaps I will need some extra properties here).
Fix a finite sequence of time points $u$ and some $t>0$. Then for any choice of $P_\emptyset\in\wprocesses_\rateset$ and, for all $x_u\in\states^u$, $P_{x_u}\in\wprocesses_\rateset$, there is some $P\in\wprocesses_\rateset$ such that *** still need to complete this statement (je kunt dit aan elkaar plakken) ***
\end{theorem}
\begin{proof}
*** heb dit ergens staan ***
\end{proof}

\subsection{Room for Jasper}

*** only to be touched by Jasper ***
*** als ik toch perse iets buiten de bewijzen wil invoeren doe ik dat voorlopig hier ***

\section{An Important Lower Transition Operator}
\label{sec:lowertrans}

Having introduced the notion of lower expectations with respect to sets of (non-)Markov processes, one might wonder how to compute these quantities, either numerically or for analytical purposes. 

Obviously, one way to go about doing this is to work directly with the definitions. That is, explicitly generate the entire set $\mprocesses_\rateset$ (or $\processes_\rateset$) for a given $\rateset$, compute expectations of a function $f$ for each element of this set, and then find the infimum of these expectations. It should be clear that this approach is fairly unwieldy, not in the least because for arbitrary $\rateset$ the corresponding set of processes may be infinite.

Therefore, we will instead provide an alternative characterization of these lower expectations. In particular, we will in this section introduce a specific \emph{lower transition operator}, which is a map from $\gamblesX$ to $\gamblesX$ that generalizes the notion of a transition matrix. We will here focus on introducing the relevant concepts, and showing that the operator of interest is well-defined. We end this section by relating this operator to existing work from the literature. In Sections~\ref{sec:connections} and~\ref{sec:funcs_multi_time_points} we will then establish the relation between this operator and lower expectations, and show that we can indeed use it to compute the quantities of interest.

\subsection{Lower Transition (Rate) Operators}

It is clear from Section {\bf SEC REF CTMC} that there is a strong connection between rate matrices and transition matrices corresponding to Markov chains. We here generalize these two concepts to \emph{lower transition rate operators} and \emph{lower transition operators}, respectively.

\begin{definition}[Lower Transition Rate Operator]\label{def:coh_low_trans_rate}
We will call a map $\lrate$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition rate operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, all constant functions $\mu\in\gamblesX$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=LR\arabic*:,ref=LR\arabic*]
\item\label{LR:constantzero}
$\lrate(\mu)(x)=0$;
\item\label{LR:subadditive}
$\lrate(f+g)(x)\geq\lrate(f)(x)+\lrate(g)(x)$;
\item\label{LR:homo}
$\lrate(\lambda f)(x)=\lambda\lrate(f)(x)$;
\item\label{LR:nondiagpos}
$\lrate(\ind{y})(x)\geq0$ for all $y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\end{definition}


\begin{definition}[Lower Transition Operator]\label{def:coh_low_trans}
We will call a map $\lt$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=C\arabic*:]
\item
$\lt(f)(x)\geq\min\left\{f(y)\,\vert\,y\in\states\right\}$
\item
$\lt(f+g)(x)\geq\lt(f)(x)+\lt(g)(x)$;
\item
$\lt(\lambda f)(x)=\lambda\lt(f)(x)$.
\end{enumerate}
\vspace{5pt}
\end{definition}

\noindent We start by giving some useful properties of the norm of these operators.

\begin{lemma}\label{lem:normlratefinite}
For any lower transition rate operator $\lrate$, we have that $0\leq\norm{\lrate}<+\infty$.
\end{lemma}
\begin{proof}
*** {\bf TODO } ***
\end{proof}

\begin{lemma}\label{lemma:normofcoherenttrans}
For any lower transition operator $\lt$, we have that $0\leq \norm{\lt}\leq 1$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttransrate}
Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lrate$ be an arbitrary lower transition rate operator. Then, it holds that $\norm{\lrate A-\lrate B}\leq 2\norm{\lrate}\norm{A-B}$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from the definition of the norm and the properties of $\lrate$.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttrans}
Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lt$ be an arbitrary lower transition operator. Then, it holds that $\norm{\lt A-\lt B}\leq \norm{A-B}$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from coherence.
\end{proof}

We next have two results about the set of all lower transition operators, which we denote by $\underline{\mathcal{T}}$. As the first result shows, this set is closed under composition.
\begin{lemma}\label{lemma:compositioncoherence}
For any $\lt,\underline{S}\in\underline{\mathcal{T}}$, we have that $\left(\lt\underline{S}\right)\in\underline{\mathcal{T}}$.
\end{lemma}
\begin{proof}
Simply check each of the properties.
\end{proof}

\noindent Furthermore, this set is a complete metric space under our usual norm.

\begin{lemma}\label{lemma:completemetricspace}
The metric space $(\underline{\mathcal{T}},d)$ is complete under the metric $d$ induced by $\norm{\cdot}$.
\end{lemma}
\begin{proof}
Define for all $\lt\in\underline{\mathcal{T}}$ and all $x\in\states$ the map $\lt_x:\gamblesX\rightarrow\reals$ as
\begin{equation*}
\lt_x(f) \coloneqq \lt(f)(x)\,,\quad\text{for all $f\in\gamblesX$,}
\end{equation*}
and let $\underline{\mathcal{T}}_x\coloneqq\left\{\lt_x\,:\,\lt\in\underline{\mathcal{T}}\right\}$ for all $x\in\states$. Then clearly, any $\lt\in\underline{\mathcal{T}}$ is a finite vector of elements $\lt_1\in\underline{\mathcal{T}}_1,\ldots,\lt_m\in\underline{\mathcal{T}}_m$, and $\underline{\mathcal{T}}=\underline{\mathcal{T}}_1\times\cdots\times \underline{\mathcal{T}}_m$.

Furthermore, for all $x\in\states$ and all $\lt_x\in\underline{\mathcal{T}}_x$, because of Definition~\ref{def:coh_low_trans}, $\lt_x$ is a map from $\gamblesX$ to $\reals$ that is super-additive, positively homogeneous, and bounded below by the minimum operator. Hence, by definition~\cite[Definition~2.3.3]{Walley:1991vk}, $\lt_x$ is a coherent lower prevision on $\gamblesX$, and $\underline{\mathcal{T}}_x$ corresponds to the space of all coherent lower previsions on $\gamblesX$.

Let now $\gambles_{\leq1}(\states)\coloneqq\left\{f\in\gamblesX\,:\,\forall x\in\states, 0\leq f(x)\leq 1\right\}$ be the set of non-negative functions $f\in\gamblesX$ with $\norm{f}\leq 1$. It was previously shown~\cite{DeBock:2015ck} that the metric space $(\underline{\mathcal{T}}_x,d_x)$ is compact under the topology generated by the metric $d_x$, defined for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$ by
\begin{equation*}
d_x(\lt_x,\underline{S}_x) \coloneqq \sup\bigl\{ \abs{\lt_xf - \underline{S}_xf} \,:\,f\in\gambles_{\leq1}(\states) \bigr\}\,.
\end{equation*}
Now consider the metric $d_x^*$, which we define for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$ by
\begin{equation*}
d_x^*(\lt_x,\underline{S}_x) \coloneqq \sup\left\{ \abs{\lt_xf - \underline{S}_xf}\,:\, f\in\gamblesX, \norm{f}=1\right\}\,.
\end{equation*}
It is fairly easy to see that for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$, we have that
\begin{equation*}
d_x(\lt_x,\underline{S}_x) \leq d_x^*(\lt_x,\underline{S}_x) \leq 2d_x(\lt_x,\underline{S}_x)\,,
\end{equation*}
from which it follows that any subset of $\underline{\mathcal{T}}_x$ that is open with respect to $d_x$ is also open with respect to $d_x^*$. Hence, $d_x$ and $d_x^*$ generate the same topology on $\underline{\mathcal{T}}_x$, and therefore because $(\underline{\mathcal{T}}_x, d_x)$ is compact, so is $(\underline{\mathcal{T}}_x,d_x^*)$. Because $(\underline{\mathcal{T}}_x,d_x^*)$ is a metric space, compactness now implies that $(\underline{\mathcal{T}}_x,d_x^*)$ is complete.

Therefore~\cite[Theorem 10.5.1]{OSearcoid:2006}, and because $\underline{\mathcal{T}}$ is a finite product of $\underline{\mathcal{T}}_1,\ldots,\underline{\mathcal{T}}_m$, the metric space $(\underline{\mathcal{T}},D)$ is complete under any metric $D$ that is \emph{conserving}~\cite[Definition 1.6.2]{OSearcoid:2006} with respect to $d_1^*,\ldots,d_m^*$. For $D$ to be conserving, it suffices to show that for all $\lt,\underline{S}\in\underline{\mathcal{T}}$,
\begin{equation*}
D(\lt, \underline{S}) = \max_{x\in\states}\bigl\{d_x^*(\lt_x, \underline{S}_x)\bigr\}\,.
\end{equation*}
Consider the metric $d$ on $\underline{\mathcal{T}}$ that is induced by $\norm{\cdot}$. We have, for any $\lt,\underline{S}\in\underline{\mathcal{T}}$, 
\begin{align*}
d(\lt,\underline{S}) &= \norm{\lt - \underline{S}} \\
% &= \sup\left\{ \norm{\lt f - \underline{S}f}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
% &= \sup\left\{ \max_{x\in\states}\{\abs{\lt(f)(x) - \underline{S}(f)(x)}\}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
 &= \sup\left\{ \max_{x\in\states}\{\abs{\lt_xf - \underline{S}_xf}\}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
 &=  \max_{x\in\states}\bigl\{\sup\left\{\abs{\lt_xf - \underline{S}_xf}\,:\,f\in\gamblesX, \norm{f}=1 \bigr\}\right\} \\
 &=  \max_{x\in\states}\left\{d_x^*(\lt_x, \underline{S}_x)\right\}\,.
\end{align*}
Thus, $d$ is conserving with respect to $d_1^*,\ldots,d_m^*$, and hence $(\underline{\mathcal{T}},d)$ is complete.
\end{proof}

We conclude this section by establishing that there is a correspondence between lower transition rate operators and lower transition operators that is analogous to the one found in Section {\bf SEC REF CTMC}. 

\begin{lemma}\label{lemma:normQsmallenough}
Consider any lower transition rate operator $\lrate$, and any $0\leq\Delta\leq\nicefrac{1}{\norm{\lrate}}$. Then, $(I+\Delta\lrate)$ is a lower transition operator.
\end{lemma}
\begin{proof}
Just check each of the three defining properties. C2 and C3 are trivial. C1 requires a bit more work. *** {\bf TODO?} ***
\end{proof}

\noindent We therefore have the following result.
\begin{lemma}\label{lemma:productiscoherent}
Consider any $t,s\in\realsnonneg$ such that $t<s$, any lower transition rate operator $\lrate$, and any sequence $u\in\mathcal{U}_{[t,s]}$ of time points such that $\sigma(u)\leq\nicefrac{1}{\norm{\lrate}}$. Then
\begin{equation*}
\prod_{k=1}^n(I+\Delta_i\lrate)\coloneqq (I+\Delta_1\lrate)(I+\Delta_2\lrate)\cdots (I+\Delta_n\lrate)
\end{equation*}
is a lower transition operator.
\end{lemma}
\begin{proof}
Trivial consequence of Lemma~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}.
\end{proof}

\subsection{The Operator of Interest}

We will next introduce a specific lower transition operator on which we will focus for the remainder of this work. We will assume in this section that we are given some arbitrary lower transition rate operator $\lrate$, and define for any $u\in\mathcal{U}_{[t,s]}$ the operator
\begin{equation*}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate)\,.
\end{equation*}
The \emph{lower transition operator corresponding to $\lrate$} is then defined as follows.

\begin{definition}[Corresponding Lower Transition Operator]\label{def:low_trans}
Consider any $t,s\in\realsnonneg$ such that $t<s$ and let $\lrate$ be an arbitrary lower transition rate operator. The \emph{corresponding lower transition operator} $\lbound_t^s$ is a map from $\gamblesX$ to $\gamblesX$, defined by
\begin{equation}\label{eq:lowerbound}
\lbound_t^s\coloneqq\lim_{\sigma(u)\to0}\left\{ \Phi_u\,\Big\vert\,u\in\mathcal{U}_{[t,s]}\right\},
\end{equation}
where the limit is taken with respect to the set $\mathcal{U}_{[t,s]}$ of all finite sequences $u$ of time points that partition the interval $[t,s]$.
\end{definition}

\noindent The following result makes clear what exactly we mean by Equation~\eqref{eq:lowerbound}, and establishes that this limit exists and is well-defined.

\begin{theorem}\label{theo:convergencelowerbound}
For any $t,s\in\realsnonneg$ such that $t<s$ and any lower transition rate operator $\lrate$, there is a lower transition operator $\lbound_t^s$ such that 
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon.
\end{equation*}
\end{theorem}
\quad\newline\newline
\noindent The remainder of this section serves to prove Theorem~\ref{theo:convergencelowerbound}. Some technically required lemmas can be found in Appendix~\ref{sec:proof_appendix}. We start with a bound on the distance between two operators $\Phi_u$ and $\Phi_{u*}$.

\begin{proposition}\label{prop:differencebetweenu}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and any $u,u^*\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\alpha$ and $\sigma(u^*)<\alpha$, with $0<\alpha\leq\nicefrac{1}{\norm{\lrate}}$. Let $\Delta\coloneqq s-t$. Then,
\begin{equation*}
\norm{\Phi_u-\Phi_{u^*}}\leq 2\alpha\Delta\norm{\lrate}^2
\end{equation*}
\end{proposition}
\begin{proof}
Consider any $u'\in\mathcal{U}_{[t,s]}$ that is finer than $u$ and $u^*$, meaning that the timepoints it consists of contain the timepoints in $u$ and the timepoints in $u^*$. For example, let $u'$ be the ordered union of the timepoints in $u$ and $u^*$.

This implies that, for all $k\in\{1,\dots,n\}$, there is some sequence $\Delta_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta_k=\sum_{i=1}^{n_k}\Delta_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_u}\leq\alpha\Delta\norm{\lrate}^2$. 

Similarly, for all $k\in\{1,\dots,n^*\}$, there is some sequence $\Delta^*_{k,i}>0$, $i\in\{1,\dots,n^*_k\}$, such that $\Delta^*_k=\sum_{i=1}^{n^*_k}\Delta^*_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^{n^*}\left(\prod_{i=1}^{n^*_k}(I+\Delta^*_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_{u^*}}\leq\alpha\Delta\norm{\lrate}^2$.

Hence, we find that
\begin{equation*}
\norm{\Phi_{u}-\Phi_{u^*}}
=
\norm{\Phi_{u}-\Phi_{u'}+\Phi_{u'}-\Phi_{u^*}}
\leq
\norm{\Phi_{u}-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u^*}}
\leq2\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{proof}

\begin{corollary}\label{corol:cauchy}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ is a \emph{Cauchy sequence}, meaning that
\begin{equation*}
(\forall \epsilon>0)(\exists N\in\nats)(\forall n,m\geq N)
\norm{\Phi_{u_n}-\Phi_{u_m}}<\epsilon.
\end{equation*}
\end{corollary}
\begin{proof}
This follows almost directly from Proposition~\ref{prop:differencebetweenu}.
\end{proof}

\begin{corollary}\label{corol:limitexistsandiscoherent}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a lower transition operator.
\end{corollary}
\begin{proof}
Since $\lim_{n\to\infty}\sigma(u_n)=0$, and because of Lemma~\ref{lemma:productiscoherent}, there is some index $i$ such that the sequence $\Phi_{u_i},\Phi_{u_{i+1}},\dots,\Phi_{u_n},\dots$ consists of lower transition operators. Due to Corollary~\ref{corol:cauchy}, this sequence is Cauchy and therefore, because of Lemma~\ref{lemma:completemetricspace}, this sequence has a limit that is also a lower transition operator. Since the limit starting from $i$ and the limit starting from $1$ are identical (initial elements do not influence the limit), we find that the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ has a limit, and that this limit is a lower transition operator.
\end{proof}

\noindent The proof of Theorem~\ref{theo:convergencelowerbound} now follows.

\begin{proof}[Proof of Theorem~\ref{theo:convergencelowerbound}]
Start by considering any sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$. Due to Corollary~\ref{corol:limitexistsandiscoherent}, the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a lower transition operator, which we denote by $\lbound_t^s$. 

Consider now any $\epsilon>0$ and let $\Delta\coloneqq s-t$ and
\begin{equation*}
\delta\coloneqq\min\left\{\frac{\epsilon}{4\Delta\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
\end{equation*}

\noindent Since $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to $\lbound_t^s$, there is some $N\in\nats$ such that
\begin{equation*}
(\forall n\geq N)~\norm{\lbound_t^s - \Phi_{u_n}}<\frac{\epsilon}{2}.
\end{equation*}
Therefore, since $\lim_{n\to\infty}\sigma(u_n)=0$, there is some $N^*\geq N$ such that
\begin{equation*}
\sigma(u_{N^*})<\delta\text{ and }\norm{\lbound_t^s - \Phi_{u_{N^*}}}<\frac{\epsilon}{2}
\end{equation*}

\noindent Consider now any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then

\begin{equation*}
\norm{\lbound_t^s - \Phi_u}\leq\norm{\lbound_t^s-\Phi_{u_{N^*}}}
+\norm{\Phi_{u_{N^*}}-\Phi_u}
<\frac{\epsilon}{2}+2\delta\Delta\norm{\lrate}^2\leq\epsilon,
\end{equation*}
where the strict inequality follows from Proposition~\ref{prop:differencebetweenu}.
In summary, we have shown that there is lower transition operator $\lbound_t^s$ such that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon\,.
\end{equation*}
\end{proof}

\subsection{Properties of the Operator of Interest}

\begin{definition}[Lower Transition Operator System]
Let $\lrate$ be an arbitrary lower transition rate operator. Then, the \emph{lower transition operator system} corresponding to $\lrate$ is the family $\underline{\mathcal{T}}_{\lrate}$ of lower transition operators corresponding to $\lrate$, defined as
\begin{equation*}
\underline{\mathcal{T}}_{\lrate} \coloneqq \Bigl\{L_t^s\,\Big\vert\,\forall t,s\in\realsnonneg, t<s\Bigr\}\,.
\end{equation*}
\end{definition}

\begin{proposition}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s,r\in\realsnonneg$ such that $t< r<s$,
\begin{equation*}
L_t^s = L_t^rL_r^s\,.
\end{equation*}
Furthermore, for all $t\in\realsnonneg$, we have that $L_t^t=I$.
\end{proposition}
\begin{proof}
*** trivial ***
\end{proof}

\begin{proposition}\label{prop:lower_transition_is_homogeneous}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t<s$, and all $\Delta\in\realsnonneg$, we have that
\begin{equation*}
L_{t+\Delta}^{s+\Delta} = L_t^s\,.
\end{equation*}
\end{proposition}
\begin{proof}
*** {\bf TODO} ***
\end{proof}

\begin{proposition}\label{prop:lower_transition_has_deriv}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t<s$, it holds that
\begin{equation*}
\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s\,,\quad\text{and,}\quad\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate\,.
\end{equation*}
% Then $\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s$ and $\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate$, meaning that
\end{proposition}
\noindent The proof for the above proposition can be found in Appendix~\ref{sec:proof_appendix}.

\section{Connections Between $L_t^s$ and Imprecise Continuous-Time Markov Chains}\label{sec:connections}

One of the objectives of this paper is to establish a connection between the operator $\lbound_t^s$ that we have just introduced, and the different types of imprecise continous-time Markov chains that were discussed in Section~\ref{sec:iCTMC}. Since the former is derived from a lower transition rate operator $\lrate$ and the latter are derived from a non-empty bounded set of rate matrices $\rateset$, an obvious first step is to investigate the connection between lower transition rate operators and non-empty bounded sets of rate matrices.

\subsection{Connections Between $\lrate$ and Sets of Rate Matrices $\rateset$}\label{sec:connections_rate}

% $\rateset$ and $\lrate$. 
%In order to do that, we start by discussing some properties of sets of rate matrices.

We start by considering a non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices. For any $f\in\gamblesX$,
\begin{equation}\label{eq:correspondinglowertrans}
\lrate f\coloneqq\inf\{Qf\colon Q\in\rateset\}\\[2mm]
\end{equation}
is then again an element of $\gamblesX$.\footnote{%Since $\rateset$ is non-empty, the components of $\lrate f$ cannot be $+\infty$.
Since $\rateset$ is bounded,~\ref{N:normAf} implies that $\norm{Qf}\leq\norm{Q}\norm{f}<+\infty$ for all $Q\in\rateset$. Therefore, and since $\rateset$ is non-empty, the components of $\lrate f$ cannot be infinite. Hence, $\lrate f$ is a real-valued function on $\states$.}
Therefore, $\lrate$ is a map from $\gamblesX$ to $\gamblesX$. We call this operator $\lrate$, as defined by Equation~\eqref{eq:correspondinglowertrans}, the \emph{lower envelope} of $\rateset$. It is a matter of straightforward verification to see that $\lrate$ is a lower transition operator.

\begin{proposition}\label{prop:lowerenvelopeislowertrans}
For any non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices, the corresponding operator $\lrate\colon\gamblesX\to\gamblesX$, as defined by Equation~\eqref{eq:correspondinglowertrans}, is a lower transition rate operator.
\end{proposition}
\begin{proof}
Consider any $Q\in\rateset$. It then follows from Definition~\ref{def:rate_matrix} that the matrix $Q$, when regarded as a map from $\gamblesX$ to $\gamblesX$, satisfies \ref{LR:constantzero}--\ref{LR:nondiagpos}. Since each of these properties is preserved under taking lower envelopes, it follows that $\lrate$ satisfies \ref{LR:constantzero}--\ref{LR:nondiagpos}, which means that $\lrate$ is a lower transition rate operator.
\end{proof}

\noindent
Inspired by this result, we will also refer to the lower envelope of $\rateset$ as the \emph{lower transition rate operator that corresponds to $\rateset$}. %As we have just seen, every non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices has such a corresponding lower transition rate operator $\lrate$. 
However, this correspondence is not one-to-one. As the following example establishes, different non-empty bounded sets of rate matrices may have the same corresponding lower transition rate operator.

\begin{exmp}
*** TO BE COMPLETED ***
\exampleend
\end{exmp}

Next, we consider some fixed lower transition rate operator $\lrate$.
All the non-empty bounded sets $\rateset$ of rate matrices that have $\lrate$ as their lower envelope then share a common property: they consist of rate matrices $Q$ that dominate $\lrate$, in the sense that $Qf\geq\lrate f$ for all $f\in\gamblesX$. Therefore, each of these sets $\rateset$ is contained in the following set of dominating rate matrices:
\begin{equation}\label{eq:dominatingratematrices}
\rateset_{\lrate}\coloneqq
\left\{
Q\in\mathcal{R}
\colon
Qf\geq\lrate f\text{ for all $f\in\gamblesX$}
\right\}.
\end{equation}
As our next result shows, this set $\rateset_{\lrate}$ is non-empty and bounded, and has $\lrate$ as its lower envelope. Even stronger, the infimum in Equation~\eqref{eq:correspondinglowertrans} is reached---can be replaced by a minimum.

\begin{proposition}\label{prop:dominating_nonempty_bounded}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is non-empty and bounded and, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $\lrate f=Qf$.
\end{proposition}
\begin{proof}
Fix any $f\in\gamblesX$. Choose $\Delta>0$ small enough such that $0\leq\Delta\norm{\lrate}\leq 1$ (this always possible because of Lemma~\ref{lem:normlratefinite}). Define $\lt\coloneqq I+\Delta\lrate$. Since $\lrate$ is a lower transition rate operator, it follows from Lemma~\ref{lemma:normQsmallenough} that $\lt$ is a lower transition operator. For any $x\in\states$, we now let
\begin{equation*}
\lt_xg\coloneqq(\lt g)(x)
\text{~~for all $g\in\gamblesX$.}
\end{equation*}
Since $\lt$ is a lower transition operator, it follows that $\lt_x\colon \gamblesX\to\reals$ is subadditive, positively homogeneous and bounded below by the minimum operator. Hence, by definition~\cite[Definition~2.3.3]{Walley:1991vk}, $\lt_x$ is a coherent lower prevision on $\gamblesX$. Because of \cite[Theorem~3.3.3(b)]{Walley:1991vk}, this implies the existence of an expectation operator $E_x$ on $\gamblesX$---Reference~\cite{Walley:1991vk} calls this a linear prevision on $\gamblesX$---such that $E_xg\geq\lt_xg$ for all $g\in\gamblesX$ and $E_xf=\lt_xf$. Let $P_x$ be the unique probability mass function that corresponds to $E_x$. For all $x,y\in\states$, we now let $T(x,y)\coloneqq P_x(y)\coloneqq E_x(\ind{x})$. Then $T$ is clearly a stochastic matrix. Furthermore, for every $x\in\states$ and $g\in\gamblesX$, we have that $(Tg)(x)=E_xg$. Hence, it follows that $Tg\geq\lt g$ for all $g\in\gamblesX$ and that $Tf=\lt f$. Now let $Q\coloneqq\nicefrac{1}{\Delta}(T-I)$, which, because of Proposition~\ref{prop:rate_from_stochastic_matrix}, is a rate matrix. Since $Tf=\lt f$, it then follows that
\begin{equation*}
Qf=\frac{1}{\Delta}(Tf-f)\geq\frac{1}{\Delta}{\lt f-f}=\lrate f.
\end{equation*}
Similarly, since $Tg\geq\lt g$ for all $g\in\gamblesX$, it follows that $Qg\geq\lrate g$, or equivalently, since $Q$ is a rate matrix, that $Q\in\rateset_{\lrate}$. Since $f$ was arbitrary, this proofs that, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $Qf=\lrate f$. Since $\gamblesX$ is non-empty, this clearly implies that $\rateset_{\lrate}$ is non-empty.

We end this proof by showing that $\rateset_{\lrate}$ is bounded. Consider any $x\in\states$. Then for all $Q\in\rateset_{\lrate}$, we have that $Q(x,x)=(Q\ind{x})(x)\geq(\lrate\ind{x})(x)$, which implies that
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset_{\lrate}\right\}\geq(\lrate\ind{x})(x)>-\infty.
\end{equation*}
Since $x\in\states$ is arbitary, Proposition~\ref{prop:alternativedefforbounded} now guarantees that $\rateset_{\lrate}$ is bounded. 
\end{proof}

\noindent
Because of this result, and since---as discussed above---every non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope is a subset of $\rateset_{\lrate}$, it follows that $\rateset_{\lrate}$ is the largest non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope.
Furthermore, as we will show in Proposition~\ref{prop:dominatingproperties} below, this set $\rateset_{\lrate}$ is also closed and convex, and has \emph{separately specified rows}.

\begin{definition}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ has separately specified rows if
\begin{equation*}
\rateset=\left\{
Q\in\mathcal{R}
\colon
(\forall x\in\states)~Q(x,\cdot)\in\rateset_x\right\},
\end{equation*}
where, for every $x\in\states$, $\rateset_x\coloneqq\{Q(x,\cdot)\colon Q\in\rateset\}$ is some set of rows from which the $x$-th row of the rate matrices in $\rateset$ are taken.
\end{definition}
Thus, a set of rate matrices has separately specified rows if it is closed under taking arbitrary combinations of rows from its elements. We will later need the following result.
\begin{lemma}\label{lemma:rateset_has_arginf}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$ and $\epsilon\in\realspos$, there exists a $Q\in\rateset$ such that
\begin{equation*}
\norm{\lrate f - Qf} < \epsilon\,.
\end{equation*}
\end{lemma}
\begin{proof}
This is immediate from the definition of the lower envelope of $\rateset$, as given by Equation~\eqref{eq:correspondinglowertrans}, and the fact that $\rateset$ has separately specified rows.
\end{proof}


\begin{proposition}\label{prop:dominatingproperties}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is closed and convex, and has separately specified rows.
\end{proposition}
\begin{proof}
*** TO BE COMPLETED ***
\end{proof}


\begin{exmp}
*** TO BE COMPLETED ***
\exampleend
\end{exmp}

\noindent
These additional properties characterise $\rateset_{\lrate}$ completely, in the sense that no other set satisfies them.

\begin{proposition}
Consider any non-empty, bounded, closed and convex set of rate matrices $\rateset\subseteq\mathcal{R}$ with separately specified rows that has $\lrate$ as its lower envelope. Then $\rateset=\rateset_{\lrate}$.
\end{proposition}
\begin{proof}
*** TO BE COMPLETED ***
\end{proof}

We conclude from all of this that non-empty bounded sets of rate matrices are more informative than lower transition rate matrices. Different non-empty bounded sets of rate matrices $\rateset$ may have the same lower transition rate operator $\lrate$ and therefore, in general, knowledge of $\lrate$ does not suffice to reconstruct $\rateset$; we can only reconstruct an outer approximation $\rateset_{\lrate}$, which is guaranteed to include $\rateset$. This changes if, besides non-empty and bounded, $\rateset$ is also closed and convex and has separately specified rows. In that case, $\lrate$ serves as an alternative representation for $\rateset$ because, since $\rateset=\rateset_{\lrate}$, we can use $\lrate$ to reconstruct $\rateset$. In other words: there is a one-to-one correspondence between lower transition rate operators and non-empty, bounded, closed and convex sets of rate matrices that have separately specified rows.

%\section{Imprecise Continuous-Time Markov Chains}\label{sec:imp_markov}

%\subsection{New Version}

%This section contains the new, simplified proofs.

\subsection{Connections Between $L_t^s$ and Lower Expectations $\underline{\mathbb{E}}$}\label{sec:single_var_lower_exp}

Having established a strong connection between the operator $\lrate$ and non-empty bounded sets of rate matrices $\rateset$, we will now turn to the connection between the operator $L_t^s$ and lower expectations $\underline{\mathbb{E}}$ with respect to sets of (non-)Markov processes. Specifically, we will in this section focus on the lower expectation of functions defined on the state space at a single point in time. In Section~\ref{sec:funcs_multi_time_points} we will then use and generalize these results when we consider functions defined on the state space at multiple time points.

As the following result shows, the operator $L_t^s$ induced by a lower transition rate operator $\lrate$ computes a lower bound on the expectation of a function $g\in\gambles(\states^{\{s\}})$, with respect to the set $\processes_\rateset$ of non-Markov processes induced by any set $\rateset$ of which $\lrate$ is the lower envelope.

\begin{theorem}\label{theorem:nonmarkov_single_var_lower_bounded}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\processes_\rateset$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $g\in\gambles(\states^{\{s\}})$,
\begin{equation*}
[L_t^s g](x_{t_n}) \leq \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\processes_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, and any $g\in\gambles(\states^{\{s\}})$. We will show that for all $\epsilon\in\realspos$,
\begin{equation*}
[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,,
\end{equation*}
which then implies $[L_t^s g](x_{t_n}) \leq \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]$. Start by choosing any $\epsilon\in\realspos$, and let $C\coloneqq (s-t)$.

Because $P\in\processes_\rateset$, it follows from Definition~\ref{def:set_non_markov_process} that there is some $\delta\in\realspos$ such that
\begin{align}\label{eq:nonmarkov_bound_proof_deriv_bounded}
\begin{split}
 &(\forall \tau\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall v\in\mathcal{U}_{[0,\tau]})\,(\forall(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}})\,(\exists Q\in\rateset)\,: \\
 &(\forall f\in\gambles(\states^{v\cup\{\tau+\Delta\}}))\,(\forall x_{\tau_m}\in\states^{\{\tau_m\}}): \\
 &\left\lvert \frac{\mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}] 
 - f(x_{\tau_0},\ldots,x_{\tau_m},x_{\tau_m})}{\Delta} \right. \\
 &\quad\quad\quad\quad\quad\quad - \left[Q f(x_{\tau_0},\ldots,x_{\tau_{m}},X_{\tau+\Delta})\right](x_{\tau_m})\biggr\rvert \quad < \quad\frac{\epsilon}{2C\norm{g}}\cdot\norm{f}\,.
\end{split}
\end{align}
Furthermore, it follows from Theorem~\ref{theo:convergencelowerbound} that there is some $\delta'\in\realspos$ such that
\begin{equation}\label{eq:nonmarkov_bound_proof_lbound_approx}
(\forall v\in\mathcal{U}_{[t,s]}\,:\,\sigma(v)<\delta')\,(\forall x_t\in\states)\abs{\left[L_t^s g\right](x_t) - \left[\prod_{k=1}^n(I+\Delta_i\lrate)g\right](x_t)} < \frac{\epsilon}{2}\,.
\end{equation}

Let $\delta^*\coloneqq\min\{\delta,\delta'\}$, and choose any $n>\nicefrac{C}{\delta^*}$. Then, for $\Delta\coloneqq\nicefrac{C}{n}$, we have $\Delta<\delta$ and $\Delta<\delta'$.

Equation \eqref{eq:nonmarkov_bound_proof_deriv_bounded} now implies that for all $\tau\in\realspos$, all $v\in\mathcal{U}_{[0,\tau]}$ such that $v=\tau_0,\ldots,\tau_m$, and all $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, there is some $Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}}\in\rateset$ such that, for all $f\in\gambles(\states^{v\cup\{\tau+\Delta\}})$ and all $x_{\tau_m}\in\states^{\{\tau_m\}}$,
\begin{align}\label{eq:nonmarkov_bound_proof_deriv_inequal}
\begin{split}
 &\left[(I + \Delta Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}})f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\right](x_{\tau_m}) - \frac{\Delta\epsilon\norm{f}}{2C\norm{g}} \\
 &\quad< \mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}]\,.
\end{split}
\end{align}
Now, note that by the basic properties of probability,
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
By choosing $\tau=(s-\Delta)$, setting $v=t_0,\ldots,t_n,(s-\Delta)$, and noting that $g\in\gambles(\states^{\{s\}})=\gambles(\states^{\{\tau+\Delta\}})\subset\gambles(\states^{v\cup\{\tau+\Delta\}})$, we find from Equation \eqref{eq:nonmarkov_bound_proof_deriv_inequal} that for all $(x_{t_0},\ldots,x_{t_n})\in\states^{u}$ there is some $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$ such that, for all $x_{s-\Delta}\in\states^{\{s-\Delta\}}$,
\begin{equation*}
\left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}=x_{s-\Delta}]\,.
\end{equation*}
Furthermore, we find using Equation~\eqref{eq:correspondinglowertrans} and the fact that $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$, that
\begin{equation*}
\left[(I + \Delta \lrate)g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} \leq \left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C}\,.
\end{equation*}
Noting that an expectation takes a convex combination of values, we find by substitution that
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] \\
&> \mathbb{E}\bigl[[(I+\Delta\lrate)g](X_{s-\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] - \frac{\Delta\epsilon}{2C}\,.
\end{align*}

Note also that
\begin{equation*}
[(I+\Delta\lrate)g](X_{s-\Delta})\in\gambles(\states^{\{s-\Delta\}})\subset\gambles(\states^{u\cup\{s-\Delta\}})\,.
\end{equation*}
Hence, we can repeat this argument, factoring the expectation at time points $(s-2\Delta),\ldots,(s-(n-1)\Delta)$, to obtain
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - n\cdot\frac{\Delta\epsilon}{2C} \\
 &= \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2}\,.
\end{align*}

It follows from Equation \eqref{eq:nonmarkov_bound_proof_lbound_approx} and the fact that $\Delta<\delta'$,
\begin{equation*}
\left[L_t^s g\right](x_{t_n}) - \frac{\epsilon}{2} < \left[(I+\Delta\lrate)^n g\right](x_{t_n})\,,
\end{equation*}
so that by substitution,
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2} \\
 &> [L_t^s g](x_{t_n}) - \frac{\epsilon}{2} - \frac{\epsilon}{2} \\
 &= [L_t^s g](x_{t_n}) - \epsilon\,.
\end{align*}
Thus, we have found that
\begin{equation*}
[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,.
\end{equation*}
Because the $\epsilon\in\realspos$ was arbitrary, this concludes the proof.
\end{proof}

As this previous result showed, $L_t^sg$ is a lower bound on the expectation of a function $g\in\gambles(\states^{\{s\}})$, with respect to a set of non-Markov processes $\processes_\rateset$ induced by some non-empty bounded set of rate matrices $\rateset$. Our next result establishes that this bound is tight if $\rateset$ also has separately specified rows. Specifically, we show that $L_t^sg$ can then be approximated to arbitrary precision by carefully choosing a Markov process $P$ from the set $\mprocesses_\rateset$.

\begin{theorem}\label{theorem:lower_markov_bound_is_tight}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$ and $\epsilon\in\realspos$, there exists a $P\in\mprocesses_{\rateset}$ such that
\begin{equation*}
\norm{\lbound_t^sf-T_t^sf} < \epsilon\,.
\end{equation*}
\end{theorem}
\begin{proof}
Choose any $f\in\gamblesX$ and any $\epsilon\in\realspos$. Let $C\coloneqq (s-t)$. 

The proof works by selecting a rate matrix at each point in time, showing that there is a $P\in\mprocesses_\rateset$ that is characterized by this construction, and finally establishing that this $P$ satisfies the inequality of interest.

By Lemma~\ref{lemma:rateset_has_arginf}, for all $\tau\in[t,s]$ there is some rate matrix $Q_\tau\in\rateset$ such that
\begin{equation}\label{eq:lower_char_rate_matrix}
\norm{\lrate \lbound_\tau^sf - Q_\tau \lbound_\tau^sf} < \frac{\epsilon}{2C}\,.
\end{equation}
To fix $Q_\tau$'s values outside of the interval $[t,s]$, let $Q_\tau \coloneqq Q_t$ for all $\tau\in[0,t]$, and let $Q_\tau\coloneqq Q_s$ for all $\tau>s$.
%Define a \emph{lower-characterizing rate matrix} $Q_\tau$ as
%\begin{equation}\label{eq:lower_char_rate_matrix}
%Q_\tau(x_\tau,\cdot)\coloneqq \argmin\left\{ Q(x_\tau,\cdot)\bigl[\lbound_\tau^sf\bigr]\,:\,Q(x_\tau,\cdot)\in\mathcal{Q}_{x_\tau}\right\}\quad\text{for all $x_\tau\in\states$ and $\tau\in[t,s]$}\,.
%\end{equation}
%Note that the $\argmin\{\cdot\}$ may be set-valued, in which case take an arbitrary element.
%Furthermore, let $Q_\tau \coloneqq Q_t$ for all $\tau\in[0,t]$, and let $Q_\tau\coloneqq Q_s$ for all $\tau>s$.

Next, define
\begin{equation}\label{eq:delta_required_for_tight_bound}
\delta \coloneqq \min\left\{\frac{\epsilon}{4C\norm{\mathcal{Q}}^2\norm{f}},\frac{1}{\norm{\lrate}}\right\}\,,
\end{equation}
and choose any $n>\nicefrac{C}{\delta}$. Then, for $\Delta\coloneqq \nicefrac{C}{n}$, we have $\Delta<\delta$.

For all $k\in\{0,\ldots,n\}$, define $t_k=t+k\Delta$. Let $u\coloneqq t_0,t_1,\ldots,t_n$. Finally, define a piecewise-constant approximation $Q_\tau^u$ to $Q_\tau$ such that, for all $k\in\{1,\ldots,n\}$,
\begin{equation}\label{eq:lower_char_matrix_linear_approx}
Q_\tau^u \coloneqq Q_{t_k},\quad\text{for all $\tau\in (t_{k-1},t_k]$}\,.
\end{equation}
Let $Q_\tau^u\coloneqq Q_{t_0}$ for all $\tau\leq t_0$ and let $Q_\tau^u\coloneqq Q_{t_n}$ for all $\tau>t_n$.

{\bf ATTN:} *** $Q_\tau^u$ is left-continuous, so the statement below may need some work. ***

Then, by Theorem~\ref{theorem:continuous_rate_matrix_has_process}, there is some $P\in\mprocesses_\mathcal{Q}$ that is characterized by $Q_\tau^u$, with corresponding transition matrix $T_t^s$. It remains to show that $\norm{L_t^sf - T_t^sf}<\epsilon$. We have
\begin{align*}
\norm{L_t^sf - T_t^sf} &= \norm{L_{t_0}^{t_n}f - T_{t_0}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{{T_{t_0}^{t_{n-1}}}}\cdot\norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \,.
\end{align*}
Recursion on the first summand yields
\begin{align*}
\norm{L_t^sf - T_t^sf} &\leq \norm{L_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-2}}\left[L_{t_{n-2}}^{t_n}f\right] - T_{t_0}^{t_{n-2}}\left[L_{t_{n-2}}^{t_n}f\right]} \\
 &\quad\quad\quad+ \norm{L_{t_{n-2}}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_{n-2}}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
&\vdots \\
 &\leq \sum_{i=1}^{n} \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,.
\end{align*}
For all $i\in\{1,\ldots,n\}$, we have using Lemma~\ref{lemma:justthelinearpart} and the facts that $\Delta<\delta\leq\nicefrac{1}{\norm{\lrate}}$ and $\norm{\lrate}\leq\norm{\rateset}$,
\begin{align*}
&\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - \left[I+\Delta\lrate\right]L_{t_i}^{t_n}f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \norm{L_{t_{i-1}}^{t_i} - \left[I+\Delta\lrate\right]}\cdot\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \Delta^2\norm{\lrate}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,.
\end{align*}
By Equation~\eqref{eq:lower_char_rate_matrix}, we have $\norm{Q_{t_i}L_{t_i}^{t_n}f - \lrate L_{t_i}^{t_n}f} < \nicefrac{\epsilon}{2C}$. Furthermore, by Equation~\eqref{eq:lower_char_matrix_linear_approx}, we have $Q_{t_i}^u=Q_{t_i}$, so that
\begin{align*}
 &\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \norm{\left[I+\Delta \lrate\right]L_{t_i}^{t_n}f - \left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f} \\
 &= \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \Delta\norm{\lrate L_{t_i}^{t_n}f - Q_{t_i}L_{t_i}^{t_n}f} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \frac{\Delta\epsilon}{2C} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right] - T_{t_{i-1}}^{t_i}}\cdot\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &\leq 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C}\,,
\end{align*}
where the last step used Lemma~\ref{lemma:bound_precise_linear_approx}.

Thus, using the fact that $n\Delta=C$, we find
\begin{align*}
\norm{L_t^sf - T_t^sf} &\leq \sum_{i=1}^n \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \sum_{i=1}^n 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &= 2n\Delta^2\norm{\mathcal{Q}}^2\norm{f} + n\frac{\Delta\epsilon}{2C}\\
 &= 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2}\,,
\end{align*}
so that by Equation~\eqref{eq:delta_required_for_tight_bound} and the fact that $\Delta<\delta$, we have
\begin{equation*}
\norm{L_t^sf - T_t^sf} \leq 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} < 2C\delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\,.
\end{equation*}
\end{proof}

Together, Theorems~\ref{theorem:nonmarkov_single_var_lower_bounded} and~\ref{theorem:lower_markov_bound_is_tight} establish a strong connection between $L_t^s$ and lower expectations $\underline{\mathbb{E}}$. In particular, for functions defined on a single point in time, they turn out to be equivalent, as shown by the result below.

\begin{corollary}\label{cor:lower_operator_is_infimum}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $g\in\gambles(\states^{\{s\}})$ and $x\in\states$,
\begin{equation*}
\left[L_t^sg\right](x) = \underline{\mathbb{E}}^\mathrm{M}[g(X_s)\,\vert\,X_t=x]\,,%\inf\Bigl\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\Bigr\}\,.
\end{equation*}
and furthermore,
\begin{equation*}
\left[L_t^sg\right](x) = \underline{\mathbb{E}}[g(X_s)\,\vert\,X_t=x]\,.%\inf\Bigl\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\Bigr\}\,.
\end{equation*}
\end{corollary}
\begin{proof}
We start by proving the first statement. By Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} and Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, we have that for all $P\in\mprocesses_\rateset$ it holds that $\left[L_t^sg\right](x) \leq \mathbb{E}[g(X_s)\,\vert\,X_t=x]$. Furthermore, by Theorem~\ref{theorem:lower_markov_bound_is_tight}, for all $\epsilon\in\realspos$ there is some $P\in\mprocesses_\rateset$ such that $\norm{L_t^sg - T_t^sg} < \epsilon$. Together this implies, using Definition~\ref{def:lower_markov}, that
\begin{equation*}
\left[L_t^sg\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\right\} = \underline{\mathbb{E}}^{\mathrm{M}}\left[g(X_s)\,\vert\,X_t=x\right]\,.
\end{equation*}

We now move on to the second statement. By Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} we have that for all $P\in\processes_\rateset$ it holds that $\left[L_t^sg\right](x) \leq \mathbb{E}[g(X_s)\,\vert\,X_t=x]$.
Furthermore, by Theorem~\ref{theorem:lower_markov_bound_is_tight} and Proposition~\ref{prop:markov_set_subset_of_nonmarkov_set}, for all $\epsilon\in\realsnonneg$ there is some $P\in\processes_\rateset$ such that
$\norm{L_t^sg - T_t^sg} < \epsilon$.
Together this implies, using Definition~\ref{def:lower_non_markov}, that
\begin{equation*}
\left[L_t^sg\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\processes_\rateset\right\} = \underline{\mathbb{E}}\left[g(X_s)\,\vert\,X_t=x\right]\,.
\end{equation*}
\end{proof}

%\begin{definition}\label{def:lower_expectation}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices with separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $f\in\gamblesX$, we define the \emph{lower expectation of $f$ at time $s$, given the state at time $t$, with respect to $\mprocesses_\rateset$}, as
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}\left[f(X_s)\,\vert\,X_t\right]\coloneqq L_t^sf\,.
%\end{equation*}
%\end{definition}

One somewhat unsurprising result is therefore that $L_t^s$ computes the lower expectation of functions $f\in\gamblesX$ with respect to sets of Markov processes $\mprocesses_\rateset$. The reason that this is to be expected is the previously established correspondence between $L_t^s$ and the solution of the differential equation introduced in {\bf DAMJANREF}, which was there shown to compute exactly this quantity.

A rather more surprising result, perhaps, is that this \emph{same} operator also computes lower expectations with respect to sets $\processes_\rateset$ of non-Markov processes.

*** misschien nog iets over dat de precieze non-Markov dingen normaal als ``moeilijk'' gezien worden? ***

Recall from Section~\ref{sec:connections_rate} that for a given lower transition rate operator $\lrate$, its set of dominating rate matrices $\rateset_{\lrate}$ is---by definition---the largest set of rate matrices which has $\lrate$ as its lower envelope. Our next result shows that the set $\wprocesses_{\rateset_{\lrate}}$ of well-behaved stochastic processes consistent with $\rateset_{\lrate}$ is the largest set of stochastic processes which has $L_t^s$ as its lower envelope.

\begin{theorem}
Let $\lrate$ be an arbitrary lower transition rate operator, with $\rateset_{\lrate}$ its set of dominating rate matrices. Then, for all $t,s\in\realsnonneg$ such that $t<s$, all $f\in\gamblesX$, and all $x\in\states$,
\begin{equation*}
\left[L_t^sf\right](x) = \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x]\,:\,P\in\wprocesses_{\rateset_{\lrate}}\right\}\,.
\end{equation*}
Furthermore, for all $P\in\processes$ such that $P\notin\wprocesses_{\rateset_{\lrate}}$, there is some $f\in\gamblesX$, and some $t,s\in\realsnonneg$ with $t<s$, such that for some $x\in\states$,
\begin{equation*}
\mathbb{E}\left[f(X_s)\,\vert\,X_t=x\right] < \left[L_t^sf\right](x)\,.
\end{equation*}
\end{theorem}
\begin{proof}
By Propositions~\ref{prop:dominating_nonempty_bounded} and \ref{prop:dominatingproperties}, the set $\rateset_{\lrate}$ is non-empty, bounded, has separately specified rows, and by definition has $\lrate$ as its corresponding lower transition rate operator. The first statement therefore follows directly from Corollary~\ref{cor:lower_operator_is_infimum}.

For the second statement, take any $P\in\processes$ such that $P\notin\wprocesses_{\rateset_{\lrate}}$. Clearly, we must have that either $P\in\wprocesses$ or $P\notin\wprocesses$. We will start by assuming the former, and show that the statement then follows.

Because $P\notin\wprocesses_{\rateset_{\lrate}}$, by Definition~\ref{def:set_non_markov_process}, there must be some $t\in\realsnonneg$ such that $\smash{\overline{\partial}}T_{t,x_u}^{t}\nsubseteq\rateset_{\lrate}$. Furthermore, by Proposition~\ref{prop:boundednon-emptyandclosed}, we have that $\smash{\overline{\partial}}T_{t,x_u}^{t}\neq\emptyset$. Hence, there must be some rate matrix $Q\in\smash{\overline{\partial}}T_{t,x_u}^{t}$ such that $Q\notin\rateset_{\lrate}$, which by Equation~\eqref{eq:dominatingratematrices} implies that there is some $f\in\gamblesX$ and some $x\in\states$ such that $\left[Qf\right](x)<\left[\lrate f\right](x)$.

Now, assume \emph{ex absurdo} that the statement is false. Because $Q\in\smash{\overline{\partial}}T_{t,x_u}^{t}$, we must have that either $Q\in\smash{\overline{\partial}}_+T_{t,x_u}^{t}$ or $Q\in\smash{\overline{\partial}}_-T_{t,x_u}^{t}$. Assume its the former. Then, by Equation~\ref{eq:rightouterderivative}, there is some $\{\Delta_i\}_{i\in\nats}\to0^+$ such that $\lim_{i\to+\infty}\nicefrac{1}{\Delta_i}(T_{t,x_u}^{t+\Delta_i}-I)=Q$. Because we assumed that the claim is false, we must have that for all $\Delta\in\realspos$, it holds that $\left[L_t^{t+\Delta}f\right](x) \leq \left[T_{t,x_u}^{t+\Delta}f\right](x)$,
or equivalently, that
\begin{equation*}
\frac{1}{\Delta}\left[(L_t^{t+\Delta} - I)f\right](x) \leq \frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)f\right](x)\,.
\end{equation*}
Taking limits on both sides with respect to $\{\Delta_i\}_{i\in\nats}$, we find that
\begin{align*}
\lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(L_t^{t+\Delta_i} - I)f\right](x) &\leq \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(T_{t,x_u}^{t+\Delta_i} - I)f\right](x) \\
\left[\lrate f\right](x) &\leq \left[Qf\right](x)\,,
\end{align*}
where the left-hand limit follows from Proposition~\ref{prop:lower_transition_has_deriv}. This contradicts the earlier observation that $\left[Qf\right](x)<\left[\lrate f\right](x)$. Hence, if the statement is indeed false, we must instead have that $Q\in\smash{\overline{\partial}}_-T_{t,x_u}^{t}$. In that case, there is some other $\{\Delta_i\}_{i\in\nats}\to0^+$, such that $\lim_{i\to+\infty}\nicefrac{1}{\Delta_i}(T_{t-\Delta_i,x_u}^{t}-I)=Q$, and similarly, we should have for all $\Delta\in\realspos$ that
\begin{equation*}
\frac{1}{\Delta}\left[(L_{t-\Delta}^{t} - I)f\right](x) \leq \frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^{t} - I)f\right](x)\,.
\end{equation*}
By Proposition~\ref{prop:lower_transition_is_homogeneous}, we have that $L_{t-\Delta}^{t}=L_{t}^{t+\Delta}$, and so after taking limits on both sides,
\begin{align*}
\lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(L_t^{t+\Delta_i} - I)f\right](x) &\leq \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(T_{t-\Delta_i,x_u}^{t} - I)f\right](x) \\
\left[\lrate f\right](x) &\leq \left[Qf\right](x)\,,
\end{align*}
we again find a contradiction. Therefore, if $P\in\wprocesses$, the statement follows.

Now, suppose that instead $P\notin\wprocesses$. Assume \emph{ex absurdo} that the statement is false. Because $P\notin\wprocesses$, by Definition~\ref{def:well-behaved} there must be some $t\in\realsnonneg$ and some $x,y\in\states$ such that either 
\begin{equation*}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t+\Delta}=y\,\vert\,X_t=x)-\delta_{xy}}\nless+\infty\,,
\end{equation*}
or,
\begin{equation*}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t}=y\,\vert\,X_{t-\Delta}=x)-\delta_{xy}}\nless+\infty\,.
\end{equation*}
Start by assuming that its the former. Clearly, we have for all $\Delta\in\realspos$ that $\left[T_t^{t+\Delta}\ind{y}\right](x)=P(X_{t+\Delta}=y\,\vert\,X_t=x)$, and furthermore, that $\left[I\ind{y}\right](x)=\delta_{xy}$. Hence, we have that
\begin{equation*}
\left[(T_t^{t+\Delta}-I)\ind{y}\right](x) = P(X_{t+\Delta}=y\,\vert\,X_t=x) - \delta_{xy}\,.
\end{equation*}
By Proposition~\ref{prop:rate_from_stochastic_matrix}, we have that $\nicefrac{1}{\Delta}(T_t^{t+\Delta}-I)$ is a transition rate matrix, and hence by Definition~\ref{def:rate_matrix} we must have that $\left[(T_t^{t+\Delta}-I)\ind{y}\right](y)\leq 0$, and $\abs{\left[(T_t^{t+\Delta}-I)\ind{y}\right](x)}\leq\abs{\left[(T_t^{t+\Delta}-I)\ind{y}\right](y)}$. It therefore follows that also
\begin{equation}\label{eq:nonwellbehaved_limit_diagonal}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{\left[(T_t^{t+\Delta}-I)\ind{y}\right](y)} \nless +\infty\,.
\end{equation}
Similarly, by Proposition {\bf ADD-THIS}, we have that $\nicefrac{1}{\Delta}(L_t^{t+\Delta}-I)$ is a lower transition rate operator, and hence by Definition~\ref{def:coh_low_trans_rate}, we must have that $\left[\nicefrac{1}{\Delta}(L_t^{t+\Delta}-I)\ind{y}\right](y)\leq 0$. 

*** That last claim kinda still needs a lemma somewhere ***

Because we assumed that the statement is false, it must hold for all $\Delta\in\realspos$ that
\begin{equation*}
\frac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{y}\right](y) \leq \frac{1}{\Delta}\left[(T_{t}^{t+\Delta} - I)\ind{y}\right](y)\,.
\end{equation*}
From our earlier observations, we know that both sides of this inequality are negative. Hence, we must have that
\begin{equation*}
\abs{\frac{1}{\Delta}\left[(T_{t}^{t+\Delta} - I)\ind{y}\right](y)} \leq \abs{\frac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{y}\right](y)}\,.
\end{equation*}
By the definition of the norm, we have that $\abs{\nicefrac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{y}\right](y)}\leq\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)\ind{y}}$, and because $\ind{y}\in\gamblesX$ and $\norm{\ind{y}}=1$, we have $\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)\ind{y}}\leq \norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)}$. Hence, we must have that
\begin{equation*}
\abs{\frac{1}{\Delta}\left[(T_{t}^{t+\Delta} - I)\ind{y}\right](y)} \leq \norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)}\,.
\end{equation*}

Now, by Proposition~\ref{prop:lower_transition_has_deriv}, we have that $\lim_{\Delta\to0^+}\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I) = \lrate$, and hence it follows that $\lim_{\Delta\to0^+}\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate}$. By Proposition~\ref{lem:normlratefinite}, we furthermore know that $\norm{\lrate}<+\infty$. Taking limits on both sides of our earlier inequality, we therefore find that
\begin{align*}
\limsup_{\Delta\to0^+}\abs{\frac{1}{\Delta}\left[(T_{t}^{t+\Delta} - I)\ind{y}\right](y)} &\leq \limsup_{\Delta\to0^+}\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)} \\
\limsup_{\Delta\to0^+}\abs{\frac{1}{\Delta}\left[(T_{t}^{t+\Delta} - I)\ind{y}\right](y)} &\leq \norm{\lrate} < +\infty\,,
\end{align*}
which, by Equation~\eqref{eq:nonwellbehaved_limit_diagonal}, is a contradiction.

*** need to add proof for left-sided. works with a similar argument ***
\end{proof}

*** lalala sterker nog, zoals we in de volgende sectie gaan zien, kunnen we met $L_t^s$ dingen uitrekenen voor niet-Markov sets die niet kunnen voor Markov sets ***

\section{Relation to Previous Work}\label{sec:prev_work}

*** Taking $\lrate$ as lower envelope of $\rateset$, $L_t^s$ corresponds to solution of Damjan's differential equation ***

*** Can therefore be computed relatively easily ***

\section{Functions Defined on Multiple Time Points}\label{sec:funcs_multi_time_points}

%\subsection{wat notatie voor meerdere variabelen}\label{sec:imp_non_markov}

%We will now move on to consider non-Markovian stochastic processes. Due to the need to consider multiple time points in such a process' history, we will find it convenient to switch from a transition-matrix notation to using (conditional) expectation operators. 
%We will start by introducing some notation for functions which makes explicit the time points that we are considering.

%\begin{definition}
%Consider any stochastic process $P\in\processes$, any $t,t',s\in\realsnonneg$ such that $t<t'<s$, any $u\in\mathcal{U}_{[t,t']}$, and any $f\in\gambles(\states^{u\cup\{s\}})$. Then, the \emph{expectation of $f$ at time $s$ given the states at times $u$}, with respect to $P$, is a map $\mathbb{E}$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, defined for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, as
%\begin{equation*}
%\mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \coloneqq \sum_{x_s\in\states^{\{s\}}} f(x_{t_0},\ldots,x_{t_n},x_s)P(X_s=x_s\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n})\,.
%\end{equation*}
%\end{definition}
%\noindent Note that $\gambles(\states^{\{s\}})\subset\gambles(\states^{u\cup\{s\}})$, so that for any $g\in\gambles(\states^{\{s\}})$,
%\begin{equation*}
%\mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \sum_{x_s\in\states^{\{s\}}} g(x_s)P(X_s=x_s\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n})\,.
%\end{equation*}

% \begin{definition}
% Consider any $t,t',s\in\realsnonneg$ such that $t<t'<s$, any $u\in\mathcal{U}_{[t,t']}$, any $g\in\gambles(\states^{\{s\}})$ and any $f\in\gambles(\states^{u\cup\{s\}})$. Let $\rateset$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. We will define the \emph{lower expectation operator} $\underline{\mathbb{E}}$, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, as
% \begin{equation*}
% \underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \coloneqq \left[L_{t_n}^sg\right](x_{t_n})\,,
% \end{equation*}
% and,
% \begin{equation*}
% \underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \coloneqq \left[L_{t_n}^sf(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
% \end{equation*}
% \end{definition}

Having shown that the operator $L_t^s$ computes lower expectations for functions defined on a single point in time, we will now turn our attention to functions defined on multiple time points. Because we are considering \emph{conditional} expectations, where the conditioning is done with respect to states in a (non-)Markov chain's history, it makes sense to distinguish between two different classes of functions defined at multiple time points. 

First, in Section~\ref{sec:function_single_future_multiple_past}, we will consider functions defined on a single time point in a chain's future, and multiple time points in the chain's history. Thus, we will consider functions $f\in\gambles(\states^{u\cup\{s\}})$, and lower expectations of the form
\begin{equation*}
\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
We will see that it is a straightforward implication of our results from Section~\ref{sec:single_var_lower_exp} that such lower expectations are computable using $L_t^s$, both with respect to sets of Markov chains and with respect to sets of non-Markov chains.

In Section~\ref{sec:decomposition} we will generalize this to functions defined on multiple time points in a chain's future and history, considering functions $f\in\gambles(\states^{u\cup v})$ and lower expectations of the form
\begin{equation*}
\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
As we will see, for functions of this kind the lower expectations with respect to sets of non-Markov chains no longer correspond to those taken with respect to sets of Markov chains. One of the main results of this paper, however, is that we can still use the operator $L_t^s$ to compute lower expectations of this form if taken with respect to sets of non-Markov chains. We will see that this is because the optimization problem involved in computing lower expectations in some sense becomes simpler when we drop the Markov assumption.

Finally, in Section~\ref{sec:tractability}, we will show that although $L_t^s$ provides us with a way to compute such lower expectations, doing so for general functions $f\in\gambles(\states^{u\cup v})$ is still computationally intractable. However, we then provide algorithms to tractably compute lower expectations for large and practically useful subclasses of $\gambles(\states^{u\cup v})$.

\subsection{Multi-Variable Functions on a Single Point in the Future}\label{sec:function_single_future_multiple_past}

We start by considering functions $f\in\gambles(\states^{u\cup\{s\}})$ defined on a single time point $s$ in a (non-)Markov chain's future, and multiple time points $u=t_0,\ldots,t_n$ in a chain's history. Recall our notation from Section~\ref{sec:multivar_notation}; we write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ for a specific state assignment $(x_{t_0},\ldots,x_{t_n})$, and have defined
\begin{equation*}
\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n}) \equiv \left[L_t^sf(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
\end{equation*}
The following results are now direct implications of our results from Section~\ref{sec:single_var_lower_exp}.
\begin{proposition}\label{prop:multi_var_single_future_bounded}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for all $P\in\processes_\rateset$, all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]\,.
\end{equation*}
\end{proposition}
\begin{proof}
Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we therefore have to show that $\left[L_t^sg\right](x_{t_n}) \leq \mathbb{E}\left[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]$. Because $g\in\gambles(\states^{\{s\}})$, this inequality holds by Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
\end{proof}

\begin{proposition}\label{prop:multi_var_single_future_tight}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $\epsilon\in\realspos$, there is a $P\in\mprocesses_\rateset$ such that
\begin{equation*}
\abs{\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n}) - \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]} < \epsilon\,.
\end{equation*}
\end{proposition}
\begin{proof}
Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we now have to show that there is a $P\in\mprocesses_\rateset$ such that $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} < \epsilon$. 

Because $g\in\gambles(\states^{\{s\}})$, by Theorem~\ref{theorem:lower_markov_bound_is_tight} there must be some $P\in\mprocesses_\rateset$ such that $\norm{L_t^sg - \mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon$. Consider this $P$. By the Markov property, its expectation must satisfy $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} = \abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]}$, and by the definition of the norm, we have 
\begin{equation*}
\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]} \leq \norm{L_t^sg-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon\,.
\end{equation*}
\end{proof}

Note that this result is weaker than the corresponding Theorem~\ref{theorem:lower_markov_bound_is_tight} in Section~\ref{sec:single_var_lower_exp}. Specifically, this says that for a given history $(x_{t_0},\ldots,x_{t_n})\in\states^u$, there is a $P\in\mprocesses_\rateset$ that approaches $\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n})$. This does not imply that there is a $P\in\mprocesses_\rateset$ that approaches $\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n})$ for \emph{all} histories! 

We will see in Section~\ref{sec:decomposition} that this is exactly the reason that computing lower expectations with respect to sets of non-Markov processes is ``easy''; by dropping the Markov assumption, the corresponding optimization problems become solvable locally with respect to a given history. In contrast, the optimization over sets of Markov processes must there be done globally with respect to all possible histories, because they do not allow for minimizing selections specific to a given trajectory.

For our present purposes, Proposition~\ref{prop:multi_var_single_future_tight} is still strong enough to imply the following result.

%\begin{proposition}
%In essentie, voor alle $f\in\gambles(\states^{u\cup\{s\}})$ en alle $\epsilon\in\realspos$, is er een $P\in\mprocesses_\rateset$ zodat
%\begin{equation*}
%\norm{L_t^s f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%{\bf TODO} This is immediate.
%\end{proof}

\begin{corollary}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_t^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^{\mathrm{M}}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]
\end{equation*}
and furthermore,
\begin{equation*}
\left[L_t^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]
\end{equation*}
\end{corollary}
\begin{proof}
This is a direct consequence of Propositions \ref{prop:markov_set_subset_of_nonmarkov_set}, \ref{prop:lower_exp_markov_bounded_by_nonmarkov}, \ref{prop:multi_var_single_future_bounded}, and \ref{prop:multi_var_single_future_tight}. The proof is similar to that of Corollary~\ref{cor:lower_operator_is_infimum}.
\end{proof}
Thus, we see that the operator $L_t^s$ can also be used to compute lower expectations of functions $f\in\gambles(\states^{u\cup\{s\}})$, both with respect to sets of Markov processes and with respect to sets of non-Markov processes. We will now turn to functions defined on multiple time points in a process' future, where we will find this correspondence to no longer hold.

%\begin{theorem}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\processes_\rateset$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $f\in\gambles(\states^{u\cup\{s\}})$,
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \leq \mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Consider any $P\in\processes_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, any $f\in\gambles(\states^{u\cup\{s\}})$, and any $(x_{t_0},\ldots,x_{t_n})\in\states^u$.
%
%Note that
%\begin{equation*}
%f(x_{t_0},\ldots,x_{t_n},X_s)\in\gambles(\states^{\{s\}})
%\end{equation*}
%is the restriction of $f\in\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{s\}})$. Hence, we can define a function $g\in\gambles(\states^{\{s\}})$ as
%\begin{equation*}
%g(x_s) \coloneqq f(x_{t_0},\ldots,x_{t_n},x_s)\,,\quad\quad\text{for all $x_s\in\states^{\{s\}}$}\,.
%\end{equation*}
%Then,
%\begin{equation*}
%\mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,,
%\end{equation*}
%and
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%Thus, it remains to show that
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \leq \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%Because $g\in\gambles(\states^{\{s\}})$, this was shown to hold by Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
%\end{proof}

%\begin{theorem}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for any $u\in\mathcal{U}_{[0,t]}$, any $g\in\gambles(\states^{\{s\}})$, and any $\epsilon\in\realspos$, there is some $P\in\processes_\rateset$ such that $P$ is Markovian, i.e. also $P\in\mprocesses_\rateset$, and furthermore,
%\begin{equation*}
%\norm{\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Because $g\in\gambles(\states^{\{s\}})$, we have by Definition~{\bf REF} that
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] = \underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_n}] = L_{t_n}^s g\,.
%\end{equation*}
%Furthermore, by Theorem~{\bf REF} there is some $P\in\mprocesses_\rateset$ with
%\begin{equation*}
%\mathbb{E}_{X_{s}}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] =  \mathbb{E}_{X_{s}}[g(X_s)\,\vert\,X_{t_n}] = T_{t_n}^sf
%\end{equation*}
%by virtue of the Markov property, and
%\begin{equation*}
%\norm{\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_{s}}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}]} = \norm{L_{t_n}^sg - T_{t_n}^sg}< \epsilon\,.
%\end{equation*}
%Because $P\in\mprocesses_\rateset$ and $\mprocesses_\rateset\subseteq\processes_\rateset$, we have $P\in\processes_\rateset$, which concludes the proof.
%\end{proof}

\subsection{Multi-Variable Functions on Multiple Points in the Future}\label{sec:decomposition}


We now consider functions $f\in\gambles(\states^{u\cup v})$, where $u=t_0,\ldots,t_n$ is a sequence of time points in a process' history, and $v=s_0,\ldots,s_m$ is a sequence of time points in a process' future; hence, we assume $s_0>t_n$. As the next example shows, the lower expectation of such functions, when taken with respect to a set $\mprocesses_\rateset$ of Markov processes, no longer necessarily corresponds to the lower expectation taken with respect to a set $\processes_\rateset$ of non-Markov processes.

\begin{exmp}
{\bf TODO} Example that sometimes $\underline{\mathbb{E}}^\mathrm{M}\neq \underline{\mathbb{E}}$ for functions $f\in\gambles(\states^{u\cup v})$.
\exampleend
\end{exmp}

An obvious question is therefore what the operator $L_t^s$ computes for functions of this form, as it clearly cannot compute both $\underline{\mathbb{E}}^\mathrm{M}$ and $\underline{\mathbb{E}}$. As we will see below, it turns out that we can use $L_t^s$ to compute the lower expectation of such functions with respect to sets of non-Markov processes. We start by showing that, using a composition of these operators, we can compute a lower bound with respect to a set $\processes_\rateset$.
\begin{proposition}\label{prop:multivar_bounded}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for all $P\in\processes_\rateset$, all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{proposition}
\begin{proof}
By the basic properties of expectation, we can decompose the expectation functional as
\begin{align*}
 &\quad \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
 &= \mathbb{E}\bigl[\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s_0,\ldots,s_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
Note that the nested expectation is conditioned on the time points $t_0,\ldots,t_n,s_0,\ldots,s_{m-1}$; it therefore only computes an expectation on a single point $s_m$ in the future. Hence, by Proposition~\ref{prop:multi_var_single_future_bounded} and the fact that the (outer) expectation computes a convex combination of values, we have
\begin{align*}
&\quad\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
 &\geq \mathbb{E}\bigl[[L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_{m-1}})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
The proof is then finished by backward induction on $m$.
\end{proof}

Note that a direct implication of this, together with Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, is that $L_t^s$ can also be used to compute lower bounds on the expectation with respect to a set $\mprocesses_\rateset$ of Markov processes. However, this bound will then in general not be tight, and hence will not correspond to the lower expectation.

To see why, observe that in the term $[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n})$, the operators $L_{s_{i-1}}^{s_i}$ can take on different values depending on the choice of $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$. Hence, to approach these values of $L_{s_{i-1}}^{s_i}$ from within a set $\mprocesses$ of Markov processes, we have to be able to pick the approximating values such that they depend on the specific trajectory $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$, and this is exactly what the Markov condition prevents us from doing. As the next result shows, we can however approach this quantity from within a set $\processes_\rateset$ of non-Markov processes.

\begin{proposition}\label{prop:multivar_bound_tight}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$, all $v\in\mathcal{U}_{[s,s']}$, all $f\in\gambles(\states^{u\cup v})$, and all $\epsilon\in\realspos$, there is a $P\in\processes_\rateset$ such that
\begin{equation*}
\norm{L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
\end{equation*}
\end{proposition}
\begin{proof}
{\bf TODO} *** deze is nog een beetje ingewikkeld, we moeten weer een process bouwen en laten zien dat die in $\processes_\rateset$ zit ***
\end{proof}
\begin{corollary}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{corollary}
\begin{proof}
This is a direct consequence of Propositions~\ref{prop:multivar_bounded} and \ref{prop:multivar_bound_tight}.
\end{proof}

%\begin{theorem}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for any $u\in\mathcal{U}_{[0,t]}$, any $f\in\gambles(\states^{u\cup\{s\}})$, and any $\epsilon\in\realspos$, there is some $P\in\processes_\rateset$ such that
%\begin{equation*}
%\norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Choose any $u\in\mathcal{U}_{[0,t]}$, any $f\in\gambles(\states^{u\cup\{s\}})$, and any $\epsilon\in\realspos$.
%
%Define a \emph{lower-characterizing rate matrix} $Q_{x_{t_0},\ldots,x_{t_n},\tau}$ as
%\begin{equation*}
%Q_{x_{t_0},\ldots,x_{t_n},\tau}(x_\tau,\cdot) \coloneqq \argmin\bigl\{Q(x_\tau,\cdot)\left[\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{\tau}]\right]\,:\,Q(x_\tau,\cdot)\in\rateset_{x_\tau}\bigr\}\,,
%\end{equation*}
%for all $x_\tau\in\states^{\{\tau\}}$, all $\tau\in[t,s]$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$. Note that the $\argmin\{\cdot\}$ may be set-valued, in which case take an arbitrary element. Furthermore, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, let $Q_{x_{t_0},\ldots,x_{t_n},\tau}\coloneqq Q_{x_{t_0},\ldots,x_{t_n},t}$ for all $\tau\in[0,t]$, and let $Q_{x_{t_0},\ldots,x_{t_n},\tau}\coloneqq Q_{x_{t_0},\ldots,x_{t_n},s}$ for all $\tau>s$.
%
%Let $C\coloneqq (s-t)$, define
%\begin{equation*}
%\delta\coloneqq \min\left\{\frac{\epsilon}{2C\norm{\rateset}^2\norm{f}}, \frac{1}{\norm{\lrate}}\right\}\,,
%\end{equation*}
%and choose any $m>\nicefrac{C}{\delta}$. Then, for $\Delta\coloneqq\nicefrac{C}{m}$, we have $\Delta<\delta$.
%
%For all $k\in\{0,\ldots,m\}$, define $\tau_k\coloneqq t+k\Delta$. Let $v\coloneqq \tau_0,\tau_1,\ldots,\tau_m$. Finally, define for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$ a piecewise-constant approximation $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v$ to $Q_{x_{t_0},\ldots,x_{t_n},\tau}$ such that, for all $k\in\{1,\ldots,m\}$,
%\begin{equation*}
%Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\coloneqq Q_{x_{t_0},\ldots,x_{t_n},\tau_k}\,,\quad\text{for all $\tau\in(\tau_{k-1},\tau_k]$}\,.
%\end{equation*}
%Let $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\coloneqq Q_{x_{t_0},\ldots,x_{t_n},\tau_0}$ for all $\tau\leq\tau_0$ and let $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\coloneqq Q_{x_{t_0},\ldots,x_{t_n},\tau_m}$ for all $\tau>\tau_m$.
%
%Then, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v$ is a piecewise-constant matrix which takes values $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\in\rateset$ for all $\tau\in\realsnonneg$. Hence, by Proposition {\bf REF}, there is some $P\in\processes_\rateset$ that is characterized by the collection $Q_{X_{t_0},\ldots,X_{t_n},\tau}^v$ of all $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v$. 
%
%Let $\mathbb{E}$ be the expectation operator associated with this $P$. Then,
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad + \norm{\mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]}\,, \end{align*}
%where the last step used convexity of the expectation operator and the definition of the norm. Recursion on the first summand yields
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}\bigl[\mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}\bigl[\mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\quad\quad\quad + \norm{\mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_{\tau_{m-2}}}\bigl[\mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\quad\quad\quad + \norm{ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] - \mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\quad\quad\quad + \norm{ \underline{\mathbb{E}}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}] - \mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&\vdots \\
%&\leq \sum_{i=1}^m \norm{\underline{\mathbb{E}}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr] - \mathbb{E}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr]}\,.
%\end{align*}
%For all $i\in\{1,\ldots,m\}$, we have
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr] - \mathbb{E}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr] - [I+\Delta \lrate]\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}]} \\
%&\quad\quad + \norm{[I+\Delta \lrate]\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] - \mathbb{E}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr]} \\
%&\leq 2\Delta^2\norm{\rateset}^2\norm{f}\,,
%\end{align*}
%where the last step needs a bit more detail. Thus, we find
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\leq \sum_{i=1}^m 2\Delta^2\norm{\rateset}^2\norm{f} \\
%&= 2n\Delta^2\norm{\rateset}^2\norm{f} \\
%&= 2C\Delta\norm{\rateset}^2\norm{f}\,,
%\end{align*}
%so that by Equation {\bf REF} and the fact that $\Delta<\delta$, we have
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\leq 2C\Delta\norm{\rateset}^2\norm{f} \\
%&< 2C\delta\norm{\rateset}^2\norm{f} \\
%&\leq \epsilon\,.
%\end{align*}
%\end{proof}
%
%*** kan dit weg? ***
%\begin{corollary}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $u\in\mathcal{U}_{[0,t]}$, all $g\in\gambles(\states^{\{s\}})$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \inf\Bigl\{ \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,:\, P\in\processes_\rateset \Bigr\}\,.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%This is immediate.
%\end{proof}
%
%
%*** kan dit weg? ****
%
%\begin{corollary}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $u\in\mathcal{U}_{[0,t]}$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \inf\Bigl\{ \mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,:\, P\in\processes_\rateset \Bigr\}\,.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%This is immediate.
%\end{proof}

\subsection{Algoritmes}\label{sec:tractability}

In the sequel, for a given $f\in\gambles(\states^{u\cup v})$, we will refer to the transition operator corresponding to the distribution $P\in\mathbb{P}_\mathcal{Q}$ that satisfies Theorem~\ref{theorem:nonmarkov_multi_variable_lower_envelope} 
as $\lt_u^v$.

\begin{proposition}
For any $f\in\gambles(\states^{u\cup v})$ and given the corresponding $\lt_u^v$, computing $\left[\lt_u^v f\right](x_{t_0},\ldots,x_{t_n})$ is not more difficult than computing $\left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})$ for an arbitrary $P\in\mathbb{P}_{\mathcal{Q}}$. Specifically, this takes a number of operations that is exponential in $m$.
\end{proposition}
\begin{proof}
The first claim is immediate from the fact that $\lt_u^v$ corresponds to a $P\in\mathbb{P}_\mathcal{Q}$. The difficulty of the computation follows from the fact that it is an expectation on $m$ variables, which requires summing over all values $(x_{\tau_0},\ldots,x_{\tau_m})\in\states^v$.
\end{proof}

\begin{proposition}
Identifying the $\lt_u^v$ corresponding to a given $f\in\gambles(\states^{u\cup v})$ is intractable.
\end{proposition}
\begin{proof}
The problem is essentially that for all combinations $(x_{\tau_0},\ldots,x_{\tau_m})\in\states^v$, we need to perform an optimization to find the corresponding $\lrate$. Clearly, the number of optimizations required is exponential in $m$.
\end{proof}

\begin{proposition}
There is a subclass of gambles $\mathcal{C}(\states^{u\cup v})\subset\gambles(\states^{u\cup v})$, so that for $f\in\mathcal{C}(\states^{u\cup v})$, identifying the corresponding $\lt_u^v$ is tractable, i.e., the required number of times that $\lrate$ needs to be computed is linear in $m$.
\end{proposition}
\begin{proof}
As a trivial example, take $\mathcal{C}(\states^{u\cup v}) = \gambles(\states^{\{s'\}})$. {\bf Claim: } This class can be made a lot bigger.
\end{proof}




\section{Conclusions \& Future Work}\label{sec:conclusions}

*** I would discuss sigma-additivity things here: explain that this is possible with our approach by using Kolmogorovs extension theorem, say that this would allow us to consider for example the lower and upper expected time till absorbtion. ***

%
%\section{*** Dit mag volgens mij volledig weg ***}\label{sec:decomp}
%
%We now generalize the results from the previous sections to the lower bound of expectations on functions $f\in\gambles(\states^{u\cup v})$, where $v\in\mathcal{U}_{[s,s']}$.
%
%We again start with some notation. For any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, any $u\in\mathcal{U}_{[0,t]}$ and any $v\in\mathcal{U}_{[s,s']}$, let $A_u^v$ be a non-negatively homogeneous operator from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$. Define the norm as before, i.e.,
%\begin{equation*}
%\norm{A_u^v} \coloneqq \sup\left\{\norm{A_u^v}\,:\,f\in\gambles(\states^{u\cup v}), \norm{f}=1\right\}\,.
%\end{equation*}
%This norm satisfies all properties N6-N12 from the previous section. The default notation for sequences of time points will be $u=t_0,\ldots,t_n$ and $v=\tau_0,\ldots,\tau_m$, where we will assume that $t_n\leq \tau_0$.
%
%As a special case of such an operator, define for all $f\in\gambles(\states^{u\cup v})$ the operator
%\begin{equation*}
%T_u^vf \coloneqq \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
%\end{equation*}
%This also satisfies for all $g\in\gambles(\states^v)$ the equality
%\begin{equation*}
%T_u^vg = \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[g(X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
%\end{equation*}
%
%\begin{proposition}\label{proposition:nonmarkov_multi_variable_decompose}
%Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[s,s']}$, and any $P\in\mathbb{P}$ that has, for all $i\in\{1,\ldots,m\}$, transition operators
%\begin{equation*}
%T_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\,.
%\end{equation*}
%Then, the expectation operator $T_u^v$ corresponding to $P$ satisfies, for all $f\in\gambles(\states^{u\cup v})$,
%\begin{equation*}
%T_u^v f = T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%We will use backward induction on $m$. Note that
%\begin{equation*}
%T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} g = \mathbb{E}_{X_{\tau_m}}\left[g(X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]
%\end{equation*}
%This is immediate from the decomposition properties of expectation, i.e.,
%\begin{align*}
%&\quad T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f \\
% &= \mathbb{E}_{X_{\tau_0}}\left[\cdots\mathbb{E}_{X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\cdots\,\vert\,X_{t_0},\ldots,X_{t_n}\right] \\
% &= \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right] \\
% &= T_u^v f\,.
%\end{align*}
%\end{proof}
%
%\begin{definition}[Lower Joint Expectation Operator]\label{def:low_joint_exp_op}
%Recall the extended operator $L_u^{\{s\}}$ from Definition~\ref{def:low_full_cond_exp_op}, and define a new operator $L_u^v$ from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$, as
%\begin{equation*}
%L_u^v \coloneqq L_u^{\{\tau_0\}}L_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}.
%\end{equation*}
%\end{definition}
%
%\begin{theorem}\label{theorem:nonmarkov_multi_variable_lower_bounded}
%Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\mathbb{P}_\mathcal{Q}$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[s,s']}$, any $f\in\gambles(\states^{u\cup v})$ and any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, it holds that
%\begin{equation*}
%\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) \leq \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Consider any $(x_{t_0},\ldots,x_{t_n})\in\states^u$. Using the decomposition from Proposition~\ref{proposition:nonmarkov_multi_variable_decompose}, we find that
%\begin{align*}
%&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
%&= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
%&= \mathbb{E}_{X_{\tau_0}}\left[\cdots\mathbb{E}_{X_{\tau_m}}\left[f(x_{t_0},\ldots,x_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\cdots\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
%&= \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_{m-1}}}\left[\mathbb{E}_{X_{\tau_m}}\left[f(x_{t_0},\ldots,x_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
%&= \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{\tau_0},\ldots,x_{\tau_{m-1}})\,,
%\end{align*}
%where
%\begin{equation*}
%g(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \coloneqq \left[T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,.
%\end{equation*}
%Define a new function $g'(\cdot)$ as
%\begin{equation*}
%g'(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \coloneqq \left[L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,,
%\end{equation*}
%and note that by Theorem~\ref{theorem:nonmarkov_historic_variable_lower_bounded} we have for all $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$ that
%\begin{equation}\label{equation:nonmarkov_multiple_variable_theorem_eq}
%g'(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \leq g(x_{\tau_0},\ldots,x_{\tau_{m-1}})\,.
%\end{equation}
%Now, because
%\begin{equation*}
%\sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{\tau_0},\ldots,x_{\tau_{m-1}})
%\end{equation*}
%is a convex combination of values $g(x_{\tau_0},\ldots,x_{\tau_{m-1}})$ over all combinations $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, we have by Equation~\ref{equation:nonmarkov_multiple_variable_theorem_eq} that
%\begin{align*}
%&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
%&= \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \\
%&\geq \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g'(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \\
%&= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-2}\}}^{\{\tau_{m-1}\}}L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{align*}
%Repeatedly applying this argument, i.e. using backward induction on $m$, finally reveals
%\begin{align*}
%\left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) &= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
% &\geq \left[L_u^{\{\tau_0\}}L_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
% &= \left[L_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{align*}
%\end{proof}
%
%\begin{theorem}\label{theorem:nonmarkov_multi_variable_lower_envelope}
%Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$, there is some $P\in\mathbb{P}_\mathcal{Q}$ such that, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) = \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%*** The non-Markovian stochastic process for which, for all $i\in\{0,\ldots,{m-1}\}$ and all $(x_{\tau_0},\ldots,x_{\tau_i})\in\states^{\{\tau_0,\ldots,\tau_i\}}$,
%\begin{equation*}
%Q_{u\cup\{\tau_0,\ldots,\tau_i,\mu\}}(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_i}) \coloneqq \lrate L_\mu^{\tau_{i+1}} \left[L_{u\cup\{\tau_0,\ldots,\tau_{i+1}\}}^{\{\tau_{i+2}\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_i})
%\end{equation*}
%establishes the equality. ***
%\end{proof}


\bibliographystyle{plain} 
\bibliography{general}

\appendix

\section{Proofs of Lemmas from Section~\ref{sec:lowertrans}}\label{sec:proof_appendix}

\begin{lemma}\label{lemma:justtheindicator}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
&=\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)+\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{(I+\Delta_n\lrate)-I}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&=\Delta_n\norm{\lrate}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttrans}. By repeating this argument over and over again (actually, by induction), we find that
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
\leq \Delta_n\norm{\lrate} +\Delta_{n-1}\norm{\lrate}+\cdots
+\Delta_1\norm{\lrate}
=\Delta\norm{\lrate}.
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:justthelinearpart}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1\norm{\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I},
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttransrate}. Due to Lemma~\ref{lemma:justtheindicator}, this implies that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right).
\end{align*}
By continuing in this way (applying induction) we find that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq
\norm{\prod_{i=n}^n(I+\Delta_i\lrate)-(I+\sum_{i=n}^n\Delta_i\lrate)}
+2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
&=2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
&=2\norm{\lrate}^2\sum_{k=1}^n\Delta_k\sum_{i=k+1}^n\Delta_i\\
&\leq2\norm{\lrate}^2\frac{1}{2}\left(\sum_{k=1}^n\Delta_k\right)^2=\Delta^2\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:differencebetweennested}
For any $k\in\{1,\dots,n\}$, consider a sequence of $\Delta_{k,i}>0$, $i=1,\dots,n_k$ and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{n,k}$. Let $\Delta\coloneqq\sum_{k=1}^n\Delta_k$ and let $\alpha\coloneqq\max\{\Delta_k\colon k\in\{1,\dots,n\}\}$. If $\alpha\leq\nicefrac{1}{\norm{\lrate}}$, then
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&=\norm{\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&=\norm{
\left(
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
},
\end{align*}

\noindent
which, because of Lemma~\ref{lemma:productiscoherent}, \ref{lemma:normofcoherenttrans} and~\ref{lemma:differencenormofcoherenttrans}, implies that

\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
(I+\Delta_n\lrate)
}\\
&\leq
\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
+
\Delta_n^2\norm{\lrate}^2,
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:justthelinearpart}.

By continuing in this way (applying induction), we find that
\begin{align*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
&\leq
\Delta_1^2\norm{\lrate}^2+\cdot+\Delta_k^2\norm{\lrate}^2+\cdot
+
\Delta_n^2\norm{\lrate}^2\\
&\leq
\alpha\Delta_1\norm{\lrate}^2+\cdot+\alpha\Delta_k\norm{\lrate}^2+\cdot
+
\alpha\Delta_n\norm{\lrate}^2\\
&=
\alpha\Delta\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:lower_transition_has_deriv}]
The proposition claims that $\frac{d}{dt}L_t^s=-\lrate L_t^s$ and $\frac{d}{ds}L_t^s=L_t^s\lrate$, meaning that
\begin{equation}\label{eq:lower_deriv_backward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert <\delta)~
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation}
and
\begin{equation}\label{eq:lower_deriv_forward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert<\delta)~
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon.
\end{equation}
We start by proving Equation~\eqref{eq:lower_deriv_backward}. Choose any $\epsilon\in\realspos$, and let
\begin{equation}\label{eq:derivative_max_delta}
\delta \coloneqq \frac{\epsilon}{3\norm{\lrate}^2}.
\end{equation}
Then, consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
\begin{equation*}
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation*}
Start by rewriting the statement slightly to prevent having to consider different cases for positive and negative $\Delta$. Let $t'\coloneqq\max\{t,t+\Delta\}$, and let $\Delta'\coloneqq t'-t$. Then,
\begin{equation*}
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert = \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s}\,.
\end{equation*}
Now,
\begin{align*}
\norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &= \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^{t'}L_{t'}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}L_{t'}^s} \\
 &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}}\norm{L_{t'}^s} \\
 &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}} \\
 &= \norm{\frac{I - L_{0}^{\lvert\Delta\rvert}}{\lvert\Delta\rvert}+\lrate L_{0}^{\Delta'}} \\
 &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate L_{0}^{\Delta'}} \\
 &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate} + \norm{\lrate - \lrate L_{0}^{\Delta'}} \\
 &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + 2\norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2.
\end{align*}
Because $\Delta'$ satisfies either $\Delta'=0$ or $\Delta'=\lvert\Delta\rvert$, we have that $\Delta'\leq\lvert\Delta\rvert$. Thus,
\begin{align*}
\norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2 \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &= 3\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &< 3\delta\norm{\lrate}^2 \\
 &= \epsilon\,,
\end{align*}
where the last step used Equation~\eqref{eq:derivative_max_delta}. This concludes the proof of Equation~\eqref{eq:lower_deriv_backward}. We will now prove Equation~\eqref{eq:lower_deriv_forward}.

Again choose any $\epsilon\in\realspos$, and let $\delta$ be given by
\begin{equation*}
\delta \coloneqq \frac{\epsilon}{2\norm{\lrate}^2}\,.
\end{equation*}
Consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
\begin{equation*}
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon\,.
\end{equation*}
We again first rewrite the statement to prevent having to perform case-work in the sign of $\Delta$. Let $s'\coloneqq\min\{s,s+\Delta\}$, and let $\Delta'\coloneqq s-s'$. Then,
\begin{equation*}
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert = \norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate}\,.
\end{equation*}
Now,
\begin{align*}
\norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate} &= \norm{\frac{L_t^{s'}L_{s'}^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'}L_{s'}^{s'+\Delta'}\lrate} \\
 &\leq \norm{L_t^{s'}}\norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
 &\leq \norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
 &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{0}^{\Delta'}\lrate} \\
 &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - \lrate} + \norm{\lrate - L_{0}^{\Delta'}\lrate} \\
 &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + \norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \Delta'\norm{\lrate}^2 \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &= 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &< 2\delta\norm{\lrate}^2 \\
 &= \epsilon\,.
\end{align*}
\end{proof}

\end{document}
