\documentclass[10pt]{paper}
%\documentclass[a4paper,reqno]{amsart}
\usepackage[british]{babel}
%\usepackage[garamond]{mathdesign}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{courier}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem,multicol}
\usepackage{tikz}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
%\usepackage{hyperref}
%\usepackage{pdfsync}
%\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{exmp}{Example}%[section]
 
\renewcommand{\ttdefault}{cmtt}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

% - macros

\newcommand{\nats}{\mathbb{N}}
\newcommand{\natswith}{\nats_{0}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\realspos}{\reals_{>0}}
\newcommand{\realsnonneg}{\reals_{\geq 0}}

\newcommand{\states}{\mathcal{X}}

\newcommand{\paths}{\Omega}
%\newcommand{\path}{\omega}

\newcommand{\power}{\mathcal{P}(\paths)}
\newcommand{\nonemptypower}{\power_{\emptyset}}
\newcommand{\events}{\mathcal{E}}
%\newcommand{\nonemptyevents}{\events^{\emptyset}}
\newcommand{\filter}[1][t]{\mathcal{F}_{#1}}
\newcommand{\eventst}[1][t]{\events_{#1}}

\newcommand{\processes}{\mathbb{P}}
\newcommand{\mprocesses}{\processes^{\mathrm{M}}}

\newcommand{\hmprocesses}{\processes^{\mathrm{HM}}}

\newcommand{\wprocesses}{\processes^{\mathrm{W}}}
\newcommand{\wmprocesses}{\processes^{\mathrm{WM}}}

\newcommand{\whmprocesses}{\processes^{\mathrm{WHM}}}


\newcommand{\lt}{\underline{T}}
\newcommand{\lbound}{L}

\newcommand{\gambles}{\mathcal{L}}
\newcommand{\gamblesX}{\gambles(\states)} 

\newcommand{\ind}[1]{\mathbb{I}_{#1}}

\newcommand{\rateset}{\mathcal{Q}}
\newcommand{\lrate}{\underline{Q}}

\newcommand{\asa}{\Leftrightarrow}
\newcommand{\then}{\Rightarrow}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

\newcommand{\coloneqq}{:\!=}

\newcommand{\opinset}{\,\,\widetilde{\in}\,\,}

\newcommand{\argmin}{\arg\min}

\newcommand{\exampleend}{\hfill$\Diamond$}

\title{Imprecise Continuous-Time Markov Chains}

%\author[1]{Thomas E. Krak\thanks{t.e.krak@uu.nl}}
%\author[2]{Jasper de Bock\thanks{jasper.debock@ugent.be}}
%\affil[1]{Universiteit Utrecht}
%\affil[2]{Ghent University}

\author{Thomas Krak \and Jasper de Bock}

\begin{document}

%\author{{\bf Thomas E. Krak} \\ Utrecht}
%\address{Utrecht University}
%\curraddr{}
%\email{t.e.krak@uu.nl}
%\thanks{}

%\author{{\bf Jasper de Bock} \\ Ghent}
%\address{Ghent University}

%\author{
	%{\bf Thomas E. Krak} \quad\quad {\bf Jasper de Bock} \\
%	Utrecht University \quad Ghent University \\
	%Department of Information and Computing Sciences \\
	%Princetonplein 5, De Uithof \\
	%3584 CC Utrecht \\
	%The Netherlands \\
%	\texttt{\quad\quad t.e.krak@uu.nl} \quad\quad \texttt{jasper.debock@ugent.be}
%\and
	%{\bf Jasper de Bock} \\
%	Ghent University \\
	%SYSTeMS Research Group \\
	%Technologiepark -- Zwijnaarde 914 \\
	%9052 Zwijnaarde \\ 
	%Belgium \\
%	\texttt{jasper.debock@ugent.be}
%}
\date{}
\maketitle

\begin{abstract}
Lorem ipsum.
\end{abstract}

\section{Introduction}\label{sec:introduction}

*** wat introtekst

*** motivatie: precieze discrete-tijd markov ketens, precieze continue-tijd markov ketens, bruikbaar (en gebruikt) in verschillende domeinen (voorbeelden)

*** aanhalen eerder werk in imprecieze varianten: imprecieze discrete-tijd, imprecieze continue-tijd (werk damjan), toepassingen (werk damjan + matthias)

*** doel dit paper: wat willen we doen en waarom, wat zijn onze hoofdresultaten

*** opbouw van paper

*** {\bf lange bewijzen en lemmas die niet verhelderend zijn staan allemaal in appendix}

\section{Preliminaries}\label{sec:prelim}

We will denote the reals as $\reals$, the non-negative reals as $\realsnonneg$, and the positive reals as $\realspos$. The natural numbers will be written as $\nats$, and we will denote $\nats_0\coloneqq\nats\cup\{0\}$. The rationals will be denoted by $\mathbb{Q}$.

Throughout this work, we will consider some fixed finite \emph{state space} $\states=\{1,\dots,m\}$. Let $\gamblesX$ denote the set of all real-valued functions on $\states$. Because $\states$ is finite, we can also interpret any function $f\in\gamblesX$ as a vector in $\reals^m$. Hence, we will in the sequel use the terms `function' and `vector' interchangeably when referring to elements of $\gamblesX$.

An important type of map from $\gamblesX$ to $\gamblesX$ will be matrices. Because the domain and range of these maps is always fixed, we will simply refer to them as `matrices', with which we then always mean `square, $m\times m$, real-valued matrices'. If $A$ is a matrix, we will index its elements as $A(x,y)$ for all $x,y\in\states$, where the indexing is understood to be row-major. Furthermore, $A(x,\cdot)$ will denote the $x$-th row of $A$, and $A(\cdot,y)$ will denote its $y$-th column. The symbol $I$ will be reserved throughout to refer to the $m\times m$ identity matrix.

Infinite sequences of quantities will be denoted $\{a_i\}_{i\in\nats}$, possibly with limit statements of the form $\{a_i\}_{i\in\nats}\to c$. If this sequence is in a space endowed with an ordering relation, we may write $\{a_i\}_{i\in\nats}\to c^+$ or $\{a_i\}_{i\in\nats}\to c^-$ if the limit is approached from above or below, respectively.

Finally, we will make extensive use of finite sequences of time points. For any such sequence $u\coloneqq t_0,t_1,\ldots,t_n$, with $n\in\nats$ and $t_i\in\realsnonneg$, $i\in\{0,\ldots,n\}$, we will write $u<t$ if $\max\left\{t_i:i\in\{0,\ldots,n\}\right\}<t$ for some $t\in\realsnonneg$. For all $t\in\realsnonneg$, we will denote the set of such sequences as $\mathcal{U}_{<t}$. We will sometimes allow $u$ to be an empty sequence, to prevent having to constantly consider several cases separately. However, we will be explicit when doing so. As a special case, for any $t,s\in\realsnonneg$ such that $t<s$, we consider ordered sequences $u$ that partition the interval $[t,s]$, and include the end-points of this interval. Thus, we then have that $u$ is such that $t=t_0 < t_1 <\ldots < t_n = s$, and we will use $\mathcal{U}_{[t,s]}$ to denote the set of all such sequences. Furthermore, given any $u\in\mathcal{U}_{[t,s]}$, we define for all $i\in\{1,\ldots,n\}$ the terms $\Delta_i\coloneqq (t_i-t_{i-1})$ that denote the distance between consecutive time points in $u$, and use $\sigma(u) \coloneqq \max\bigl\{\Delta_i\,:\,i\in\{1,\ldots,n\}\bigr\}$ to denote the maximum such distance.

\subsection{Operators and Norms}\label{sec:func_oper_norm}
For any vector $f\in\gamblesX$, we let
\begin{equation*}
\norm{f}\coloneqq\norm{f}_{\infty}\coloneqq\max\{\abs {f(x)}\colon x\in\states\}
\end{equation*}
be the maximum norm. For any operator $A$ from $\gamblesX$ to $\gamblesX$ that is non-negatively homogeneous, meaning that $A(\lambda f)=\lambda A(f)$ for all $f\in\gamblesX$ and all $\lambda\geq0$, we consider the induced operator norm
\begin{equation*}
\norm{A}\coloneqq\sup\left\{\norm{Af}\colon f\in\gamblesX,\norm{f}=1\right\}.
\end{equation*}
If $A$ is a matrix, we then have that
\begin{equation*}
\norm{A}
=
\max\left\{\sum_{y\in\states}\abs{A(x,y)}\colon x\in\states\right\}.
\end{equation*}
\noindent
Finally, for any set $\mathcal{A}$ of matrices, we define $\norm{\mathcal{A}}\coloneqq\sup\{\norm{A}\colon A\in\mathcal{A}\}$.


\noindent These norms satisfy the following properties. 

\begin{proposition}\label{prop:norm_properties}
For all $f,g\in\gamblesX$, all $A,B$ from $\gamblesX$ to $\gamblesX$ that are non-negatively homogeneous, all $\lambda\in\reals$ and all $x\in\states$, we have that
\vspace{5pt}

\begin{multicols}{2}
\begin{enumerate}[label=N\arabic*:,ref=N\arabic*]
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\abs{f(x)}\leq\norm{f}$ \\
\item
$\norm{A}\geq0$
\item
$\norm{A}=0\asa A=0$
\item
$\norm{A+B}\leq\norm{A}+\norm{B}$
\item\label{N:homogeneous}
$\norm{\lambda A}=\abs{\lambda}\norm{A}$
\item\label{N:normAB}
$\norm{AB}\leq\norm{A}\norm{B}$
\item\label{N:normAf}
$\norm{Af}\leq\norm{A}\norm{f}$
\end{enumerate}
\end{multicols}
\end{proposition}

\subsection{wat notatie voor meerdere variabelen}\label{sec:multivar_notation}

*** dit kan opzich wel korter denk ik, maar kunnen we bij oppoetsen straks nog wel even checken ***

We will also find it convenient to have notation for functions defined on the state space at multiple time points.
To this end, consider any $t,s\in\realsnonneg$ such that $t<s$, and any $u\in\mathcal{U}_{[t,s]}$. We will now define
\begin{equation*}
\states^u\coloneqq \prod_{i=0}^n\states
\end{equation*}
to be the joint state space at times $t_0,\ldots,t_n$. Let $\gambles(\states^u)$ denote the set of functions on $(X_{t_0},\ldots,X_{t_n})$.

For any $t,t',s\in\realsnonneg$ such that $t<t'<s$ and any $u\in\mathcal{U}_{[t,t']}$, let $\states^{\{s\}}$ denote the state space at time $s$, and let
\begin{equation*}
\states^{u\cup\{s\}} \coloneqq \states^u\times\states^{\{s\}}
\end{equation*}
denote the joint state space at times $t_0,\ldots,t_n,s$. Similarly, for any $t,t',s,s'\in\realsnonneg$ such that $t<t'<s<s'$, any $u\in\mathcal{U}_{[t,t']}$ and $v\in\mathcal{U}_{[s,s']}$, let
\begin{equation*}
\states^{u\cup v}\coloneqq\states^u\times\states^v\,.
\end{equation*}
Let the sets of functions $\gambles(\states^{u\cup\{s\}})$ and $\gambles(\states^{u\cup v})$ be defined in the obvious manner.

For any function $f\in\gambles(\states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}})$, let the norm $\norm{f}$ be defined as
\begin{equation*}
\norm{f} \coloneqq \max\left\{ \abs{f(x_{t_0},\ldots,x_{t_n})}\,:\,(x_{t_0},\ldots,x_{t_n})\in \states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}}\right\}\,.
\end{equation*}

For the sake of brevity, we will also write a joint state assignment as
\begin{equation*}
\left(X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right)\equiv (X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})\,.
\end{equation*}

**** {\bf ATTN:} We started using $x_u\in\states^u$ as shorthand notation for the state assignment $(x_{t_0},\ldots,x_{t_n})\in\states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}}$. Still need to properly introduce that somewhere.

For a function $f\in\gambles(\states^{u\cup\{s\}})$, we will write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ specific to the joint state assignment $(x_{t_0},\ldots,x_{t_n})$. Thus, $f(x_{t_0},\ldots,x_{t_n},X_s)$ corresponds to a $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$.

Finally, we will want to use operators as defined in Section~\ref{sec:func_oper_norm} for functions defined on (multiple) explicit time points. It is clear that an operator $A$ defined on $\gamblesX$ can be applied to any element of a set of functions $\gambles(\states^{\{s\}})$ that depend on the state at just a single point in time. It remains to determine how to cope with functions defined on multiple time points. 

To avoid introducing overly complex notation, we will stipulate the following convention. If $f\in\gambles(\states^u)$ is a function defined on an ordered sequence $u$ of time points $t_0<t_1<\cdots<t_n$, and if $A$ is a non-negatively homogeneous operator from $\gamblesX$ to $\gamblesX$, we allow $A$ to be applied to $f$ by applying it to the restriction of $f$ to the state $\states^{\{t_n\}}$ at the \emph{latest} time point $t_n$ in $u$. Because such a restriction is dependent on the specific state assignment $(x_{t_0},\ldots,x_{t_{n-1}})$, the result is a function $[Af]\in\gambles(\states^{\{t_0,\ldots,t_{n-1}\}})$. In other words, we then have
\begin{equation*}
\left[Af\right](x_{t_0},\ldots,x_{t_{n-1}}) \equiv \left[A f(x_{t_0},\ldots,x_{t_{n-1}},X_{t_n})\right](x_{t_{n-1}})\,.
\end{equation*}

\subsection{Full Conditional Probabilities}\label{sec:cond_prob}

We will in Section~\ref{sec:stochastic_processes} define continuous-time stochastic processes using the framework of full conditional probabilities, which we describe here. 

Consider some arbitrary random experiment, with some (possibly infinite) \emph{possibility space} $\Omega$. An element $\omega\in\Omega$ of this set is then interpreted as one possible outcome of the random experiment. In order to describe multiple possible outcomes simultaneously, we use the notion of \emph{events}. Such an event $E$ is a subset of $\Omega$. We use $\power$ to denote the set of all events, and let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. A full conditional probability measure $P$ is then a map from the set $\power\times\nonemptypower$ of all (conditional) events to $\reals$ that quantifies the uncertainty of the outcomes of the random experiment, as follows.

\begin{definition}[Full conditional probability]\label{def:cond_prob}
A full conditional probability $P$ is a real-valued map from $\power\times\nonemptypower$ to $\reals$ that satisfies the following axioms. For all $A,B\in\power$ and all \mbox{$C,D\in\nonemptypower$}:
\vspace{5pt}

\begin{enumerate}[label=F\arabic*:,ref=F\arabic*]
\item
$P(C\vert C)=1$;\label{def:coh_prob_1}
\item
$0\leq P(A\vert C)\leq 1$;\label{def:coh_prob_2}
\item
$P(A\cup B\vert C)=P(A\vert C)+P(B\vert C)$ if $A\cap B=\emptyset$;\label{def:coh_prob_3}
\item
$P(A\vert C)=P(A\vert D)P(D\vert C)$ if $A\subseteq D\subseteq C$.\label{def:coh_prob_4}
\end{enumerate}
\vspace{5pt}

\noindent
For any $A\in\power$ and $C\in\nonemptypower$, we call $P(A\vert C)$ the probability of $A$ conditional on $C$. Also, for any $A\in\power$, we use the shorthand notation $P(A)\coloneqq P(A\vert\paths)$ and then call $P(A)$ the probability of $A$.
\end{definition}

The following notion of coherent conditional probabilities will be useful whenever we want to check if a given map $P$ from a set of events to $\reals$ is a well-defined conditional probability measure.

\begin{definition}[Coherent conditional probability]\label{def:coherence}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is said to be a \emph{coherent conditional probability} if, for all $n\in\mathbb{N}$ and every choice of $(A_i,C_i)\in\mathcal{C}$ and $\lambda_i\in\reals$, $i\in\{1,\dots,n\}$,
\begin{equation*}
\sup\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(P(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation*}
with $C_0\coloneqq\cup_{i=1}^nC_i$.
\end{definition}

\begin{theorem}\label{theo:coherentextendable}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it can be extended to a full conditional probability.
\end{theorem}

\begin{corollary}\label{corol:fullcoherent}
Let $P$ be a real-valued map from $\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it is a full conditional probability.
\end{corollary}
\begin{proof}
Trivial consequence of Theorem~\ref{theo:coherentextendable}.
\end{proof}

\subsection{Transition (Rate) Matrices}\label{sec:trans_rate_matrices}

*** some intro text, explain why these are useful, and how they are related to stochastic processes

\begin{definition}[Transition Rate Matrix]\label{def:rate_matrix}
A real-valued matrix $Q$ is said to be a \emph{transition rate matrix}, or sometimes simply \emph{rate matrix}, if

\vspace{5pt}
\begin{enumerate}[label=R\arabic*:]
\item
$\sum_{y\in\states}Q(x,y)=0$ for all $x\in\states$;
\item
$Q(x,y)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\noindent
We use $\mathcal{R}$ to denote the set of all transition rate matrices. 
\end{definition}

\noindent Clearly, $\mathcal{R}$ is closed under finite sums and multiplication with non-negative scalars. 

\begin{definition}[Transition Matrix]\label{def:stoch_matrix}
A real-valued matrix $T$ is said to be a \emph{transition matrix} if
\vspace{5pt}
\begin{enumerate}[label=S\arabic*:,ref=N\arabic*]
\item
$\sum_{y\in\states}T(x,y)=1$ for all $x\in\states$;\label{def:trans_matrix_is_stochastic}
\item
$T(x,y)\geq0$ for all $x,y\in\states$.
\end{enumerate}
\vspace{5pt}
\noindent
\end{definition}

\noindent We start by establishing some properties of the norm of these matrices. Both propositions follow trivially from the definitions.

\begin{proposition}
Let $Q$ be an arbitrary transition rate matrix. Then, $\norm{Q}<+\infty$.
\end{proposition}

\begin{proposition}
Let $T$ be an arbitrary transition matrix. Then, $\norm{T}=1$.
\end{proposition}

Consider now any set $\rateset\subseteq\mathcal{R}$ of rate matrices. Then $\rateset$ is said to be \emph{non-empty} if $\rateset\neq\emptyset$ and $\rateset$ is said to be \emph{bounded} if $\norm{\rateset}<+\infty$. The following proposition provides a simple alternative characterisation of boundedness.

\begin{proposition}\label{prop:alternativedefforbounded}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ is bounded if and only if
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset\right\}>-\infty\text{~~for all $x\in\states$.}
\end{equation*}
\end{proposition}
\begin{proof}
*** {\bf TODO}
\end{proof}

*** tralala, rate matrices and transition matrices zijn sterk gelinkt.  intuitief geven rate matrices aan hoe snel een transitie kans veranderd, in termen van $\Delta$.

\begin{proposition}\label{prop:stochastic_from_rate_matrix}
Consider any transition rate matrix $Q\in\mathcal{R}$, and any $\Delta\in\realsnonneg$ such that $\Delta \leq \nicefrac{1}{\norm{Q}}$. Then, the matrix $(I+\Delta Q)$ is a transition matrix.
\end{proposition}

\begin{proposition}\label{prop:rate_from_stochastic_matrix}
Consider any transition matrix $T$, and any $\Delta\in\realspos$. Then, the matrix $\nicefrac{1}{\Delta}(T-I)$ is a transition rate matrix.
\end{proposition}

\section{Continuous-Time Stochastic Processes}\label{sec:stochastic_processes}

We will in this section start by formalizing the notion of continuous-time stochastic processes. Next, in Section~\ref{sec:well_behaved}, we describe a specific subclass of stochastic processes, which we call well-behaved, and on which we will largely focus throughout this work. Finally, Section~\ref{sec:dynamics} provides some tools with which we can describe the dynamics of stochastic processes.

We start by defining continuous-time stochastic processes. Such a stochastic process is a construct that moves through the state space $\states$ over a continuous time line $\realsnonneg$. A specific realization or instance of this behavior will be called a path or a trajectory. The stochasticity of the process is quantified using a full conditional probability measure on a set of events, which describe specific (combinations of) states in which the process can be at certain time points. These ideas are formalized as follows.

A \emph{path} $\omega$ is a cadlag function from $\realsnonneg$ to $\states$, where cadlag means that it is right continuous for all $t\in\realsnonneg$ and that the left limit exists for all $t\in\realspos$. Let $\paths$ be the set of all paths. For any path $\omega\in\paths$ and any time point $t\in\realsnonneg$, the value of $\omega$ in $t$ is denoted by $\omega(t)$. The set $\Omega$ will form the possibility space for a full conditional probability measure. Hence, an \emph{event} is a subset $E$ of $\paths$. We again denote the set of all events by $\power$ and we let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. 

A stochastic process will be defined below as the restriction of a full conditional probability measure to a specific subset of the set $\power\times\nonemptypower$ of all (conditional) events, as follows.

For any $t\in\realsnonneg$ and $x\in\states$, we define the elementary event
\begin{equation*}
(X_t=x)\coloneqq\{\omega\in\paths\colon\omega(t)=x\}.
\end{equation*}
%We use $\events^{\mathrm{f}}$ to denote the set of all these elementary events. 
Similarly, for any finite ordered sequence of time points $u=t_1,\ldots,t_n$ with $n\in\nats_0$ and $t_i\in\realsnonneg,i\in\{1,\ldots,n\}$, and any state assignment $x_u\in\states^u$, we define the event
\begin{align*}
(X_u=x_u)\coloneqq\left(X_{t_1}=x_{t_1}, \dots, X_{t_n}=x_{t_n}\right)
\coloneqq&
%\bigcap_{i\in\{0,\dots,n\}}(X_{t_i}=x_{t_i})\\
%=&
\left\{\omega\in\paths\colon(\forall i\in\{1,\dots,n\}\,:~\omega(t_i)=x_{t_i})\right\}.%,
\end{align*}
%which we will sometimes also denote by $(X_{t_i}=x_{t_i}, i\in\{0,\dots,n\})$.
%We use $\events^{\mathrm{s}}$ to denote the set of all these events.
Consider now any $t\in\realsnonneg$. We then let $\mathcal{A}_{>t}$ be the algebra that is generated by all the elementary events $(X_s=x)$, for all $s> t$ and $x\in\states$,\footnote{This is the smallest subset of $\power$ that contains all these elementary events and that is furthermore closed under complements, finite unions and hence also finite intersections.} and we let $\mathcal{F}_{\leq t}$ be the set of all events $\left(X_{t_1}=x_{t_1}, \dots, X_{t_n}=x_{t_n}\right)$, for all finite sequences $u\leq t$ and all $x_u\in\states^u$.%, and we define $\filter\coloneqq\mathcal{C}_{\leq t}\setminus\{\emptyset\}$.

For a given $t\in\realsnonneg$, we will refer to $\mathcal{A}_{> t}$ as the set of possible future events, and to $\mathcal{F}_{\leq t}$ as the set of historic events, or possible histories.

*** Changing the definition a bit ***

For any set of events $\mathcal{E}\subseteq\power$, we use $\langle\mathcal{E}\rangle$ to denote the algebra that is generated by them. Using this notation, for any $u\in\mathcal{U}$, we let
\begin{equation*}
\mathcal{A}_u
\coloneqq
\left\langle
\left\{
(X_t=x)
\colon
x\in\states,t\in u\cup\reals_{>u}
\right\}
\right\rangle
\end{equation*}
be the algebra that is generated by the elementary events whose time point is either preceded by or belongs to $u$.

\begin{definition}[Stochastic Process]\label{def:stoch_process}
A \emph{stochastic process} is the restriction of a full conditional probability to
\begin{equation*}
\mathcal{C}^\mathrm{SP}\coloneqq\big\{
(A,X_u=x_u)
\colon
u\in\mathcal{U},~x_u\in\states^u,~A\in\mathcal{A}_u\big\}\subset\power\times\nonemptypower.
\end{equation*}
We denote the set of all such stochastic processes by $\processes$.
\end{definition}

\begin{corollary}
Let $P$ be a real-valued map from $\mathcal{C}^\mathrm{SP}$ to $\reals$. Then $P$ is a stochastic process if and only if it is a coherent conditional probability.
\end{corollary}
\begin{proof}
Trivial consequence of Theorem~\ref{theo:coherentextendable}.
\end{proof}

Because Definition~\ref{def:cond_prob} only covered finitely additive full conditional probability measures, this also restricts our definition of stochastic processes to finitely additive stochastic processes. The advantage is that this simplifies considerably the subsequent analysis, and, as shown below, the extension to a $\sigma$-additive stochastic process can always be made when this is required.

*** explain that $\sigma$-additivity is not required ***

*** discuss that a unique $\sigma$-additive extension always exists, but that for our purposes, it is not necessary to consider it ***

\begin{proposition}[\cite{berti2002coherent}, Theorem 2]
Let $P\in\processes$ be a stochastic process. Then, there exists an extension $P^*$ of $P$ to the set
\begin{equation*}
\mathcal{C}^{\mathrm{SP}*} \coloneqq \left\{(A,X_u=x_u)\colon
u\in\mathcal{U},~x_u\in\states^u,~A\in\sigma(\mathcal{A}_u)
\right\}\subset\power\times\nonemptypower\,,
\end{equation*}
where $\sigma(\mathcal{A}_u)$ is the $\sigma$-algebra generated by $\mathcal{A}_u$, such that $P^*$ is a coherent conditional probability that is $\sigma$-additive on $\mathcal{C}^{\mathrm{SP}*}$.
\end{proposition}
\begin{proof}
*** {\bf TODO}, but follows rather straightforwardly from \cite[Theorem 2]{berti2002coherent} and Lemma~\ref{lem:stoch_process_sigma_add_on_algebra}. Definitely works for a fixed $t$, maybe show that it also works for the set generated by all $t\in\realsnonneg$.
\end{proof}

\subsection{Well-behaved stochastic processes}\label{sec:well_behaved}

*** explain that stochastic process can behave in rather extreme ways. For example, the transition probabilities can jump instantaneously. In order to avoid this behavior, we restrict our attention to a subclass of stochastic processes, which we call well-behaved. ***

\begin{definition}[Well-Behaved Stochastic Process]
\label{def:well-behaved}
A stochastic process $P\in\processes$ is said to be \emph{well-behaved} if, for any time sequence $0\leq t_0<\dots<t_{n}<t$ and any set of states $x_{0},\dots,x_{n},x,y\in\states$:
\begin{equation*}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\abs{P(X_{t+\Delta}=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})-\delta_{xy}}<+\infty
\end{equation*}
and
\begin{equation*}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\abs{P(X_{t}=y\vert X_{t-\Delta}=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})-\delta_{xy}}<+\infty
\end{equation*}
The set of all well-behaved stochastic processes is denoted by $\wprocesses$.
\end{definition}

This definition of well-behavedness is related to continuity and differentiability, but stronger than the former and weaker than the latter. Example~\ref{exmp:well-behaved} below, and Example~\ref{exmp:well-behaved-no-deriv} in Section~\ref{sec:dynamics}, provide some intuition on this.

\begin{exmp}\label{exmp:well-behaved}
Suppose that a stochastic process $P\in\processes$ is such that, at some time $t\in\realsnonneg$ and for some history $x_u\in\states^u$, $u<t$, for some $x\in\states$ it satisfies for all $\Delta\in[0,1]$,
\begin{equation*}
P(X_{t+\Delta}=x\,\vert\,X_{t}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}) = 1-\sqrt{\Delta}\,.
\end{equation*}
This apparently makes sense, at least in that this probability takes values in the range $[0,1]$, and in that it holds that
\begin{equation*}
P(X_{t}=x\,\vert\,X_{t}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}) = 1\,.
\end{equation*}
Furthermore, this probability is continuous in $\Delta$. However, we clearly have that
\begin{equation*}
\frac{1}{\Delta}\abs{P(X_{t+\Delta}=x\,\vert\,X_{t}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}) - \delta_{xx}} \to +\infty\,,\quad\text{as $\Delta\to0^+$,}
\end{equation*}
and hence $P$ is not well-behaved.
\exampleend
\end{exmp}

\subsection{Process Dynamics}\label{sec:dynamics}

We will now introduce some tools to describe the behavior of stochastic processes. Rather than work with the individual probabilities, it will be convenient to jointly consider probabilities that are related by the same conditioning event. To this end, we will next introduce the notion of transition matrices corresponding to a given stochastic process.

\begin{definition}[Corresponding Transition Matrix]\label{def:trans_matrix}
Consider any stochastic process $P\in\processes$. Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, the \emph{corresponding transition matrix} $T_t^s$ is a matrix that is defined by
\begin{equation*}
T_t^s(x_t, x_s) \coloneqq P(X_s=x_s\,\vert X_t=x_t)\quad\text{for all $x_s,x_t\in\states$}\,.
\end{equation*}
We denote this family of matrices by $\mathcal{T}_P$.%, and call it the \emph{system of transition matrices} that corresponds to $P$.
\end{definition}

Because we will also want to work with conditioning events that contain more than a single time point, we furthermore introduce the following generalization.

\begin{definition}[History-Dependent Corresponding Transition Matrix]
Consider any $P\in\processes$. Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, any sequence of time points $u<t$, and any state assignment $x_u\in\states^u$, the corresponding \emph{history-dependent} transition matrix $T_{t,x_u}^s$ is a matrix that is defined by
\begin{equation*}
T^s_{t,\,x_u}(x_t,x_s)
\coloneqq
P(X_s=x_s\vert X_t=x_t, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})\quad\text{for all $x_s,x_t\in\states$}\,.
\end{equation*}
For notational convenience, we will allow $u$ to be empty, in which case we will simply have that $T_{t,x_u}^s=T_t^s$.
\end{definition}

The following proposition establishes some simple properties of these corresponding (history-dependent) transition matrices.

\begin{proposition}\label{prop:stochasticprocess:simpleproperties}
Let $P\in\processes$ be a stochastic process.  %Then for any $t,s\in\realsnonneg$ such that $t\leq s$, $T_t^s$ is a transition matrix and $T_t^t=I$ and, if $P$ is well-behaved, then also
%\begin{equation}\label{eq:wellbehavedtransitionmatrix}
%\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_t^{t+\Delta}-I}<+\infty
%\text{~~~~and~~~~}
%\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta}^t-I}<+\infty.
%\end{equation}
%Similarly, 
Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, any sequence of time points $u<t$, and any state assignment $x_u\in\states^u$, the corresponding (history dependent) transition matrix $T_{t,x_u}^s$ is---as its name suggests---a transition matrix, and $T_{t,x_u}^t=I$. Furthermore, if $P$ is well-behaved, then also
\begin{equation}\label{eq:wellbehavedtransitionmatrix}%\label{eq:wellbehavedhistorictransitionmatrix}
\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t,x_u}^{t+\Delta}-I}<+\infty
\text{~~~~and~~~~}
\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta,x_u}^t-I}<+\infty.
\end{equation}
\end{proposition}
\begin{proof}
Trivial consequence of Definitions~\ref{def:stoch_matrix}, \ref{def:cond_prob}, and \ref{def:well-behaved}.
\end{proof}

\begin{remark*}
Note that for any $P\in\processes$, a corresponding transition matrix $T_{t, x_u}^s$ is a map from $\gamblesX$ to $\gamblesX$, that can therefore be applied to any $f\in\gamblesX$. Observe that for all $x_t\in\states$, we have that
%\begin{align*}
%\left[T_t^sf\right](x_t) &= \sum_{x_s\in\states}f(x_s)P(X_s=x_s\,\vert\,X_t=x_t)
%= \mathbb{E}\left[f(X_s)\,\vert\,X_t=x_t\right]\,,
%\end{align*}
\begin{align*}
\left[T_{t,x_u}^sf\right](x_t) &= \sum_{x_s\in\states}f(x_s)P(X_s=x_s\,\vert\,X_t=x_t,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})\\
 &= \mathbb{E}\left[f(X_s)\,\vert\,X_t=x_t, X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]\,,
\end{align*}
where the expectation is taken with respect to $P$. %Similarly, for any history-dependent transition matrix $T_{t,x_u}^s$, we have that
\exampleend
\end{remark*}

Because a stochastic process is defined on a continuous time line, and because its corresponding transition matrices $T_{t,x_u}^s$ only describe the behavior of this process on a fixed point in time, we will furthermore require some tools to capture the dynamics of a given process. That is, we will be interested in how these transition matrices change over time.

One seemingly obvious way to describe these dynamics is to use the derivatives of a process' corresponding transition matrices. Unfortunately, for general stochastic processes, these derivatives do not necessarily exist. In fact, also the slightly weaker directional derivatives---which we define below---are not guaranteed to exist. Worse still, they are not even guaranteed to exist for well-behaved processes.

\begin{definition}[Directional Partial Derivatives]\label{def:direc_partial_deriv}
For any stochastic process $P\in\processes$, any $t\in\realsnonneg$, any sequence of time points $u<t$, and any state assignment $x_u\in\states^u$, the \emph{right-} and \emph{left-sided partial derivatives} of $T_{t,x_u}^t$ are defined, respectively, as
\begin{equation*}
\partial_{+}{T_{t,\,x_u}^t}
\coloneqq
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-T^t_{t,\,x_u})
=
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)
\end{equation*}

\begin{equation*}
\partial_{-}{T_{t,\,x_u}^t}
\coloneqq
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-T^t_{t,\,x_u})
=
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-I)
\end{equation*}
\noindent If these partial derivatives exist, then because of Proposition~\ref{prop:rate_from_stochastic_matrix}, they are guaranteed to belong to the set of rate matrices  $\mathcal{R}$. If they both exist and coincide, we write $\partial{T_{t,\,x_u}^t}$ to denote their common value.
\end{definition}

\begin{exmp}\label{exmp:well-behaved-no-deriv}
Suppose that a well-behaved stochastic process $P\in\wprocesses$ is such that, at some time $t\in\realsnonneg$ and for some history $x_u\in\states^u$, $u<t$, the corresponding transition matrix $T_{t,x_u}^{t+\Delta}$ satisfies, for all $\Delta\in\realspos$,
\begin{equation*}
\frac{1}{\Delta}(T_{t,x_u}^{t+\Delta} - I) = \left\{\begin{array}{cl}
Q_1 & \quad \text{if $\Delta\in\mathbb{Q}$, and} \\
Q_2 & \quad \text{otherwise,}
\end{array}\right.
\end{equation*}
for some rate matrices $Q_1,Q_2\in\mathcal{R}$ with $Q_1\neq Q_2$. Clearly, the norm of this quantity is bounded for all $\Delta\in\realspos$, and hence this does not pose a problem for the well-behavedness of $P$. On the other hand, $\partial_{+}{T_{t,\,x_u}^t}$ clearly does not exist.
\exampleend
\end{exmp}

Because these directional partial derivatives do not necessarily exist, it will be more convenient to instead work with \emph{outer partial derivatives} {\bf (REF)}. Intuitively, these can be seen as a kind of set-valued derivatives, containing all accumulation points of the difference equations in Definition~\ref{def:direc_partial_deriv}, obtained as $\Delta\to0^+$.

\begin{definition}[Outer Partial Derivatives]
For any stochastic process $P\in\processes$, any $t\in\realsnonneg$, any sequence of time points $u<t$, and any state assignment $x_u\in\states^u$, the \emph{right-} and \emph{left-sided} \emph{outer partial derivatives} of $T_{t,x_u}^t$ are defined, respectively, as
\begin{align}
\label{eq:rightouterderivative}
\begin{split}
\overline{\partial}_{+}
{T^t_{t,\,x_u}}
&\coloneqq
\left\{
Q\in\mathcal{R}
\colon
\left(\exists \,\{\Delta_i\}_{i\in\nats}\to0^+\,:\,
~
%\lim_{i\to+\infty}\Delta_i=0
%\text{~~and~}
\lim_{i\to+\infty}
\frac{1}{\Delta_i}
(T^{t+\Delta_i}_{t,\,x_u}-I)
=Q
\right)
\right\}\\
\overline{\partial}_{-}
{T^t_{t,\,x_u}}
&\coloneqq
\left\{
Q\in\mathcal{R}
\colon
\left(\exists\, \{\Delta_i\}_{i\in\nats}\to0^+\,:\,
~
%\lim_{i\to+\infty}\Delta_i=0
%\text{~~and~}
\lim_{i\to+\infty}
\frac{1}{\Delta_i}
(T^{t}_{t-\Delta_i,\,x_u}-I)
=Q
\right)
\right\}
\end{split}
\end{align}
Furthermore, the \emph{outer partial derivative} of $T_{t,x_u}^t$ is defined as
\begin{equation*}
\overline{\partial}
{T^t_{t,\,x_u}}
\coloneqq
\overline{\partial}_{+}
{T^t_{t,\,x_u}}
\cup
\overline{\partial}_{-}
{T^t_{t,\,x_u}}\,.
\end{equation*}
\end{definition}

The next result shows that, at least for well-behaved processes $P\in\wprocesses$, these outer partial derivatives always exist; in particular, that they are non-empty and bounded.

\begin{proposition}\label{prop:boundednon-emptyandclosed}
For any $P\in\wprocesses$, $\overline{\partial}_{+}
{T^t_{t,\,x_u}}$, $\overline{\partial}_{-}
{T^t_{t,\,x_u}}$ and $\overline{\partial}
{T^t_{t,\,x_u}}$ are non-empty, bounded and closed subsets of $\mathcal{R}$.
\end{proposition}

\noindent Furthermore, these right- and left-sided outer partial derivatives are clearly related to the directional partial derivatives. The next statement makes this explicit for well-behaved stochastic processes, using an $\epsilon-\delta$ expression for the limits in Definition~\ref{def:direc_partial_deriv}.

\begin{proposition}\label{prop:outerderivativebehaveslikelimit}
Consider any well-behaved stochastic process $P\in\wprocesses$. Then, for any $t\in\realsnonneg$, any sequence of time points $u<t$, any state assignment $x_u\in\states^u$, and any $\epsilon>0$, there is some $\delta>0$ such that, for all $0<\Delta<\delta$:
\begin{equation}
\label{eq:outerderivativebehaveslikelimit1}
(\exists Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}<\epsilon
\end{equation}
and
\begin{equation}
\label{eq:outerderivativebehaveslikelimit2}
(\exists Q\in\overline{\partial}_{-}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-I)-Q}<\epsilon
\end{equation}
\end{proposition}

We conclude this section by establishing that, for well-behaved processes, the outer partial derivatives are a proper generalization of the directional partial derivatives. In particular, if the latter exist, their values correspond exactly to the elements of the former.

\begin{corollary}\label{corol:outersingleton}
Consider any $P\in\wprocesses$. Then $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is a singleton if and only if $\smash{\partial_{+}
{T^t_{t,\,x_u}}}$ exists and, in that case, $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}=\{\partial_{+}
{T^t_{t,\,x_u}}\}$. Analogous results hold for $\smash{\overline{\partial}_{-}
{T^t_{t,\,x_u}}}$ and $\smash{\partial_{-}
{T^t_{t,\,x_u}}}$, and for $\smash{\overline{\partial}
{T^t_{t,\,x_u}}}$ and $\smash{\partial
{T^t_{t,\,x_u}}}$.
\end{corollary}
\begin{proof}
This follows trivially from Proposition~\ref{prop:outerderivativebehaveslikelimit}.
\end{proof}

\section{Continuous-Time Markov Chains}\label{sec:cont_time_markov_chains}

Having introduced continuous-time stochastic processes in Section~\ref{sec:stochastic_processes}, we will in this section focus on a specific class of such processes: the \emph{continuous-time Markov chains}.

\begin{definition}[Markov Property, Markov Chain]\label{def:markov_property}
A stochastic process $P\in\processes$ satisfies the \emph{Markov property} if, for any time sequence $0\leq t_0<\dots<t_{n}<t<s$ and any set of states $x_{0},\dots,x_{n},x,y\in\states$:
\begin{equation*}
P(X_s=y\vert X_{t}=x)=P(X_s=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n}).
\end{equation*}
A stochastic process that satisfies this property is called a \emph{Markov chain}. We denote the set of all Markov chains by $\mprocesses$ and use $\wmprocesses$ to refer to the subset that only contains the well-behaved Markov chains.
\end{definition}

\subsection{Transition Matrix Systems}

We know from Proposition~\ref{prop:stochasticprocess:simpleproperties} that the transition matrices of a stochastic process---and therefore also, in particular, of a Markov chain---satisfy a number of simple properties. For the specific case of a Markov chain $P\in\mprocesses$, the family of transition matrices $\mathcal{T}_P$ also satisfies an additional property---see Equation~\eqref{eq:transmatrixproduct} below. Whenever this is the case, we will call such a family of transition matrices a transition matrix system.

\begin{definition}[Transition matrix system]
A \emph{transition matrix system} $\mathcal{T}$ is a family of transition matrices $T_t^s$, defined for all $0\leq t\leq s$, such that
\begin{equation}\label{eq:transmatrixproduct}
T_t^s=T_t^r T_r^s
\text{~~for all $0\leq t\leq r\leq s$}
\end{equation}
and $T_t^t=I$ for all $t\geq0$. A transition matrix system that also satisfies Equation~\eqref{eq:wellbehavedtransitionmatrix} is called a \emph{well-behaved transition matrix system}.
\end{definition}

\begin{proposition}\label{prop:Markovhassystem}
Consider a Markov chain $P\in\mprocesses$ and let $\mathcal{T}_P$ be the corresponding family of transition matrices. Then $\mathcal{T}_P$ is a transition matrix system and, if $P$ is well behaved, then $\mathcal{T}_P$ is also well-behaved.
\end{proposition}
\begin{proof}
*** fairly easy to prove ***
\end{proof}

At this point, we already know that every (well-behaved) Markov chain has a corresponding (well-behaved) transition matrix system. Our next result establishes that the converse is true as well: every (well-behaved) transition matrix system has a corresponding (well-behaved) Markov chain, and for a given initial distribution, this Markov chain is even unique.

\begin{theorem}\label{theo:uniqueMarkovchain}
 Let $p$ be an arbitrary mass function on $\states$ and let $\mathcal{T}$ be a transition matrix system. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}$ and, for all $y\in\states$, $P(X_0=y)=p(y)$. Furthermore, if $\mathcal{T}$ is well-behaved, then $P$ is also well-behaved.
\end{theorem}
\begin{proof}
Let
\begin{multline*}
\mathcal{C}\coloneqq\{
(X_s=y,X_u=x_u)
\colon 
u\in\mathcal{U}_\emptyset,~s>u,~x_u\in\states^u,~y\in\states
\}\\
\cup
\{
(X_0=y,X_\emptyset=x_\emptyset)\colon y\in\states
\}
\end{multline*}
and consider a real-valued function $\tilde{P}$ on $\mathcal{C}$ that is defined by 
\begin{equation*}
\tilde{P}(X_s=y\vert X_u=x_u)
=
\tilde{P}(X_s=y\vert X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})
\coloneqq
\begin{cases}
p(y)&\text{~if $u=\emptyset$}\\
T_{t_n}^s(x_n,y)&\text{~otherwise}
\end{cases}
\end{equation*}
for all $(X_s=y,X_u=x_u)\in\mathcal{C}$.


We first prove that $\tilde{P}$ is a coherent conditional probability on $\mathcal{C}$. So consider any $k\in\nats$ and, for all $i\in\{1,\dots,k\}$, choose $(A_i,C_i)=(X_{s_i}=y_i,X_{u_i}=x_{u_i})\in\mathcal{C}$ and $\lambda_i\in\reals$. We need to show that
\begin{equation}\label{eq:theo:uniqueMarkovchain:coh1}
\sup\left\{\sum_{i=1}^m\lambda_i\ind{C_i}(\omega)\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation}
with $C_0\coloneqq\cup_{i=1}^mC_i$.
Since every sequence $u_i$ is finite, there is some finite set $w=\{w_0,w_1,\dots,w_m\}\subset\reals_{\geq0}$ of time points, with $m\in\nats$, such that $0=w_0<w_1<\dots<w_m$ and, for all $i\in\{1,\dots,k\}$, $u_i\subseteq w$ and $s_i\in w$.
Let $P_w$ be the restriction of $\tilde{P}$ to $\mathcal{C}_w$, with $\mathcal{C}_w$ defined as in Lemma~\ref{lemma:simplechaincoherence}. Then since $P_w$ clearly satisfies the conditions of Lemma~\ref{lemma:simplechaincoherence}, it follows that $P_w$ is a coherent conditional probability. Because of Theorem~\ref{theo:coherentextendable}, this implies that $P_w$ is the restriction to $\mathcal{C}_w$ of some full conditional probability $\tilde{P}_w$ that, because of Corollary~\ref{corol:fullcoherent}, is coherent. Hence, we find that
\begin{equation}\label{eq:theo:uniqueMarkovchain:coh2}
\sup\left\{\sum_{i=1}^m\lambda_i\ind{C_i}(\omega)\bigl(\tilde{P}_w(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0.
\end{equation}
By comparing Equations~\eqref{eq:theo:uniqueMarkovchain:coh1} and~\eqref{eq:theo:uniqueMarkovchain:coh2}, it follows that in order to prove that $\tilde{P}$ is coherent, it suffices to show that, for all $i\in\{1,\dots,k\}$, $\tilde{P}_w(A_i\vert C_i)=\tilde{P}(A_i\vert C_i)$. So fix any $i\in\{1,\dots,k\}$. If $u_i=\emptyset$, then $s_i=0=w_0$ and therefore $(A_i,C_i)\in\mathcal{C}_w$, which implies that $\tilde{P}_w(A_i\vert C_i)=P_w(A_i\vert C_i)=\tilde{P}(A_i\vert C_i)$. If $u_i\neq\emptyset$, then since $u_i\subseteq w$, $s_i\in w$ and $s_i>u_i$, it follows from Lemma~\ref{lemma:simplechainextend} that $\tilde{P}_w(A_i\vert C_i)=\tilde{P}(A_i\vert C_i)$. Hence, $\tilde{P}$ is coherent.

% \begin{equation*}
% \tilde{P}_w(A_i\vert C_i)
% =\tilde{P}_w(X_{s_i}=y_i\vert X_{u_i}=x_{u_i})
% =\tilde{P}_w(X_{s_i}=y_i\vert X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})
% =T_{t_n}^s(x_n,y)
% =
% =P_w(A_i\vert C_i)
% =\tilde{P}(A_i\vert C_i)
% \end{equation*}


Therefore, and because of Theorem~\ref{theo:coherentextendable}, $\tilde{P}$ can be extended to a full conditional probability $\tilde{P}^*$. If we now let $P$ be the restriction of $\tilde{P}^*$ to $\mathcal{C}^\mathrm{SP}$, then clearly, $P$ is a stochastic process that extends $\tilde{P}$, which implies that $P$ is a Markov chain such that $\mathcal{T}_P=\mathcal{T}$ and, for all $y\in\states$, $P(X_0=y)=p(y)$. Lemma~\ref{lemma:samepandTissameP} implies that this Markov chain is unique. If $\mathcal{T}$ is well-behaved, then since $\mathcal{T}_P=\mathcal{T}$, it follows from Lemma~\ref{lemma:PwbiffTwb} that $P$ is well-behaved.
\end{proof}

\begin{lemma}\label{lemma:simplechaincoherence}
Let $w=\{w_0,w_1,\dots,w_m\}\subset\reals_{\geq0}$ be a finite set of time points, with $m\in\nats_0$, such that $w_0<w_1<\dots<w_m$.
Let $P_w$ be a real-valued function on
\begin{equation*}
\mathcal{C}_w\coloneqq
\left\{
(X_{w_j}=y,X_u=x_u)
\colon 
j\in\{0,\dots,m\},~
u=\{w_0,\dots,w_{j-1}\},~
y\in\states,~
x_u\in\states^u
\right\}
\end{equation*}
such that, for any $j\in\{0,\dots,m\}$, $u=\{w_0,\dots,w_{j-1}\}$ and $x_u\in\states^u$, $P_w(\cdot\,\vert X_u=x_u)$ is a probability mass function on $\states$. Then $P_w$ is a coherent conditional probability on $\mathcal{C}_w$.
\end{lemma}
\begin{proof}
*** still have to do this ***

\begin{equation*}
\sup\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(P(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation*}
with $C_0\coloneqq\cup_{i=1}^nC_i$.
\end{proof}

\begin{lemma}\label{lemma:simplechainextend}
Let $w=\{w_0,w_1,\dots,w_m\}\subset\reals_{\geq0}$ be a finite set of time points, with $m\in\nats_0$, such that $w_0<w_1<\dots<w_m$. Consider a transition matrix system $\mathcal{T}$ and let $\tilde{P}_w$ be any full conditional probability such that for all $j\in\{1,\dots,m\}$ and any choice of $x_{w_{\ell}}\in\states$, $\ell\in\{0,\dots,j\}$:
\begin{equation*}
\tilde{P}_w(X_{w_j}=x_{w_j}\vert X_{w_0}=x_{w_0},\dots,X_{w_{j-1}}=x_{w_{j-1}})=T_{w_{j-1}}^{w_j}(x_{w_{j-1}},x_{w_j}).
\end{equation*}
Then for any $s\in w$ and $u\subseteq w$ such that $s>u$ and $u\neq\emptyset$, any $y\in\states$ and any $x_u\in\states^u$, we have that
\begin{equation*}
\tilde{P}_w(X_s=y\vert X_u=x_u)
=
\tilde{P}_w(X_s=y\vert X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})
=T_{t_n}^s(x_n,y).
\end{equation*}
\end{lemma}
\begin{proof}
*** still have to do this ***

So fix any $i\in\{1,\dots,k\}$. Since $u_i\subseteq w$, $s_i\in w$ and $s_i>u_i$, there must be some $j\in\{0,\dots,m\}$ such that $s_i=w_j$ and $u_i\subseteq\{w_0,\dots,w_{j-1}\}$. Let $u_i^\mathrm{c}\coloneqq\{w_0,\dots,w_{j-1}\}\setminus u_i$.

\begin{align*}
\tilde{P}_w(A_i\vert C_i)
=\tilde{P}_w(X_{s_i}=y_i\vert X_{u_i}=x_{u_i})
=...
=\tilde{P}(X_{s_i}=y_i\vert X_{u_i}=x_{u_i})
=\tilde{P}(A_i\vert C_i)
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:samepandTissameP}
Consider two markov chains $P_1,P_2\in\mprocesses$ such that $\mathcal{T}_{P_1}=\mathcal{T}_{P_2}$ and, for all $y\in\states$, $P_1(X_0=y)=P_2(X_0=y)$. Then $P_1=P_2$.
\end{lemma}
\begin{proof}
***
\end{proof}

\begin{lemma}\label{lemma:PwbiffTwb}
Consider any Markov chain $P\in\mprocesses$. Then $P$ is well-behaved if and only if $\mathcal{T}_P$ is well-behaved.
\end{lemma}
\begin{proof}
***
\end{proof}


Hence, Markov chains---and well-behaved Markov chains in particular---are completely characterised by their transition matrices and their initial distribution. 
We now focus on a number of special cases.

\subsection{Homogeneous Markov chains}

\begin{definition}[Homogeneous Markov chain]\label{def:homogeneousMarkov}
A Markov chain $P\in\mprocesses$ is called \emph{homogeneous} if its transition matrices $T_t^s$ do not depend on the absolute value of $t$ and $s$, but only on the time-difference $s-t$:
\begin{equation}\label{eq:homogeneousMarkov}
T_t^s=T_{t+\Delta}^{s+\Delta}
\text{~~for all $0\leq t\leq s$ and $\Delta\geq0$.}
\end{equation}
We denote the set of all homogeneous Markov chains by $\hmprocesses$ and use $\whmprocesses$ to refer to the subset that consists of the well-behaved homogeneous Markov chains.
\end{definition}


\begin{definition}\label{def:systemfromQ}For any rate matrix $Q\in\mathcal{R}$, we use $\mathcal{T}_Q$ to denote the family of stochastic matrices that is defined by
\begin{equation*}
T_t^s=e^{Q(s-t)}
\text{~~for all $0\leq t\leq s$.}
\end{equation*}
*** nog even uitleggen wat die matrix exponential inhoudt
\end{definition}

\begin{proposition}
\label{prop:systemQ}
For any $Q\in\mathcal{R}$, $\mathcal{T}_Q$ is a well-behaved transition matrix system.
\end{proposition}
\begin{proof}
*** trivial ***
\end{proof}

\begin{corollary}\label{cor:rate_has_unique_homogen_markov_process}
Consider any rate matrix $Q\in\mathcal{R}$ and let $p$ be an arbitrary mass function on $\states$. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$ and, furthermore, this unique Markov chain is well-behaved and homogeneous.
\end{corollary}

%As our next result shows, the converse is true as well: every well-behaved transition matrix system of a well-behaved homogeneous Markov chain can be uniquely characterised by a single transition rate matrix.

\begin{theorem}\label{theo:homogeneoushasQ}
For any well-behaved homogeneous Markov chain $P\in\whmprocesses$, there is a unique rate matrix $Q\in\mathcal{R}$ such that $\mathcal{T}_P=\mathcal{T}_Q$.
\end{theorem}

Hence, any well-behaved homogenous Markov chain $P\in\whmprocesses$ is completely characterised by its initial distribution and a rate matrix $Q\in\mathcal{R}$. We will denote this rate matrix by $Q_P$.

\subsection{Non-homogeneous Markov chains}

*** I would explicitely discuss the special cases of continous $Q_t$ and piecewice constant $Q_t$ here, as examples, and refer to some other papers for more general cases, since we don't need them.

\begin{theorem}%\label{theorem:continuous_rate_matrix_has_process}
For any continuous map $Q_t$ from $\reals_{\geq0}$ to $\mathcal{R}$, there is a unique transition matrix system $\mathcal{T}$ such that
\begin{equation*}
test
\end{equation*}
\end{theorem}
\begin{proof}
*** I guess its best to provide a reference here, although I think it should also be possible to prove this ourselves. ***

"Exact methods for the transient analysis of nonhomogeneous continuous time Markov chains"

"an empirical transition matrix for non-homogeneous markov chains based on censored observations"
\end{proof}

**** Other kinds of continuous-time non-homogeneous Markov chains also exist. For example, Proposition~\ref{prop:continuous_rate_matrix_has_process} in the appendix describes the existence of a $P\in\wmprocesses$ corresponding to a piecewise-continuous function $Q_t$.

\section{Imprecise Continuous-Time Markov chains}
\label{sec:iCTMC}

*** tralala, wat gaan we doen? impreciezie! wanneer gaan we dat doen? nu! jeej

\begin{definition}[Characterizing Set of Rate Matrices]
Consider any set $\mathcal{P}\subset\processes$ of stochastic processes. Then, a set of rate matrices $\rateset\subset\mathcal{R}$ is called the set of rate matrices that \emph{characterize} $\mathcal{P}$, if
\begin{equation*}
\rateset = \bigcup_{P\in\mathcal{P}}\,\bigcup_{t\in\realsnonneg}\,\bigcup_{u\in\mathcal{U}_{<t}}\,\bigcup_{x_u\in\states^u}\smash{\overline{\partial}}T_{t,x_u}^t\,.
\end{equation*}
For any $\mathcal{P}\subset\processes$, we will write $\rateset_{\mathcal{P}}$ for the set of rate matrices that characterize $\mathcal{P}$.
\end{definition}

\begin{definition}[Consistent Set of Rate Matrices]
Consider any set of rate matrices $\rateset$, and any stochastic process $P\in\processes$. Then $P$ is said to be \emph{consistent} with $\rateset$, if
\begin{equation*}
(\forall t\in\realsnonneg)(\forall u\in\mathcal{U}_{<t})(\forall x_u\in\states^u)\,:\, \smash{\overline{\partial}}T_{t,x_u}^t \subseteq \rateset\,.
\end{equation*}
If $P$ is consistent with $\rateset$, we will write $P\sim\rateset$.
\end{definition}

\begin{proposition}
Consider any set $\mathcal{P}\subset\processes$ of stochastic processes. Then, for all $P\in\mathcal{P}$, it holds that $P\sim\rateset_{\mathcal{P}}$.
\end{proposition}
\begin{proof}
*** {\bf TODO}, but trivial
\end{proof}

\begin{proposition}
Consider any set of rate matrices $\rateset\subset\mathcal{R}$. Let $\mathbb{P}_\rateset$ be the set of all stochastic processes consistent with $\rateset$, i.e., let
\begin{equation*}
\mathbb{P}_\rateset \coloneqq \left\{P\in\processes\,:\,P\sim\rateset\right\}\,.
\end{equation*}
Then, $\rateset$ characterizes $\mathbb{P}_\rateset$.
\end{proposition}
\begin{proof}
*** {\bf TODO}, but trivial
\end{proof}

\subsection{Types of Imprecise Continuous-Time Markov Chains}

*** lalala we behandelen verschillende gevallen ***

\subsubsection{Imprecise Homogeneous Markov Chains}

*** uitleggen dat we hier NIET op focussen, en dat dit ironisch genoeg het moeilijkste geval is; wel opgenomen voor compleetheid ***
\begin{definition}[Set of Homogeneous Markov Processes]\label{def:homogen_markov_process_set_new}
For any bounded set of rate matrices $\rateset$, we define the set $\whmprocesses_{\rateset}$ of all well-behaved homogeneous continuous-time Markov processes \emph{consistent} with $\rateset$. Formally, we let $\whmprocesses_{\rateset}$ be the set of all $P\in\whmprocesses$ such that
\begin{align*}
\whmprocesses_\rateset
\coloneqq&
\left\{
P\in\whmprocesses
\colon
\left((\forall t\in\realsnonneg)(\forall u<t)(\forall x_u\in\states^u):~
\overline{\partial}
{T^t_{t,\,x_u}}\subseteq\rateset
\right)\right\}\\
=&
\left\{
P\in\whmprocesses
\colon
\left((\forall t\in\realsnonneg):~
\overline{\partial}
{T^t_{t}}\subseteq\rateset
\right)\right\}\\
=&
\left\{
P\in\whmprocesses
\colon
Q_P\in\rateset
\right\}
\end{align*}
\end{definition}

\begin{definition}[Lower Expectation for Set of Homogeneous Markov Processes]\label{def:lower_homogen_markov} Consider any bounded set of rate matrices $\rateset$, and the set of corresponding well-behaved homogeneous continuous-time Markov processes $\whmprocesses_\rateset$. Then, the \emph{lower expectation with respect to $\whmprocesses_\rateset$} is defined as
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{WHM}[\cdot\,\vert\,\cdot] \coloneqq \inf\left\{\mathbb{E}[\cdot\,\vert\,\cdot]\,:\,P\in\whmprocesses_\rateset\right\}\,.
\end{equation*}
\end{definition}


\subsubsection{Imprecise Markov Chains}

\begin{definition}[Set of Markov Processes]\label{def:markov_process_set_new}
For any bounded set of rate matrices $\rateset$, we define the set $\wmprocesses_{\rateset}$ of all well-behaved continuous-time Markov processes \emph{consistent} with $\rateset$. Formally, we let $\wmprocesses_{\rateset}$ be the set of all $P\in\wmprocesses$ such that
%\begin{align*}\label{eq:conditionforMarkov_new}
%(\forall\epsilon\in\realspos)&\,
%(\exists\delta\in\realspos)\,
%(\forall t\in\realsnonneg)\,
%(\forall\Delta\in(0,\delta))\,
%(\exists Q\in\rateset)\,:\\
% &\,(\forall f\in\gamblesX)(\forall x\in\states)~
%\left\lvert\frac{\mathbb{E}_{X_{t+\Delta}}[f(X_{t+\Delta})\,\vert\,X_t=x]-f(x)}{\Delta}-\left[Qf\right](x)\right\rvert<\epsilon\cdot\norm{f}\,.
%\end{align*}
\begin{align*}
\wmprocesses_\rateset
\coloneqq&
\left\{
P\in\wmprocesses
\colon
\left((\forall t\in\realsnonneg)(\forall u<t)(\forall x_u\in\states^u):~
\overline{\partial}
{T^t_{t,\,x_u}}\subseteq\rateset
\right)\right\}\\
=&
\left\{
P\in\wmprocesses
\colon
\left((\forall t\in\realsnonneg):~
\overline{\partial}
{T^t_{t}}\subseteq\rateset
\right)\right\}
\end{align*}
\end{definition}

\begin{definition}[Lower Expectation for Set of Markov Processes]\label{def:lower_markov} Consider any bounded set of rate matrices $\rateset$, and the set of corresponding well-behaved continuous-time Markov processes $\wmprocesses_\rateset$. Then, the \emph{lower expectation with respect to $\wmprocesses_\rateset$} is defined as
%\begin{equation*}
%\underline{\mathbb{E}}^\mathrm{M}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\mprocesses_\rateset\right\}\,.
%\end{equation*}
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{WM}[\cdot\,\vert\,\cdot] \coloneqq \inf\left\{\mathbb{E}[\cdot\,\vert\,\cdot]\,:\,P\in\wmprocesses_\rateset\right\}\,.
\end{equation*}
\end{definition}

\subsubsection{Imprecise Non-Markov Chains}

\begin{definition}[Set of Non-Markov Processes]\label{def:set_non_markov_process}
For any bounded set of rate matrices $\rateset$, we consider the set $\wprocesses_\rateset$ of all stochastic processes \emph{consistent} with $\rateset$. Formally, we let $\wprocesses_\rateset$ be the set of all $P\in\wprocesses$ such that
%\begin{align*}
%&(\forall\epsilon\in\realspos)\,(\exists\delta\in\realspos)\,: \\
% &(\forall t\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall u\in\mathcal{U}_{[0,t]})\,(\forall(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}})\,(\exists Q\in\rateset)\,: \\
% &(\forall f\in\gambles(\states^{u\cup\{t+\Delta\}}))\,(\forall x_{t_n}\in\states^{\{t_n\}}): \\
% &\abs{\frac{\mathbb{E}_{X_{t+\Delta}}[f(x_{t_0},\ldots,x_{t_n},X_{t+\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] - f(x_{t_0},\ldots,x_{t_n},x_{t_n})}{\Delta} - \left[Q f(x_{t_0},\ldots,x_{t_{n}},X_{t+\Delta})\right](x_{t_n})} \\ 
% &\quad\quad < \epsilon\cdot\norm{f}\,.
%\end{align*}
\begin{equation}
\label{def:nonmarkovsetQ}
\wprocesses_\rateset
\coloneqq
\left\{
P\in\wprocesses
\colon
\left((\forall t\in\realsnonneg)(\forall u<t)(\forall x_u\in\states^u):~
\overline{\partial}
{T^t_{t,\,x_u}}\subseteq\rateset
\right)\right\}
\end{equation}
\end{definition}

\begin{definition}[Lower Expectation for Set of Non-Markov Processes]\label{def:lower_non_markov} Consider any bounded set of rate matrices $\rateset$, and the set of corresponding well-behaved continuous-time non-Markov processes $\wprocesses_\rateset$. Then, the \emph{lower expectation with respect to $\wprocesses_\rateset$} is defined as
%\begin{equation*}
%\underline{\mathbb{E}}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\processes_\rateset\right\}\,.
%\end{equation*}
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{W}[\cdot\,\vert\,\cdot] \coloneqq \inf\left\{\mathbb{E}[\cdot\,\vert\,\cdot]\,:\,P\in\wprocesses_\rateset\right\}\,.
\end{equation*}
\end{definition}

\subsection{Properties of Imprecise Continuous-Time Markov Chains}

*** some notes about upper expectations, and that it suffices to just consider the lower ***

\begin{proposition}\label{prop:markov_set_subset_of_nonmarkov_set}
Consider any bounded set of rate matrices $\rateset$, and the corresponding sets $\whmprocesses_\rateset$, $\wmprocesses_\rateset$, and $\wprocesses_\rateset$ of well-behaved homogeneous continuous-time Markov processes, well-behaved continuous-time Markov processes, and well-behaved continuous-time non-Markov processes, respectively. Then,
\begin{equation*}
\whmprocesses_\rateset \subseteq \wmprocesses_\rateset \subseteq \wprocesses_\rateset\,.
\end{equation*}
\end{proposition}
\begin{proof}
This is immediate from Definitions \ref{def:homogen_markov_process_set_new}, \ref{def:markov_process_set_new}, and \ref{def:set_non_markov_process}.
\end{proof}

\begin{proposition}\label{prop:lower_exp_markov_bounded_by_nonmarkov}
Consider any bounded set of rate matrices $\rateset$, and the corresponding sets $\whmprocesses_\rateset$, $\wmprocesses_\rateset$, and $\wprocesses_\rateset$ of well-behaved homogeneous continuous-time Markov processes, well-behaved continuous-time Markov processes, and well-behaved continuous-time non-Markov processes, respectively. Then,
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{W}[\cdot\,\vert\,\cdot] \leq
\underline{\mathbb{E}}_\rateset^\mathrm{WM}[\cdot\,\vert\,\cdot] \leq
\underline{\mathbb{E}}_\rateset^\mathrm{WHM}[\cdot\,\vert\,\cdot]\,.
\end{equation*}
\end{proposition}
\begin{proof}
This is immediate from Proposition \ref{prop:markov_set_subset_of_nonmarkov_set} and Definitions \ref{def:lower_homogen_markov}, \ref{def:lower_markov}, and \ref{def:lower_non_markov}.
\end{proof}

\begin{theorem}
*** only to be touched by Jasper *** Consider a non-empty convex set of rate matrices $\rateset\subseteq\mathcal{R}$ (perhaps I will need some extra properties here).
Fix a finite sequence of time points $u$ and some $t>0$. Then for any choice of $P_\emptyset\in\wprocesses_\rateset$ and, for all $x_u\in\states^u$, $P_{x_u}\in\wprocesses_\rateset$, there is some $P\in\wprocesses_\rateset$ such that *** still need to complete this statement (je kunt dit aan elkaar plakken) ***
\end{theorem}
\begin{proof}
*** heb dit ergens staan ***
\end{proof}

\subsection{Room for Jasper}

*** only to be touched by Jasper ***
*** als ik toch perse iets buiten de bewijzen wil invoeren doe ik dat voorlopig hier ***

\section{An Important Lower Transition Operator}
\label{sec:lowertrans}

Having introduced the notion of lower expectations with respect to sets of (non-)Markov processes, one might wonder how to compute these quantities, either numerically or for analytical purposes. 

Obviously, one way to go about doing this is to work directly with the definitions. That is, explicitly generate the entire set $\mprocesses_\rateset$ (or $\processes_\rateset$) for a given $\rateset$, compute expectations of a function $f$ for each element of this set, and then find the infimum of these expectations. It should be clear that this approach is fairly unwieldy, not in the least because for arbitrary $\rateset$ the corresponding set of processes may be infinite. Therefore, we will instead provide an alternative characterization of these lower expectations. 

In particular, we will in this section introduce a specific \emph{lower transition operator}, which is a map from $\gamblesX$ to $\gamblesX$ that generalizes the notion of a transition matrix. We will here focus on introducing the relevant concepts, and showing that this operator of interest is well-defined. We end this section by establishing that this (family) of operators is in many ways intuitively comparable to the transition matrix system of a well-behaved homogeneous Markov chain. In Section~\ref{sec:connections} we will then establish the relation between this operator and lower expectations, and show that we can indeed use it to compute the quantities of interest. In Section~\ref{sec:prev_work} we will show how this operator is related to previous work from the literature.

\subsection{Lower Transition (Rate) Operators}

It is clear from Section~\ref{sec:trans_rate_matrices} that there is a strong connection between transition rate matrices and transition matrices. We here generalize these two concepts to \emph{lower transition rate operators} and \emph{lower transition operators}, respectively.

\begin{definition}[Lower Transition Rate Operator]\label{def:coh_low_trans_rate}
We will call a map $\lrate$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition rate operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, all constant functions $\mu\in\gamblesX$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=LR\arabic*:,ref=LR\arabic*]
\item\label{LR:constantzero}
$\lrate(\mu)(x)=0$;
\item\label{LR:subadditive}
$\lrate(f+g)(x)\geq\lrate(f)(x)+\lrate(g)(x)$;
\item\label{LR:homo}
$\lrate(\lambda f)(x)=\lambda\lrate(f)(x)$;
\item\label{LR:nondiagpos}
$\lrate(\ind{y})(x)\geq0$ for all $y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\end{definition}


\begin{definition}[Lower Transition Operator]\label{def:coh_low_trans}
We will call a map $\lt$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=C\arabic*:]
\item
$\lt(f)(x)\geq\min\left\{f(y)\,\vert\,y\in\states\right\}$
\item
$\lt(f+g)(x)\geq\lt(f)(x)+\lt(g)(x)$;
\item
$\lt(\lambda f)(x)=\lambda\lt(f)(x)$.
\end{enumerate}
%\vspace{5pt}
\noindent We use $\underline{\mathcal{T}}$ to denote the set of all lower transition operators.
\vspace{5pt}
\end{definition}

\noindent We start by giving some useful properties of the norm of these operators.

\begin{proposition}\label{lem:normlratefinite}
For any lower transition rate operator $\lrate$, we have that $0\leq\norm{\lrate}<+\infty$.
\end{proposition}

\begin{proposition}\label{lemma:normofcoherenttrans}
For any lower transition operator $\lt$, we have that $0\leq \norm{\lt}\leq 1$.
\end{proposition}

We next establish that there is a correspondence between lower transition rate operators and lower transition operators that is analogous to the one found in Section~\ref{sec:trans_rate_matrices}.

\begin{proposition}\label{lemma:normQsmallenough}
Consider any lower transition rate operator $\lrate$, and any $\Delta\in\realsnonneg$ such that $\Delta\leq\nicefrac{1}{\norm{\lrate}}$. Then, the operator $(I+\Delta\lrate)$ is a lower transition operator.
\end{proposition}

\begin{proposition}\label{lemma:lower_trans_to_lower_rate}
Consider any lower transition operator $\lt$, and any $\Delta\in\realspos$. Then, the operator $\nicefrac{1}{\Delta}(\lt - I)$ is a lower transition rate operator.
\end{proposition}

We now have two results about the set $\underline{\mathcal{T}}$ of all lower transition operators. As the first result shows, this set is closed under composition.
\begin{proposition}\label{lemma:compositioncoherence}
For any $\lt,\underline{S}\in\underline{\mathcal{T}}$, we have that $\left(\lt\underline{S}\right)\in\underline{\mathcal{T}}$.
\end{proposition}
\begin{proof}
Simply check each of the properties.
\end{proof}

\noindent Furthermore, this set is a complete metric space under our usual norm.

\begin{proposition}\label{lemma:completemetricspace}
The metric space $(\underline{\mathcal{T}},d)$ is complete under the metric $d$ induced by our usual norm $\norm{\cdot}$.
\end{proposition}

\noindent We conclude this section with the following result.
\begin{proposition}\label{lemma:productiscoherent}
Consider any $t,s\in\realsnonneg$ such that $t<s$, any lower transition rate operator $\lrate$, and any sequence $u\in\mathcal{U}_{[t,s]}$ of time points such that $\sigma(u)\leq\nicefrac{1}{\norm{\lrate}}$. Then
\begin{equation*}
\prod_{k=1}^n(I+\Delta_i\lrate)\coloneqq (I+\Delta_1\lrate)(I+\Delta_2\lrate)\cdots (I+\Delta_n\lrate)
\end{equation*}
is a lower transition operator.
\end{proposition}
\begin{proof}
Trivial consequence of Propositions~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}.
\end{proof}

\subsection{The Operator of Interest}

We will in this section introduce a specific lower transition operator on which we will focus for the remainder of this work. We will assume here that we are given some arbitrary lower transition rate operator $\lrate$, and define for any $u\in\mathcal{U}_{[t,s]}$ the auxiliary operator
\begin{equation*}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate)\,.
\end{equation*}
Before defining the operator of interest, we first give some preliminary results to provide intuition on how this operator will be constructed. We start with a bound on the distance between two operators $\Phi_u$ and $\Phi_{u*}$.

\begin{proposition}\label{prop:differencebetweenu}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and any $u,u^*\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\alpha$ and $\sigma(u^*)<\alpha$, with $0<\alpha\leq\nicefrac{1}{\norm{\lrate}}$. Let $\Delta\coloneqq s-t$. Then,
\begin{equation*}
\norm{\Phi_u-\Phi_{u^*}}\leq 2\alpha\Delta\norm{\lrate}^2
\end{equation*}
\end{proposition}

\begin{corollary}\label{corol:cauchy}
For every sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$, the corresponding sequence $\{\Phi_{u_i}\}_{i\in\nats}$ is a \emph{Cauchy sequence}, meaning that
\begin{equation*}
(\forall \epsilon>0)(\exists n\in\nats)(\forall i,j\geq n)
\norm{\Phi_{u_i}-\Phi_{u_j}}<\epsilon.
\end{equation*}
\end{corollary}
\begin{proof}
This follows almost directly from Proposition~\ref{prop:differencebetweenu}.
\end{proof}

\noindent Due to Proposition~\ref{lemma:completemetricspace}, we therefore have the following result.

\begin{corollary}\label{corol:limitexistsandiscoherent}
For every sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$, the corresponding sequence $\{\Phi_{u_i}\}_{i\in\nats}$ converges to a lower transition operator.
\end{corollary}

\noindent We are now ready to define the \emph{lower transition operator corresponding to $\lrate$}.

\begin{definition}[Corresponding Lower Transition Operator]\label{def:low_trans}
Consider any $t,s\in\realsnonneg$ such that $t<s$ and let $\lrate$ be an arbitrary lower transition rate operator. The \emph{corresponding lower transition operator} $\lbound_t^s$ is a map from $\gamblesX$ to $\gamblesX$, defined by
\begin{equation}\label{eq:lowerbound}
\lbound_t^s\coloneqq\lim_{\sigma(u)\to0}\left\{ \Phi_u\,\Big\vert\,u\in\mathcal{U}_{[t,s]}\right\},
\end{equation}
where the limit is taken with respect to the set $\mathcal{U}_{[t,s]}$ of all finite sequences $u$ of time points that partition the interval $[t,s]$.
\end{definition}

\noindent The next result makes clear what exactly we mean by Equation~\eqref{eq:lowerbound}, and establishes that this limit exists and is well-defined.

\begin{theorem}\label{theo:convergencelowerbound}
For any $t,s\in\realsnonneg$ such that $t<s$ and any lower transition rate operator $\lrate$, there is a lower transition operator $\lbound_t^s$ such that 
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon.
\end{equation*}
\end{theorem}

\subsection{Properties of the Operator of Interest}

*** tralala, beschouw nu de familie van die dingen op elk tijdstip en over elk interval

\begin{definition}[Lower Transition Operator System]
Let $\lrate$ be an arbitrary lower transition rate operator. Then, the \emph{lower transition operator system} corresponding to $\lrate$ is the family $\underline{\mathcal{T}}_{\lrate}$ of lower transition operators corresponding to $\lrate$, defined as
\begin{equation*}
\underline{\mathcal{T}}_{\lrate} \coloneqq \Bigl\{L_t^s\,\Big\vert\,\forall t,s\in\realsnonneg, t<s\Bigr\}\,.
\end{equation*}
\end{definition}

*** tralala, kijk, het lijkt qua eigenschappen op een homogene markov keten

\begin{proposition}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s,r\in\realsnonneg$ such that $t< r<s$,
\begin{equation*}
L_t^s = L_t^rL_r^s\,.
\end{equation*}
Furthermore, for all $t\in\realsnonneg$, we have that $L_t^t=I$.
\end{proposition}
\begin{proof}
*** trivial ***
\end{proof}

\begin{proposition}\label{prop:lower_transition_is_homogeneous}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t<s$, and all $\Delta\in\realsnonneg$, we have that
\begin{equation*}
L_t^s = L_{t+\Delta}^{s+\Delta}\,.
\end{equation*}
\end{proposition}
\begin{proof}
*** {\bf TODO}
\end{proof}

*** kijk, differentiaalvergelijking die lijkt op die van matrix exponential:

\begin{proposition}\label{prop:lower_transition_has_deriv}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t<s$, it holds that
\begin{equation*}
\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s\,,\quad\text{and,}\quad\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate\,.
\end{equation*}
% Then $\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s$ and $\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate$, meaning that
\end{proposition}

\section{Connections Between $L_t^s$ and Imprecise Continuous-Time Markov Chains}\label{sec:connections}

One of the objectives of this paper is to establish a connection between the operator $\lbound_t^s$ that we have just introduced, and the different types of imprecise continous-time Markov chains that were discussed in Section~\ref{sec:iCTMC}. Since the former is derived from a lower transition rate operator $\lrate$ and the latter are derived from a non-empty bounded set of rate matrices $\rateset$, an obvious first step is to investigate the connection between lower transition rate operators and non-empty bounded sets of rate matrices.

\subsection{Connections Between $\lrate$ and Sets of Rate Matrices $\rateset$}\label{sec:connections_rate}

% $\rateset$ and $\lrate$. 
%In order to do that, we start by discussing some properties of sets of rate matrices.

We start by considering a non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices. For any $f\in\gamblesX$,
\begin{equation}\label{eq:correspondinglowertrans}
\lrate f\coloneqq\inf\{Qf\colon Q\in\rateset\}\\[2mm]
\end{equation}
is then again an element of $\gamblesX$.\footnote{%Since $\rateset$ is non-empty, the components of $\lrate f$ cannot be $+\infty$.
Since $\rateset$ is bounded,~\ref{N:normAf} implies that $\norm{Qf}\leq\norm{Q}\norm{f}<+\infty$ for all $Q\in\rateset$. Therefore, and since $\rateset$ is non-empty, the components of $\lrate f$ cannot be infinite. Hence, $\lrate f$ is a real-valued function on $\states$.}
Therefore, $\lrate$ is a map from $\gamblesX$ to $\gamblesX$. We call this operator $\lrate$, as defined by Equation~\eqref{eq:correspondinglowertrans}, the \emph{lower envelope} of $\rateset$. It is a matter of straightforward verification to see that $\lrate$ is a lower transition rate operator.

\begin{proposition}\label{prop:lowerenvelopeislowertrans}
For any non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices, the corresponding operator $\lrate\colon\gamblesX\to\gamblesX$, as defined by Equation~\eqref{eq:correspondinglowertrans}, is a lower transition rate operator.
\end{proposition}

\noindent
Inspired by this result, we will also refer to the lower envelope of $\rateset$ as the \emph{lower transition rate operator that corresponds to $\rateset$}. %As we have just seen, every non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices has such a corresponding lower transition rate operator $\lrate$. 
However, this correspondence is not one-to-one. As the following example establishes, different non-empty bounded sets of rate matrices may have the same corresponding lower transition rate operator.

\begin{exmp}
*** TO BE COMPLETED ***
\exampleend
\end{exmp}

Next, we consider some fixed lower transition rate operator $\lrate$.
All the non-empty bounded sets $\rateset$ of rate matrices that have $\lrate$ as their lower envelope then share a common property: they consist of rate matrices $Q$ that dominate $\lrate$, in the sense that $Qf\geq\lrate f$ for all $f\in\gamblesX$. Therefore, each of these sets $\rateset$ is contained in the following set of dominating rate matrices:
\begin{equation}\label{eq:dominatingratematrices}
\rateset_{\lrate}\coloneqq
\left\{
Q\in\mathcal{R}
\colon
Qf\geq\lrate f\text{ for all $f\in\gamblesX$}
\right\}.
\end{equation}
As our next result shows, this set $\rateset_{\lrate}$ is non-empty and bounded, and has $\lrate$ as its lower envelope. Even stronger, the infimum in Equation~\eqref{eq:correspondinglowertrans} is reached---can be replaced by a minimum.

\begin{proposition}\label{prop:dominating_nonempty_bounded}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is non-empty and bounded and, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $\lrate f=Qf$.
\end{proposition}

\noindent
Because of this result, and since---as discussed above---every non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope is a subset of $\rateset_{\lrate}$, it follows that $\rateset_{\lrate}$ is the largest non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope.
Furthermore, as we will show in Proposition~\ref{prop:dominatingproperties} below, this set $\rateset_{\lrate}$ is also closed and convex, and has \emph{separately specified rows}.

\begin{definition}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ has separately specified rows if
\begin{equation*}
\rateset=\left\{
Q\in\mathcal{R}
\colon
(\forall x\in\states)~Q(x,\cdot)\in\rateset_x\right\},
\end{equation*}
where, for every $x\in\states$, $\rateset_x\coloneqq\{Q(x,\cdot)\colon Q\in\rateset\}$ is some set of rows from which the $x$-th row of the rate matrices in $\rateset$ are taken.
\end{definition}
\noindent Thus, a set of rate matrices has separately specified rows if it is closed under taking arbitrary combinations of rows from its elements. 

\begin{proposition}\label{prop:dominatingproperties}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is closed and convex, and has separately specified rows.
\end{proposition}

\begin{exmp}
*** TO BE COMPLETED ***
\exampleend
\end{exmp}

\noindent
These additional properties characterize $\rateset_{\lrate}$ completely, in the sense that no other set satisfies them.

\begin{proposition}\label{prop:dominating_unique_characterization}
Consider any non-empty, bounded, closed and convex set of rate matrices $\rateset\subseteq\mathcal{R}$ with separately specified rows that has $\lrate$ as its lower envelope. Then $\rateset=\rateset_{\lrate}$.
\end{proposition}

We conclude from all of this that non-empty bounded sets of rate matrices are more informative than lower transition rate operators. Different non-empty bounded sets of rate matrices $\rateset$ may have the same lower transition rate operator $\lrate$ and therefore, in general, knowledge of $\lrate$ does not suffice to reconstruct $\rateset$; we can only reconstruct an outer approximation $\rateset_{\lrate}$, which is guaranteed to include $\rateset$. This changes if, besides non-empty and bounded, $\rateset$ is also closed and convex and has separately specified rows. In that case, $\lrate$ serves as an alternative representation for $\rateset$ because, since $\rateset=\rateset_{\lrate}$, we can use $\lrate$ to reconstruct $\rateset$. In other words: there is a one-to-one correspondence between lower transition rate operators and non-empty, bounded, closed and convex sets of rate matrices that have separately specified rows.

%\section{Imprecise Continuous-Time Markov Chains}\label{sec:imp_markov}

%\subsection{New Version}

%This section contains the new, simplified proofs.

\subsection{Connections Between $L_t^s$ and Lower Expectations $\underline{\mathbb{E}}$}\label{sec:single_var_lower_exp}

Having established a strong connection between the operator $\lrate$ and non-empty bounded sets of rate matrices $\rateset$, we will now turn to the connection between the operator $L_t^s$ and lower expectations $\underline{\mathbb{E}}$ with respect to sets of (non-)Markov processes. Specifically, we will in this section focus on the lower expectation of functions defined on the state space at a single point in time. In Section~\ref{sec:funcs_multi_time_points} we will then use and generalize these results when we consider functions defined on the state space at multiple time points.

As the following result shows, the operator $L_t^s$ corresponding to a lower transition rate operator $\lrate$ computes a lower bound on the expectation of a function $f\in\gamblesX$, with respect to the set $\wprocesses_\rateset$ of well-behaved stochastic processes consistent with any set $\rateset$ of which $\lrate$ is the lower envelope.

\begin{proposition}\label{theorem:nonmarkov_single_var_lower_bounded}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for any $P\in\wprocesses_\rateset$, any $s\in\realsnonneg$, any $u\in\mathcal{U}_{<s}$, any $x_u\in\states^u$, and any $f\in\gamblesX$,
\begin{equation*}
[L_{t_n}^s f](x_{t_n}) \leq \mathbb{E}[f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,.
\end{equation*}
\end{proposition}

As this result shows, $L_t^sf$ is a lower bound on the expectation of a function $f\in\gamblesX$, with respect to a set of stochastic processes $\wprocesses_\rateset$ induced by some non-empty bounded set of rate matrices $\rateset$. Our next result establishes that this bound is tight if $\rateset$ also has separately specified rows. Specifically, we show that $L_t^sf$ can then be approximated to arbitrary precision by carefully choosing a Markov process $P$ from the set $\wmprocesses_\rateset$.

\begin{proposition}\label{theorem:lower_markov_bound_is_tight}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then for all $t,s\in\realsnonneg$ such that $t<s$, all $f\in\gamblesX$, and all $\epsilon\in\realspos$, there exists a $P\in\wmprocesses_{\rateset}$ such that
\begin{equation*}
\norm{\lbound_t^sf-T_t^sf} < \epsilon\,.
\end{equation*}
\end{proposition}

Together, Propositions~\ref{theorem:nonmarkov_single_var_lower_bounded} and~\ref{theorem:lower_markov_bound_is_tight} establish a strong connection between $L_t^s$ and lower expectations $\underline{\mathbb{E}}$. In particular, for functions defined on a single point in time, they turn out to be equivalent, as shown by the result below.

\begin{corollary}\label{cor:lower_operator_is_infimum}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $f\in\gamblesX$, all $t,s\in\realsnonneg$ such that $t<s$, all $u\in\mathcal{U}_{<t}$, all $x_u\in\states^u$, and all $x\in\states$,
\begin{align*}
\left[L_t^sf\right](x) &= \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
 &= \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x]\,,%\inf\Bigl\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\Bigr\}\,.
\end{align*}
and furthermore,
\begin{equation*}
\left[L_t^sf\right](x) = \underline{\mathbb{E}}^{\mathrm{W}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,.%\inf\Bigl\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\Bigr\}\,.
\end{equation*}
\end{corollary}

*** This needs some rewording ***

One somewhat unsurprising result is therefore that $L_t^s$ computes the lower expectation of functions $f\in\gamblesX$ with respect to sets of Markov processes $\mprocesses_\rateset$. The reason that this is to be expected is the previously established correspondence between $L_t^s$ and the solution of the differential equation introduced in {\bf DAMJANREF}, which was there shown to compute exactly this quantity.

A rather more surprising result, perhaps, is that this \emph{same} operator also computes lower expectations with respect to sets $\processes_\rateset$ of non-Markov processes. Our next result strengthens this connection between $L_t^s$ and sets of non-Markov processes.

Recall from Section~\ref{sec:connections_rate} that for a given lower transition rate operator $\lrate$, its set of dominating rate matrices $\rateset_{\lrate}$ is---by definition---the largest set of rate matrices which has $\lrate$ as its lower envelope. The following result shows that the set $\wprocesses_{\rateset_{\lrate}}$ of well-behaved stochastic processes consistent with $\rateset_{\lrate}$ is the largest set of stochastic processes which has $L_t^s$ as its lower envelope.

\begin{theorem}\label{theo:dominating_rate_processes_max_set}
Let $\lrate$ be an arbitrary lower transition rate operator, with $\rateset_{\lrate}$ its set of dominating rate matrices, and let  $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $f\in\gamblesX$, all $t,s\in\realsnonneg$ such that $t<s$, all $u\in\mathcal{U}_{<t}$, all $x_u\in\states^u$, and all $x\in\states$,
\begin{equation*}
\left[L_t^sf\right](x) = \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x,X_{t_0},\ldots,X_{t_n}=x_{t_n}]\,:\,P\in\wprocesses_{\rateset_{\lrate}}\right\}\,,
\end{equation*}
and, contrariwise, for all $P\in\processes$ such that $P\notin\wprocesses_{\rateset_{\lrate}}$, there is some $f\in\gamblesX$, some $t,s\in\realsnonneg$ with $t<s$, some $u\in\mathcal{U}_{<t}$, and some $x_u\in\states^u$, such that for some $x\in\states$,
\begin{equation*}
\mathbb{E}\left[f(X_s)\,\vert\,X_t=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] < \left[L_t^sf\right](x)\,.
\end{equation*}
\end{theorem}

\section{Relation to Previous Work}\label{sec:prev_work}

To the best of our knowledge, the concept of imprecise continuous-time Markov chains was first introduced in the literature by the work of {\v{S}}kulj~\cite{Skulj:2015cq}. There, the idea was used to define $\lrate$ as a lower envelope of a set of rate matrices $\rateset$ that has separately specified rows. It was shown that then, for a given $f\in\gamblesX$, the differential equation
\begin{align}\label{eq:damjans_diff}
\begin{split}
\frac{d \underline{f}_{\,t}}{d t} &\coloneqq \lrate\,\underline{f}_{\,t}\,,\quad
\underline{f}_{\,0} \coloneqq f\,,
\end{split}
\end{align}
has a unique solution, and that this solution satisfies
\begin{equation}\label{eq:damjans_lower}
\underline{f}_{\,s} = \underline{\mathbb{E}}_{\rateset}^{\mathrm{WM}}[f(X_s)\,\vert\,X_0]\,.
\end{equation}
Our work extends these results in several ways. First, as shown by Proposition~\ref{prop:lower_transition_has_deriv}, the differential equation~\eqref{eq:damjans_diff} has a uniform solution, i.e. one that is independent of $f$, and this solution is given\footnote{There are some notational discrepancies between this previous work and our work here. To see this correspondence, take from Proposition~\ref{prop:lower_transition_has_deriv} the quantity $\nicefrac{\partial}{\partial t}L_t^s=-\lrate L_t^s$, with boundary condition $L_s^s=I$, and ``integrate'' backward from $s$ to $0$. This fixes the directional inconsistency of the derivatives, and yields the proper correspondence to differential equation~\eqref{eq:damjans_diff}.} by the operator $L_0^s$ corresponding to $\lrate$. Second, Corollary~\ref{cor:lower_operator_is_infimum} confirms Equation~\eqref{eq:damjans_lower}, but furthermore shows that the solution $\underline{f}_{\,s}$ also corresponds to the lower expectation with respect to a set of non-Markov processes.

Our present work furthermore adds to the literature in several ways. In~\cite{Skulj:2015cq}, the arguments were cast purely in terms of lower envelopes of expectation functionals; but side-stepped the question of which sets of processes these envelopes correspond to. In contrast, our work in Section~\ref{sec:iCTMC} makes explicit the sets of processes that we are dealing with, and Theorem~\ref{theo:dominating_rate_processes_max_set} exactly characterizes the largest set of processes for which our lower transition operator---and indeed the differential equation~\eqref{eq:damjans_diff}---computes a lower envelope.

This earlier work also contains several results that are of significance to us. In particular, {\v{S}}kulj discusses several practical techniques to numerically compute, or approximate to arbitrary precision, the quantity $\underline{f}_{\,s}$ for a given $f\in\gamblesX$. Hence, because of the correspondence between $\underline{f}_{\,s}$ and $L_0^sf$, these results can be applied when numerically working with our lower transition operator.

Finally, previous work only focused on functions $f\in\gamblesX$ defined at a single point in time. We will now turn to functions defined on multiple time points, where we will find that it is crucial to be aware of the set of processes with respect to which one is taking lower envelopes.

\section{Functions Defined on Multiple Time Points}\label{sec:funcs_multi_time_points}

Having shown in Section~\ref{sec:single_var_lower_exp} that the operator $L_t^s$ computes lower expectations for functions $f\in\gamblesX$ defined on a single point in time, we will now turn our attention to functions defined on multiple time points. Because we are considering \emph{conditional} expectations, where the conditioning is done with respect to states in a (non-)Markov chain's history, it makes sense to distinguish between two different classes of functions defined at multiple time points. 

First, in Section~\ref{sec:function_single_future_multiple_past}, we will consider functions defined on a single time point in a chain's future, and multiple time points in the chain's history. Thus, we will consider functions $f\in\gambles(\states^{u\cup\{s\}})$, and lower expectations of the form
\begin{equation*}
\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
We will see that it is a straightforward implication of our results from Section~\ref{sec:single_var_lower_exp} that such lower expectations are computable using $L_t^s$, both with respect to sets of Markov chains and with respect to sets of non-Markov chains.

In Section~\ref{sec:decomposition} we will generalize this to functions defined on multiple time points in a chain's future and history, considering functions $f\in\gambles(\states^{u\cup v})$ and lower expectations of the form
\begin{equation*}
\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
As we will see, for functions of this kind the lower expectations with respect to sets of non-Markov chains no longer correspond to those taken with respect to sets of Markov chains. One of the main results of this paper, however, is that we can still use the operator $L_t^s$ to compute lower expectations of this form if taken with respect to sets of non-Markov chains. We will see that this is because the optimization problem involved in computing lower expectations in some sense becomes simpler when we drop the Markov assumption.

Finally, in Section~\ref{sec:tractability}, we will show that although $L_t^s$ provides us with a way to compute such lower expectations, doing so for general functions $f\in\gambles(\states^{u\cup v})$ is still computationally intractable. However, we then provide algorithms to tractably compute lower expectations for large and practically useful subclasses of $\gambles(\states^{u\cup v})$.

\subsection{Multi-Variable Functions on a Single Point in the Future}\label{sec:function_single_future_multiple_past}

We start by considering functions $f\in\gambles(\states^{u\cup\{s\}})$ defined on a single time point $s$ in a (non-)Markov chain's future, and multiple time points $u=t_0,\ldots,t_n$ in a chain's history. Recall our notation from Section~\ref{sec:multivar_notation}; we write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ for a specific state assignment $(x_{t_0},\ldots,x_{t_n})$, and have defined
\begin{equation*}
\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n}) \equiv \left[L_{t_n}^sf(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
\end{equation*}
The following results are now direct implications of, and analogies to, our results from Section~\ref{sec:single_var_lower_exp}.

\begin{proposition}\label{prop:multi_var_single_future_bounded}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for any $P\in\wprocesses_\rateset$, any $s\in\realsnonneg$, any $u\in\mathcal{U}_{<s}$, any $x_u\in\states^u$, and any $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]\,.
\end{equation*}
\end{proposition}

\begin{proposition}\label{prop:multi_var_single_future_tight}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $s\in\realsnonneg$, all $u\in\mathcal{U}_{<s}$, all $x_u\in\states^u$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $\epsilon\in\realspos$, there is a $P\in\wmprocesses_\rateset$ such that
\begin{equation*}
\abs{\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n}) - \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]} < \epsilon\,.
\end{equation*}
\end{proposition}

Note that this result is weaker than the corresponding Theorem~\ref{theorem:lower_markov_bound_is_tight} in Section~\ref{sec:single_var_lower_exp}. Specifically, this says that for a given history $x_u\in\states^u$, there is a $P\in\wmprocesses_\rateset$ that approaches $\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n})$. This does not imply that there is a $P\in\wmprocesses_\rateset$ that approaches $\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n})$ for \emph{all} histories! 

We will see in Section~\ref{sec:decomposition} that this is exactly the reason that computing lower expectations with respect to sets of non-Markov processes is ``easy''; by dropping the Markov assumption, the corresponding optimization problems become solvable locally with respect to a given history. In contrast, the optimization over sets of Markov processes must there be done globally with respect to all possible histories, because they do not allow for minimizing selections specific to a given trajectory.

For our present purposes, Proposition~\ref{prop:multi_var_single_future_tight} is still strong enough to imply the following result.

%\begin{proposition}
%In essentie, voor alle $f\in\gambles(\states^{u\cup\{s\}})$ en alle $\epsilon\in\realspos$, is er een $P\in\mprocesses_\rateset$ zodat
%\begin{equation*}
%\norm{L_t^s f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%{\bf TODO} This is immediate.
%\end{proof}

\begin{corollary}\label{cor:inf_works_for_single_future_var}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $s\in\realsnonneg$, all $u\in\mathcal{U}_{<s}$, all $x_u\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_{t_n}^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,,
\end{equation*}
and furthermore,
\begin{equation*}
\left[L_{t_n}^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^\mathrm{W}_{\,\rateset}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,.
\end{equation*}
\end{corollary}

Thus, we see that the operator $L_t^s$ can also be used to compute lower expectations of functions $f\in\gambles(\states^{u\cup\{s\}})$, both with respect to sets of Markov processes and with respect to sets of non-Markov processes. We will now turn to functions defined on multiple time points in a process' future, where we will find this correspondence to no longer hold.


\subsection{Multi-Variable Functions on Multiple Points in the Future}\label{sec:decomposition}

**** notatie moet nog geupdate worden

We now consider functions $f\in\gambles(\states^{u\cup v})$, where $u=t_0,\ldots,t_n$ is a sequence of time points in a process' history, and $v=s_0,\ldots,s_m$ is a sequence of time points in a process' future; hence, we assume $s_0>t_n$. As the next example shows, the lower expectation of such functions, when taken with respect to a set $\wmprocesses_\rateset$ of Markov processes, no longer necessarily corresponds to the lower expectation taken with respect to a set $\wprocesses_\rateset$ of non-Markov processes.

\begin{exmp}
{\bf TODO} Example that sometimes $\underline{\mathbb{E}}^\mathrm{M}\neq \underline{\mathbb{E}}$ for functions $f\in\gambles(\states^{u\cup v})$.
\exampleend
\end{exmp}

An obvious question is therefore what the operator $L_t^s$ computes for functions of this form, as it clearly cannot compute both $\underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}$ and $\underline{\mathbb{E}}^\mathrm{W}_{\,\rateset}$. As we will see below, it turns out that we can use $L_t^s$ to compute the lower expectation of such functions with respect to sets of non-Markov processes. We start by showing that, using a composition of these operators, we can compute a lower bound with respect to a set $\wprocesses_\rateset$.
\begin{proposition}\label{prop:multivar_bounded}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for all $P\in\processes_\rateset$, all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{proposition}

Note that a direct implication of this, together with Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, is that $L_t^s$ can also be used to compute lower bounds on the expectation with respect to a set $\mprocesses_\rateset$ of Markov processes. However, this bound will then in general not be tight, and hence will not correspond to the lower expectation.

To see why, observe that in the term $[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n})$, the operators $L_{s_{i-1}}^{s_i}$ can take on different values depending on the choice of $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$. Hence, to approach these values of $L_{s_{i-1}}^{s_i}$ from within a set $\mprocesses$ of Markov processes, we have to be able to pick the approximating values such that they depend on the specific trajectory $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$, and this is exactly what the Markov condition prevents us from doing. As the next result shows, we can however approach this quantity from within a set $\processes_\rateset$ of non-Markov processes.

\begin{proposition}\label{prop:multivar_bound_tight}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$, all $v\in\mathcal{U}_{[s,s']}$, all $f\in\gambles(\states^{u\cup v})$, and all $\epsilon\in\realspos$, there is a $P\in\processes_\rateset$ such that
\begin{equation*}
\norm{L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
\end{equation*}
\end{proposition}

\begin{corollary}\label{cor:inf_works_for_multivar}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{corollary}

\subsection{Algoritmes}\label{sec:tractability}

*** We gaan nu laten zien hoe dingen berekenbaar zijn door compositie van onder-transitie operatoren ***

*** Neem aan dat we een methode hebben om voor gegeven $f\in\gamblesX$, de term $\hat{f}_t^s :\approx L_t^sf$ numeriek uit te rekenen, of te benaderen tot arbitraire precisie. Bijvoorbeeld, gebruik de methode van Damjan. Noteer de complexiteit van deze methode als $O(c)$.

\begin{exmp}
*** Voorbeeld dat uitrekenen voor arbitraire $f\in\gambles(\states^{u\cup v})$ exponentieel moeilijk is. \exampleend
\end{exmp}

\begin{remark*}
*** Merk op dat uitrekenen voor arbitraire $f\in\gambles(\states^{u\cup \{s\}})$ ook exponentieel moeilijk is, maar alleen als je dat voor alle $x_u$ wil doen. In dat geval is de functie sowieso problematisch, maar dat heeft niks met verwachtingswaardes te maken. We nemen dus aan dat ofwel $n$ klein genoeg is dat dit niet vervelend is, of dat we het maar voor een of enkele waarden $x_u$ willen doen. In het bijzonder is het geen probleem als we vanuit tijdstip 0 vertrekken, dwz, de initiele state. \exampleend
\end{remark*}

*** er zijn subklasses van functies waarvoor het uitrekenen lineair is in $m$

\begin{definition}[Single-Future-State Sum Functions]
Consider any $u\coloneqq t_0,\ldots,t_n$ and any $v\coloneqq s_0,\ldots,s_m$. A function $f\in\gambles(\states^{u\cup v})$ is called a \emph{single-future-state sum function} if there exist functions $g_0,\ldots,g_m$ such that $g_i\in\gambles(\states^{u\cup\{s_i\}})$, $i\in\{0,\ldots,m\}$, and such that, for all $x_u\in\states^u$ and all $x_v\in\states^v$,
\begin{equation*}
f(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_m}) = \sum_{i=0}^m g_i(x_{t_0},\ldots,x_{t_n},x_{s_i})\,.
\end{equation*}
We use $\gambles_+(\states^{u\cup v})\subset\gambles(\states^{u\cup v})$ to denote the set of all single-future-state sum functions on $\states^{u\cup v}$.
\end{definition}

\begin{definition}[Single-Future-State Product Functions]
Consider any $u\coloneqq t_0,\ldots,t_n$ and any $v\coloneqq s_0,\ldots,s_m$. A function $f\in\gambles(\states^{u\cup v})$ is called a \emph{single-future-state product function} if there exist functions $g_0,\ldots,g_m$ such that $g_i\in\gambles(\states^{u\cup\{s_i\}})$, $i\in\{0,\ldots,m\}$, and such that, for all $x_u\in\states^u$ and all $x_v\in\states^v$,
\begin{equation*}
f(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_m}) = \prod_{i=0}^m g_i(x_{t_0},\ldots,x_{t_n},x_{s_i})\,.
\end{equation*}
We use $\gambles_\times(\states^{u\cup v})\subset\gambles(\states^{u\cup v})$ to denote the set of all single-future-state product functions on $\states^{u\cup v}$.
\end{definition}

*** opmerking dat ze misschien vrij specifiek lijken, maar dat veel praktisch gebruikte dingen daar wel inzitten; veel utility/loss-functies zijn lineair, de GBR zit daarin, misschien nog wat andere voorbeelden (referenties zijn wel boeiend hier)

\begin{algorithm}[H]
  \caption{Numerically compute lower expectation of $f\in\gambles_+(\states^{u\cup v}).$
    \label{alg:compute_singlestate_sum}}
  \begin{algorithmic}[1]
    \Require{A non-empty, bounded set of rate matrices $\rateset$ with separately specified rows, a state assignment $x_u\in\states^u$, and a function $f\in\gambles_+(\states^{u\cup v})$.}
    \Ensure{$\hat{\underline{\mathbb{E}}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]$.}
    \Statex
    \Function{ComputeSFSSum}{$\rateset, x_u, f$}
      \State $\hat{\underline{f}}\gets 0$\Comment{Partial solution starts with empty sum}
		\For{$i\in\{0,\ldots,m\}$}\Comment{Run through all time points}
		\State $g_i\gets f(X_{t_0},\ldots,X_{t_n},1,\ldots,X_{s_i},\ldots,1)$ \Comment{$g_i$ is restriction of $f$ to $\gambles(\states^{u\cup\{s_i\}})$}% for state assignment $X_{s_j}=1$, for all $j\in\{0,\ldots,m\}\setminus\{i\}$}
		\State $\hat{\underline{g}}_{\,i}\gets [L_{s_{i-1}}^{s_i}g_i](x_u)$\Comment{Compute lower expectation of $g_i$}
		\State $\hat{\underline{f}}\gets\hat{\underline{f}} + \hat{\underline{g}}_{\,i}$\Comment{Add to partial solution}
		\EndFor
      \State \Return{$\hat{\underline{f}}$}\Comment{Return lower expectation}
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\begin{lemma}
*** proper setup

Consider any $f\in\gambles_+(\states^{u\cup v})$ such that $f=\sum_{i=0}^m g_i$, and let $f'\coloneqq \sum_{i=0}^{m-1} g_i$. Then,
\begin{align*}
 &\quad \underline{\mathbb{E}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
 &= \left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) \\
 &= \left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-2}}^{s_{m-1}}f' \right](x_{t_0},\ldots,x_{t_n}) + \left[L_{s_{m-1}}^{s_m}g_m\right](x_{t_0},\ldots,x_{t_n})\,.
\end{align*}
\end{lemma}
\begin{proof}
*** {\bf TODO}, but trivial.
\end{proof}

\begin{proposition}
*** proper setup

Consider any $f\in\gambles_+(\states^{u\cup v})$ such that $f=\sum_{i=0}^m g_i$. For notational convenience, let $s_{(-1)}\coloneqq t_n$. Then,
\begin{align*}
 &\quad \underline{\mathbb{E}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
 &= \left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) \\
 &= \sum_{i=0}^m \left[L_{s_{i-1}}^{s_i}g_i\right](x_{t_0},\ldots,x_{t_n}) \\
 &\approx \sum_{i=0}^m \hat{\underline{g}}_{\,i}(x_{t_0},\ldots,x_{t_n})\,,
\end{align*}
where $\hat{\underline{g}}_{\,i}:\!\approx L_{s_{i-1}}^{s_i}g_i$ is the numerical result of computing $L_{s_{i-1}}^{s_i}g_i$.
\end{proposition}
\begin{proof}
Trivial consequence of Lemma~{\bf REF} and backward induction on $m$.
\end{proof}

\begin{corollary}
Algorithm~\ref{alg:compute_singlestate_sum} correctly computes the lower expectation of any single-future-state sum function $f\in\gambles_+(\states^{u\cup v})$ with respect to the set $\wprocesses_\rateset$, in time complexity $O(mc)$. If the numerical method to compute $L_{s_{i-1}}^{s_i}g_i$ is an approximation with error at most $\epsilon$, then the output of Algorithm~\ref{alg:compute_singlestate_sum} is an approximation of this lower expectation with error at most $m\cdot\epsilon$.
\end{corollary}
\begin{proof}
*** Correctness and complexity are trivial. Approximation error needs to be properly defined first (do this).
\end{proof}

*** we now move on to single-future-state product functions. we use the following (backward) induction step
%
%\begin{lemma}\label{lem:single_future_state_product_factorizes}
%*** proper setup
%
%Consider any $f\in\gambles_\times(\states^{u\cup v})$ such that $f=\prod_{i=0}^m g_i$, and let $f'\coloneqq \prod_{i=0}^{m-1} g_i$. Then, if $[L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\geq 0$,
%\begin{align*}
%&\quad \underline{\mathbb{E}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
% &= [L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\cdot\underline{\mathbb{E}}_\rateset^\mathrm{W}\left[ f'(x_{t_0},\ldots,t_n,X_{s_0},\ldots,X_{s_{m-1}})\,\Big\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]\,,
%\end{align*}
%and, if $[L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})<0$, 
%\begin{align*}
%&\quad \underline{\mathbb{E}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
% &= -[L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\cdot\underline{\mathbb{E}}_\rateset^\mathrm{W}\left[ -f'(x_{t_0},\ldots,t_n,X_{s_0},\ldots,X_{s_{m-1}})\,\Big\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] \\
% &= [L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\cdot\overline{\mathbb{E}}_\rateset^\mathrm{W}\left[f'(x_{t_0},\ldots,t_n,X_{s_0},\ldots,X_{s_{m-1}})\,\Big\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] \,,
%\end{align*}
%where $\overline{\mathbb{E}}_\rateset^\mathrm{W}$ is the upper expectation with respect to $\wprocesses_\rateset$, as defined by {\bf REF}. Similarly, if $[L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\geq 0$, then
%\begin{align*}
%&\quad \overline{\mathbb{E}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
% &= [L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\cdot\overline{\mathbb{E}}_\rateset^\mathrm{W}\left[ f'(x_{t_0},\ldots,t_n,X_{s_0},\ldots,X_{s_{m-1}})\,\Big\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]\,,
%\end{align*}
%and, if $[L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})<0$, 
%\begin{align*}
%&\quad \overline{\mathbb{E}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
% &= -[L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\cdot\overline{\mathbb{E}}_\rateset^\mathrm{W}\left[ -f'(x_{t_0},\ldots,t_n,X_{s_0},\ldots,X_{s_{m-1}})\,\Big\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] \\
% &= [L_{s_{m-1}}^{s_m}g_m](x_{t_0},\ldots,x_{t_n})\cdot\underline{\mathbb{E}}_\rateset^\mathrm{W}\left[f'(x_{t_0},\ldots,t_n,X_{s_0},\ldots,X_{s_{m-1}})\,\Big\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] \,.
%\end{align*}
%\end{lemma}
%
%\begin{proposition}
%*** {\bf TODO:} proper proof of induction for product case
%\end{proposition}
%\begin{proof}
%*** todo
%\end{proof}
%
%\begin{corollary}
%Algorithm~\ref{alg:compute_singlestate_prod} correctly computes the lower expectation of any single-future-state product function $f\in\gambles_\times(\states^{u\cup v})$ with respect to the set $\wprocesses_\rateset$, in time complexity $O(mc)$. If the numerical method to compute $L_{s_{i-1}}^{s_i}g_i$ is an approximation with error at most $\epsilon$, then the output of Algorithm~\ref{alg:compute_singlestate_prod} is an approximation of this lower expectation with error at most {\bf TODO}.
%\end{corollary}
%\begin{proof}
%*** Complexity is trivial. Correctness is also trivial (once we have the proposition for induction). Approximation error needs to be properly defined first (do this), and we need to figure out what it is.
%\end{proof}
%
%\begin{algorithm}[H]
%  \caption{Numerically compute lower expectation of $f\in\gambles_\times(\states^{u\cup v}).$ 
%    \label{alg:compute_singlestate_prod}}
%  \begin{algorithmic}[1]
%    \Require{A non-empty, bounded set of rate matrices $\rateset$ with separately specified rows, a state assignment $x_u\in\states^u$, and a function $f\in\gambles_\times(\states^{u\cup v})$.}
%    \Ensure{$\hat{\underline{\mathbb{E}}}_\rateset^\mathrm{W}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]$.}
%    \Statex
%    \Function{ComputeSFSProduct}{$\rateset, x_u, f$}
%		\For{$i\in\{0,\ldots,m\}$}\Comment{Run through all time points}
% for state assignment $X_{s_j}=1$, for all $j\in\{0,\ldots,m\}\setminus\{i\}$}
%		\State $\hat{\underline{g}}_{\,i}\gets [L_{s_{i-1}}^{s_i}g_i](x_u)$\Comment{Precompute lower expectation of $g_i$}
%	%	\State $\hat{\overline{g}}_{\,i}\gets -[L_{s_{i-1}}^{s_i}-g_i](x_u)$\Comment{Precompute upper expectation of $g_i$}
%		\EndFor
%      \State $\hat{\underline{f}}\gets 1$\Comment{Partial solution starts with empty product}
%		\State $z \gets \text{``low"}$\Comment{Flag indicating lower or upper expectation}
%		\For{$i=m\to 0$}\Comment{Run through time points, from latest to earliest}
%		\State $g_i\gets f(X_{t_0},\ldots,X_{t_n},1,\ldots,X_{s_i},\ldots,1)$ \Comment{$g_i$ is restriction of $f$ to $\gambles(\states^{u\cup\{s_i\}})$}
%		\State $\hat{g}_i\gets 0$\Comment{Buffer for expectation}
%		\If{$z=\text{``low"}$}\Comment{Check whether to compute lower or upper}
%			\State $\hat{g}_i\gets [L_{s_{i-1}}^{s_i}g_i](x_u)$\Comment{Compute lower expectation}
%		\ElsIf{$z=\text{``up"}$}
%			\State $\hat{g}_i\gets -[L_{s_{i-1}}^{s_i}(-g_i)](x_u)$\Comment{Compute upper expectation}
%		\EndIf
%		\State $\hat{\underline{f}}\gets\hat{\underline{f}}\cdot\hat{g}_i$\Comment{Multiply with partial solution}
%		\If{$\hat{g}_i<0$}\Comment{If expectation was negative, we should switch}
%			\If{$z=\text{``low"}$}
%				\State $z\gets \text{``up"}$
%			\ElsIf{$z=\text{``up"}$}
%				\State $z\gets \text{``low"}$
%			\EndIf
%		\EndIf
%		\EndFor
%      \State \Return{$\hat{\underline{f}}$}\Comment{Return lower expectation}
%    \EndFunction
%  \end{algorithmic}
%\end{algorithm}


\section{Conclusions \& Future Work}\label{sec:conclusions}

*** kijk, dit hebben we besproken

*** wat hebben we daarvan geleerd?

*** wat kunnen we nog meer doen? Nou, bijvoorbeeld:

*** I would discuss sigma-additivity things here: explain that this is possible with our approach by using Kolmogorovs extension theorem, say that this would allow us to consider for example the lower and upper expected time till absorbtion. ***

*** mogelijk nog opmerking over dat dit Hidden-ICTMCs mogelijk maakt omdat GBR in subklasse van berekenbare functies zit

*** mogelijk iets over dat het handig kan zijn om voor berekenbaarheid nieuwe methodes te vinden om $\hat{f}_t^s$ numeriek uit te rekenen.


\bibliographystyle{plain} 
\bibliography{general}

\appendix

\section{Proofs and Lemmas for Section~\ref{sec:prelim}}

\begin{proof}[Proof of Proposition~\ref{prop:norm_properties}]
*** Most are obvious, as that's what it means to be a ``norm''. Others follow trivially from the definitions. Properties~\ref{N:normAB} and \ref{N:normAf} probably require a proof.
\end{proof}

\begin{lemma}\label{lemma:alt_axiom_prob}
Let $P$ be any map from $\power\times\nonemptypower$ to $\reals$. Then, $P$ is a full conditional probability if and only if it satisfies
\begin{enumerate}[label=P\arabic*:,ref=P\arabic*]
\item $P(\cdot\,\vert\,C)$ is a non-negative, (finitely-)additive function on $\power$, for all $C\in\nonemptypower$;\label{lem:alt_axiom_prob_1}
\item $P(A\,\vert\,C)=1$ for all $A\in\power$ and $C\in\nonemptypower$ such that $C\subseteq A$;\label{lem:alt_axiom_prob_2}
\item $P(A\cap D\,\vert\,C)=P(A\,\vert\,D\cap C)P(D\,\vert\,C)$ for all $A\in\power$ and $C,D\cap C\in\nonemptypower$.\label{lem:alt_axiom_prob_3}
\end{enumerate}
Note that because $\power$ and $\nonemptypower$ both contain subsets of $\Omega$, the subset and intersection relations in~\ref{lem:alt_axiom_prob_2} and~\ref{lem:alt_axiom_prob_3} are properly defined.
\end{lemma}
\begin{proof}
We start by proving the ``if" direction; if $P$ satisfies~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3}, it is a full conditional probability, i.e., it then satisfies~\ref{def:coh_prob_1}-\ref{def:coh_prob_4}.

Property~\ref{def:coh_prob_1} is immediate from~\ref{lem:alt_axiom_prob_2}. Furthermore, property~\ref{def:coh_prob_3} is the formal definition of (finite-)additivity from~\ref{lem:alt_axiom_prob_1}. The non-negativity in~\ref{def:coh_prob_2} follows from the non-negativity in~\ref{lem:alt_axiom_prob_1}. To fully prove~\ref{def:coh_prob_2}, it remains to show that for all $A\in\power$ and all $C\in\nonemptypower$, it holds that $P(A\,\vert\,C)\leq 1$. Note that $A\cap A^c=\emptyset$, where $A^c$ denotes the complement of $A$ in $\Omega$. Hence, by~\ref{def:coh_prob_3}, we have $P(A\,\vert\,C)+P(A^c\,\vert\,C)=P(A\cup A^c\,\vert\,C)=P(\Omega\,\vert\,C)=1$, where the last equality follows from~\ref{lem:alt_axiom_prob_2}, since $C\subseteq \Omega$. Hence, by non-negativity of $P(A\,\vert\,C)$ and $P(A^c\,\vert\,C)$, we find that $P(A\,\vert\,C)\leq 1$.

For property~\ref{def:coh_prob_4}, take any $A\in\power$ and any $C,D\in\nonemptypower$ such that $A\subseteq D\subseteq C$. Then clearly, $A\cap D=A$ and $D\cap C=D$. Hence, it also holds that $D\cap C\in\nonemptypower$. Therefore, by property~\ref{lem:alt_axiom_prob_3}, we have that $P(A\,\vert\,C)=P(A\cap D\,\vert\,C)=P(A\,\vert\,D\cap C)P(D\,\vert\,C)=P(A\,\vert\,D)P(D\,\vert\,C)$, which confirms~\ref{def:coh_prob_4}.

We now prove the other direction; if $P$ satisfies~\ref{def:coh_prob_1}-\ref{def:coh_prob_4}, it also satisfies~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3}.

The non-negativity in~\ref{lem:alt_axiom_prob_1} follows from~\ref{def:coh_prob_2}. The (finite-)additivity is equal to~\ref{def:coh_prob_3}, which concludes the proof for~\ref{lem:alt_axiom_prob_1}.

For property~\ref{lem:alt_axiom_prob_2}, take any $A\in\power$ and any $C\in\nonemptypower$ such that $C\subseteq A$. Let $B\coloneqq A\setminus C$. Then clearly, $B\in\power$, $B\cup C=A$, and $B\cap C=\emptyset$. Hence, we have $P(A\,\vert\,C)=P(B\cup C\,\vert\,C)$, and by~\ref{def:coh_prob_3}, we have $P(B\cup C\,\vert\,C)=P(B\,\vert\,C)+P(C\,\vert\,C)$. Using properties~\ref{def:coh_prob_1} and~\ref{def:coh_prob_2}, it then follows that $P(B\,\vert\,C)=0$, and furthermore, $P(A\,\vert\,C)=P(B\cup C\,\vert\,C)=1$, confirming~\ref{lem:alt_axiom_prob_2}. 

For property~\ref{lem:alt_axiom_prob_3}, we first show that $P(A\cap C\,\vert\,C) = P(A\,\vert\,C)$ for all $A\in\power$ and all $C\in\nonemptypower$. To show this, take any $A\in\power$, any $C\in\nonemptypower$, and let $B\coloneqq A\setminus(A\cap C)$. Then clearly, $A=B\cup(A\cap C)$ and $B\cap (A\cap C)=\emptyset$. Hence, by~\ref{def:coh_prob_3}, we have
\begin{align*}
P(A\,\vert\,C) = P(B\cup(A\cap C)\,\vert\,C)= P(B\,\vert\,C) + P(A\cap C\,\vert\,C).
\end{align*}
It remains to show that $P(B\vert\,C)=0$. Note that because $B=A\setminus(A\cap C)$, it follows that $B\cap C=\emptyset$. Therefore, by~\ref{def:coh_prob_3},
\begin{align*}
P(B\cup C\,\vert\,C)=P(B\,\vert\,C) + P(C\,\vert\,C) \\
P(B\cup C\,\vert\,C)=P(B\,\vert\,C) + 1\,,
\end{align*}
using~\ref{def:coh_prob_1}. Since $0\leq P(B\,\vert\,C)$ and $P(B\cup C\,\vert\,C)\leq 1$ by~\ref{def:coh_prob_2}, it follows that $P(B\,\vert\,C)=0$. Hence,
\begin{align*}
P(A\,\vert\,C) = P(B\cup(A\cap C)\,\vert\,C)= P(B\,\vert\,C) + P(A\cap C\,\vert\,C) = P(A\cap C\,\vert\,C)\,.
\end{align*}

Now, to prove property~\ref{lem:alt_axiom_prob_3}, take any $A\in\power$ and any $C,D\cap C\in\nonemptypower$. Let $E\coloneqq A\cap D\cap C$, and let $G\coloneqq D\cap C$. Then clearly, $E\subseteq G\subseteq C$. Hence, by~\ref{def:coh_prob_4},
\begin{align*}
P(E\,\vert\,C) &= P(E\,\vert\,G)P(G\,\vert\,C) \\
P(A\cap D\cap C\,\vert\,C) &= P(A\cap D\cap C\,\vert\,D\cap C)P(D\cap C\,\vert\,C)\,.
\end{align*}
Using the previously established property, we have that $P(A\cap D\cap C\,\vert\,C)=P(A\cap D\,\vert\,C)$, $P(A\cap D\cap C\,\vert\,D\cap C)=P(A\,\vert\,D\cap C)$, and $P(D\cap C\,\vert\,C)=P(D\,\vert\,C)$. Hence, we find
\begin{align*}
P(A\cap D\cap C\,\vert\,C) &= P(A\cap D\cap C\,\vert\,D\cap C)P(D\cap C\,\vert\,C) \\
P(A\cap D\,\vert\,C) &= P(A\,\vert\,D\cap C)P(D\,\vert\,C)\,,
\end{align*}
which proves~\ref{lem:alt_axiom_prob_3}.
\end{proof}

\begin{definition}
*** coherent on set
\end{definition}

\begin{corollary}
*** if map from subset of conditional events to R is coherent on this subset, it is a coherent conditional probability
\end{corollary}
\begin{proof}
(duh)
\end{proof}

\begin{lemma}
if map is coherent on set, then it is also coherent on any subset
\end{lemma}
\begin{proof}
todo
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:coherentextendable}]
We start by proving the ``if" direction; if $P$ can be extended to a full conditional probability, it is a coherent conditional probability on $\mathcal{C}\subseteq\power\times\nonemptypower$. Let $P$ be a map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$ such that it has an extension $P^*$ on $\power\times\nonemptypower$ that is a full conditional probability. 

Because events are subsets of $\Omega$, we have that the set $\power$ of all events is the power set of $\Omega$. Hence, $\power$ is an algebra of events. Furthermore, because $P^*$ satisfies~\ref{def:coh_prob_1}-\ref{def:coh_prob_4}, by Lemma~\ref{lemma:alt_axiom_prob}, it also satisfies~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3}. Therefore~\cite{berti1991coherent}, $P^*$ is a coherent conditional probability on $\power\times\nonemptypower$. Hence {(\bf LEMMA?)}, $P^*$ is coherent on the subset $\mathcal{C}\subseteq\power\times\nonemptypower$. Note that because $P^*$ is an extension of $P$, $P$ and $P^*$ take the same values on $\mathcal{C}$. Therefore, and because $P^*$ is coherent on $\mathcal{C}$, $P$ is also coherent on $\mathcal{C}$. Hence, $P$ is a coherent conditional probability.

We now prove the other direction; if $P$ is coherent on $\mathcal{C}\subseteq\power\times\nonemptypower$, it can be extended to a full conditional probability.
Let $P$ be a coherent conditional probability on $\mathcal{C}\subseteq\power\times\nonemptypower$. Then, there exists~\cite[Theorem 4]{regazzini1985finitely} an extension $P^*$ of $P$ to the set $\power\times\nonemptypower$, such that $P^*$ is a coherent conditional probability.

Recall that $\power$ is an algebra of events. Therefore~\cite{berti1991coherent}, and because $P^*$ is coherent on $\power\times\nonemptypower$, $P^*$ satisfies~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3}. Hence, by Lemma~\ref{lemma:alt_axiom_prob}, $P^*$ also satisfies~\ref{def:coh_prob_1}-\ref{def:coh_prob_4}. Therefore, $P^*$ is an extension of $P$ that is a full conditional probability.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:stochastic_from_rate_matrix}]
Let $T=[I+\Delta Q]$. We will verify the properties from Definition~\ref{def:stoch_matrix}.

We start with property S1. Consider any $x\in\states$. Then
\begin{equation*}
\sum_{y\in\states} T(x,y) = \sum_{y\in\states} [I + \Delta Q](x,y) = \sum_{y\in\states}I(x,y) + \Delta \sum_{y\in\states}Q(x,y) = 1\,,
\end{equation*}
where we used property R1 from Definition~\ref{def:rate_matrix}.

For property S2, note that $0\leq \Delta \leq \nicefrac{1}{\norm{Q}}$. Hence, for all $x\in\states$, we have $-1\leq \Delta Q(x,x) \leq 0$, so that $[I+\Delta Q](x,x) \geq 0$. Furthermore, for all $x,y\in\states$ s.t. $x\neq y$, we have $0\leq \Delta Q(x,y) \leq 1$, so that $[I+\Delta Q](x,y)\geq 0$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:rate_from_stochastic_matrix}]
This follows from a similar argument as the proof of Proposition~\ref{prop:stochastic_from_rate_matrix}; simply verify the properties from Definition~\ref{def:rate_matrix}.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:stochastic_processes}}

*** Next 2 Lemmas use old definitions, ignore for now please

\begin{lemma}
For any $t\in\realsnonneg$, let $\mathcal{A}_{>t}$ be the algebra of subsets of $\Omega$ generated by all the elementary events $(X_s=x)= \{\omega\in\Omega\,:\,\omega(s)=x\}$, for all $x\in\states$ and all $s>t$. Consider any pairwise-disjoint sequence $\{A_i\}_{i\in\nats}$ such that $A_i\in\mathcal{A}_{>t}$ for all $i\in\nats$ and $A_i\cap A_j=\emptyset$ for all $i,j\in\nats$ with $i\neq j$, and for which $\cup_{i=1}^\infty A_i\in\mathcal{A}_{>t}$. Then, there is some $n\in\nats$ such that, for all $j>n$, it holds that $A_j=\emptyset$.
\end{lemma}
\begin{proof}
{\bf TODO}, but fairly trivial since $\mathcal{A}_{>t}$ is the algebra generated by all elementary events.
\end{proof}

\begin{lemma}\label{lem:stoch_process_sigma_add_on_algebra}
Let $P:\mathcal{C}^{\mathrm{SP}}\to\reals$ be a stochastic process, where $\mathcal{C}^{\mathrm{SP}}$ is defined as in Definition~\ref{def:stoch_process}. Then, for all $t\in\realsnonneg$ and all $C\in\mathcal{F}_{\leq t}$, the map $P(\cdot\,\vert\,C):\mathcal{A}_{>t}\to\reals$ is $\sigma$-additive on $\mathcal{A}_{>t}$, meaning that for every pairwise-disjoint sequence $\{A_i\}_{i\in\nats}$ such that $A_i\in\mathcal{A}_{>t}$ for all $i\in\nats$ and $A_i\cap A_j=\emptyset$ for all $i,j\in\nats$ with $i\neq j$, and for which $\cup_{i=1}^\infty A_i\in\mathcal{A}_{>t}$, it holds that
\begin{equation*}
P\left(\bigcup_{i=1}^\infty A_i\,\Big\vert\,C\right) = \sum_{i=1}^\infty P(A_i\,\vert\,C)\,.
\end{equation*}
\end{lemma}
\begin{proof}
Consider any $t\in\realsnonneg$, any $C\in\mathcal{F}_{\leq t}$, and any pairwise-disjoint sequence $\{A_i\}_{i\in\nats}$ in $\mathcal{A}_{>t}$ such that $\cup_{i=1}^\infty A_i\in\mathcal{A}_{>t}$. Then, by Lemma {\bf REF}, there is some $n\in\nats$ such that, for all $j>n$, it holds that $A_j=\emptyset$. Hence, we find that the claim reduces to
\begin{align*}
P\left(\bigcup_{i=1}^\infty A_i\,\Big\vert\,C\right) &= \sum_{i=1}^\infty P(A_i\,\vert\,C) \\
%P\left(\bigcup_{i=1}^n A_i\,\Big\vert\,C\right) &= \sum_{i=1}^n P(A_i\,\vert\,C) + \sum_{i=n+1}^\infty P(A_i\,\vert\,C) \\
P\left(\bigcup_{i=1}^n A_i\,\Big\vert\,C\right) &= \sum_{i=1}^n P(A_i\,\vert\,C) + \sum_{i=n+1}^\infty P(\emptyset\,\vert\,C) \\
P\left(\bigcup_{i=1}^n A_i\,\Big\vert\,C\right) &= \sum_{i=1}^n P(A_i\,\vert\,C)\,,
\end{align*}
which we know to be true from finite-additivity of $P(\cdot\,\vert\,C)$.

**** Maybe expand this last argument a bit by linking it to~\ref{def:coh_prob_3}.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:boundednon-emptyandclosed}]
We only give the proof for $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. The proof for $\smash{\overline{\partial}_{-}
{T^t_{t,\,x_u}}}$ is completely analogous. The proof for $\smash{\overline{\partial}
{T^t_{t,\,x_u}}}$ then follows trivially because a union of two bounded, non-empty and closed sets is always bounded, non-empty and closed itself.

We start by establishing the boundedness of $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Since $P$ is well-behaved, it follows from Definition~\ref{def:well-behaved} that there is some $B>0$ and $\delta>0$ such that
\begin{equation}\label{eq:boundedbyB}
(\forall 0<\Delta<\delta)
~
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)}\leq B.
\end{equation}
Consider now any $Q\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Because of Equation~\eqref{eq:rightouterderivative}, $Q$ is the limit of a sequence of matrices $Q_k$, $k\in\nats$, defined by
\begin{equation}\label{eq:sequenceofQsinproof}
Q_k\coloneqq\frac{1}{\Delta_k}
(T^{t+\Delta_k}_{t,\,x_u}-I)
\text{~~for all $k\in\nats$}.
\end{equation}
Because of Equation~\eqref{eq:boundedbyB}, the norms $\norm{Q_k}$ of these matrices are eventually (for large enough $k$) bounded above by $B$. Hence, it follows that $\norm{Q}\leq B$. Since this is true for any $Q\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, we find that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is bounded.


In order to prove that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is non-empty, we consider any sequence $\Delta_k\to0^+$, $k\in\nats$. The corresponding sequence of matrices $Q_k$, $k\in\nats$, as defined by Equation~\eqref{eq:sequenceofQsinproof}, is then bounded because $P$ is well-behaved---see Definition~\ref{def:well-behaved}---and therefore, it follows from the Bolzano-Weierstrass theorem that it has a convergent subsequence $Q_{k_i}$, $i\in\nats$, of which we denote the limit by $Q^*$. Hence, we have found a sequence $\Delta_{k_i}\to0^+$, $i\in\nats$, such that $Q_{k_i}\to Q^*$.
Since we know from Lemma~\ref{prop:rate_from_stochastic_matrix} that each of the matrices $Q_{k_i}$, $i\in\nats$, is a rate matrix, the limit $Q^*$ is also a rate matrix, which therefore clearly belongs to $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$.

We end by showing that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is closed, or equivalently, that for any converging sequence $Q^*_k$, $k\in\nats$, of rate matrices in $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, the limit point $Q^*\coloneqq\lim_{k\to+\infty}Q^*_k$ is again an element of $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Since the rate matrices $Q^*_k$ belong to the bounded set $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, their limit $Q^*$ is a (real-valued) rate matrix. For any $k\in\nats$, it now follows from Definition~\eqref{eq:rightouterderivative} that there is some $0<\Delta_k<\nicefrac{1}{k}$ such that $\norm{Q_k-Q^*_k}\leq\nicefrac{1}{k}$, with $Q_k$ defined as in Equation~\eqref{eq:sequenceofQsinproof}.
Since
\begin{equation*}
0\leq\lim_{k\to+\infty}\norm{Q^*-Q_k}\leq\lim_{k\to+\infty}\norm{Q^*-Q^*_k}+\lim_{k\to+\infty}\norm{Q^*_k-Q_k}=0,
\end{equation*}
we find that the sequence $Q_k$, $k\in\nats$, converges to $Q^*$: $\lim_{k\to+\infty}Q_k=Q^*$. Hence, because $\lim_{k\to+\infty}\Delta_k=0$, we conclude that $Q^*\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:outerderivativebehaveslikelimit}]
Fix any $\epsilon>0$.
Assume \emph{ex absurdo} that
\begin{equation*}
(\forall\delta>0)(\exists0<\Delta<\delta)
(\forall Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}\geq\epsilon.
\end{equation*}
Clearly, this implies the existence of a sequence $\Delta_k\to0^+$, $k\in\nats$, such that
\begin{equation}\label{eq:boundednonelement}
\norm{Q_k-Q}\geq\epsilon
\text{~~for all $k\in\nats$ and all $Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}}$},
\end{equation}
with $Q_k$ defined as in Equation~\eqref{eq:sequenceofQsinproof}. As we know from the proof of Proposition~\ref{prop:boundednon-emptyandclosed}, the sequence $Q_k$, $k\in\nats$, has a convergent subsequence $Q_{k_i}$, $i\in\nats$, of which the limit $Q^*$ belongs to $\overline{\partial}_{+}
{T^t_{t,\,x_u}}$. Hence, since Equation~\eqref{eq:boundednonelement} implies that $\norm{Q^*-Q}\geq\epsilon$ for all $Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}}$, we find that $0=\norm{Q^*-Q^*}\geq\epsilon$. From this contradiction, it follows that there must be some $\delta_1>0$ such that Equation~\eqref{eq:outerderivativebehaveslikelimit1} holds for all $0<\Delta<\delta_1$. Similarly, using a completely analogous argument, we infer that there must be some $\delta_2>0$ such that Equation~\eqref{eq:outerderivativebehaveslikelimit2} holds for all $0<\Delta<\delta_1$. Now let $\delta\coloneqq\min\{\delta_1,\delta_2\}$.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:cont_time_markov_chains}}

\begin{proof}[Proof of Corollary~\ref{cor:rate_has_unique_homogen_markov_process}]
Since we know from Proposition~\ref{prop:systemQ} that $\mathcal{T}_Q$ is a well-behaved transition matrix system, it follows from Theorem~\ref{theo:uniqueMarkovchain} that there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$, and that this Markov chain is furthermore well-behaved. Since it---trivially---follows from Definition~\ref{def:systemfromQ} that $\mathcal{T}_Q$ satisfies Equation~\eqref{eq:homogeneousMarkov}, Definition~\ref{def:homogeneousMarkov} implies that $P$ is homogeneous.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:homogeneoushasQ}]
Because of Proposition~\ref{prop:boundednon-emptyandclosed}, we know that $\overline{\partial}_{+}
{T^0_{0}}$ is a non-empty bounded set of rate matrices, which implies that there is some real $B>0$ such that $\norm{Q'}\leq B$ for all $Q'\in\overline{\partial}_{+}
{T^0_{0}}$. Let $Q$ be any of element of $\overline{\partial}_{+}
{T^0_{0}}$.


Fix any $c\geq0$, $\epsilon>0$ and $\delta>0$. 
It then follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and~\ref{N:homogeneous} that there is some $\delta^*>0$ such that
\begin{equation}
\label{eq:homogeneoushasQ1}
(\forall 0<\Delta^*<\delta^*)
~
(\exists Q^*\in\overline{\partial}_{+}
{T^0_{0}})
~
\norm{T_0^{\Delta^*}-(I+\Delta^*Q^*)}<\Delta^*\epsilon.
\end{equation}
Furthermore, because of Equation~\eqref{eq:rightouterderivative} and~\ref{N:homogeneous}, there is some $0<\Delta<\min\{\delta,\delta^*\}$ such that
\begin{equation}
\label{eq:homogeneoushasQ2}
\norm{T^{\Delta}_{0}-(I+\Delta Q)}<\Delta\epsilon.
\end{equation}
If we now define $n\coloneqq\lfloor\nicefrac{c}{\Delta}\rfloor$ and $d\coloneqq c-n\Delta$, then $n\Delta\leq c<(n+1)\Delta$ and therefore also $0\leq d<\Delta$. Because of Proposition~\ref{prop:Markovhassystem}, Equation~\eqref{eq:transmatrixproduct} and Definition~\ref{def:homogeneousMarkov}, we know that
\begin{equation*}
T_0^c=\left(
\prod_{j=1}^{n}
T_{(j-1)\Delta}^{j\Delta}
\right)
T_{n\Delta}^c
=\left(T_0^{\Delta}\right)^{n}
T_0^{d}
\end{equation*}
and therefore, it follows from *** SOME LEMMA WITH A GENERAL VERSION OF THE RECURSIVELY PROVEN NORM INEQUALITY WE USE A LOT IN THIS PAPER *** that
\begin{equation}
\label{eq:homogeneoushasQ3}
\norm{
	e^{Qc}-T_0^c
}
=
\norm{
\left(T_0^{\Delta}\right)^{n}
T_0^{d}
-
\left(
e^{Q\Delta}
\right)^{n}
e^{Qd}
}
\leq
n\norm{T_0^{\Delta}-e^{Q\Delta}}
+\norm{T_0^{d}-e^{Qd}}.
\end{equation}
From Equation~\eqref{eq:homogeneoushasQ2} and Lemma~\ref{lemma:linearpartofexponential}, we infer that
\begin{equation}
\label{eq:homogeneoushasQ4}
\norm{T_0^{\Delta}-e^{Q\Delta}}
\leq
\norm{T_0^{\Delta}-(I+\Delta Q)}
+
\norm{(I+\Delta Q)-e^{Q\Delta}}
\leq
\Delta\epsilon
+
\Delta^2\norm{Q}^2.
\end{equation}
Since $d<\Delta<\delta^*$, we infer from Equation~\eqref{eq:homogeneoushasQ1} that there is some $Q^*\in\overline{\partial}_{+}
{T^0_{0}}$ such that $\norm{T_0^{d}-(I+d Q^*)}<d\epsilon$. Hence, also using Lemma~\ref{lemma:linearpartofexponential}, we find that
\begin{align}
\norm{T_0^{d}-e^{Qd}}
&\leq
\norm{T_0^{d}-(I+d Q^*)}
+
\norm{(I+d Q^*)-(I+d Q)}
+
\norm{(I+d Q)-e^{Qd}}\notag\\
&\leq
d\epsilon+d\norm{Q^*-Q}
+d^2\norm{Q}^2
\leq
d\epsilon+d\norm{Q^*}
+d\norm{Q}
+d^2\norm{Q}^2.\label{eq:homogeneoushasQ5}
\end{align}
By combining Equations~\eqref{eq:homogeneoushasQ3}, \eqref{eq:homogeneoushasQ4} and~\eqref{eq:homogeneoushasQ5}, it follows that
\begin{equation*}
\norm{
	e^{Qc}-T_0^c
}
\leq
n\Delta\epsilon
+
n\Delta^2\norm{Q}^2
+
d\epsilon
+d\norm{Q^*}
+d\norm{Q}
+d^2\norm{Q}^2.
\end{equation*}
Taking into account that $\norm{Q}\leq B$, $\norm{Q^*}\leq B$, $n\Delta\leq c$ and $d<\Delta<\delta$, this implies that
\begin{equation*}
\norm{
	e^{Qc}-T_0^c
}
\leq
c\epsilon
+
c\delta B^2
+
\delta\epsilon
+2\delta B
+\delta^2 B^2.
\end{equation*}
Since this is true for any $\epsilon>0$ and $\delta>0$, it follows that $\norm{e^{Qc}-T_0^c}\leq0$, which implies that $T_0^c=e^{Qc}$. Since this is true for all $c\geq0$, it follows from Definition~\ref{def:homogeneousMarkov} that
\begin{equation}\label{eq:homogeneoushasQ6}
T_t^s=T_0^{s-t}=e^{Q(s-t)}
\text{~~for all $0\leq t\leq s$,}
\end{equation}
or equivalently, that $\mathcal{T}_P=\mathcal{T}_Q$.

Finally, we prove that $Q$ is unique. Assume \emph{ex absurdo} that this is not the case, or equivalently, that there are rate matrices $Q_1$ and $Q_2$, with $Q_1\neq Q_2$, such that $\mathcal{T}_P=\mathcal{T}_{Q_1}$ and $\mathcal{T}_P=\mathcal{T}_{Q_2}$. It then follows from Definition~\ref{def:systemfromQ} that $\partial_{+}{T^0_{0}}=Q_1$ and $\partial_{+}{T^0_{0}}=Q_2$, which implies that $Q_1=Q_2$. From this contradiction, it follows $Q$ is indeed unique.
\end{proof}

\begin{lemma}\label{lemma:nonhomogen_trans_mat_system}
Let $Q_t$ be a left-continuous, piecewise-constant function that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$, such that $Q_t$ takes different values on at most a finite number of intervals. Specifically, let $Q_t$ be such that there exists some ordered $u\in\mathcal{U}$ with $u=t_0,\ldots,t_n$ and rate matrices $\tilde{Q}_1,\ldots,\tilde{Q}_n\in\mathcal{R}$ such that, for all $t\in\realsnonneg$,
\begin{equation*}
Q_t = \tilde{Q}_i\,,\quad\text{if $t\in(t_{i-1},t_{i}]$, for all $i\in\{1,\ldots,n\}$}\,,
\end{equation*}
and $Q_t=\tilde{Q}_1$ if $t\leq t_0$, and $Q_t=\tilde{Q}_n$ if $t>t_n$. For notational convenience, let $t_{-1}\coloneqq-1$, $t_{n+1}\coloneqq +\infty$, $\tilde{Q}_0\coloneqq \tilde{Q}_1$, and $\tilde{Q}_{n+1}\coloneqq \tilde{Q}_n$. Then, for all $t\in\realsnonneg$, it holds that
\begin{equation*}
Q_t = \tilde{Q}_i\,,\quad\text{if $t\in(t_{i-1},t_i]$, for all $i\in\{0,\ldots,n+1\}$.}
\end{equation*}

Consider now a family $\mathcal{T}_{Q_t}$ of matrices $T_t^s$, defined recursively for all $t,s\in\realsnonneg$ with $t\leq s$, such that
\begin{equation*}
T_t^s \coloneqq \left\{\begin{array}{ll}
e^{Q_s(s-t)} & \text{if $\bigl(\exists i\in\{0,\ldots,n+1\}\,:\,t,s\in[t_{i-1},t_i]\bigr)$, and} \\
T_t^{t_{i-1}}T_{t_{i-1}}^s & \text{where $i\in\{1,\ldots,n+1\}$ is such that $s\in(t_{i-1},t_i]$, otherwise.}
\end{array}\right.
\end{equation*}
Then, $\mathcal{T}_{Q_t}$ is a well-behaved transition matrix system.
\end{lemma}
\begin{proof}
For any $T_t^s\in\mathcal{T}_{Q_t}$, it is clear that $T_t^s$ is a transition matrix if $t$ and $s$ coincide on some interval $[t_{i-1},t_i]$, with $i\in\{0,\ldots,n+1\}$, because then $T_t^s=e^{Q_s(s-t)}$. If they do not coincide, say $t\in[t_{i-1},t_i]$ and $s\in[t_{j-1},t_j]$ for some $i,j\in\{0,\ldots,n+1\}$ with $i<j$, by recursion, we find that
\begin{equation*}
T_t^s = T_t^{t_i}T_{t_i}^{t_{i+1}}\cdots T_{t_{j-2}}^{t_{j-1}}T_{t_{j-1}}^s\,.
\end{equation*}
Note that on the right-hand side of this equality, for each factor $T_a^b$, the time points $a$ and $b$ coincide on some interval, and hence each $T_a^b$ is a transition matrix. Hence, the right-hand side of this equality is a composition of transition matrices, and hence $T_t^s$ is a transition matrix. Therefore, $\mathcal{T}_{Q_t}$ is clearly a family of transition matrices.

Furthermore, it is clear that for any $T_t^t\in\mathcal{T}_{Q_t}$, it holds that $T_t^t=I$, because $t$ and $t$ trivially coincide on some interval, so that we find $T_t^t=e^{Q_t\cdot 0}=I$. Hence, for $\mathcal{T}_{Q_t}$ to be a transition matrix system, it remains to show that all $T_t^s\in\mathcal{T}_{Q_t}$ satisfy Equation~\eqref{eq:transmatrixproduct}.

If $t$ and $s$ coincide on some interval, Equation~\eqref{eq:transmatrixproduct} trivially holds for $T_t^s$. If they do not coincide, a similar factorization as found above yields the same result. Hence, $\mathcal{T}_{Q_t}$ is a transition matrix system. 

It remains to show that $\mathcal{T}_{Q_t}$ is well-behaved, i.e., that it also satisfies Equation~\eqref{eq:wellbehavedtransitionmatrix}.
Note that in the first limit expression of~\eqref{eq:wellbehavedtransitionmatrix}, for small enough $\Delta$, the time points $t$ and $t+\Delta$ coincide on some interval. On this interval, it holds that there is some $Q\in\rateset$ such that $Q_{t+\Delta}=Q$ for all $\Delta>0$ such that $t$ and $t+\Delta$ coincide. Hence, we find that
\begin{equation*}
\lim_{\Delta\to0^+}\frac{1}{\Delta}\norm{T_t^{t+\Delta}-I} = \lim_{\Delta\to0^+}\frac{1}{\Delta}\norm{e^{Q_{t+\Delta}\cdot\Delta}-I} = \lim_{\Delta\to0^+}\frac{1}{\Delta}\norm{e^{Q\cdot\Delta}-I} = \norm{Q} < +\infty\,,
\end{equation*}
where the last equality used the well-known derivative of the matrix exponential. Similarly, for the second limit expression of~\eqref{eq:wellbehavedtransitionmatrix}, for small enough $\Delta$, it holds that $t-\Delta$ and $t$ coincide on some interval. Hence
\begin{equation*}
\lim_{\Delta\to0^+}\frac{1}{\Delta}\norm{T_{t-\Delta}^{t}-I} = \lim_{\Delta\to0^+}\frac{1}{\Delta}\norm{e^{Q_t\cdot \Delta}-I}=\norm{Q_t}<+\infty\,.
\end{equation*}
Therefore, $\mathcal{T}_{Q_t}$ is well-behaved.
\end{proof}

\begin{proposition}\label{prop:continuous_rate_matrix_has_process}
Consider any left-continuous, piecewise-constant function $Q_t$ that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$. Then, there exists a well-behaved continuous-time Markov chain $P\in\wmprocesses$ such that $\mathcal{T}_P=\mathcal{T}_{Q_t}$, where $\mathcal{T}_{Q_t}$ is defined as in Lemma~\ref{lemma:nonhomogen_trans_mat_system}.
\end{proposition}
\begin{proof}
By Lemma~\ref{lemma:nonhomogen_trans_mat_system}, $\mathcal{T}_{Q_t}$ is a well-behaved transition matrix system. Therefore, by Theorem~{\bf REF}, there exists a well-behaved continuous-time Markov chain $P\in\wmprocesses$ such that $\mathcal{T}_P=\mathcal{T}_{Q_t}$.
\end{proof}

\begin{lemma}\label{lemma:nonhomogeneous_in_process_set}
Let $P\in\wmprocesses$ be such that there exists a left-continuous, piecewise-constant function $Q_t$ such that $\mathcal{T}_P=\mathcal{T}_{Q_t}$, where $\mathcal{T}_{Q_t}$ is defined as in Lemma~\ref{lemma:nonhomogen_trans_mat_system}. Then, for any set of rate matrices $\rateset$ for which it holds that $Q_t\in\rateset$ for all $t\in\realsnonneg$, it holds that $P\in\wmprocesses_\rateset$.
\end{lemma}
\begin{proof}
Note that, by the definition of $\mathcal{T}_{Q_t}$, it holds for all $t\in\realsnonneg$ that $\partial_+T_t^t$ and $\partial_-T_t^t$ exist, and that they satisfy $\partial_+T_t^t,\partial_-T_t^t\in\rateset$. Therefore, and because $P$ is well-behaved, by Corollary~\ref{corol:outersingleton} we have that $\smash{\overline{\partial}}_+T_t^t=\{\partial_+T_t^t\}$ and $\smash{\overline{\partial}}_-T_t^t=\{\partial_-T_t^t\}$. Hence, we find that
\begin{equation*}
\smash{\overline{\partial}}T_t^t = \smash{\overline{\partial}}_+T_t^t\cup \smash{\overline{\partial}}_-T_t^t = \{\partial_+T_t^t, \partial_-T_t^t\}\,,
\end{equation*}
from which it follows that, for all $t\in\realsnonneg$, it holds that $\smash{\overline{\partial}}T_t^t\subseteq\rateset$. Therefore, by Definition~\ref{def:markov_process_set_new}, we find that $P\in\wmprocesses_\rateset$.
\end{proof}

\begin{lemma}\label{lemma:linearpartofexponential}
Consider any $Q\in\mathcal{R}$ and any $\Delta\geq0$. Then
\begin{equation*}
\norm{e^{Q\Delta}-(I+\Delta Q)}\leq
\Delta^2\norm{Q}^2.
\end{equation*}
\end{lemma}
\begin{proof}
*** this follows by combining Lemma~\ref{lemma:justthelinearpart} with the limit expression for the exponential ***
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:iCTMC}}

\begin{lemma}\label{lemma:bound_on_linear_approx_partition}
Consider any $P\in\wprocesses_\rateset$, any $0<t<s$ and any $x_u$. Then for all $\epsilon>0$ and $\delta>0$, there is some $v\in\mathcal{U}_{[t,s]}$ such that $\sigma(v)<\delta$ and, for all $i\in\{0,\dots,n-1\}$:
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{t_{i+1}}_{t_i,\,x_u}-(I+\Delta_{i+1}Q)
}<\Delta_{i+1}\epsilon
\end{equation*}
\end{lemma}
\begin{proof}
Fix any $\epsilon>0$ and $\delta>0$.
For any $r\in[t,s]$, it follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and Definition~\eqref{def:nonmarkovsetQ} that there is some $0<\delta_r<\delta$ such that, for all $0<\Delta<\delta_r$:
\begin{equation}\label{eq:epsilonboundsforlemma}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{r+\Delta}_{r,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{r}_{r-\Delta,\,x_u}-I)-Q}<\epsilon.
\end{equation}
Let $U_r\coloneqq(r-\delta_r,r+\delta_r)$. Then the set $C\coloneqq\{U_r\colon r\in[t,s]\}$ is an open cover of $[t,s]$. By the Heine-Borel theorem, $C$ contains a finite subcover $C^*$ of $[t,s]$. Without loss of generality, we can take this subcover to be minimal, in the sense that if we remove any of its elements, it is no longer a cover. Let $m$ be the cardinality of $C^*$ and let $r_1<r_2<\dots<r_m$ be the ordered sequence of the midpoints of the intervals in $C^*$.

We will now prove that
\begin{equation}\label{eq:orderingofbounds}
r_i-\delta_{r_i}<r_j-\delta_{r_j}
\text{~~and~~}
r_i+\delta_{r_i}<r_j+\delta_{r_j}
\text{~~for all $1\leq i<j\leq m$.}
\end{equation}
Assume \emph{ex absurdo} that this statement is not true. Then this implies that there are $1\leq i<j\leq m$ such that either $r_i-\delta_{r_i}\geq r_j-\delta_{r_j}$ or $r_i+\delta_{r_i}\geq r_j+\delta_{r_j}$. If $r_i-\delta_{r_i}\geq r_j-\delta_{r_j}$, then since $i<j$ implies that $r_i<r_j$, it follows that $\delta_{r_j}\geq\delta_{r_i}+r_j-r_i>\delta_{r_i}$ and therefore, that $r_j+\delta_{r_j}>r_i+\delta_{r_i}$. 
Hence, we find that $U_{r_i}\subseteq U_{r_j}$. Since $C^*$ was taken to be a minimal cover, this is a contradiction.
Similarly, if $r_i+\delta_{r_i}\geq r_j+\delta_{r_j}$, then since $i<j$ implies that $r_i<r_j$, it follows that $\delta_{r_i}\geq\delta_{r_j}+r_j-r_i>\delta_{r_j}$ and therefore, that $r_i-\delta_{r_i}<r_j-\delta_{r_i}$. Hence, we find that $U_{r_j}\subseteq U_{r_i}$. Since $C^*$ was taken to be a minimal cover, this is again a contradiction. From these two contradictions, it follows that Equation~\eqref{eq:orderingofbounds} is indeed true.

Next, we prove that
\begin{equation}\label{eq:overlapasyouwantit}
r_{k+1}-\delta_{r_{k+1}}<r_k+\delta_{r_k}
\text{~~for all $k\in\{1,\dots,m-1\}$.}
\end{equation}
Assume \emph{ex absurdo} that this statement is not true or, equivalently, that there is some $k\in\{1,\dots,m-1\}$ such that $r_k+\delta_{r_k}\leq r_{k+1}-\delta_{r_{k+1}}$. For all $i\in\{k+1,\dots, m\}$, it then follows from Equation~\eqref{eq:orderingofbounds} that $r_k+\delta_{r_k}\leq r_i-\delta_{r_i}$, which implies that $r_k+\delta_{r_k}\notin U_{r_i}$. Similarly, for all $i\in\{1,\dots,k\}$, it follows from Equation~\eqref{eq:orderingofbounds} that $r_i+\delta_{r_i}\leq r_k+\delta_{r_k}$, which again implies that $r_k+\delta_{r_k}\notin U_{r_i}$. 
Hence, for all $i\in\{1,\dots,m\}$, we have found that $r_k+\delta_{r_k}\notin U_{r_i}$. 
Since $C^*$ is a cover of $[t,s]$, this implies that $r_k+\delta_{r_k}\notin[t,s]$, which, since $r_k\in[t,s]$, implies that $r_k+\delta_{r_k}>s$. Hence, since we know from Equation~\eqref{eq:orderingofbounds} that $r_k-\delta_{r_k}<r_m-\delta_{r_m}$, it follows that $U_{r_m}\cap[t,s]\subseteq U_{r_k}\cap[t,s]$. This contradicts the fact that $C^*$ was taken to be a minimal cover.

For all $k\in\{1,\dots,m-1\}$, we now define $q_k\coloneqq\nicefrac{1}{2}(r_k+\delta_{r_k}+r_{k+1}-\delta_{r_{k+1}})$.
Using Equation~\eqref{eq:orderingofbounds}, it then follows that
\begin{equation*}
q_k<\frac{r_{k+1}+\delta_{r_{k+1}}+r_{k+1}-\delta_{r_{k+1}}}{2}=r_{k+1}
\text{~~and~~}
q_k>\frac{r_{k}+\delta_{r_{k}}+r_{k}-\delta_{r_{k}}}{2}=r_{k},
\end{equation*}
and Equation~\eqref{eq:overlapasyouwantit} trivially implies that $r_{k+1}-\delta_{r_{k+1}}<q_k<r_k+\delta_{r_k}$. Hence,
\begin{equation*}
r_k<q_k<r_k+\delta_{r_k}
\text{~~and~~}
r_{k+1}-\delta_{r_{k+1}}<q_k<r_{k+1}.
\end{equation*}
Because of Equation~\eqref{eq:epsilonboundsforlemma}, and with $\Delta^*_k\coloneqq q_k-r_k$ and $\Delta^{**}_k\coloneqq r_{k+1}-q_k$, this implies that
\begin{equation}\label{eq:epsilonboundsforqk}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta^*_k}
(T^{q_k}_{r_k,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta^{**}_k}
(T^{r_{k+1}}_{q_k,\,x_u}-I)-Q}<\epsilon.
\end{equation}

For all $k\in\{1,\dots,m-1\}$, we now let $s_{2k}\coloneqq q_k$ and, for all $k\in\{1,\dots,m\}$, we let $s_{2k-1}\coloneqq r_k$. For the resulting sequence $s_1<s_2<\dots<s_{2m-2}<s_{2m-1}$, it then follows from Equation~\eqref{eq:epsilonboundsforqk} and~\ref{N:homogeneous} that, for all $i\in\{1,\dots,2m-2\}$:
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{s_{i+1}}_{s_i,\,x_u}-(I+\Delta'_{i+1}Q)
}<\Delta'_{i+1}\epsilon,
\end{equation*}
with $\Delta'_{i+1}\coloneqq s_{i+1}-s_i<\delta$. 

If $s_1\neq t$, we also define $s_0\coloneqq t$ and $\Delta'_1\coloneqq s_1-s_0=r_1-t>0$. Since $C^*$ is a minimal cover, and because of Equation~\eqref{eq:orderingofbounds}, it follows that $r_1-\delta_{r_1}<t<r_1$, which, because of Equation~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous}, implies that
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{s_{1}}_{s_0,\,x_u}-(I+\Delta'_{1}Q)
}<\Delta'_{1}\epsilon.
\end{equation*}

If $s_{2m-1}\neq s$, we also define $s_{2m}\coloneqq s$ and $\Delta'_{2m}\coloneqq s_{2m}-s_{2m-1}=s-r_m>0$. Since $C^*$ is a minimal cover, and because of Equation~\eqref{eq:orderingofbounds}, it follows that $r_m<s<r_m+\delta_{r_m}$, which, because of Equation~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous}, implies that
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{s_{2m}}_{s_{2m-1},\,x_u}-(I+\Delta'_{2m}Q)
}<\Delta'_{2m}\epsilon.
\end{equation*}

We now consider four cases.
If $s_1=t$ and $s_{2m-1}=s$, the result follows by letting $n\coloneqq 2m-2$ and defining $t_i\coloneqq s_{i+1}$ for all $i\in\{0,\dots,n\}$. 
If $s_1=t$ and $s_{2m-1}\neq s$, the result follows by letting $n\coloneqq 2m-1$ and defining $t_i\coloneqq s_{i+1}$ for all $i\in\{0,\dots,n\}$.
If $s_1\neq t$ and $s_{2m-1}=s$, the result follows by letting $n\coloneqq 2m-1$ and defining $t_i\coloneqq s_{i}$ for all $i\in\{0,\dots,n\}$.
If $s_1\neq t$ and $s_{2m-1}\neq s$, the result follows by letting $n\coloneqq 2m$ and defining $t_i\coloneqq s_{i}$ for all $i\in\{0,\dots,n\}$.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:lowertrans}}

***** Still need to add proofs for \ref{lem:normlratefinite} \ref{lemma:normofcoherenttrans} \ref{lemma:normQsmallenough} \ref{lemma:lower_trans_to_lower_rate}.

\begin{proof}[Proof of Proposition~\ref{lemma:completemetricspace}]
Define for all $\lt\in\underline{\mathcal{T}}$ and all $x\in\states$ the map $\lt_x:\gamblesX\rightarrow\reals$ as
\begin{equation*}
\lt_x(f) \coloneqq \lt(f)(x)\,,\quad\text{for all $f\in\gamblesX$,}
\end{equation*}
and let $\underline{\mathcal{T}}_x\coloneqq\left\{\lt_x\,:\,\lt\in\underline{\mathcal{T}}\right\}$ for all $x\in\states$. Then clearly, any $\lt\in\underline{\mathcal{T}}$ is a finite vector of elements $\lt_1\in\underline{\mathcal{T}}_1,\ldots,\lt_m\in\underline{\mathcal{T}}_m$, and $\underline{\mathcal{T}}=\underline{\mathcal{T}}_1\times\cdots\times \underline{\mathcal{T}}_m$.

Furthermore, for all $x\in\states$ and all $\lt_x\in\underline{\mathcal{T}}_x$, because of Definition~\ref{def:coh_low_trans}, $\lt_x$ is a map from $\gamblesX$ to $\reals$ that is super-additive, positively homogeneous, and bounded below by the minimum operator. Hence, by definition~\cite[Definition~2.3.3]{Walley:1991vk}, $\lt_x$ is a coherent lower prevision on $\gamblesX$, and $\underline{\mathcal{T}}_x$ corresponds to the space of all coherent lower previsions on $\gamblesX$.

Let now $\gambles_{\leq1}(\states)\coloneqq\left\{f\in\gamblesX\,:\,\forall x\in\states, 0\leq f(x)\leq 1\right\}$ be the set of non-negative functions $f\in\gamblesX$ with $\norm{f}\leq 1$. It was previously shown~\cite{DeBock:2015ck} that the metric space $(\underline{\mathcal{T}}_x,d_x)$ is compact under the topology generated by the metric $d_x$, defined for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$ by
\begin{equation*}
d_x(\lt_x,\underline{S}_x) \coloneqq \sup\bigl\{ \abs{\lt_xf - \underline{S}_xf} \,:\,f\in\gambles_{\leq1}(\states) \bigr\}\,.
\end{equation*}
Now consider the metric $d_x^*$, which we define for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$ by
\begin{equation*}
d_x^*(\lt_x,\underline{S}_x) \coloneqq \sup\left\{ \abs{\lt_xf - \underline{S}_xf}\,:\, f\in\gamblesX, \norm{f}=1\right\}\,.
\end{equation*}
It is fairly easy to see that for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$, we have that
\begin{equation*}
d_x(\lt_x,\underline{S}_x) \leq d_x^*(\lt_x,\underline{S}_x) \leq 2d_x(\lt_x,\underline{S}_x)\,,
\end{equation*}
from which it follows that any subset of $\underline{\mathcal{T}}_x$ that is open with respect to $d_x$ is also open with respect to $d_x^*$. Hence, $d_x$ and $d_x^*$ generate the same topology on $\underline{\mathcal{T}}_x$, and therefore because $(\underline{\mathcal{T}}_x, d_x)$ is compact, so is $(\underline{\mathcal{T}}_x,d_x^*)$. Because $(\underline{\mathcal{T}}_x,d_x^*)$ is a metric space, compactness now implies that $(\underline{\mathcal{T}}_x,d_x^*)$ is complete.

Therefore~\cite[Theorem 10.5.1]{OSearcoid:2006}, and because $\underline{\mathcal{T}}$ is a finite product of $\underline{\mathcal{T}}_1,\ldots,\underline{\mathcal{T}}_m$, the metric space $(\underline{\mathcal{T}},D)$ is complete under any metric $D$ that is \emph{conserving}~\cite[Definition 1.6.2]{OSearcoid:2006} with respect to $d_1^*,\ldots,d_m^*$. For $D$ to be conserving, it suffices to show that for all $\lt,\underline{S}\in\underline{\mathcal{T}}$,
\begin{equation*}
D(\lt, \underline{S}) = \max_{x\in\states}\bigl\{d_x^*(\lt_x, \underline{S}_x)\bigr\}\,.
\end{equation*}
Consider the metric $d$ on $\underline{\mathcal{T}}$ that is induced by $\norm{\cdot}$. We have, for any $\lt,\underline{S}\in\underline{\mathcal{T}}$, 
\begin{align*}
d(\lt,\underline{S}) &= \norm{\lt - \underline{S}} \\
% &= \sup\left\{ \norm{\lt f - \underline{S}f}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
% &= \sup\left\{ \max_{x\in\states}\{\abs{\lt(f)(x) - \underline{S}(f)(x)}\}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
 &= \sup\left\{ \max_{x\in\states}\{\abs{\lt_xf - \underline{S}_xf}\}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
 &=  \max_{x\in\states}\bigl\{\sup\left\{\abs{\lt_xf - \underline{S}_xf}\,:\,f\in\gamblesX, \norm{f}=1 \bigr\}\right\} \\
 &=  \max_{x\in\states}\left\{d_x^*(\lt_x, \underline{S}_x)\right\}\,.
\end{align*}
Thus, $d$ is conserving with respect to $d_1^*,\ldots,d_m^*$, and hence $(\underline{\mathcal{T}},d)$ is complete.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttransrate}
Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lrate$ be an arbitrary lower transition rate operator. Then, it holds that $\norm{\lrate A-\lrate B}\leq 2\norm{\lrate}\norm{A-B}$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from the definition of the norm and the properties of $\lrate$.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttrans}
Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lt$ be an arbitrary lower transition operator. Then, it holds that $\norm{\lt A-\lt B}\leq \norm{A-B}$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:justtheindicator}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
&=\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)+\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{(I+\Delta_n\lrate)-I}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&=\Delta_n\norm{\lrate}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttrans}. By repeating this argument over and over again (actually, by induction), we find that
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
\leq \Delta_n\norm{\lrate} +\Delta_{n-1}\norm{\lrate}+\cdots
+\Delta_1\norm{\lrate}
=\Delta\norm{\lrate}.
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:justthelinearpart}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1\norm{\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I},
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttransrate}. Due to Lemma~\ref{lemma:justtheindicator}, this implies that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right).
\end{align*}
By continuing in this way (applying induction) we find that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq
\norm{\prod_{i=n}^n(I+\Delta_i\lrate)-(I+\sum_{i=n}^n\Delta_i\lrate)}
+2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
&=2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
&=2\norm{\lrate}^2\sum_{k=1}^n\Delta_k\sum_{i=k+1}^n\Delta_i\\
&\leq2\norm{\lrate}^2\frac{1}{2}\left(\sum_{k=1}^n\Delta_k\right)^2=\Delta^2\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:differencebetweennested}
For any $k\in\{1,\dots,n\}$, consider a sequence of $\Delta_{k,i}>0$, $i=1,\dots,n_k$ and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{n,k}$. Let $\Delta\coloneqq\sum_{k=1}^n\Delta_k$ and let $\alpha\coloneqq\max\{\Delta_k\colon k\in\{1,\dots,n\}\}$. If $\alpha\leq\nicefrac{1}{\norm{\lrate}}$, then
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&=\norm{\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&=\norm{
\left(
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
},
\end{align*}

\noindent
which, because of Lemma~\ref{lemma:productiscoherent}, \ref{lemma:normofcoherenttrans} and~\ref{lemma:differencenormofcoherenttrans}, implies that

\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
(I+\Delta_n\lrate)
}\\
&\leq
\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
+
\Delta_n^2\norm{\lrate}^2,
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:justthelinearpart}.

By continuing in this way (applying induction), we find that
\begin{align*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
&\leq
\Delta_1^2\norm{\lrate}^2+\cdot+\Delta_k^2\norm{\lrate}^2+\cdot
+
\Delta_n^2\norm{\lrate}^2\\
&\leq
\alpha\Delta_1\norm{\lrate}^2+\cdot+\alpha\Delta_k\norm{\lrate}^2+\cdot
+
\alpha\Delta_n\norm{\lrate}^2\\
&=
\alpha\Delta\norm{\lrate}^2
\end{align*}
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:differencebetweenu}]
Consider any $u'\in\mathcal{U}_{[t,s]}$ that is finer than $u$ and $u^*$, meaning that the timepoints it consists of contain the timepoints in $u$ and the timepoints in $u^*$. For example, let $u'$ be the ordered union of the timepoints in $u$ and $u^*$.

This implies that, for all $k\in\{1,\dots,n\}$, there is some sequence $\Delta_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta_k=\sum_{i=1}^{n_k}\Delta_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_u}\leq\alpha\Delta\norm{\lrate}^2$. 

Similarly, for all $k\in\{1,\dots,n^*\}$, there is some sequence $\Delta^*_{k,i}>0$, $i\in\{1,\dots,n^*_k\}$, such that $\Delta^*_k=\sum_{i=1}^{n^*_k}\Delta^*_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^{n^*}\left(\prod_{i=1}^{n^*_k}(I+\Delta^*_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_{u^*}}\leq\alpha\Delta\norm{\lrate}^2$.

Hence, we find that
\begin{equation*}
\norm{\Phi_{u}-\Phi_{u^*}}
=
\norm{\Phi_{u}-\Phi_{u'}+\Phi_{u'}-\Phi_{u^*}}
\leq
\norm{\Phi_{u}-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u^*}}
\leq2\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{proof}

\begin{proof}[Proof of Corollary~\ref{corol:limitexistsandiscoherent}]
Since $\lim_{n\to\infty}\sigma(u_n)=0$, and because of Lemma~\ref{lemma:productiscoherent}, there is some index $i$ such that the sequence $\Phi_{u_i},\Phi_{u_{i+1}},\dots,\Phi_{u_n},\dots$ consists of lower transition operators. Due to Corollary~\ref{corol:cauchy}, this sequence is Cauchy and therefore, because of Lemma~\ref{lemma:completemetricspace}, this sequence has a limit that is also a lower transition operator. Since the limit starting from $i$ and the limit starting from $1$ are identical (initial elements do not influence the limit), we find that the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ has a limit, and that this limit is a lower transition operator.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:convergencelowerbound}]
Start by considering any sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$. Due to Corollary~\ref{corol:limitexistsandiscoherent}, the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a lower transition operator, which we denote by $\lbound_t^s$. 

Consider now any $\epsilon>0$ and let $\Delta\coloneqq s-t$ and
\begin{equation*}
\delta\coloneqq\min\left\{\frac{\epsilon}{4\Delta\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
\end{equation*}

\noindent Since $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to $\lbound_t^s$, there is some $N\in\nats$ such that
\begin{equation*}
(\forall n\geq N)~\norm{\lbound_t^s - \Phi_{u_n}}<\frac{\epsilon}{2}.
\end{equation*}
Therefore, since $\lim_{n\to\infty}\sigma(u_n)=0$, there is some $N^*\geq N$ such that
\begin{equation*}
\sigma(u_{N^*})<\delta\text{ and }\norm{\lbound_t^s - \Phi_{u_{N^*}}}<\frac{\epsilon}{2}
\end{equation*}

\noindent Consider now any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then

\begin{equation*}
\norm{\lbound_t^s - \Phi_u}\leq\norm{\lbound_t^s-\Phi_{u_{N^*}}}
+\norm{\Phi_{u_{N^*}}-\Phi_u}
<\frac{\epsilon}{2}+2\delta\Delta\norm{\lrate}^2\leq\epsilon,
\end{equation*}
where the strict inequality follows from Proposition~\ref{prop:differencebetweenu}.
In summary, we have shown that there is lower transition operator $\lbound_t^s$ such that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon\,.
\end{equation*}
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:lower_transition_has_deriv}]
The proposition claims that $\frac{d}{dt}L_t^s=-\lrate L_t^s$ and $\frac{d}{ds}L_t^s=L_t^s\lrate$, meaning that
\begin{equation}\label{eq:lower_deriv_backward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert <\delta)~
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation}
and
\begin{equation}\label{eq:lower_deriv_forward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert<\delta)~
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon.
\end{equation}
We start by proving Equation~\eqref{eq:lower_deriv_backward}. Choose any $\epsilon\in\realspos$, and let
\begin{equation}\label{eq:derivative_max_delta}
\delta \coloneqq \frac{\epsilon}{3\norm{\lrate}^2}.
\end{equation}
Then, consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
\begin{equation*}
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation*}
Start by rewriting the statement slightly to prevent having to consider different cases for positive and negative $\Delta$. Let $t'\coloneqq\max\{t,t+\Delta\}$, and let $\Delta'\coloneqq t'-t$. Then,
\begin{equation*}
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert = \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s}\,.
\end{equation*}
Now,
\begin{align*}
\norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &= \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^{t'}L_{t'}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}L_{t'}^s} \\
 &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}}\norm{L_{t'}^s} \\
 &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}} \\
 &= \norm{\frac{I - L_{0}^{\lvert\Delta\rvert}}{\lvert\Delta\rvert}+\lrate L_{0}^{\Delta'}} \\
 &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate L_{0}^{\Delta'}} \\
 &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate} + \norm{\lrate - \lrate L_{0}^{\Delta'}} \\
 &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + 2\norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2.
\end{align*}
Because $\Delta'$ satisfies either $\Delta'=0$ or $\Delta'=\lvert\Delta\rvert$, we have that $\Delta'\leq\lvert\Delta\rvert$. Thus,
\begin{align*}
\norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2 \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &= 3\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &< 3\delta\norm{\lrate}^2 \\
 &= \epsilon\,,
\end{align*}
where the last step used Equation~\eqref{eq:derivative_max_delta}. This concludes the proof of Equation~\eqref{eq:lower_deriv_backward}. We will now prove Equation~\eqref{eq:lower_deriv_forward}.

Again choose any $\epsilon\in\realspos$, and let $\delta$ be given by
\begin{equation*}
\delta \coloneqq \frac{\epsilon}{2\norm{\lrate}^2}\,.
\end{equation*}
Consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
\begin{equation*}
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon\,.
\end{equation*}
We again first rewrite the statement to prevent having to perform case-work in the sign of $\Delta$. Let $s'\coloneqq\min\{s,s+\Delta\}$, and let $\Delta'\coloneqq s-s'$. Then,
\begin{equation*}
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert = \norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate}\,.
\end{equation*}
Now,
\begin{align*}
\norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate} &= \norm{\frac{L_t^{s'}L_{s'}^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'}L_{s'}^{s'+\Delta'}\lrate} \\
 &\leq \norm{L_t^{s'}}\norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
 &\leq \norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
 &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{0}^{\Delta'}\lrate} \\
 &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - \lrate} + \norm{\lrate - L_{0}^{\Delta'}\lrate} \\
 &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + \norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \Delta'\norm{\lrate}^2 \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &= 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &< 2\delta\norm{\lrate}^2 \\
 &= \epsilon\,.
\end{align*}
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:connections}}

\begin{proof}[Proof of Proposition~\ref{prop:lowerenvelopeislowertrans}]
Consider any $Q\in\rateset$. It then follows from Definition~\ref{def:rate_matrix} that the matrix $Q$, when regarded as a map from $\gamblesX$ to $\gamblesX$, satisfies \ref{LR:constantzero}--\ref{LR:nondiagpos}. Since each of these properties is preserved under taking lower envelopes, it follows that $\lrate$ satisfies \ref{LR:constantzero}--\ref{LR:nondiagpos}, which means that $\lrate$ is a lower transition rate operator.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:dominating_nonempty_bounded}]
Fix any $f\in\gamblesX$. Choose $\Delta>0$ small enough such that $0\leq\Delta\norm{\lrate}\leq 1$ (this always possible because of Lemma~\ref{lem:normlratefinite}). Define $\lt\coloneqq I+\Delta\lrate$. Since $\lrate$ is a lower transition rate operator, it follows from Lemma~\ref{lemma:normQsmallenough} that $\lt$ is a lower transition operator. For any $x\in\states$, we now let
\begin{equation*}
\lt_xg\coloneqq(\lt g)(x)
\text{~~for all $g\in\gamblesX$.}
\end{equation*}
Since $\lt$ is a lower transition operator, it follows that $\lt_x\colon \gamblesX\to\reals$ is super-additive, positively homogeneous and bounded below by the minimum operator. Hence, by definition~\cite[Definition~2.3.3]{Walley:1991vk}, $\lt_x$ is a coherent lower prevision on $\gamblesX$. Because of \cite[Theorem~3.3.3(b)]{Walley:1991vk}, this implies the existence of an expectation operator $E_x$ on $\gamblesX$---Reference~\cite{Walley:1991vk} calls this a linear prevision on $\gamblesX$---such that $E_xg\geq\lt_xg$ for all $g\in\gamblesX$ and $E_xf=\lt_xf$. Let $P_x$ be the unique probability mass function that corresponds to $E_x$. For all $x,y\in\states$, we now let $T(x,y)\coloneqq P_x(y)\coloneqq E_x(\ind{x})$. Then $T$ is clearly a stochastic matrix. Furthermore, for every $x\in\states$ and $g\in\gamblesX$, we have that $(Tg)(x)=E_xg$. Hence, it follows that $Tg\geq\lt g$ for all $g\in\gamblesX$ and that $Tf=\lt f$. Now let $Q\coloneqq\nicefrac{1}{\Delta}(T-I)$, which, because of Proposition~\ref{prop:rate_from_stochastic_matrix}, is a rate matrix. Since $Tf=\lt f$, it then follows that
\begin{equation*}
Qf=\frac{1}{\Delta}(Tf-f)\geq\frac{1}{\Delta}{\lt f-f}=\lrate f.
\end{equation*}
Similarly, since $Tg\geq\lt g$ for all $g\in\gamblesX$, it follows that $Qg\geq\lrate g$, or equivalently, since $Q$ is a rate matrix, that $Q\in\rateset_{\lrate}$. Since $f$ was arbitrary, this proves that, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $Qf=\lrate f$. Since $\gamblesX$ is non-empty, this clearly implies that $\rateset_{\lrate}$ is non-empty.

We end this proof by showing that $\rateset_{\lrate}$ is bounded. Consider any $x\in\states$. Then for all $Q\in\rateset_{\lrate}$, we have that $Q(x,x)=(Q\ind{x})(x)\geq(\lrate\ind{x})(x)$, which implies that
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset_{\lrate}\right\}\geq(\lrate\ind{x})(x)>-\infty.
\end{equation*}
Since $x\in\states$ is arbitary, Proposition~\ref{prop:alternativedefforbounded} now guarantees that $\rateset_{\lrate}$ is bounded. 
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:dominatingproperties}]
We start by showing that $\rateset_{\lrate}$ is closed, or equivalently, that for any converging sequence $\{Q_i\}_{i\in\nats}$ in $\rateset_{\lrate}$, the limit $Q\coloneqq\lim_{i\to+\infty}Q_i$ is again an element of $\rateset_{\lrate}$. Because $\{Q_i\}_{i\in\nats}$ is in the bounded set $\rateset_{\lrate}$ of rate matrices, we know that the limit $Q$ is again a rate matrix. Now, assume \emph{ex absurdo} that $Q\notin\rateset_{\lrate}$. Then, by Equation~\eqref{eq:dominatingratematrices}, there is some $f\in\gamblesX$ and some $x\in\states$ such that $\left[Qf\right](x) < \left[\lrate f\right](x)$. This means that there is some $\epsilon\in\realspos$ such that also $\left[Qf\right](x) + \epsilon < \left[\lrate f\right](x)$. Because $\lim_{i\to+\infty}Q_i=Q$, we must eventually have that, for large enough $i$, it holds that $\left[Q_if\right](x) < \left[Qf\right](x) + \epsilon$. This implies that also $\left[Q_if\right](x) < \left[\lrate f\right](x)$, which, because $Q_i\in\rateset_{\lrate}$, is a contradiction. Therefore, $Q\in\rateset_{\lrate}$, and because the converging sequence $\{Q_i\}_{i\in\nats}$ was arbitrary, this proves that $\rateset_{\lrate}$ is closed.

We now show that $\rateset_{\lrate}$ is convex, meaning that for any rate matrices $Q_1,Q_2\in\rateset_{\lrate}$, and any $\lambda\in[0,1]$, the matrix $Q_\lambda\coloneqq\lambda Q_1 + (1-\lambda)Q_2$ also satisfies $Q_\lambda\in\rateset_{\lrate}$. It is easily verified from Definition~\ref{def:rate_matrix} that $Q_\lambda$ is a rate matrix. Now, assume \emph{ex absurdo} that $Q_\lambda\notin\rateset_{\lrate}$. Then, by Equation~\eqref{eq:dominatingratematrices}, there is some $f\in\gamblesX$ and some $x\in\states$ such that $\left[Q_\lambda f\right](x) < \left[\lrate f\right](x)$. This implies that $\left(\lambda\left[Q_1 f\right](x)+(1-\lambda)\left[Q_2 f\right](x)\right) < \left[\lrate f\right](x)$, which because $Q_1,Q_2\in\rateset_{\lrate}$, is a contradiction. Therefore, $Q_\lambda\in\rateset_{\lrate}$.

We finally show that $\rateset_{\lrate}$ has separately specified rows. For all $x\in\states$, let $\rateset_x\coloneqq\{Q(x,\cdot)\,:\,Q\in\rateset_{\lrate}\}$. Now, let $Q$ be any matrix such that, for all $x\in\states$, $Q(x,\cdot)\in\rateset_x$. It is easily verified from Definition~\ref{def:rate_matrix} that $Q$ is a rate matrix. Now, assume \emph{ex absurdo} that $Q\notin\rateset_{\lrate}$. Then, by Equation~\eqref{eq:dominatingratematrices}, there is some $f\in\gamblesX$ and some $x\in\states$ such that $\left[Qf\right](x) < \left[\lrate f\right](x)$. Because $Q(x,\cdot)\in\rateset_x$, this implies that there is some $Q'\in\rateset_{\lrate}$ such that also $\left[Q'f\right](x) < \left[\lrate f\right](x)$, a contradiction. Therefore, $Q\in\rateset_{\lrate}$.
\end{proof}

\begin{lemma}\label{lemma:rows_nonempty_bounded_closed_convex}
Let $\rateset$ be any non-empty, bounded, closed and convex set of rate matrices that has separately specified rows. Then, for all $x\in\states$, the set $\rateset_x\coloneqq\{Q(x,\cdot)\,:\,Q\in\rateset\}$ is non-empty, bounded, closed, and convex.
\end{lemma}
\begin{proof}
Take any $x\in\states$. Non-emptiness, boundedness, and convexity of $\rateset_x$ follows trivially from the fact that $\rateset$ is non-empty, bounded, and convex. It remains to show that $\rateset_x$ is closed, or, equivalently, that for any converging sequence $\{Q_i(x,\cdot)\}_{i\in\nats}$ in $\rateset_x$, the limit $Q^*(x,\cdot)\coloneqq\lim_{i\to+\infty}Q_i(x,\cdot)$ also satisfies $Q^*(x,\cdot)\in\rateset_x$.

For all $x'\in\states$ such that $x'\neq x$, choose some $Q(x',\cdot)\in\rateset_{x'}$. Construct a sequence of matrices $\{\hat{Q}_i\}_{i\in\nats}$ such that, for all $i\in\nats$ and all $x'\in\states$, $x'\neq x$, $\hat{Q}_i(x',\cdot)=Q(x',\cdot)$, and such that $\hat{Q}_i(x,\cdot)=Q_i(x,\cdot)$ for all $i\in\nats$. Because $\rateset$ has separately specified rows, and because $\hat{Q}_i(y,\cdot)\in\rateset_y$ for all $y\in\states$ and all $i\in\nats$, we then have that $\hat{Q}_i\in\rateset$, for all $i\in\nats$.

Furthermore, in the sequence $\{\hat{Q}_i\}_{i\in\nats}$, only the $x$-th row is changing. Therefore, because $\{Q_i(x,\cdot)\}_{i\in\nats}$ is converging, so is $\{\hat{Q}_i\}_{i\in\nats}$. In particular, $\{\hat{Q}_i(x,\cdot)\}_{i\in\nats}\to Q^*(x,\cdot)$. Let $Q^*\coloneqq\lim_{i\to+\infty}\hat{Q}_i$. Because $\rateset$ is closed, and $\{\hat{Q}_i\}_{i\in\nats}$ is in $\rateset$, it follows that $Q^*\in\rateset$. Therefore, we find that $Q^*(x,\cdot)\in\rateset_x$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:dominating_unique_characterization}]
Let $\rateset$ be any non-empty, bounded, closed and convex set of rate matrices with separately specified rows, with corresponding lower transition rate operator $\lrate$. Assume \emph{ex absurdo} that $\rateset\neq\rateset_{\lrate}$.

Then, we must have that either $\rateset\subset\rateset_{\lrate}$, or, because $\rateset$ is non-empty, there must be some $Q\in\rateset$ such that $Q\notin\rateset_{\lrate}$. In the latter case, because $\rateset$ has $\lrate$ as its lower envelope, we immediately find a contradiction with Equation~\eqref{eq:dominatingratematrices}. Hence, if indeed $\rateset\neq\rateset_{\lrate}$, we must instead have that $\rateset\subset\rateset_{\lrate}$. Therefore, and because $\rateset_{\lrate}$ is non-empty, there must be some $Q\in\rateset_{\lrate}$ such that $Q\notin\rateset$.

Because $\rateset$ has separately specified rows, there must therefore be some $x\in\states$ such that $Q(x,\cdot)\notin\rateset_x$, where $\rateset_x\coloneqq\{Q'(x,\cdot)\,:\,Q'\in\rateset\}$. Because $\rateset$ is non-empty, bounded, closed, convex, and has separately specified rows, it follows from Lemma~\ref{lemma:rows_nonempty_bounded_closed_convex} that $\rateset_x$ is non-empty, bounded, closed, and convex. 

Note that we can interpret $\rateset_x\subset\reals^m$ as a subset of the vector space $\reals^m$, where $m$ is the size of $\states$. Furthermore, because $\rateset_x$ is closed and bounded, we have that $\rateset_x$ is compact. Because $Q(x,\cdot)\notin\rateset_x$, the (non-empty, closed, convex) singleton set $\{Q(x,\cdot)\}$ is clearly disjoint from $\rateset_x$.

Therefore, by the hyperplane separation theorem\footnote{That is, by one of them.} ({\bf REF}), there must be some $f\in\gamblesX$ and some $c_1,c_2\in\reals$, with $c_1<c_2$, such that $Q(x,\cdot)f = [Qf](x) \leq c_1$, and such that, for all $Q'(x,\cdot)\in\rateset_x$, $Q'(x,\cdot)f = [Q'f](x) \geq c_2$.

This clearly implies that, for all $Q'\in\rateset$, it holds that $[Qf](x) \leq c_1 < c_2 \leq [Q'f](x)$. In turn, this implies the existence of some $\epsilon\in\realspos$ for which $[Qf](x) + \epsilon < [Q'f](x)$, for all $Q'\in\rateset$. Because $\lrate$ is the lower envelope of $\rateset$, by Equation~\eqref{eq:correspondinglowertrans}, we therefore find that $[Qf](x) < [\lrate f](x)$. Hence, we have found a $f\in\gamblesX$ and a $x\in\states$, such that for some $Q\in\rateset_{\lrate}$, it holds that $[Qf](x) < [\lrate f](x)$. By Equation~\eqref{eq:dominatingratematrices}, this is a contradiction, and hence $\rateset=\rateset_{\lrate}$.
\end{proof}

\begin{lemma}\label{lemma:rateset_has_arginf}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$ and $\epsilon\in\realspos$, there exists a $Q\in\rateset$ such that
\begin{equation*}
\norm{\lrate f - Qf} < \epsilon\,.
\end{equation*}
\end{lemma}
\begin{proof}
This is immediate from the definition of the lower envelope of $\rateset$, as given by Equation~\eqref{eq:correspondinglowertrans}, and the fact that $\rateset$ has separately specified rows.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded}]
Consider any $P\in\wprocesses_\rateset$, any $s\in\realsnonneg$, any $u\in\mathcal{U}_{<s}$, any $x_u\in\states^u$, and any $f\in\gamblesX$. We will show that for all $\epsilon\in\realspos$, it holds that 
\begin{equation*}
[L_{t_n}^s f](x_{t_n}) < \mathbb{E}[f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] + \epsilon\,,
\end{equation*}
which then implies that $[L_{t_n}^s f](x_{t_n}) \leq \mathbb{E}[f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]$. Start by choosing any $\epsilon\in\realspos$, and let $C\coloneqq(s-t_n)$. 

Now, by Theorem~\ref{theo:convergencelowerbound}, there is some $\delta\in\realspos$ such that
\begin{equation*}
(\forall v\in\mathcal{U}_{[t_n,s]}\,:\,\sigma(v)<\delta) \norm{L_{t_n}^s - \Phi_v} < \frac{\epsilon}{2\norm{f}}\,,
\end{equation*}
which implies that for all $v\in\mathcal{U}_{[t_n,s]}$ with $\sigma(v)<\delta$, and all $x\in\states$,
\begin{equation}\label{eq:lowerbound_proof_linear_approx_lbound}
\left[L_{t_n}^sf\right](x) - \frac{\epsilon}{2} < \left[\Phi_vf\right](x)\,.
\end{equation}

Now, let $w\coloneqq t_0,\ldots,t_{n-1}$, and let $x_w\coloneqq(x_{t_0},\ldots,x_{t_{n-1}})$. Then, because $P\in\wprocesses_\rateset$, by Proposition~\ref{prop:outerderivativebehaveslikelimit}, there is some $\delta'\in\realspos$ such that for all $\Delta\in\realspos$ with $\Delta<\delta'$, there is a $Q_0\in\rateset$ such that
\begin{equation*}
\norm{T_{t_n,x_w}^{t_n+\Delta} - (I+\Delta Q_0)} < \frac{\Delta\epsilon}{2C\norm{f}}\,.
\end{equation*}
Choose any $\Delta_0$ such that $\Delta_0<\min\{\delta,\delta'\}$. This implies that for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$, we have that
\begin{equation*}
\norm{T_{t_n,x_w}^{t_n+\Delta_0}g - (I+\Delta_0 Q_0)g} < \frac{\Delta_0\epsilon}{2C}\,,
\end{equation*}
which in turn implies that for all $x\in\states$,
\begin{equation*}
\left[(I+\Delta_0 Q_0)g\right](x) - \frac{\Delta_0\epsilon}{2C} < \left[T_{t_n,x_w}^{t_n+\Delta_0}g\right](x)\,.
\end{equation*}
Because $Q_0\in\rateset$, using Equation~\eqref{eq:correspondinglowertrans}, we find that, for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$,
\begin{equation}\label{eq:lowerbound_proof_linear_approx_last_part}
\left[(I+\Delta_0 \lrate)g\right](x) - \frac{\Delta_0\epsilon}{2C} < \left[T_{t_n,x_w}^{t_n+\Delta_0}g\right](x)\,.
\end{equation}

Let $\delta^*\coloneqq\min\{\delta,\nicefrac{1}{\norm{\lrate}}\}$. 
Because $P\in\wprocesses_\rateset$, Lemma~\ref{lemma:bound_on_linear_approx_partition} implies that there is some $v\in\mathcal{U}_{[t_n+\Delta_0,s]}$ such that $\sigma(v)<\delta^*$, with $v=\tau_0,\ldots,\tau_m$, and $\tau_0=t_n+\Delta_0$, $\tau_m=s$, and, for all $i\in\{1,\ldots,m\}$,
\begin{equation*}
(\exists Q\in\rateset)\norm{T_{\tau_{i-1},x_u}^{\tau_{i}} - (I+\Delta_{i}Q)} < \frac{\Delta_{i}\epsilon}{2C\norm{f}}\,,
\end{equation*}
which implies that there exist rate matrices $Q_{1},\ldots,Q_{m}$ such that, for all functions $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$ and all $i\in\{1,\ldots,m\}$,
\begin{equation*}
\norm{T_{\tau_{i-1},x_u}^{\tau_{i}}g - (I+\Delta_{i}Q_i)g} < \frac{\Delta_{i}\epsilon}{2C}\,.
\end{equation*}
This in turn implies that for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$, all $i\in\{1,\ldots,m\}$, and all $x\in\states$,
\begin{equation*}
\left[(I+\Delta_{i}Q_{i})g\right](x) - \frac{\Delta_{i}\epsilon}{2C} < \left[T_{\tau_{i-1},x_u}^{\tau_{i}}g\right](x)\,.
\end{equation*}
Because for all $i\in\{1,\ldots,m\}$ it holds that $Q_i\in\rateset$, by Equation~\eqref{eq:correspondinglowertrans} we find that for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$ and all $x\in\states$,
\begin{equation}\label{eq:lowerbound_proof_linear_approx_lrate}
\left[(I+\Delta_{i}\lrate)g\right](x) - \frac{\Delta_{i}\epsilon}{2C} < \left[T_{\tau_{i-1},x_u}^{\tau_{i}}g\right](x)\,.
\end{equation}

Now, observe that because $\tau_m=s$,
\begin{align*}
\left[T_{\tau_{m-1},x_u}^{\tau_{m}}f\right](x) &= \mathbb{E}[f(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\\
 &= \mathbb{E}[f(X_{s})\,\vert\,X_{s-\Delta_m}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,.
\end{align*}
Furthermore, by the basic properties of expectation, we have that
\begin{align*}
\mathbb{E}[&f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
 & = \mathbb{E}\bigl[\mathbb{E}[f(X_s)\,\vert\,X_{s-\Delta_m},X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\bigr]\,,
\end{align*}
and because this (outer) expectation computes a convex combination of values, we find by substitution of Equation~\eqref{eq:lowerbound_proof_linear_approx_lrate}, that
\begin{align*}
\mathbb{E}[&f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
& > \mathbb{E}\bigl[[(I+\Delta_m\lrate)f](X_{\tau_{m-1}})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\bigr] - \frac{\Delta_{m}\epsilon}{2C}\,.
\end{align*}

Because $\Delta_m\leq\sigma(v)<\delta^*\leq\nicefrac{1}{\norm{\lrate}}$, we find by Proposition~\ref{lemma:normQsmallenough} that $(I+\Delta_m\lrate)$ is a lower transition operator. Therefore, by Proposition~\ref{lemma:normofcoherenttrans}, we have that $\norm{(I+\Delta_m\lrate)f}\leq\norm{f}$. Hence, Equation~\eqref{eq:lowerbound_proof_linear_approx_lrate} applies, and we can use backward induction on $m$ to find
\begin{align*}
\mathbb{E}[&f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] \\
& > \mathbb{E}\bigl[[(I+\Delta_m\lrate)f](X_{\tau_{m-1}})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\bigr] - \frac{\Delta_{m}\epsilon}{2C} \\
 &> \mathbb{E}\bigl[[(I+\Delta_{m-1}\lrate)(I+\Delta_m\lrate)f](X_{\tau_{m-2}})\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\bigr] - \frac{\Delta_{m-1}\epsilon}{2C} - \frac{\Delta_{m}\epsilon}{2C} \\
&\vdots \\
%& > \mathbb{E}\left[\left[\left(\prod_{i=2}^m(I+\Delta_i\lrate)\right)f\right](X_{\tau_{1}})\,\Bigg\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] - \sum_{i=2}^m\frac{\Delta_{i}\epsilon}{2C} \\
& > \mathbb{E}\left[\left[\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](X_{\tau_{0}})\,\Bigg\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C} \\
& = \mathbb{E}\left[\left[\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](X_{t_n+\Delta_0})\,\Bigg\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C} \\
&= \left[T_{t_n,x_w}^{t_n+\Delta_0}\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](x_{t_n}) - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C}\,.
%& = \left[\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](x_{t_n}) - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C}\\
%& = \left[\Phi_v f\right](x_{t_n}) - \frac{\epsilon}{2}\,,
\end{align*}
Because $\prod_{i=1}^m(I+\Delta_i\lrate)$ is a lower transition operator, we have that $\norm{\prod_{i=1}^m(I+\Delta_i\lrate)f}\leq\norm{f}$. Therefore, Equation~\eqref{eq:lowerbound_proof_linear_approx_last_part} applies, and hence we find that
\begin{equation*}
\mathbb{E}[f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] > \left[\prod_{i=0}^m(I+\Delta_i\lrate)f\right](x_{t_n}) - \sum_{i=0}^m\frac{\Delta_{i}\epsilon}{2C}\,.
\end{equation*}
Note that $\sum_{i=0}^m\Delta_i=C$. Let now $v^*\coloneqq t_n,\tau_0,\ldots,\tau_m$. Then clearly, $v^*\in\mathcal{U}_{[t_n,s]}$. We therefore have that
\begin{equation*}
\mathbb{E}[f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] > \left[\Phi_{v^*}f\right](x_{t_n}) - \frac{\epsilon}{2}\,,
\end{equation*}
and because $\sigma(v^*)<\delta$, we now find from Equation~\eqref{eq:lowerbound_proof_linear_approx_lbound} that
\begin{equation*}
\mathbb{E}[f(X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}] > \left[L_{t_n}^sf\right](x_{t_n}) - \epsilon\,.
\end{equation*}
Because the $\epsilon\in\realspos$ was arbitrary, this completes the proof.
\end{proof}

%
%**** {\bf the one below is deprecated}
%\begin{proof}[Proof of Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded}]
%Consider any $P\in\processes_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, and any $g\in\gambles(\states^{\{s\}})$. We will show that for all $\epsilon\in\realspos$,
%\begin{equation*}
%[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,,
%\end{equation*}
%which then implies $[L_t^s g](x_{t_n}) \leq \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]$. Start by choosing any $\epsilon\in\realspos$, and let $C\coloneqq (s-t)$.
%
%Because $P\in\processes_\rateset$, it follows from Definition~\ref{def:set_non_markov_process} that there is some $\delta\in\realspos$ such that
%\begin{align}\label{eq:nonmarkov_bound_proof_deriv_bounded}
%\begin{split}
% &(\forall \tau\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall v\in\mathcal{U}_{[0,\tau]})\,(\forall(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}})\,(\exists Q\in\rateset)\,: \\
% &(\forall f\in\gambles(\states^{v\cup\{\tau+\Delta\}}))\,(\forall x_{\tau_m}\in\states^{\{\tau_m\}}): \\
% &\left\lvert \frac{\mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}] 
% - f(x_{\tau_0},\ldots,x_{\tau_m},x_{\tau_m})}{\Delta} \right. \\
% &\quad\quad\quad\quad\quad\quad - \left[Q f(x_{\tau_0},\ldots,x_{\tau_{m}},X_{\tau+\Delta})\right](x_{\tau_m})\biggr\rvert \quad < \quad\frac{\epsilon}{2C\norm{g}}\cdot\norm{f}\,.
%\end{split}
%\end{align}
%Furthermore, it follows from Theorem~\ref{theo:convergencelowerbound} that there is some $\delta'\in\realspos$ such that
%\begin{equation}\label{eq:nonmarkov_bound_proof_lbound_approx}
%(\forall v\in\mathcal{U}_{[t,s]}\,:\,\sigma(v)<\delta')\,(\forall x_t\in\states)\abs{\left[L_t^s g\right](x_t) - \left[\prod_{k=1}^n(I+\Delta_i\lrate)g\right](x_t)} < \frac{\epsilon}{2}\,.
%\end{equation}
%
%Let $\delta^*\coloneqq\min\{\delta,\delta'\}$, and choose any $n>\nicefrac{C}{\delta^*}$. Then, for $\Delta\coloneqq\nicefrac{C}{n}$, we have $\Delta<\delta$ and $\Delta<\delta'$.
%
%Equation \eqref{eq:nonmarkov_bound_proof_deriv_bounded} now implies that for all $\tau\in\realspos$, all $v\in\mathcal{U}_{[0,\tau]}$ such that $v=\tau_0,\ldots,\tau_m$, and all $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, there is some $Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}}\in\rateset$ such that, for all $f\in\gambles(\states^{v\cup\{\tau+\Delta\}})$ and all $x_{\tau_m}\in\states^{\{\tau_m\}}$,
%\begin{align}\label{eq:nonmarkov_bound_proof_deriv_inequal}
%\begin{split}
% &\left[(I + \Delta Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}})f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\right](x_{\tau_m}) - \frac{\Delta\epsilon\norm{f}}{2C\norm{g}} \\
% &\quad< \mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}]\,.
%\end{split}
%\end{align}
%Now, note that by the basic properties of probability,
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
%\end{align*}
%By choosing $\tau=(s-\Delta)$, setting $v=t_0,\ldots,t_n,(s-\Delta)$, and noting that $g\in\gambles(\states^{\{s\}})=\gambles(\states^{\{\tau+\Delta\}})\subset\gambles(\states^{v\cup\{\tau+\Delta\}})$, we find from Equation \eqref{eq:nonmarkov_bound_proof_deriv_inequal} that for all $(x_{t_0},\ldots,x_{t_n})\in\states^{u}$ there is some $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$ such that, for all $x_{s-\Delta}\in\states^{\{s-\Delta\}}$,
%\begin{equation*}
%\left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}=x_{s-\Delta}]\,.
%\end{equation*}
%Furthermore, we find using Equation~\eqref{eq:correspondinglowertrans} and the fact that $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$, that
%\begin{equation*}
%\left[(I + \Delta \lrate)g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} \leq \left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C}\,.
%\end{equation*}
%Noting that an expectation takes a convex combination of values, we find by substitution that
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] \\
%&> \mathbb{E}\bigl[[(I+\Delta\lrate)g](X_{s-\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] - \frac{\Delta\epsilon}{2C}\,.
%\end{align*}
%
%Note also that
%\begin{equation*}
%[(I+\Delta\lrate)g](X_{s-\Delta})\in\gambles(\states^{\{s-\Delta\}})\subset\gambles(\states^{u\cup\{s-\Delta\}})\,.
%\end{equation*}
%Hence, we can repeat this argument, factoring the expectation at time points $(s-2\Delta),\ldots,(s-(n-1)\Delta)$, to obtain
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - n\cdot\frac{\Delta\epsilon}{2C} \\
% &= \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2}\,.
%\end{align*}
%
%It follows from Equation \eqref{eq:nonmarkov_bound_proof_lbound_approx} and the fact that $\Delta<\delta'$,
%\begin{equation*}
%\left[L_t^s g\right](x_{t_n}) - \frac{\epsilon}{2} < \left[(I+\Delta\lrate)^n g\right](x_{t_n})\,,
%\end{equation*}
%so that by substitution,
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2} \\
% &> [L_t^s g](x_{t_n}) - \frac{\epsilon}{2} - \frac{\epsilon}{2} \\
% &= [L_t^s g](x_{t_n}) - \epsilon\,.
%\end{align*}
%Thus, we have found that
%\begin{equation*}
%[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,.
%\end{equation*}
%Because the $\epsilon\in\realspos$ was arbitrary, this concludes the proof.
%\end{proof}

\begin{proof}[Proof of Proposition~\ref{theorem:lower_markov_bound_is_tight}]
Choose any $f\in\gamblesX$ and any $\epsilon\in\realspos$. Let $C\coloneqq (s-t)$. 

The proof works by selecting a rate matrix at each point in time, showing that there is a $P\in\mprocesses_\rateset$ that is characterized by this construction, and finally establishing that this $P$ satisfies the inequality of interest.

By Lemma~\ref{lemma:rateset_has_arginf}, for all $\tau\in[t,s]$ there is some rate matrix $Q_\tau\in\rateset$ such that
\begin{equation}\label{eq:lower_char_rate_matrix}
\norm{\lrate \lbound_\tau^sf - Q_\tau \lbound_\tau^sf} < \frac{\epsilon}{2C}\,.
\end{equation}
To fix $Q_\tau$'s values outside of the interval $[t,s]$, let $Q_\tau \coloneqq Q_t$ for all $\tau\in[0,t]$, and let $Q_\tau\coloneqq Q_s$ for all $\tau>s$.
%Define a \emph{lower-characterizing rate matrix} $Q_\tau$ as
%\begin{equation}\label{eq:lower_char_rate_matrix}
%Q_\tau(x_\tau,\cdot)\coloneqq \argmin\left\{ Q(x_\tau,\cdot)\bigl[\lbound_\tau^sf\bigr]\,:\,Q(x_\tau,\cdot)\in\mathcal{Q}_{x_\tau}\right\}\quad\text{for all $x_\tau\in\states$ and $\tau\in[t,s]$}\,.
%\end{equation}
%Note that the $\argmin\{\cdot\}$ may be set-valued, in which case take an arbitrary element.
%Furthermore, let $Q_\tau \coloneqq Q_t$ for all $\tau\in[0,t]$, and let $Q_\tau\coloneqq Q_s$ for all $\tau>s$.

Next, define
\begin{equation}\label{eq:delta_required_for_tight_bound}
\delta \coloneqq \min\left\{\frac{\epsilon}{4C\norm{\mathcal{Q}}^2\norm{f}},\frac{1}{\norm{\lrate}}\right\}\,,
\end{equation}
and choose any $n>\nicefrac{C}{\delta}$. Then, for $\Delta\coloneqq \nicefrac{C}{n}$, we have $\Delta<\delta$.

For all $k\in\{0,\ldots,n\}$, define $t_k=t+k\Delta$. Let $u\coloneqq t_0,t_1,\ldots,t_n$. Finally, define a piecewise-constant approximation $Q_\tau^u$ to $Q_\tau$ such that, for all $k\in\{1,\ldots,n\}$,
\begin{equation}\label{eq:lower_char_matrix_linear_approx}
Q_\tau^u \coloneqq Q_{t_k},\quad\text{for all $\tau\in (t_{k-1},t_k]$}\,.
\end{equation}
Let $Q_\tau^u\coloneqq Q_{t_0}$ for all $\tau\leq t_0$ and let $Q_\tau^u\coloneqq Q_{t_n}$ for all $\tau>t_n$.

Then, $Q_\tau^u$ is a left-continuous, piecewise-constant function that gives for each time point $\tau\in\realsnonneg$ a rate matrix $Q_\tau^u\in\rateset\subset\mathcal{R}$, such that the value of $Q_\tau^u$ differs on a finite number of intervals. Therefore, by Proposition~\ref{prop:continuous_rate_matrix_has_process}, there is some $P\in\wmprocesses$ such that $\mathcal{T}_P=\mathcal{T}_{Q_\tau^u}$, where $\mathcal{T}_{Q_\tau^u}$ is defined as in Lemma~\ref{lemma:nonhomogen_trans_mat_system}. Furthermore, because $Q_\tau^u$ takes values in $\rateset$, by Lemma~\ref{lemma:nonhomogeneous_in_process_set}, $P$ furthermore satisfies $P\in\wmprocesses_\rateset$. It remains to show that, for this $P$, it holds that $\norm{L_t^sf - T_t^sf}<\epsilon$. 

We start by factorizing the interval $[t,s]$ according to $u$, as follows.
\begin{align*}
\norm{L_t^sf - T_t^sf} &= \norm{L_{t_0}^{t_n}f - T_{t_0}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{{T_{t_0}^{t_{n-1}}}}\cdot\norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \,.
\end{align*}
Recursion on the first summand yields
\begin{align*}
\norm{L_t^sf - T_t^sf} &\leq \norm{L_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-2}}\left[L_{t_{n-2}}^{t_n}f\right] - T_{t_0}^{t_{n-2}}\left[L_{t_{n-2}}^{t_n}f\right]} \\
 &\quad\quad\quad+ \norm{L_{t_{n-2}}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_{n-2}}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
&\vdots \\
 &\leq \sum_{i=1}^{n} \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,.
\end{align*}
For all $i\in\{1,\ldots,n\}$, we have using Lemma~\ref{lemma:justthelinearpart} and the facts that $\Delta<\delta\leq\nicefrac{1}{\norm{\lrate}}$ and $\norm{\lrate}\leq\norm{\rateset}$,
\begin{align*}
&\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - \left[I+\Delta\lrate\right]L_{t_i}^{t_n}f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \norm{L_{t_{i-1}}^{t_i} - \left[I+\Delta\lrate\right]}\cdot\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \Delta^2\norm{\lrate}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,.
\end{align*}
By Equation~\eqref{eq:lower_char_rate_matrix}, we have $\norm{Q_{t_i}L_{t_i}^{t_n}f - \lrate L_{t_i}^{t_n}f} < \nicefrac{\epsilon}{2C}$. Furthermore, by Equation~\eqref{eq:lower_char_matrix_linear_approx}, we have $Q_{t_i}^u=Q_{t_i}$, so that
\begin{align*}
 &\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \norm{\left[I+\Delta \lrate\right]L_{t_i}^{t_n}f - \left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f} \\
 &= \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \Delta\norm{\lrate L_{t_i}^{t_n}f - Q_{t_i}L_{t_i}^{t_n}f} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \frac{\Delta\epsilon}{2C} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right] - T_{t_{i-1}}^{t_i}}\cdot\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &= \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}\right] - e^{Q_{t_i}\cdot(t_i-t_{i-1})}}\cdot\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &\leq 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C}\,,
\end{align*}
where we used the definition of $T_{t_{i-1}}^{t_i}$ from Lemma~\ref{lemma:nonhomogen_trans_mat_system}, and where the last step used Lemma~\ref{lemma:linearpartofexponential} and the fact that $\norm{Q_{t_i}}\leq\norm{\rateset}$, since $Q_{t_i}\in\rateset$.

Thus, using the fact that $n\Delta=C$, we find
\begin{align*}
\norm{L_t^sf - T_t^sf} &\leq \sum_{i=1}^n \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \sum_{i=1}^n 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &= 2n\Delta^2\norm{\mathcal{Q}}^2\norm{f} + n\frac{\Delta\epsilon}{2C}\\
 &= 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2}\,,
\end{align*}
so that by Equation~\eqref{eq:delta_required_for_tight_bound} and the fact that $\Delta<\delta$, we have
\begin{equation*}
\norm{L_t^sf - T_t^sf} \leq 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} < 2C\delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\,.
\end{equation*}
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:lower_operator_is_infimum}]
We start by proving the first statement. By Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} and Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, we have that for all $P\in\mprocesses_\rateset$ it holds that $\left[L_t^sg\right](x) \leq \mathbb{E}[g(X_s)\,\vert\,X_t=x]$. Furthermore, by Theorem~\ref{theorem:lower_markov_bound_is_tight}, for all $\epsilon\in\realspos$ there is some $P\in\mprocesses_\rateset$ such that $\norm{L_t^sg - T_t^sg} < \epsilon$. Together this implies, using Definition~\ref{def:lower_markov}, that
\begin{equation*}
\left[L_t^sg\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\right\} = \underline{\mathbb{E}}^{\mathrm{M}}\left[g(X_s)\,\vert\,X_t=x\right]\,.
\end{equation*}

We now move on to the second statement. By Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} we have that for all $P\in\processes_\rateset$ it holds that $\left[L_t^sg\right](x) \leq \mathbb{E}[g(X_s)\,\vert\,X_t=x]$.
Furthermore, by Theorem~\ref{theorem:lower_markov_bound_is_tight} and Proposition~\ref{prop:markov_set_subset_of_nonmarkov_set}, for all $\epsilon\in\realsnonneg$ there is some $P\in\processes_\rateset$ such that
$\norm{L_t^sg - T_t^sg} < \epsilon$.
Together this implies, using Definition~\ref{def:lower_non_markov}, that
\begin{equation*}
\left[L_t^sg\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\processes_\rateset\right\} = \underline{\mathbb{E}}\left[g(X_s)\,\vert\,X_t=x\right]\,.
\end{equation*}
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:dominating_rate_processes_max_set}]
By Propositions~\ref{prop:dominating_nonempty_bounded} and \ref{prop:dominatingproperties}, the set $\rateset_{\lrate}$ is non-empty, bounded, has separately specified rows, and by definition has $\lrate$ as its corresponding lower transition rate operator. The first statement therefore follows directly from Corollary~\ref{cor:lower_operator_is_infimum}.

For the second statement, take any $P\in\processes$ such that $P\notin\wprocesses_{\rateset_{\lrate}}$. Clearly, we must have that either $P\in\wprocesses$ or $P\notin\wprocesses$. We will start by assuming the former, and show that the statement then follows.

Because $P\notin\wprocesses_{\rateset_{\lrate}}$, by Definition~\ref{def:set_non_markov_process}, there must be some $t\in\realsnonneg$, some $u\in\mathcal{U}_{<t}$, and some $x_u\in\states^u$, such that $\smash{\overline{\partial}}T_{t,x_u}^{t}\nsubseteq\rateset_{\lrate}$. Furthermore, by Proposition~\ref{prop:boundednon-emptyandclosed}, we have that $\smash{\overline{\partial}}T_{t,x_u}^{t}\neq\emptyset$. Hence, there must be some rate matrix $Q\in\smash{\overline{\partial}}T_{t,x_u}^{t}$ such that $Q\notin\rateset_{\lrate}$, which by Equation~\eqref{eq:dominatingratematrices} implies that there is some $f\in\gamblesX$ and some $x\in\states$ such that $\left[Qf\right](x)<\left[\lrate f\right](x)$.

Now, assume \emph{ex absurdo} that the statement is false. Because $Q\in\smash{\overline{\partial}}T_{t,x_u}^{t}$, we must have that either $Q\in\smash{\overline{\partial}}_+T_{t,x_u}^{t}$ or $Q\in\smash{\overline{\partial}}_-T_{t,x_u}^{t}$. Assume it is the former. Then, by Equation~\ref{eq:rightouterderivative}, there is some $\{\Delta_i\}_{i\in\nats}\to0^+$ such that $\lim_{i\to+\infty}\nicefrac{1}{\Delta_i}(T_{t,x_u}^{t+\Delta_i}-I)=Q$. Because we assumed that the claim is false, we must have that for all $\Delta\in\realspos$, it holds that $\left[L_t^{t+\Delta}f\right](x) \leq \left[T_{t,x_u}^{t+\Delta}f\right](x)$,
or equivalently, that
\begin{equation*}
\frac{1}{\Delta}\left[(L_t^{t+\Delta} - I)f\right](x) \leq \frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)f\right](x)\,.
\end{equation*}
Taking limits on both sides with respect to $\{\Delta_i\}_{i\in\nats}$, we find that
\begin{align*}
\lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(L_t^{t+\Delta_i} - I)f\right](x) &\leq \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(T_{t,x_u}^{t+\Delta_i} - I)f\right](x) \\
\left[\lrate f\right](x) &\leq \left[Qf\right](x)\,,
\end{align*}
where the left-hand limit follows from Proposition~\ref{prop:lower_transition_has_deriv}. This contradicts the earlier observation that $\left[Qf\right](x)<\left[\lrate f\right](x)$. Hence, if the statement is indeed false, we must instead have that $Q\in\smash{\overline{\partial}}_-T_{t,x_u}^{t}$. In that case, there is some other $\{\Delta_i\}_{i\in\nats}\to0^+$, such that $\lim_{i\to+\infty}\nicefrac{1}{\Delta_i}(T_{t-\Delta_i,x_u}^{t}-I)=Q$, and similarly, we should have for all $\Delta\in\realspos$ that
\begin{equation*}
\frac{1}{\Delta}\left[(L_{t-\Delta}^{t} - I)f\right](x) \leq \frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^{t} - I)f\right](x)\,.
\end{equation*}
By Proposition~\ref{prop:lower_transition_is_homogeneous}, we have that $L_{t-\Delta}^{t}=L_{t}^{t+\Delta}$, and so after taking limits on both sides,
\begin{align*}
\lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(L_t^{t+\Delta_i} - I)f\right](x) &\leq \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(T_{t-\Delta_i,x_u}^{t} - I)f\right](x) \\
\left[\lrate f\right](x) &\leq \left[Qf\right](x)\,,
\end{align*}
we again find a contradiction. Therefore, if $P\in\wprocesses$, the statement follows.

Now, suppose that instead $P\notin\wprocesses$. Assume \emph{ex absurdo} that the statement is false. Because $P\notin\wprocesses$, by Definition~\ref{def:well-behaved} there must be some $t\in\realsnonneg$, some $u\in\mathcal{U}_{<t}$, some $x_u\in\states^u$, and some $x,y\in\states$, such that either 
\begin{equation*}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t+\Delta}=y\,\vert\,X_t=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})-\delta_{xy}}\nless+\infty\,,
\end{equation*}
or,
\begin{equation*}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t}=y\,\vert\,X_{t-\Delta}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})-\delta_{xy}}\nless+\infty\,.
\end{equation*}
Start by assuming that it is the former. Clearly, we have for all $\Delta\in\realspos$ that $\left[T_{t,x_u}^{t+\Delta}\ind{y}\right](x)=P(X_{t+\Delta}=y\,\vert\,X_t=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})$, and furthermore, that $\left[I\ind{y}\right](x)=\delta_{xy}$. Hence, we have that
\begin{equation*}
\left[(T_{t,x_u}^{t+\Delta}-I)\ind{y}\right](x) = P(X_{t+\Delta}=y\,\vert\,X_t=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}) - \delta_{xy}\,.
\end{equation*}
By Proposition~\ref{prop:rate_from_stochastic_matrix}, we have that $\nicefrac{1}{\Delta}(T_{t,x_u}^{t+\Delta}-I)$ is a transition rate matrix, and hence by Definition~\ref{def:rate_matrix} we must have that $\nicefrac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta}-I)\ind{x}\right](x)\leq 0$, and $\nicefrac{1}{\Delta}\abs{\left[(T_{t,x_u}^{t+\Delta}-I)\ind{y}\right](x)}\leq\nicefrac{1}{\Delta}\abs{\left[(T_{t,x_u}^{t+\Delta}-I)\ind{x}\right](x)}$. It therefore follows that also
\begin{equation}\label{eq:nonwellbehaved_limit_diagonal}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{\left[(T_{t,x_u}^{t+\Delta}-I)\ind{x}\right](x)} \nless +\infty\,.
\end{equation}
Similarly, by Lemma~\ref{lemma:lower_trans_to_lower_rate}, we have that $\nicefrac{1}{\Delta}(L_t^{t+\Delta}-I)$ is a lower transition rate operator, and hence by Definition~\ref{def:coh_low_trans_rate}, we must have that $\nicefrac{1}{\Delta}\left[(L_t^{t+\Delta}-I)\ind{x}\right](x)\leq 0$. 

Because we assumed that the statement is false, and because $\ind{x}\in\gamblesX$, it must hold for all $\Delta\in\realspos$ that $\left[L_{t}^{t+\Delta}\ind{x}\right](x)\leq\left[T_{t,x_u}^{t+\Delta}\ind{x}\right](x)$, or equivalently, that
\begin{equation*}
\frac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{x}\right](x) \leq \frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)\,.
\end{equation*}
From our earlier observations, we know that both sides of this inequality are non-positive. Hence, we must have that
\begin{equation*}
\abs{\frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)} \leq \abs{\frac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{x}\right](x)}\,.
\end{equation*}
By the definition of the norm, we have that $\abs{\nicefrac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{x}\right](x)}\leq\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)\ind{x}}$, and because $\ind{x}\in\gamblesX$ and $\norm{\ind{x}}=1$, we have $\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)\ind{x}}\leq \norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)}$. Hence, we must have that
\begin{equation*}
\abs{\frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)} \leq \norm{\frac{1}{\Delta}(L_{t}^{t+\Delta} - I)}\,.
\end{equation*}

Now, by Proposition~\ref{prop:lower_transition_has_deriv}, we have that $\lim_{\Delta\to0^+}\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I) = \lrate$, and hence it follows that $\lim_{\Delta\to0^+}\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate}$. By Lemma~\ref{lem:normlratefinite}, we furthermore know that $\norm{\lrate}<+\infty$. Taking limits on both sides of our earlier inequality, we therefore find that
\begin{equation*}
\limsup_{\Delta\to0^+}\abs{\frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)} \leq \limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate} < +\infty\,,
\end{equation*}
which, by Equation~\eqref{eq:nonwellbehaved_limit_diagonal}, is a contradiction. Therefore, if the statement is indeed false, we must instead have that for some (other) $t\in\realsnonneg$, some (other) $u\in\mathcal{U}_{<t}$, some (other) $x_\in\states^u$, and some (other) $x,y\in\states$,
\begin{equation}\label{eq:nonwellbehaved_limit_diagonal_left}
\limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t}=y\,\vert\,X_{t-\Delta}=x,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})-\delta_{xy}}\nless+\infty\,.
\end{equation}
Using the same argument as before, we must then have for all $\Delta\in\realspos$ that
\begin{equation*}
\abs{\frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^t - I)\ind{x}\right](x)} \leq \norm{\frac{1}{\Delta}(L_{t-\Delta}^t - I)}\,.
\end{equation*}
Because by Proposition~\ref{prop:lower_transition_is_homogeneous} we have that $L_{t-\Delta}^t=L_t^{t+\Delta}$, we again find after taking limits on both sides that
\begin{equation*}
\limsup_{\Delta\to0^+}\abs{\frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^t - I)\ind{x}\right](x)} \leq \limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate} < +\infty\,,
\end{equation*}
a contradiction with Equation~\eqref{eq:nonwellbehaved_limit_diagonal_left}. Therefore, if $P\notin\wprocesses$, the statement follows.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:funcs_multi_time_points}}

\begin{proof}[Proof of Proposition~\ref{prop:multi_var_single_future_bounded}]
Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we therefore have to show that $\left[L_t^sg\right](x_{t_n}) \leq \mathbb{E}\left[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]$. Because $g\in\gambles(\states^{\{s\}})$, this inequality holds by Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:multi_var_single_future_tight}]
Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we now have to show that there is a $P\in\mprocesses_\rateset$ such that $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} < \epsilon$. 

Because $g\in\gambles(\states^{\{s\}})$, by Theorem~\ref{theorem:lower_markov_bound_is_tight} there must be some $P\in\mprocesses_\rateset$ such that $\norm{L_t^sg - \mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon$. Consider this $P$. By the Markov property, its expectation must satisfy $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} = \abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]}$, and by the definition of the norm, we have 
\begin{equation*}
\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]} \leq \norm{L_t^sg-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon\,.
\end{equation*}
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:inf_works_for_single_future_var}]
This is a direct consequence of Propositions \ref{prop:markov_set_subset_of_nonmarkov_set}, \ref{prop:lower_exp_markov_bounded_by_nonmarkov}, \ref{prop:multi_var_single_future_bounded}, and \ref{prop:multi_var_single_future_tight}. The proof is similar to that of Corollary~\ref{cor:lower_operator_is_infimum}.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:multivar_bounded}]
By the basic properties of expectation, we can decompose the expectation functional as
\begin{align*}
 &\quad \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
 &= \mathbb{E}\bigl[\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s_0,\ldots,s_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
Note that the nested expectation is conditioned on the time points $t_0,\ldots,t_n,s_0,\ldots,s_{m-1}$; it therefore only computes an expectation on a single point $s_m$ in the future. Hence, by Proposition~\ref{prop:multi_var_single_future_bounded} and the fact that the (outer) expectation computes a convex combination of values, we have
\begin{align*}
&\quad\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
 &\geq \mathbb{E}\bigl[[L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_{m-1}})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
The proof is then finished by backward induction on $m$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:multivar_bound_tight}]
{\bf TODO} *** deze is nog een beetje ingewikkeld, we moeten weer een process bouwen en laten zien dat die in $\processes_\rateset$ zit ***
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:inf_works_for_multivar}]
This is a direct consequence of Propositions~\ref{prop:multivar_bounded} and \ref{prop:multivar_bound_tight}.
\end{proof}


\end{document}
