\documentclass[10pt,a4paper]{paper}
%\documentclass[3p]{elsarticle}
%\documentclass[a4paper,reqno]{amsart}
\usepackage[british]{babel}
%\usepackage[garamond]{mathdesign}

\usepackage{hyperref,url}

\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{courier}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem,multicol}
\usepackage{tikz}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

%\usepackage{hyperref}
%\usepackage{pdfsync}
%\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{exmp}{Example}%[section]
 
\renewcommand{\ttdefault}{cmtt}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}

\newtheorem{claim}{Claim}[theorem]
\newtheorem*{claim*}{Claim}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

% - macros

\newcommand{\nats}{\mathbb{N}}
\newcommand{\natswith}{\nats_{0}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\realspos}{\reals_{>0}}
\newcommand{\realsnonneg}{\reals_{\geq 0}}

\newcommand{\states}{\mathcal{X}}

\newcommand{\paths}{\Omega}
%\newcommand{\path}{\omega}

\newcommand{\power}{\mathcal{P}(\paths)}
\newcommand{\nonemptypower}{\power_{\emptyset}}
\newcommand{\events}{\mathcal{E}}
%\newcommand{\nonemptyevents}{\events^{\emptyset}}
\newcommand{\filter}[1][t]{\mathcal{F}_{#1}}
\newcommand{\eventst}[1][t]{\events_{#1}}

\newcommand{\processes}{\mathbb{P}}
\newcommand{\mprocesses}{\processes^{\mathrm{M}}}

\newcommand{\hmprocesses}{\processes^{\mathrm{HM}}}

\newcommand{\wprocesses}{\processes^{\mathrm{W}}}
\newcommand{\wmprocesses}{\processes^{\mathrm{WM}}}

\newcommand{\whmprocesses}{\processes^{\mathrm{WHM}}}


\newcommand{\lt}{\underline{T}}
\newcommand{\lbound}{L}

\newcommand{\gambles}{\mathcal{L}}
\newcommand{\gamblesX}{\gambles(\states)} 

\newcommand{\ind}[1]{\mathbb{I}_{#1}}

\newcommand{\rateset}{\mathcal{Q}}
\newcommand{\lrate}{\underline{Q}}

\newcommand{\asa}{\Leftrightarrow}
\newcommand{\then}{\Rightarrow}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

\newcommand{\coloneqq}{:\!=}

\newcommand{\opinset}{\,\,\widetilde{\in}\,\,}

\newcommand{\argmin}{\arg\min}

\newcommand{\exampleend}{\hfill$\Diamond$}

\def\presuper#1#2%
  {\mathop{}%
   \mathopen{\vphantom{#2}}^{#1}%
   \kern-\scriptspace%
   #2}

\title{Imprecise Continuous-Time Markov Chains}

%\author[1]{Thomas E. Krak\thanks{t.e.krak@uu.nl}}
%\author[2]{Jasper de Bock\thanks{jasper.debock@ugent.be}}
%\affil[1]{Universiteit Utrecht}
%\affil[2]{Ghent University}

\author{Thomas Krak \and Jasper De Bock}
%\author{Thomas Krak and Jasper de Bock}

\begin{document}

%\author{{\bf Thomas E. Krak} \\ Utrecht}
%\address{Utrecht University}
%\curraddr{}
%\email{t.e.krak@uu.nl}
%\thanks{}

%\author{{\bf Jasper de Bock} \\ Ghent}
%\address{Ghent University}

%\author{
	%{\bf Thomas E. Krak} \quad\quad {\bf Jasper de Bock} \\
%	Utrecht University \quad Ghent University \\
	%Department of Information and Computing Sciences \\
	%Princetonplein 5, De Uithof \\
	%3584 CC Utrecht \\
	%The Netherlands \\
%	\texttt{\quad\quad t.e.krak@uu.nl} \quad\quad \texttt{jasper.debock@ugent.be}
%\and
	%{\bf Jasper de Bock} \\
%	Ghent University \\
	%SYSTeMS Research Group \\
	%Technologiepark -- Zwijnaarde 914 \\
	%9052 Zwijnaarde \\ 
	%Belgium \\
%	\texttt{jasper.debock@ugent.be}
%}
\date{}
\maketitle

\begin{abstract}
Lorem ipsum.
\end{abstract}

\section{Introduction}\label{sec:introduction}

Continuous-time Markov chains are models that are used to describe the behavior of dynamical systems under stochastic uncertainty. They describe the stochastic evolution of such a system through a discrete state-space and over a continuous time-dimension. This class of models has found widespread applications in such fields as queuing theory~\cite{asmussen2008applied,bolch2006queueing}, mathematical finance~\cite{rolski2009stochastic, elliott2013default, sass2004optimizing}, epidemiology~\cite{jackson2003multistate, lemey2009reconstructing, duffy1995estimation}, and system reliability analysis~\cite{gokhale2004analysis, wang2007reliability, besnard2010approach}, among others~\cite{yin2012continuous} {\bf CITE} {\bf CITE}.

We here consider \emph{imprecise continuous-time Markov chains}, which can be used to robustly model systems for which often-made assumptions need not hold. Such assumptions may include time-homogeneity, whereby the behavior of the system is independent of the absolute point in time at which it is considered. Other such assumptions are the Markov property and differentiability of the state-transition probabilities. The former means that the future behavior of the system is independent of its history, given the current state. The differentiability assumption leads to convenient analytic expressions but might not hold if, e.g., the system is exposed to external perturbations. 

%As in any modeling task, simplifying assumptions tend to be made when working with these models. We would argue, however, that these assumptions are sometimes grounded more in pragmatism than in informed consideration of the underlying system. Such simplifying assumptions may include time-homogeneity, whereby the behavior of the system is independent of the absolute point in time at which it is considered; the Markov assumption, that the future behavior of the system only depends on its current state, and not on its history; or differentiability of the state-transition probabilities, which leads to convenient analytic expressions but which need not hold if, for example, the system is exposed to unknown external perturbations. 

%Therefore, we consider in this paper \emph{imprecise continuous-time Markov chains}, which can be used to robustly model systems for which the above assumptions need not hold. 

The following provides a motivating toy example that will return throughout this work. Although the model in this example is clearly too simple to be of practical use, it does already illustrate that the simplifying assumptions above are often unrealistic.
\begin{exmp}\label{ex:health_sick_exmp}
Suppose that we want to model a person periodically becoming sick, and recovering after some time. We model this behavior using a binary state space $\{\text{\tt healthy, sick}\}$, and consider the two possible state-transitions: from {\tt healthy} to {\tt sick}, and from {\tt sick} to {\tt healthy}. A \emph{rate parameter} is then associated with each of these transitions, which quantifies the ``speed'' with which the model completes such a transition. A continuous-time Markov chain can then be used to answer probabilistic questions about this system.

For instance, we might be interested in the probability $P(X_s={\tt sick}\,\vert\,X_t={\tt healthy})$ of the person being sick at time $s$, given that he or she is healthy at time $t$. Under an assumption of time-homogeneity, this probability would not depend on the absolute values of $t$ and $s$, but only on the relative difference $(s-t)$. This assumption might therefore be invalid if, e.g., there is a seasonal trend to the disease (it might be more likely to contract this disease during the winter).

As another example, the Markov assumption states that, for time points $t< r< s$,
\begin{equation*}
P(X_s={\tt sick}\,\vert\, X_r={\tt healthy}, X_t={\tt sick}) = P(X_s={\tt sick}\,\vert\, X_r={\tt healthy})\,,
\end{equation*}
that is, the probability of being sick at time $s$ \emph{only} depends on the person's condition at time $r$, and not on whether he or she has ever been sick before time $r$. When it is for instance possible to develop immunity to the disease, the Markov assumption is thus clearly incorrect.

As a final example, differentiability assumptions are often made about the probabilities involved. For instance, we might assume that $\nicefrac{\partial}{\partial s}\left[P(X_s\,\vert\,X_t)\right]$ exists. However, if the person that we are modeling is exposed to someone carrying a virus, the probability of becoming sick could suddenly and drastically change. More generally, this might be the case when our specified model fails to account for possible external perturbations.
\exampleend
\end{exmp}
From a practical point of view, once such a model is formulated and numerical estimates for its parameters have been obtained, one is typically interested in computing the expected value of some function with respect to this model. Such a function might be the expected utility of some financial strategy~\cite{sass2004optimizing}; the expected time until some component in a system breaks down~\cite{besnard2010approach}; or the expected speed with which clinical symptoms of a disease develop~\cite{duffy1995estimation}. It is of interest, therefore, to robustify such assessments not only against uncertain parameter estimates, but also against possibly incorrect assumptions like the ones discussed above. 

In particular, it would be useful to have a framework that can jointly deal with these different types of uncertainty and model misspecifications. Indeed, methods for working with uncertain parameter estimates include for example Bayesian techniques~\cite{insua2012bayesian}, and plenty of work has been done on modeling non-homogeneous systems (see, e.g.,~\cite{rindos1995exact,aalen1978empirical,johnson1989nonhomogeneous}). However, as far as we can tell, much less work has been done on dropping the Markovian and differentiability assumptions. Frameworks for jointly dealing with these different types of uncertainty are also scarce, if not non-existent.

In order to formulate such a framework here, we turn in this work to the field of \emph{imprecise probability}~\cite{Walley:1991vk}. Here, rather than work with a single model, say $P$, we jointly consider some \emph{set} of models $\mathcal{P}$. And, instead of computing the expectation $\mathbb{E}_P[f(\cdot)]$ of some function $f$ with respect to a single model $P$, we are then interested in the \emph{lower-} and \emph{upper expectation} with respect to this set $\mathcal{P}$\footnote{We adopt in this work the ``sensitivity analysis'' interpretation of imprecise probability, by explicitly considering sets of stochastic models. Specifically, it is also possible to instead take the lower- and upper expectation as themselves being the elementary objects of interest. We refer to {\bf CITE} {\bf CITE} {\bf CITE} for further discussion on this.}. These quantities are defined, respectively, as
\begin{equation*}
\underline{\mathbb{E}}[f(\cdot)] \coloneqq \inf\left\{\mathbb{E}_P[f(\cdot)]\,:\,P\in\mathcal{P}\right\}\,,\quad\text{and,}\quad\overline{\mathbb{E}}[f(\cdot)] \coloneqq \sup\left\{\mathbb{E}_P[f(\cdot)]\,:\,P\in\mathcal{P}\right\}\,.
\end{equation*}
This can intuitively be interpreted as providing best- and worst-case expectations with respect to all models in $\mathcal{P}$. For example, $\mathcal{P}$ may contain models under various plausible parameter estimates. Then, if the lower- and upper expectations of some function are roughly similar, we can say that the assessment is robust against choosing a (somewhat) incorrect parameter estimate. If, on the other hand, the lower- and upper expectation vary wildly, the assessment is clearly sensitive to the specific parameter estimate. In that case, e.g., policy may have to be adapted accordingly. In our specific setting, such a set might contain not only models under various parameter estimates, but also models for which, e.g., the Markov assumption does not hold. 

This use of imprecise probability has previously been applied to ``normal''---discrete-time---Markov chains, resulting in \emph{imprecise Markov chains} {\bf CITE} {\bf CITE} {\bf CITE}. This has found applications in {\bf CITE} {\bf CITE}, and has been shown to {\bf CITE}. **** dingen

The notion of imprecise \emph{continuous-time} Markov chains was only recently introduced to the literature by the work of {\v{S}}kulj~\cite{Skulj:2015cq}. This work focused on providing tools to tractably compute the lower expectation of functions on the state-space at a single time-point. Note that it suffices to focus on either the lower- or the upper expectation, because they can be defined through the conjugacy relation $\overline{\mathbb{E}}[f(\cdot)]=-\underline{\mathbb{E}}[-f(\cdot)]$. These results have since been applied to the robust analysis of failure-rates and repair-times in power-grid networks~\cite{troffaes2015using}.

Our aims with this present paper are threefold: firstly, to solidify the theoretical foundations of imprecise continuous-time Markov chains. Secondly, to extend and generalize previous results about the computability of lower expectations for this model class. Thirdly, to provide analytical tools for future analysis of these models, and to aid the development of algorithms.

Our main contributions are the following:
\begin{enumerate}
\item We provide a unified framework for describing continuous-time, discrete-space stochastic processes, using the formalism of full conditional probabilities. This framework covers the full range of (non-)homogeneous, (non-)Markovian, and (non-)differentiable stochastic processes.
\item We use this framework to formalize ``imprecise continuous-time Markov chains"; sets of stochastic processes that are in a specific sense ``consistent'' with a given set of parameters. We derive closure properties of such sets of processes, as well as factorization properties of their corresponding lower expectations.
\item We introduce a ``lower transition operator'' for imprecise continuous-time Markov chains, and show that this operator satisfies convenient algebraic properties like time-homogeneity, differentiability, and Markovian-like factorization---even if the underlying set of processes does not. We show that we can use this operator to compute lower expectations for arbitrary functions on the state space, and provide an exact characterization of the largest set of processes for which a given such operator does this.
\end{enumerate}

The remainder of this paper is organized as follows. In Section~\ref{sec:prelim}, we introduce notation and some basic concepts that will be used throughout. In Section~\ref{sec:systems}, we define some tools for representing the behavior of stochastic processes. Section~\ref{sec:stochastic_processes} formally introduces stochastic processes, and formalizes the tools for describing their dynamics. Section~\ref{sec:cont_time_markov_chains} focuses on the special case of continuous-time Markov chains. 

In Section~\ref{sec:iCTMC}, we formalize the notion of imprecise continuous-time Markov chains, and state some powerful properties that can aid in their analysis. Section~\ref{sec:lowertrans} introduces a lower transition operator for imprecise continuous-time Markov chains, and we show in Sections~\ref{sec:connections} and~\ref{sec:funcs_multi_time_points} that this operator can be used to compute lower expectations of arbitrary functions on the state space. In Section~\ref{sec:prev_work}, we relate and compare our results to previous work from the literature. Section~\ref{sec:conclusions} finally contains some conclusions, along with pointers to possible future work.

Throughout, most proofs and technical lemmas related to our results can be found in the appendix, where they are organized by section and chronological appearance.

\section{Preliminaries}\label{sec:prelim}

We denote the reals as $\reals$, the non-negative reals as $\realsnonneg$, and the positive reals as $\realspos$. For any $c\in\reals$, $\reals_{\geq c}$ and $\reals_{>c}$ have a similar meaning. The natural numbers are denoted by $\nats$, and we also define $\nats_0\coloneqq\nats\cup\{0\}$. The rationals will be denoted by $\mathbb{Q}$.

Infinite sequences of quantities will be denoted $\{a_i\}_{i\in\nats}$, possibly with limit statements of the form $\{a_i\}_{i\in\nats}\to c$, which should be interpreted as $\lim_{i\to\infty}a_i=c$. If the elements of such a sequence belong to a space that is endowed with an ordering relation, we may write $\{a_i\}_{i\in\nats}\to c^+$ or $\{a_i\}_{i\in\nats}\to c^-$ if the limit is approached from above or below, respectively.

For any set $A$ and any subset $C$ of $A$, we use $\ind{C}$ to denote the indicator of $C$, defined for all $a\in A$ by $\ind{C}(a)=1$ if $a\in C$ and $\ind{C}(a)=0$, otherwise. If $C$ is a singleton $C=\{c\}$, we may instead write $\ind{c}\coloneqq\ind{\{c\}}$.

\subsection{Sequences of Time Points}

We will make extensive use of finite sequences of time points. Such a sequence is of the form $u\coloneqq t_0,t_1,\ldots,t_n$, with $n\in\natswith$ and, for all $i\in\{0,\ldots,n\}$, $t_i\in\realsnonneg$. These sequences are taken to be ordered, meaning that for all $i,j\in\{0,\ldots,n\}$ with $i<j$, it holds that $t_i\leq t_j$. Let $\mathcal{U}$ denote the set of all such finite sequences that are \emph{non-degenerate}, meaning that for all $u\in\mathcal{U}$ with $u=t_0,\ldots,t_n$, it holds that $t_i\neq t_j$ for all $i,j\in\{0,\ldots,n\}$ such that $i\neq j$. Note that this does not prohibit \emph{empty} sequences. We therefore also define $\mathcal{U}_\emptyset\coloneqq \mathcal{U}\setminus\{\emptyset\}$.

For any finite sequence $u$ of time points, let $\max u\coloneqq \max\{t_i:i\in\{0,\ldots,n\}\}$. For any time point $t\in\realsnonneg$, we then write $t>u$ if $t>\max u$, and similarly for other inequalities. If $u=\emptyset$, then $t>u$ is taken to be trivially true, regardless of the value of $t$. We use $\mathcal{U}_{<t}$ to denote the subset of $\mathcal{U}$ that consists of those sequences $u\in\mathcal{U}$ for which $u<t$, and, again, similarly for other inequalities.

Since a sequence $u\in\mathcal{U}$ is a subset of $\realsnonneg$, we can use set-theoretic notation to operate on such sequences. The result of such operations is again taken to be ordered. For example, for any $u,v\in\mathcal{U}$, we use $u\cup v$ to denote the ordered union of $u$ and $v$. Similarly, for any $s\in\realsnonneg$ and any $u\in\mathcal{U}_{<s}$ with $u=t_0,\ldots,t_n$, we use $u\cup\{s\}$ to denote the sequence $t_0,\ldots,t_n,s$.

As a special case, we consider finite sequences of time points that partition a given time interval $[t,s]$, with $t,s\in\realsnonneg$ such that $t\leq s$. Such a sequence is taken to include the end-points of this interval.  Thus, the sequence is of the form $t=t_0< t_1<\cdots< t_n=s$. We denote the set of all such sequences by $\mathcal{U}_{[t,s]}$. Since these sequences are non-degenerate, it follows that $\mathcal{U}_{[t,t]}$ consists of a single sequence $u=t$. For any $u\in\mathcal{U}_{[t,s]}$ with $u=t_0,\ldots,t_n$, we also define the sequential differences $\Delta_i^u\coloneqq t_i-t_{i-1}$, for all $i\in\{1,\ldots,n\}$. We then use $\sigma(u)\coloneqq \max\{\Delta_i^u:i\in\{1,\ldots,n\}\}$ to denote the maximum such difference.

\subsection{States and Functions}\label{sec:multivar_notation}

Throughout this work, we will consider some fixed finite \emph{state space} $\states=\{1,\dots,m\}$. A generic element of this set is called a state and will be denoted by $x$.

We use $\gamblesX$ to denote the set of all real-valued functions on $\states$. Because $\states$ is finite, a function $f\in\gamblesX$ can be interpreted as a vector in $\reals^m$. Hence, we will in the sequel use the terms `function' and `vector' interchangeably when referring to elements of $\gamblesX$.

We will often find it convenient to explicitely indicate the time point $t$ that is being considered, in which case we write $\states_t\coloneqq\states$ to denote the state space at time $t$, and $x_t$ to denote a state at time $t$. This notational trick also allows us to introduce some notation for the joint state at (multiple) explicit time points. For any finite sequence of time points $u\in\mathcal{U}$ such that $u=t_0,\ldots,t_n$, we use
\begin{equation*}
\states_u \coloneqq \prod_{t\in u}\states_t
\end{equation*}
to denote the joint state space at the time points in $u$. A joint state $x_u\in\states_u$ is a tuple $(x_{t_0},\ldots,x_{t_n})\in\states_{t_0}\times\cdots\times\states_{t_n}$ that specifies a state $x_{t_k}$ for every time point $t_k$ in $u$. Note that if $u$ only contains a single time point $t$, then we simply have that $\states_u=\states_{\{t\}}=\states_t=\states$. If $u=\emptyset$, then $x_\emptyset\in\states_\emptyset$ is a ``dummy'' placeholder, which typically leads to statements that are vacuously true. 
For any $u\in\mathcal{U}_\emptyset$, we use $\gambles(\states_u)$ to denote the set of all real-valued functions on $\states_u$. 

%Finally, we will want to use operators as defined in Section~\ref{sec:func_oper_norm} for functions defined on (multiple) explicit time points. To this end, for any $f\in\gambles(\states^{u\cup\{s\}})$, with $u<s$, and any non-negatively homogeneous operator $A$ from $\gamblesX$ to $\gamblesX$, we stipulate the following convention. If $A$ is applied to $f$, it will be applied to the restriction of $f$ to the \emph{latest} time point at which it is defined---the time point $s$, in this case. Because this restriction depends on the specific state assignment $x_u$ corresponding to the other time points $u=t_0,\ldots,t_n$, the result is a function $[Af]\in\gambles(\states^u)$. In other words, we stipulate that
%\begin{equation*}
%[Af](x_u) \coloneqq [Af(x_u,X_s)](x_{t_n})\,.
%\end{equation*}


%Finally, as some notes on notational conventions, we typically use $t,r,s\in\realsnonneg$ to denote time points, $u,v,w\in\mathcal{U}$ to denote sequences of time points, and $x,y,z\in\states$ to denote state assignments. 

\subsection{Norms and Operators}\label{sec:func_oper_norm}

For any $u\in\mathcal{U}_\emptyset$ and any $f\in\gambles(\states_u)$, let the norm $\norm{f}$ be defined as
\begin{equation*}
\norm{f} \coloneqq \norm{f}_{\infty} \coloneqq \max\{\abs{f(x_u)}\,:\,x_u\in\states_u\}\,.
\end{equation*}
As a special case, we then have for any $f\in\gamblesX$ that $\norm{f}=\max\{\abs{f(x)}\,:\,x\in\states\}$.

Linear maps from $\gamblesX$ to $\gamblesX$ will play an important role in this work, and will be represented by matrices. Because the state space $\states$ is fixed throughout, we will always consider square, $m\times m$, real-valued matrices, where $m$ is the size of $\states$. As such, we will for the sake of brevity simply refer to them as `matrices'. If $A$ is such a matrix, we will index its elements as $A(x,y)$ for all $x,y\in\states$, where the indexing is understood to be row-major. Furthermore, $A(x,\cdot)$ will denote the $x$-th row of $A$, and $A(\cdot,y)$ will denote its $y$-th column. The symbol $I$ will be reserved throughout to refer to the $m\times m$ identity matrix.

Because we will also be interested in non-linear maps, we consider as a more general case operators that are \emph{non-negatively homogeneous}. An operator $A$ from $\gamblesX$ to $\gamblesX$ is non-negatively homogeneous if $A(\lambda f)=\lambda \left[Af\right]$ for all $f\in\gamblesX$ and all $\lambda\in\realsnonneg$. Note that this includes matrices as a special case.

For any non-negatively homogeneous operator $A$ from $\gamblesX$ to $\gamblesX$, we consider the induced operator norm
\begin{equation*}
\norm{A}\coloneqq\sup\left\{\norm{Af}\colon f\in\gamblesX,\norm{f}=1\right\}.
\end{equation*}
If $A$ is a matrix, it is easily verified that then
\begin{equation}\label{eq:normofmatrix}
\norm{A}
=
\max\left\{\sum_{y\in\states}\abs{A(x,y)}\colon x\in\states\right\}.
\end{equation}
\noindent
Finally, for any set $\mathcal{A}$ of matrices, we define $\norm{\mathcal{A}}\coloneqq\sup\{\norm{A}\colon A\in\mathcal{A}\}$.

These norms satisfy the following properties; Reference~\cite{DeBock:2016} provides a proof for the non-trivial ones.

\begin{proposition}\label{prop:norm_properties}
For all $f,g\in\gamblesX$, all $A,B$ from $\gamblesX$ to $\gamblesX$ that are non-negatively homogeneous, all $\lambda\in\reals$ and all $x\in\states$, we have that
\vspace{5pt}

\begin{multicols}{2}
\begin{enumerate}[label=N\arabic*:,ref=N\arabic*]
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\abs{f(x)}\leq\norm{f}$ \\
\item
$\norm{A}\geq0$
\item
$\norm{A}=0\asa A=0$
\item
$\norm{A+B}\leq\norm{A}+\norm{B}$
\item\label{N:homogeneous}
$\norm{\lambda A}=\abs{\lambda}\norm{A}$
\item\label{N:normAB}
$\norm{AB}\leq\norm{A}\norm{B}$
\item\label{N:normAf}
$\norm{Af}\leq\norm{A}\norm{f}$
\end{enumerate}
\end{multicols}
\end{proposition}

\section{Transition Matrix Systems}\label{sec:systems}

We provide in this section some definitions that will later be useful for characterizing continuous-time Markov chains. Because we have not yet formally introduced the concept of a continuous-time Markov chain, for now, the definitions below can be taken to be purely algebraic constructs. Nevertheless, whenever possible, we will of course attempt to provide them with some intuition.

%Our reason for providing these definitions here, instead of where we first require them, is because we do not whish to interrupt the line of reasoning in Sections~\ref{sec:stochastic_processes} and~\ref{sec:cont_time_markov_chains} with technical definitions. However, whenever possible, we attempt to provide some intuition on the context in which these definitions will be used.

For the purposes of this section, it suffices to say that a continuous-time Markov chain is a process which at each time $t$ is in some state $x\in\states$. As time elapses, the proces moves through the state space $\states$ in some stochastic fashion. We here define tools with which this stochastic behavior can be conveniently described.

\subsection{Transition (Rate) Matrices}\label{sec:trans_rate_matrices}

%\emph{Transition matrices} are convenient tools for describing both discrete-time and continuous-time Markov chains. 
A \emph{transition matrix} $T$ is a matrix that is row-stochastic, meaning that, for each $x\in\states$, the row $T(x,\cdot)$ is a probability mass function on $\states$.
\begin{definition}[Transition Matrix]\label{def:stoch_matrix}
A real-valued matrix $T$ is said to be a \emph{transition matrix} if
\vspace{5pt}
\begin{enumerate}[label=T\arabic*:,ref=T\arabic*]
\item\label{def:T:sumone}
$\sum_{y\in\states}T(x,y)=1$ for all $x\in\states$;\label{def:trans_matrix_is_stochastic}
\item\label{def:T:nonneg}
$T(x,y)\geq0$ for all $x,y\in\states$.
\end{enumerate}
\vspace{5pt}
\noindent
\end{definition}

\begin{proposition}\label{lemma:compositiontransitionmatrix}
For any two transition matrices $T_1$ and $T_2$, their composition $T_1T_2$ is also a transition matrix.
\end{proposition}

The interpretation in the context of Markov chains goes as follows. The elements $T(x,y)$ of a transition matrix $T$ describe the probability of the Markov chain ending up in state $y$ at the next time point, given that it is currently in state $x$. In other words, the row $T(x,\cdot)$ contains the state-transition probabilities, conditional on currently being in state $x$. We will make this connection more explicit when we formalize continuous-time stochastic processes in Section~\ref{sec:stochastic_processes}.

For now, we note that in a continuous-time setting, this notion of ``next'' time-point is less obvious than in a discrete-time setting, because the state-transition probabilities are then continuously dependent on the evolution of time. To capture this aspect, the notion of \emph{transition rate matrices}~\cite{norris1998markov} is used.
\begin{definition}[Transition Rate Matrix]\label{def:rate_matrix}
A real-valued matrix $Q$ is said to be a \emph{transition rate matrix}, or sometimes simply \emph{rate matrix}, if

\vspace{5pt}
\begin{enumerate}[label=R\arabic*:,ref=R\arabic*]
\item\label{def:Q:sumzero}
$\sum_{y\in\states}Q(x,y)=0$ for all $x\in\states$;
\item\label{def:Q:nonnegoffdiagonal}
$Q(x,y)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\noindent
We use $\mathcal{R}$ to denote the set of all transition rate matrices. 
\vspace{5pt}
\end{definition}

The connection between transition matrices and rate matrices is perhaps best illustrated as follows. Suppose that at some time point $t$, we want to describe for any state $x$ the probability of ending up in state $y$ at some time $s\geq t$. Let $T_t^s$ denote the transition matrix that contains all these probabilities. Note first of all that it is reasonable to assume that, if time does not evolve, then the system should not change. That is, if we are in state $x$ at time $t$, then the probability of still being in state $x$ at time $s=t$, should be one. Hence, we should have $T_t^t=I$, with $I$ the identity matrix. 

A rate matrix $Q$ is then used to describe the transition matrix $T_t^{t+\Delta}$ after a small period of time, $\Delta$, has elapsed. Specifically, the scaled matrix $\Delta Q$ serves as a linear approximation of the change from $T_t^t$ to $T_t^{t+\Delta}$. The following proposition states that, for small enough $\Delta$, this linear change still results in a transition matrix.

\begin{proposition}\label{prop:stochastic_from_rate_matrix}
Consider any transition rate matrix $Q\in\mathcal{R}$, and any $\Delta\in\realsnonneg$ such that $\Delta \norm{Q}\leq 1$. Then the matrix $(I+\Delta Q)$ is a transition matrix.
\end{proposition}
This also explains the terminology used; a rate matrix describes the ``rate of change'' of a (continuously) time-dependent transition matrix over a small period of time.

Of course, this notion can also be reversed; given a transition matrix $T_t^{t+\Delta}$, what is the change that it underwent compared to $T_t^t=I$? The following proposition states that such a change can always be described using a rate matrix.
\begin{proposition}\label{prop:rate_from_stochastic_matrix}
Consider any transition matrix $T$, and any $\Delta\in\realspos$. Then, the matrix $\nicefrac{1}{\Delta}(T-I)$ is a transition rate matrix.
\end{proposition}
Note that Proposition~\ref{prop:rate_from_stochastic_matrix} essentially states that the finite-difference $\nicefrac{1}{\Delta}(T_t^{t+\Delta} - T_t^t)$ is a rate matrix. Intuitively, if we now take the limit as this $\Delta$ goes to zero, this states that the derivative of a continuously time-dependent transition matrix is given by some rate matrix $Q\in\mathcal{R}$---assuming that this limit exists, of course. We will make this connection more explicit in Section~\ref{sec:stochastic_processes}.

We next introduce a function that is often seen in the context of continuous-time Markov chains: the \emph{matrix exponential}~\cite{van2006study} $e^{Q\Delta}$ of $Q \Delta$, with $Q$ a rate matrix and $\Delta\in\reals_{\geq0}$. %For any matrix $A$, its matrix exponential is denoted $e^A$. 
There are various equivalent ways in which such a matrix exponential can be defined. We refer to~\cite{van2006study} for some examples, and will consider some specific definitions later on in this work.
For now, we restrict ourselves to stating the following well-known result.
\begin{proposition}\cite[Theorem 2.1.2]{norris1998markov}\label{prop:stochastic_from_exponential}
Consider a rate matrix $Q\in\mathcal{R}$ and any $\Delta\in\realsnonneg$. Then $e^{Q\Delta}$ is a transition matrix.
\end{proposition}

We conclude this section with some comments about \emph{sets} of rate matrices. First, note that the set of \emph{all} rate matrices, $\mathcal{R}$, is closed under finite sums and multiplication with non-negative scalars. Consider now any set $\rateset\subseteq\mathcal{R}$ of rate matrices. Then $\rateset$ is said to be \emph{non-empty} if $\rateset\neq\emptyset$ and $\rateset$ is said to be \emph{bounded} if $\norm{\rateset}<+\infty$. The following proposition provides a simple alternative characterization of boundedness.

\begin{proposition}\label{prop:alternativedefforbounded}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ is bounded if and only if
\begin{equation}\label{eq:alternative_bounded}
\inf\left\{Q(x,x)\colon Q\in\rateset\right\}>-\infty\text{~~for all $x\in\states$.}
\end{equation}
\end{proposition}


\subsection{Transition Matrix Systems}

In the previous section, we used the notation $T_t^s$ to refer to a transition matrix that contains the probabilities of moving from a state at time $t$, to a state at time $s$. We now consider \emph{families} of these transition matrices. Such a family $\mathcal{T}$ specifies a transition matrix $T_t^s$ for every $t,s\in\realsnonneg$ such that $t\leq s$. 

We already explained in the previous section that it is reasonable to assume that $T_t^t=I$. If the transition matrices of a family $\mathcal{T}$ satisfy this property, and if they furthermore satisfy the \emph{semi-group} property---see Equation~\ref{eq:transmatrixproduct} below---we call this family a \emph{transition matrix system}.

\begin{definition}[Transition Matrix System]\label{def:trans_mat_system}
A \emph{transition matrix system} $\mathcal{T}$ is a family of transition matrices $T_t^s$, defined for all $t,s\in\realsnonneg$ with $t\leq s$, such that for all $t,r,s\in\realsnonneg$ with $t\leq r\leq s$, it holds that
\begin{equation}\label{eq:transmatrixproduct}
T_t^s=T_t^r T_r^s\,,
\end{equation}
and for all $t\in\realsnonneg$, $T_t^t=I$.
\end{definition}
It will turn out that there is a strong connection between transition matrix systems and continuous-time Markov chains. We will return to this in Section~\ref{sec:cont_time_markov_chains}.

In the previous section, we have seen that for any transition matrix $T$ and any $\Delta\in\realspos$, the matrix $\nicefrac{1}{\Delta}(T - I)$ is a rate matrix, and therefore, in particular, that the finite difference $\nicefrac{1}{\Delta}(T_t^{t+\Delta} - I)$ is a rate matrix. We here note that this is also the case for the term $\nicefrac{1}{\Delta}(T_{t-\Delta}^t - I)$ whenever $(t-\Delta)\geq0$.

We now consider this property in the context of a transition matrix system $\mathcal{T}$. For all $t\in\realsnonneg$ and all $\Delta\in\realspos$, such a transition matrix system specifies a transition matrix $T_t^{t+\Delta}$ and---if $(t-\Delta)\geq0$---a transition matrix $T_{t-\Delta}^t$. We now consider the behavior of these matrices for various values of $\Delta$. In particular, we look what happens to these finite differences if we take $\Delta$ to be increasingly smaller. 

For each $\Delta\in\realspos$, due the property that we have just recalled, there will be a rate matrix that corresponds to these finite differences. If the norm of these rate matrices never diverges to $+\infty$ as we take $\Delta$ to zero, we call the family $\mathcal{T}$ \emph{well-behaved}.

\begin{definition}[Well-Behaved Transition Matrix System]\label{def:well_behaved_trans_mat_system}
A transition matrix system $\mathcal{T}$ is called \emph{well-behaved} if 
\begin{equation}\label{eq:wellbehavedtransitionmatrixsystem}%\label{eq:wellbehavedhistorictransitionmatrix}
(\forall t\in\realsnonneg)~\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t}^{t+\Delta}-I}<+\infty\,
\text{~and~}
(\forall t\in\realspos)~\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta}^t-I}<+\infty\,.
\end{equation}
\end{definition}

Observe that this notion of well-behavedness does not imply differentiability; the limit $\lim_{\Delta\to0^+}\nicefrac{1}{\Delta}(T_t^{t+\Delta}-I)$ need not exist. Rather, it implies that the rate of change of the transition matrices in $\mathcal{T}$ is bounded at all times and on any given interval.

We finally consider an important special type of transition matrix systems. We have seen in the previous section that for any $Q\in\mathcal{R}$ and any $\Delta\in\realsnonneg$, the matrix exponential $e^{Q\Delta}$ is a transition matrix. We here consider for any $Q\in\mathcal{R}$ the family $\mathcal{T}_Q$ that is generated by such transition matrices.
\begin{definition}\label{def:systemfromQ}For any rate matrix $Q\in\mathcal{R}$, we use $\mathcal{T}_Q$ to denote the family of transition matrices that is defined by
\begin{equation*}
T_t^s=e^{Q(s-t)}
\text{~~for all $t,s\in\realsnonneg$ such that $t\leq s$.}
\end{equation*}
We call this family $\mathcal{T}_Q$ the \emph{exponential transition matrix system} corresponding to $Q$.
\end{definition}

%The following result explains this use of terminology.

\begin{proposition}
\label{prop:systemQ}
For any $Q\in\mathcal{R}$, $\mathcal{T}_Q$ is a well-behaved transition matrix system.
\end{proposition}

This exponential transition matrix system corresponding to a $Q\in\mathcal{R}$ will turn out to play a large role in the context of continuous-time Markov chains. We return to this in Section~\ref{sec:cont_time_markov_chains}.

\subsection{Restricted Transition Matrix Systems}

We finally consider yet another construct that will be useful later: the restriction of a transition matrix system $\mathcal{T}$ to a closed interval $\mathbf{I}$ in the time-line $\realsnonneg$.

By a closed interval $\mathbf{I}$, we here mean a non-empty closed subset $\mathbf{I}\subseteq\realsnonneg$ that is connected, in the sense that for any $t,s\in\mathbf{I}$ such that $t\leq s$, and any $r\in[t,s]$, it holds that $r\in\mathbf{I}$. Note that for any $c\in\realsnonneg$, $[c,+\infty)$ is such a closed interval.

For any transition matrix system $\mathcal{T}$ and any such closed interval $\mathbf{I}\subseteq\realsnonneg$, we use $\mathcal{T}^\mathbf{I}$ to denote the restriction of $\mathcal{T}$ to $\mathbf{I}$. Such a restriction is a family of transition matrices $T_t^s$ that is defined for all $t,s\in\mathbf{I}$ such that $t\leq s$.
%\begin{equation*}
%\mathcal{T}^{\mathbf{I}} \coloneqq \left\{T_t^s\in\mathcal{T}\,:\,\forall t,s\in\mathbf{I},\, t\leq s\right\}\,.
%\end{equation*}
We call such a family $\mathcal{T}^{\mathbf{I}}$ a \emph{restricted transition matrix system} on $\mathbf{I}$.

%**** misschien iets zeggen dat we ook open intervallen toelaten; in het bijzonder heb ik $\mathcal{T}^{[t,\infty)}$ nodig.

\begin{proposition}\label{prop:restr_trans_mat_system_if_semigroup}
Consider any closed interval $\mathbf{I}\subseteq\realsnonneg$, and let $\mathcal{T}^{\mathbf{I}}$ be a family of transition matrices $T_t^s$ that is defined for all $t,s\in\mathbf{I}$ with $t\leq s$. Then $\mathcal{T}^{\mathbf{I}}$ is a restricted transition matrix system on $\mathbf{I}$ if and only if, for all $t,r,s\in\mathbf{I}$ with $t\leq r\leq s$, it holds that $T_t^s = T_t^rT_r^s$ and $T_t^t=I$.
\end{proposition}

We call a restricted transition matrix system $\mathcal{T}^{\mathbf{I}}$ well-behaved if it is the restriction to $\mathbf{I}$ of a well-behaved transition matrix system.

\begin{proposition}\label{prop:well_restr_trans_mat_system_if_limsup}
Consider any closed interval $\mathbf{I}\subseteq\realsnonneg$, and let $\mathcal{T}^{\mathbf{I}}$ be a restricted transition matrix system on $\mathbf{I}$. Then $\mathcal{T}^{\mathbf{I}}$ is well-behaved if and only if
\begin{equation}\label{eq:wellbehavedrestrictedtransitionmatrixsystem}%\label{eq:wellbehavedhistorictransitionmatrix}
(\forall t\in\mathbf{I}^+)~\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t}^{t+\Delta}-I}<+\infty\,
\text{~and~}
(\forall t\in\mathbf{I}^-)~\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta}^t-I}<+\infty\,,
\end{equation}
where $\mathbf{I}^+\coloneqq\mathbf{I}\setminus\{\sup\mathbf{I}\}$ and $\mathbf{I}^-\coloneqq\mathbf{I}\setminus\{\min\mathbf{I}\}$.%\footnote{Since $\mathbf{I}$ may not have a maximum $\max\mathbf{I}$, for example if $\mathbf{I}=\realsnonneg$, we consider its supremum $\sup\mathbf{I}$.}
\end{proposition}

%it satisfies Equation~\eqref{eq:wellbehavedtransitionmatrixsystem} on $\mathbf{I}$, instead of on the entirety of $\realsnonneg$.

%For all $t\in\mathbf{I}\setminus\min\mathbf{I}$

Now, because these restricted transition matrix systems are only defined on some given closed interval, it will be useful to define a concatenation operator between two such systems. % defined on adjacent intervals, where we say that two intervals $\mathbf{I},\mathbf{J}\subseteq\realsnonneg$ are \emph{adjacent} if, without loss of generality, $\mathbf{I}$ is closed on the right and $\mathbf{J}$ is closed on the left, and $\max\mathbf{I}=\min\mathbf{J}$. 

\begin{definition}[Concatenation Operator]
For any two closed intervals $\mathbf{I},\mathbf{J}\subseteq\realsnonneg$ such that $\max\mathbf{I}=\min\mathbf{J}$, and any two restricted transition matrix systems $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$, the concatenation of $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$ is denoted by $\mathcal{T}^\mathbf{I}\otimes\mathcal{T}^\mathbf{J}$, and defined as the family of transition matrices $T_t^s$ that is given by
\begin{equation*}
T_t^s \coloneqq \begin{cases}
\presuper{i}T_t^s & \text{if $t,s\in\mathbf{I}$} \\
\presuper{j}T_t^s & \text{if $t,s\in\mathbf{J}$} \\
\presuper{i}T_t^r\presuper{j}T_r^s & \text{if $t\in\mathbf{I}$ and $s\in\mathbf{J}$}
\end{cases}\text{~~~for all $t,s\in\mathbf{I}\cup\mathbf{J}$ such that $t\leq s$,}
\end{equation*}
where $r\coloneqq\max\mathbf{I}=\min\mathbf{J}$, and $\presuper{i}T_t^s$ and $\presuper{j}T_t^s$ denote the transition matrices corresponding to $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$, respectively.
\end{definition}
%Then, for any two adjacent intervals $\mathbf{I},\mathbf{J}\subseteq\realsnonneg$, and any two restricted transition matrix systems $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$, let $r\coloneqq\max\mathbf{I}=\min\mathbf{J}$, and let $\presuper{i}T_t^s$ and $\presuper{j}T_t^s$ denote the transition matrices corresponding to $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$, respectively. Consider the family of transition matrices $T_t^s$, defined by
%\begin{equation*}
%T_t^s \coloneqq \begin{cases}
%\presuper{i}T_t^s & \text{if $t,s\in\mathbf{I}$} \\
%\presuper{j}T_t^s & \text{if $t,s\in\mathbf{J}$} \\
%\presuper{i}T_t^r\presuper{j}T_r^s & \text{if $t\in\mathbf{I}$ and $s\in\mathbf{J}$}
%\end{cases}\text{~~~for all $t,s\in\mathbf{I}\cup\mathbf{J}$ such that $t\leq s$.}
%\end{equation*}
%We denote this family of transition matrices by $\mathcal{T}^\mathbf{I}\otimes\mathcal{T}^\mathbf{J}$.

%\begin{align*}
%\mathcal{T}^{\mathbf{I}} \otimes \mathcal{T}^{\mathbf{J}} \coloneqq &\mathcal{T}^{\mathbf{I}} \cup \mathcal{T}^{\mathbf{J}} 
% \cup \left\{T_t^s\coloneqq T_t^rT_r^{s}\,\Big\vert\,T_{t}^r\in\mathcal{T}^{\mathbf{I}},\,T_r^{s}\in\mathcal{T}^{\mathbf{J}},\,\forall t\in\mathbf{I},\,\forall s\in\mathbf{J}\right\}\,.
%\end{align*}
\begin{proposition}\label{prop:concat_restr_trans_mat_systems_is_system}
Consider two closed intervals $\mathbf{I},\mathbf{J}\subseteq\realsnonneg$ such that $\max\mathbf{I}=\min\mathbf{J}$, and any two restricted transition matrix systems $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$. Then their concatenation $\mathcal{T}^{\mathbf{I}\cup \mathbf{J}} \coloneqq \mathcal{T}^{\mathbf{I}}\otimes \mathcal{T}^{\mathbf{J}}$ is a restricted transition matrix system on $\mathbf{I}\cup\mathbf{J}$. Furthermore, if both $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$ are well behaved, then $\mathcal{T}^{\mathbf{I}\cup\mathbf{J}}$ is also well-behaved.
\end{proposition}

\begin{exmp}
Consider any two rate matrices $Q_1,Q_2\in\mathcal{R}$ such that $Q_1\neq Q_2$, and let $\mathcal{T}_{Q_1}$ and $\mathcal{T}_{Q_2}$ be their exponential transition matrix systems. Consider any $r\in\realsnonneg$, and define
\begin{equation*}
\mathcal{T} \coloneqq \mathcal{T}_{Q_1}^{[0,r]} \otimes \mathcal{T}_{Q_2}^{[r,+\infty)}\,.
\end{equation*}
Then, $\mathcal{T}$ is a transition matrix system, and because $\mathcal{T}_{Q_1}^{[0,r]}$ and $\mathcal{T}_{Q_2}^{[r,+\infty)}$ are well-behaved, so is $\mathcal{T}$. Furthermore, for any $t,s\in\realsnonneg$ such that $t\leq r\leq s$, the transition matrix $T_t^s$ corresponding to $\mathcal{T}$ satisfies $T_t^s = T_t^rT_r^s = e^{Q_1(r-t)}e^{Q_2(s-r)}$.
\exampleend
\end{exmp}

We will finally find it useful to define a metric between any two restricted transition matrix systems $\mathcal{T}^\mathbf{I}$ and $\mathcal{S}^\mathbf{I}$ defined on the same interval. We define this metric $d$ as
\begin{equation}\label{eq:trans_mat_system_metric}
d(\mathcal{T}^{\mathbf{I}},\mathcal{S}^{\mathbf{I}}) \coloneqq \sup\left\{\norm{T_t^s - S_t^s}\,:\,t,s\in\mathbf{I},\,t\leq s\right\}\,,
\end{equation}
where, for all $t,s\in\mathbf{I}$, it is understood that $T_t^s$ corresponds to $\mathcal{T}^{\mathbf{I}}$ and $S_t^s$ to $\mathcal{S}^{\mathbf{I}}$.

We now give a powerful theoretical result that we will return to later. 

\begin{proposition}\label{lemma:restricted_trans_mat_system_cauchy_converges}
Consider any interval $\mathbf{I}\subseteq\realsnonneg$, let $\mathbb{T}^{\mathbf{I}}$ be the set of all restricted transition matrix systems on $\mathbf{I}$ and let $d$ be the metric that is defined in Equation~\eqref{eq:trans_mat_system_metric}. The metric space $\smash{(\mathbb{T}^{\mathbf{I}},d)}$ is then complete.
\end{proposition}
Note that this result includes as a special case completeness of the set $\mathbb{T}^{\realsnonneg}$ of all (unrestricted) transition matrix systems. The following provides an example of how this result can be used. 

\begin{exmp}\label{exmp:limit_trans_mat_system}
Consider any two commuting rate matrices $Q_1,Q_2\in\mathcal{R}$ such that $Q_1\neq Q_2$, and let $\mathcal{T}_{Q_1}$ and $\mathcal{T}_{Q_2}$ be their exponential transition matrix systems. Consider now the sequence $\{\Delta_i\}_{i\in\nats}\coloneqq \{\nicefrac{1}{2^i}\}_{i\in\nats}$, define $\mathcal{T}_0\coloneqq \mathcal{T}_{Q_1}$, and, for all $i\in\nats$, let
\begin{equation*}
\mathcal{T}_i \coloneqq \begin{cases}
\mathcal{T}_{Q_1}^{[0,\Delta_i]}\otimes \mathcal{T}_{i-1}^{[\Delta_i,+\infty)} & \text{if $i$ is odd, and} \\
\mathcal{T}_{Q_2}^{[0,\Delta_i]}\otimes \mathcal{T}_{i-1}^{[\Delta_i,+\infty)} & \text{otherwise.}
\end{cases}
\end{equation*}
The sequence $\{\mathcal{T}_i\}_{i\in\nats}$ is then clearly in $\mathbb{T}^{\realsnonneg}$. Furthermore, for any $i\in\nats$, the systems $\mathcal{T}_{i-1}$ and $\mathcal{T}_i$ differ the ``most'' on the interval $[0,\Delta_i]$. It should therefore be intuitively clear that $d(\mathcal{T}_{i-1},\mathcal{T}_i)\propto \Delta_i$, which can also be formally shown by combining Equations~\eqref{eq:trans_mat_system_metric} and~\eqref{eq:transmatrixproduct} with Lemmas~\ref{lemma:recursive} and~\ref{lemma:linearpartofexponential} in the appendix. 

Therefore, and because $\{\Delta_i\}_{i\in\nats}\to0^+$, the limit $\mathcal{T}_*\coloneqq \lim_{i\to\infty}\mathcal{T}_i$ clearly exists and, by Proposition~\ref{lemma:restricted_trans_mat_system_cauchy_converges}, is a transition matrix system. It should also be intuitively clear that $\mathcal{T}_*$ is well-behaved, however a formal proof is rather lengthy and left as an exercise.
\end{exmp}

Thus, Proposition~\ref{lemma:restricted_trans_mat_system_cauchy_converges} allows us to prove the existence of limits of sequences of (restricted) transition matrix systems. Because this limit is then again a (restricted) transition matrix system, it will be a family $\mathcal{T}_*$ of transition matrices $\presuper{*}T_t^s$. In some cases, we will be interested in these matrices $\presuper{*}T_t^s$. The following example sketches how, for the particular case of the limit from Example~\ref{exmp:limit_trans_mat_system}, we may find closed-form expressions for these transition matrices. This example is included mostly in preparation of some other examples later in this work.
\begin{exmp}\label{exmp:limit_trans_mat_system_matrices}
Consider again the sequence $\{\mathcal{T}_i\}_{i\in\nats}$ from Example~\ref{exmp:limit_trans_mat_system}, with limit $\mathcal{T}_*\coloneqq \lim_{i\to\infty}\mathcal{T}_i$. We will be interested in the transition matrices $\presuper{*}T_t^s$ corresponding to this transition matrix system $\mathcal{T}_*$. 

It follows from the proof of Proposition~\ref{lemma:restricted_trans_mat_system_cauchy_converges} that these transition matrices are given by $\presuper{*}T_t^s = \lim_{i\to\infty} \presuper{i}T_t^s$, where for all $i\in\nats$, $\presuper{i}T_t^s$ corresponds to $\mathcal{T}_i$. In what follows, we derive an explicit expression for the matrices $\presuper{*}T_t^{s}$, for the specific case where $t=0$ and $s\in\{\Delta_i\}_{i\in\nats}$.

To this end, fix any $j\in\nats$, and consider first the $\presuper{i}T_0^{\Delta_j}$ corresponding to $\mathcal{T}_i$. Assume without loss of generality that $j$ is odd. Then, the sequence $\presuper{j}T_0^{\Delta_j},\presuper{j+1}T_0^{\Delta_j},\presuper{j+2}T_0^{\Delta_j},\ldots$ is clearly given by
\begin{equation*}
\left(\presuper{j}T_0^{\Delta_j}\right), ~~\left(\presuper{j+1}T_0^{\Delta_{j+1}}\right)\left(\presuper{j}T_{\Delta_{j+1}}^{\Delta_j}\right), ~~\left(\presuper{j+2}T_0^{\Delta_{j+2}}\right)\left(\presuper{j+1}T_{\Delta_{j+2}}^{\Delta_{j+1}}\right)\left(\presuper{j}T_{\Delta_{j+1}}^{\Delta_j}\right), \ldots
\end{equation*}
Because each of these component transition matrices corresponds to either $\mathcal{T}_{Q_1}$ or $\mathcal{T}_{Q_2}$, this can equivalently be expressed as
\begin{equation*}
\left(e^{Q_1\Delta_j}\right), ~~\left(e^{Q_2\Delta_{j+1}}\right)\left(e^{Q_1(\Delta_j-\Delta_{j+1})}\right), ~~\left(e^{Q_1\Delta_{j+2}}\right)\left(e^{Q_2(\Delta_{j+1}-\Delta_{j+2})}\right)\left(e^{Q_1(\Delta_j-\Delta_{j+1})}\right), \ldots
\end{equation*}
Because $Q_1$ and $Q_2$ were taken to commute, meaning that $Q_1Q_2=Q_2Q_1$, the matrix exponential is well-known~\cite[Theorem 5]{van2006study} to satisfy $e^{Q_1\Delta_1'}e^{Q_2\Delta_2'}=e^{Q_1\Delta_1' + Q_2\Delta_2'}$, for any $\Delta_1',\Delta_2'\in\realsnonneg$. Therefore, the above sequence can equivalently be written
\begin{align*}
%& e^{Q_1\Delta_j}, ~~e^{Q_2\Delta_{j+1}}e^{Q_1(\Delta_j-\Delta_{j+1})}, ~~e^{Q_2(\Delta_{j+1}-\Delta_{j+2})}e^{Q_1(\Delta_j-\Delta_{j+1} + \Delta_{j+2})},\ldots \\
& e^{Q_1\Delta_j}, ~~e^{Q_1(\Delta_j-\Delta_{j+1}) + Q_2\Delta_{j+1}}, ~~e^{Q_1(\Delta_j-\Delta_{j+1} + \Delta_{j+2}) + Q_2(\Delta_{j+1}-\Delta_{j+2})},\ldots
\end{align*}
%Furthermore, because $\{\Delta_i\}_{i\in\nats}=\{\nicefrac{1}{2^i}\}_{i\in\nats}$, it follows that, in the exponents above, $(\Delta_j-\Delta_{j+1})=\Delta_j(1-\nicefrac{1}{2})=\Delta_j(\nicefrac{1}{2})$, that $(\Delta_{j}-\Delta_{j+1}+\Delta_{j+2})=\Delta_j(1 - \nicefrac{1}{2} + \nicefrac{1}{4}) = \Delta_j(\nicefrac{1}{2} + \nicefrac{1}{4})$, and that $(\Delta_{j+1} - \Delta_{j+2})=\Delta_j(\nicefrac{1}{2} - \nicefrac{1}{4})=\Delta_j(\nicefrac{1}{4})$. Hence, the sequence can be written as
%\begin{equation*}
%e^{\Delta_j\left(Q_1(1) + Q_2(0)\right)}, ~~e^{\Delta_j\left(Q_1(\nicefrac{1}{2}) + Q_2(\nicefrac{1}{2})\right)}, ~~e^{\Delta_j(Q_1\left(\nicefrac{1}{2} + \nicefrac{1}{4}) + Q_2(\nicefrac{1}{4})\right)}, \ldots
%\end{equation*}
By focusing only on the subsequence $\presuper{j}T_0^{\Delta_j},\presuper{j+2}T_0^{\Delta_j},\presuper{j+4}T_0^{\Delta_j},\ldots$ containing the odd elements (note that $j$ is itself odd), it can rather easily---but not concisely---be shown that because $\{\Delta_i\}_{i\in\nats}=\{\nicefrac{1}{2^i}\}_{i\in\nats}$, it holds for all $k\in\natswith$ that
\begin{equation*}
\presuper{j+2k}T_0^{\Delta_j} = e^{\Delta_j\left(Q_1(1-s_k) + Q_2s_k\right)}\,,
\end{equation*}
with $s_k\coloneqq \sum_{\ell=0}^k \nicefrac{1}{4^\ell} - 1$. Clearly, the limiting behavior of the sequence is therefore governed by the limiting behavior of the geometric power series $s_k$, and since $\lim_{k\to\infty}s_k=\nicefrac{1}{3}$, we find
\begin{equation*}
\presuper{*}T_0^{\Delta_j} = \lim_{i\to\infty} \presuper{i}T_0^{\Delta_j} = \lim_{k\to\infty} \presuper{j+2k}T_{0}^{\Delta_j} = \lim_{k\to\infty} e^{\Delta_j\left(Q_1(1-s_k) + Q_2s_k\right)} = e^{\Delta_j(Q_1(\nicefrac{2}{3}) + Q_2(\nicefrac{1}{3}))}\,,
\end{equation*}
where the first equality follows from the proof of Proposition~\ref{lemma:restricted_trans_mat_system_cauchy_converges}. Using a completely analogous argument, if $j$ was taken to be even instead of odd, then instead $Q_2$ will dominate the limiting behavior, and $\presuper{*}T_0^{\Delta_j}=e^{\Delta_j(Q_1(\nicefrac{1}{3}) + Q_2(\nicefrac{2}{3}))}$. Note that the mixing proportion between $Q_1$ and $Q_2$ is then reversed. We will use these observations in other examples later in this work.
\exampleend
\end{exmp}

\section{Continuous-Time Stochastic Processes}\label{sec:stochastic_processes}

We will in this section formalize the notion of continuous-time stochastic processes, using the framework of full conditional probabilities. This approach differs somewhat from the traditional one, which is motivated below.

In a classical---Kolmogorovian, measure-theoretic---setting, such a stochastic process is typically defined as a collection of random variables $\{X_t\}_{t\in\realsnonneg}$ taking values in the space $\states$. These random variables $X_t$ are then defined with respect to some probability space $(\Omega,\mathcal{F},P)$, with $P$ a probability measure on a measurable set of events $\mathcal{F}$, typically a $\sigma$-algebra on an outcome space $\Omega$. Under certain independence assumptions, this leads to the special case of continuous-time Markov chains. We refer to {\bf CITE CITE CITE} for examples of this approach.

There are several reasons why we deviate from this approach here. First, as far as we can tell, very little work has been done on considering general stochastic processes \emph{without} the Markov assumption. As such, although straightforward, we have to formalize this notion here anyway, regardless of the framework used. Second, we will simplify considerably our analysis by focusing only on algebras that are closed under finitely many operations. Hence, we do not require special considerations for $P$ to be $\sigma$-additive on our domain of interest. On the other hand, we would like for these results to also extend to larger domains, specifically to $\sigma$-algebras. 

Finally, and most importantly, most authors appear to simply assume the existence of such a probability space $(\Omega,\mathcal{F},P)$, and others even leave it implicit without making mention of it. While valid---because under some weak assumptions such a space can always be shown to exist---it does pose some problems for our present work. Specifically, we require for some existence results a rather exact characterization of stochastic processes and the structure of the space on which they are defined. Full conditional probabilities provide a convenient framework for working around these issues and providing this characterization.

We will therefore in Section~\ref{sec:cond_prob} start by introducing full conditional probabilities, and the notion of \emph{coherence}. In Section~\ref{sec:def_stochastic_processes}, we then formalize continuous-time stochastic processes. Next, in Section~\ref{sec:well_behaved}, we describe a specific subclass of stochastic processes, which we call well-behaved, and on which we will largely focus throughout this work. Finally, Section~\ref{sec:dynamics} provides some tools with which we can describe the dynamics of stochastic processes.

\subsection{Full Conditional Probabilities}\label{sec:cond_prob}

%We will in Section~\ref{sec:def_stochastic_processes} define continuous-time stochastic processes using the framework of full conditional probabilities {\bf CITE}, which we describe here. 

Consider some arbitrary random experiment, with some (non-empty, possibly infinite) \emph{outcome space} $\Omega$. An element $\omega\in\Omega$ of this set is then interpreted as one possible outcome of the random experiment. In order to describe multiple possible outcomes simultaneously, we use the notion of \emph{events}. Such an event $E$ is a subset of $\Omega$. We use $\power$ to denote the set of all events, and let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. An element $(A,C)\in\power\times\nonemptypower$ is called \emph{the event $A$ conditional on the event $C$}, and the set $\power\times\nonemptypower$ is called the set of all \emph{conditional} events. 

A full conditional probability $P$ is then a map from the set $\power\times\nonemptypower$ to $\reals$ that quantifies the uncertainty of the outcomes of the random experiment, as follows.

\begin{definition}[Full conditional probability]\label{def:cond_prob}
A full conditional probability $P$ is a real-valued map from $\power\times\nonemptypower$ to $\reals$ that satisfies the following axioms. For all $A,B\in\power$ and all \mbox{$C,D\in\nonemptypower$}:
\vspace{5pt}

\begin{enumerate}[label=F\arabic*:,ref=F\arabic*]
\item
$P(A\vert C)\geq 0$;\label{def:coh_prob_2}
\item
$P(A\vert C)=1$ if $C\subseteq A$;\label{def:coh_prob_1}
\item
$P(A\cup B\vert C)=P(A\vert C)+P(B\vert C)$ if $A\cap B=\emptyset$;\label{def:coh_prob_3}
\item
$P(A\cap D\vert C)=P(A\vert D\cap C)P(D\vert C)$ if $D\cap C\neq\emptyset$.\label{def:coh_prob_6}
\end{enumerate}
\vspace{5pt}

\noindent
For any $A\in\power$ and $C\in\nonemptypower$, we call $P(A\vert C)$ the probability of $A$ conditional on $C$. Also, for any $A\in\power$, we use the shorthand notation $P(A)\coloneqq P(A\vert\paths)$ and then call $P(A)$ the probability of $A$.
The following additional properties can easily be shown to follow from \ref{def:coh_prob_2}--\ref{def:coh_prob_3}; see Appendix~\ref{app:stoch_proc} for a proof. For all $A\in\power$ and all $C\in\nonemptypower$:
\vspace{5pt}
\begin{enumerate}[label=F\arabic*:,ref=F\arabic*]
\setcounter{enumi}{4}
\item
$0\leq P(A\vert C)\leq 1$;\label{def:coh_prob_2b}
\item
$P(A\vert C)=P(A\cap C\vert C)$;\label{def:coh_prob_7}
\item
$P(\emptyset\vert C)=0$;\label{def:coh_prob_8}
\item
$P(\Omega\vert C)=1$.\label{def:coh_prob_5}
% \item
% $P(A\vert C)=P(A\vert D)P(D\vert C)$ if $A\subseteq D\subseteq C$.\label{def:coh_prob_4}
\end{enumerate}
\vspace{5pt}
\end{definition}

Note first of all that this definition takes conditional probabilities as the basic entity; $P(A\vert C)$ is well-defined even if $P(C)=0$. Furthermore, such a map $P$ is defined on the entire set $\power\times\nonemptypower$, and in particular not on a subset thereof that forms a (``conditional'') $\sigma$-algebra. Also, by property~\ref{def:coh_prob_3}, it only requires that the probability is finitely-additive; similar to the above, $\sigma$-additivity is not imposed by the axiomatization, regardless of the size of $\power$.  We refer to {\bf CITE CITE} for philosophical motivations behind this.

However, note that because $\power$ is the set of all events---the power set of $\Omega$---it is somewhat unwieldy to work with for arbitrary $\Omega$. Specifically, if for a given map $P$ from $\power\times\nonemptypower$ to $\reals$ we want to check if $P$ is a full conditional probability, this is difficult to do from the definition alone.

Therefore, we next introduce the notion of \emph{coherent conditional probabilities}~{\bf CITE}. %This notion was developed from DeFinetti's work on coherent (unconditional) probabilities~{\bf CITE}. We refer to {\bf CITE CITE CITE} for philosophical discussions on its merits, but will give some interpretation below.
%The following notion of coherent conditional probabilities will be useful whenever we want to check if a given map $P$ from a set of events to $\reals$ is a well-defined conditional probability.
This notion of coherence is perhaps easiest to understand in a gambling framework. In this framework, we first consider a set $\mathcal{C}\subseteq\power\times\nonemptypower$ of possible events. Gambles---lottery tickets, if you will---are then bought and sold prior to observing the outcome $\omega$ of a random experiment. 

In particular, for each event $(A,C)\in\mathcal{C}$, a bettor will specify a price $P(A\vert C)$ for which she is always willing to either buy or sell the gamble that event $A$ will come true, given that event $C$ comes true. Such a gamble yields a reward of one currency unit to its holder---paid by the seller---if the event $A$ occurs. These gambles are then bought and sold under the proviso that, should event $C$ not happen, then all parties involved are refunded. 

Suppose now that the outcome is $\omega$. For each ticket that the bettor sold, she has received $P(A\vert C)$ currency units in advance, but she must now pay one currency unit to the buyer if event $A$ came true. Hence, she must pay $\ind{A}(\omega)$ per ticket sold. Because this is conditional on event $C$ coming true, her remaining profit after paying is $\ind{C}(\omega)(P(A\vert C) - \ind{A}(\omega))$, with negative profit being loss. Note that if $\ind{C}(\omega)=0$, she neither gains nor loses anything. Allowing for fractional tickets, if she has sold a total of $\lambda\in\realsnonneg$ such tickets, her total profit is $\lambda \ind{C}(\omega)(P(A\vert C) - \ind{A}(\omega))$.

Similarly, for each ticket that she bought, she will receive one unit of currency if event $A$ came true. Her profit is then $(\ind{A}(\omega) - P(A\vert C))$ per ticket. However, she only receives this profit if event $C$ also came to pass, and otherwise gets refunded. Hence, her actual profit is $\ind{C}(\omega)(\ind{A}(\omega) - P(A\vert C))$. Comparing to the above, it follows that for any given outcome $\omega$, her resulting profit from buying is equal to her loss from selling, and vice versa.

Now, note that the bettor quoted prices for which she is \emph{always} willing to buy or sell these gambles. Therefore, the bettor can be forced---by an adversarial gambler---to buy and/or sell any number of gambles in any combination. That is, for any $n\in\nats$ and any $i\in\{1,\ldots,n\}$, she can be forced to sell him $\lambda_i\in\reals$ gambles on any event $(A_i,C_i)\in\mathcal{C}$ (which is equivalent to buying these gambles from him, if $\lambda_i$ is negative). After being forced into this combined gamble, her (possibly negative) profit on the outcome $\omega$ is
\begin{equation*}
\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\left(P(A_i\vert C_i) - \ind{A_i}(\omega)\right)\,.
\end{equation*}
%but \emph{only} if at least one of the conditioning events $C_i$ came true, i.e., if $\omega\in \cup_{i=1}^n C_i$. In particular, if $\omega\notin \cup_{i=1}^n C_i$, the bettor will get refunded for all tickets involved, and hence would not make a loss.
The prices $P$ are now said to be \emph{coherent} if they \emph{avoid sure loss}. That is, if for any such combination of gambles, there exists a ``non-trivial'' outcome $\omega$ where she does not make a loss. The trivial case is when none of the events $C_i$ come true, i.e., if $\omega\notin\cup_{i=1}^n C_i$, because she then gets refunded on all the gambles. However, to be coherent, the prices specifically have to avoid sure loss if (some) of the conditioning events do come true; there must always be an $\omega\in\cup_{i=1}^n C_i$ that yields non-negative profit.

If the prices $P$ satisfy this criteria, they are called a coherent conditional probability.

\begin{definition}[Coherent conditional probability]\label{def:coherence}\footnote{*** Explain that most authors consider a supremum. However, since $n$ is finite and because, for every $i\in\{1,\dots,n\}$, $\ind{A_i}$ and $\ind{C_i}$ can only take two values---$0$ or $1$---it follows that this supremum is taken over a finite set of real numbers, which implies that it is actually a maximum. Also explain that some authors also require this with $\min$ and $\leq0$; this is not necessary because it is implied. ***}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is said to be a \emph{coherent conditional probability} on $\mathcal{C}$ if, for all $n\in\mathbb{N}$ and every choice of $(A_i,C_i)\in\mathcal{C}$ and $\lambda_i\in\reals$, $i\in\{1,\dots,n\}$,
\begin{equation*}
\max\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(P(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation*}
with $C_0\coloneqq\cup_{i=1}^nC_i$.
\end{definition}

The reason that these prices $P$ are called \emph{probabilities} is due to the following well-known result from coherent probability theory; the bettor's prices $P$ are coherent if and only if they satisfy the axioms of probability, i.e., properties~\ref{def:coh_prob_2}-\ref{def:coh_prob_6} above.

\begin{theorem}{\cite[Theorem 3]{regazzini1985finitely}}\label{theo:fullcoherent}
Let $P$ be a real-valued map from $\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it is a full conditional probability.
\end{theorem}
Note that the result above assumes that the bettor quoted prices for the entire domain $\power\times\nonemptypower$. Contrariwise, our explanation started by considering a subset $\mathcal{C}$ of this domain. The reason for this is the following well-known result: if the prices $P$ are coherent on some subset $\mathcal{C}$ of $\power\times\nonemptypower$, it is always possible to extend the domain of these prices to some superset $\mathcal{C}^*\supseteq \mathcal{C}$, while maintaining coherence.

\begin{theorem}{\cite[Theorem 4]{regazzini1985finitely}}\label{theo:largerdomain}
Let $P$ be a coherent conditional probability on $\mathcal{C}\subseteq\power\times\nonemptypower$. Then for any $\mathcal{C}\subseteq\mathcal{C}^*\subseteq\power\times\nonemptypower$, $P$ can be extended to a coherent conditional probability on $\mathcal{C}^*$.
\end{theorem}
In particular, it is therefore always possible to extend a coherent conditional probability $P$ on $\mathcal{C}$, to a coherent conditional probability on $\power\times\nonemptypower$. Due to Theorem~\ref{theo:fullcoherent}, this extension is a full conditional probability. The following makes this explicit.

\begin{corollary}\label{corol:coherentextendable}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it can be extended to a full conditional probability.
\end{corollary}
Note, therefore, that if $P$ is a coherent conditional probability on $\mathcal{C}$, we can equivalently say that it is the restriction of a full conditional probability. One only needs to check Definition~\ref{def:cond_prob} to see that $P$ will therefore satisfy properties~\ref{def:coh_prob_2}-\ref{def:coh_prob_6}.

\subsection{Stochastic Processes}\label{sec:def_stochastic_processes}

We are now ready to define continuous-time stochastic processes. Such a stochastic process is a construct that moves through the state space $\states$ over a continuous time line $\realsnonneg$. A specific realization of this behavior will be called a path or a trajectory. The stochasticity of the process is quantified using a full conditional probability on a set of events, which describe specific (combinations of) states in which the process can be at certain time points. These ideas are formalized as follows.

%*** Section 1.1 of ``Continuous-Time Markov Chains: An Applications-Oriented Approach'' has made me doubt (but only slightly) whether using cadlag paths is always possible. I still think they are, but we should check. If not, then we can use the set of all possible functions from $\realsnonneg$ to $\states$, but that would make the whole unique extension to $\sigma$-algebra story less elegant. (see page 15 and onwards in that book as well!!!) ***

A \emph{path} $\omega$ is a function from $\realsnonneg$ to $\states$, and we denote with $\omega(t)$ the value of $\omega$ at time $t$. For any sequence of time points $u\in\mathcal{U}$ and any path $\omega$, we will write $\omega\vert_{u}$ to denote the restriction of $\omega$ to $u\subset\realsnonneg$. Using this notation, we write for any $x_u\in\states^u$ that $\omega\vert_u=x_u$ if, for all $t\in u$, it holds that $\omega(t)=x_{t}$, with $x_t\in x_u$.

Let $\paths$ be any set of paths such that
\begin{equation}\label{eq:path_exists_for_finite_points}
(\forall u\in\mathcal{U}_\emptyset)(\forall x_u\in\states^u)(\exists \omega\in\Omega)\,:\,\omega\vert_u=x_u\,.
\end{equation}
Thus, $\Omega$ must be at least such that, for any non-empty finite sequence of time points $u\in\mathcal{U}_\emptyset$ and any state assignment $x_u\in\states^u$ on those time points, there is a path $\omega\in\Omega$ that agrees with $x_u$. This set $\Omega$ will form the outcome space for a full conditional probability. Essentially, Equation~\eqref{eq:path_exists_for_finite_points} guarantees that $\Omega$ is ``large enough to be interesting''\footnote{Note that other authors typically define $\Omega$ more specifically, for example as the set of \emph{all} functions from $\realsnonneg$ to $\states$ {\bf CITE}. Other definitions could be to let $\Omega$ be the set of all (right-)continuous paths, or the set of all right-continuous paths with left-sided limits (paths that are \emph{cadlag} functions) {\bf CITE}. Observe that all these choices for $\Omega$ will satisfy Equation~\eqref{eq:path_exists_for_finite_points}, and would therefore be valid in our setting. However, we do not specifically require any of these additional properties in this paper. Hence, we have chosen to omit them from our definition.}.

An \emph{event} is a subset $E$ of $\paths$. We again denote the set of all events by $\power$ and we let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. For any set of events $\mathcal{E}\subseteq\power$, we use $\langle\mathcal{E}\rangle$ to denote the algebra that is generated by them. That is, $\langle\mathcal{E}\rangle$ is the smallest subset of $\power$ that contains all elements of $\mathcal{E}$, and that is furthermore closed under complements in $\Omega$ and finite unions, and therefore also under finite intersections. 

A stochastic process will be defined below as the restriction of a full conditional probability to a specific subset of the set $\power\times\nonemptypower$ of all (conditional) events, as follows. For any $t\in\realsnonneg$ and $x\in\states$, we define the elementary event
\begin{equation*}
(X_t=x)\coloneqq\{\omega\in\paths\colon\omega(t)=x\}.
\end{equation*}
%We use $\events^{\mathrm{f}}$ to denote the set of all these elementary events. 
%Similarly, for any finite ordered sequence of time points $u=t_1,\ldots,t_n$ with $n\in\nats_0$ and $t_i\in\realsnonneg,i\in\{1,\ldots,n\}$, and any state assignment $x_u\in\states^u$, we define the event
%\begin{align*}
%(X_u=x_u)\coloneqq\left(X_{t_1}=x_{t_1}, \dots, X_{t_n}=x_{t_n}\right)
%\coloneqq&
%\bigcap_{i\in\{0,\dots,n\}}(X_{t_i}=x_{t_i})\\
%=&
%\left\{\omega\in\paths\colon(\forall i\in\{1,\dots,n\}\,:~\omega(t_i)=x_{t_i})\right\}.%,
%\end{align*}
%which we will sometimes also denote by $(X_{t_i}=x_{t_i}, i\in\{0,\dots,n\})$.
%We use $\events^{\mathrm{s}}$ to denote the set of all these events.
%Consider now any $t\in\realsnonneg$. We then let $\mathcal{A}_{>t}$ be the algebra that is generated by all the elementary events $(X_s=x)$, for all $s> t$ and $x\in\states$,\footnote{This is the smallest subset of $\power$ that contains all these elementary events and that is furthermore closed under complements, finite unions and hence also finite intersections.} and we let $\mathcal{F}_{\leq t}$ be the set of all events $\left(X_{t_1}=x_{t_1}, \dots, X_{t_n}=x_{t_n}\right)$, for all finite sequences $u\leq t$ and all $x_u\in\states^u$.%, and we define $\filter\coloneqq\mathcal{C}_{\leq t}\setminus\{\emptyset\}$.
%
%For a given $t\in\realsnonneg$, we will refer to $\mathcal{A}_{> t}$ as the set of possible future events, and to $\mathcal{F}_{\leq t}$ as the set of historic events, or possible histories.
%
%*** Changing the definition a bit ***
Then, for any $u\in\mathcal{U}$, we let
\begin{equation*}
\mathcal{E}_u \coloneqq \left\{
(X_t=x)
\colon
x\in\states,t\in u\cup\reals_{>u}
\right\}
\end{equation*}
be the set of elementary events whose time point is either preceded by or belongs to $u$. Let now $\mathcal{A}_u\coloneqq\langle\mathcal{E}_u\rangle$ be the algebra that is generated by this set of elementary events. Then clearly, for any $A\in\mathcal{A}_u$, it holds that $A\in\power$.

Furthermore, due to Equation~\eqref{eq:path_exists_for_finite_points}, for any $u\in\mathcal{U}$ and any $x_u\in\states^u$, the event
\begin{equation*}
(X_u=x_u) \coloneqq \{\omega\in\Omega\,:\,\omega\vert_u=x_u\}
\end{equation*}
is non-empty, and therefore satisfies $(X_u=x_u)\in\nonemptypower$. Observe that if $u=\emptyset$, then $\omega\vert_u=x_u$ is vacuously true, and hence in that case $(X_u=x_u)=\Omega$.

Hence, for any $u\in\mathcal{U}$, any $x_u\in\states^u$, and any $A\in\mathcal{A}_u$, we find that the conditional event $(A,X_u=x_u)\coloneqq (A,(X_u=x_u))$ will satisfy $(A,X_u=x_u)\in\power\times\nonemptypower$.

\begin{definition}[Stochastic Process]\label{def:stoch_process}
A \emph{stochastic process} is the restriction of a full conditional probability to
\begin{equation*}
\mathcal{C}^\mathrm{SP}\coloneqq\big\{
(A,X_u=x_u)
\colon
u\in\mathcal{U},~x_u\in\states^u,~A\in\mathcal{A}_u\big\}\subset\power\times\nonemptypower.
\end{equation*}
We denote the set of all such stochastic processes by $\processes$.
\end{definition}

\begin{corollary}\label{corol:processiffcoherent}
Let $P$ be a real-valued map from $\mathcal{C}^\mathrm{SP}$ to $\reals$. Then $P$ is a stochastic process if and only if it is a coherent conditional probability.
\end{corollary}



%Because Definition~\ref{def:cond_prob} only covered finitely additive full conditional probability measures, this also restricts our definition of stochastic processes to finitely additive stochastic processes. The advantage is that this simplifies considerably the subsequent analysis, and, as shown below, the extension to a $\sigma$-additive stochastic process can always be made when this is required.
%
%*** explain that $\sigma$-additivity is not required ***
%
%*** discuss that a unique $\sigma$-additive extension always exists, but that for our purposes, it is not necessary to consider it ***
%
%\begin{proposition}{\cite[Theorem 2]{berti2002coherent}} 
%Let $P\in\processes$ be a stochastic process. Then, there exists an extension $P^*$ of $P$ to the set
%\begin{equation*}
%\mathcal{C}^{\mathrm{SP}*} \coloneqq \left\{(A,X_u=x_u)\colon
%u\in\mathcal{U},~x_u\in\states^u,~A\in\sigma(\mathcal{A}_u)
%\right\}\subset\power\times\nonemptypower\,,
%\end{equation*}
%where $\sigma(\mathcal{A}_u)$ is the $\sigma$-algebra generated by $\mathcal{A}_u$, such that $P^*$ is a coherent conditional probability that is $\sigma$-additive on $\mathcal{C}^{\mathrm{SP}*}$.
%\end{proposition}
%\begin{proof}
%*** {\bf TODO}, but follows rather straightforwardly from \cite[Theorem 2]{berti2002coherent} and Lemma~\ref{lem:stoch_process_sigma_add_on_algebra}. Definitely works for a fixed $t$, maybe show that it also works for the set generated by all $t\in\realsnonneg$.
%\end{proof}

\subsection{Well-behaved stochastic processes}\label{sec:well_behaved}

Stochastic processes, as they are defined in the previous section, can behave in rather extreme ways. For example, the transition probabilities can jump instantaneously. While we do not want to impose any differentiability assumptions on these processes, we do want this rate of change to be bounded. We call a stochastic process for which this holds \emph{well-behaved}, as follows.

%*** explain that stochastic process can behave in rather extreme ways. For example, the transition probabilities can jump instantaneously. In order to avoid this behavior, we restrict our attention to a subclass of stochastic processes, which we call well-behaved. ***

\begin{definition}[Well-Behaved Stochastic Process]
\label{def:well-behaved}
A stochastic process $P\in\processes$ is said to be \emph{well-behaved} if, for any (possibly empty) time sequence $u\in\mathcal{U}$, any $x_u\in\states^u$ and any $x,y\in\states$:
\begin{equation}\label{eq:def:well-behaved:right}
(\forall t\in\reals_{\geq0}
\colon t> u)~~
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\abs{P(X_{t+\Delta}=y\vert X_t=x, X_u=x_u)-\ind{x}(y)}<+\infty
\end{equation}
and
\begin{equation}\label{eq:def:well-behaved:left}
(\forall t\in\reals_{>0}
\colon t> u)~~
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\abs{P(X_{t}=y\vert X_{t-\Delta}=x, X_u=x_u)-\ind{x}(y)}<+\infty.
\end{equation}
The set of all well-behaved stochastic processes is denoted by $\wprocesses$.
\end{definition}

This definition of well-behavedness is related to continuity and differentiability, but stronger than the former and weaker than the latter. Example~\ref{exmp:well-behaved} below, and Example~\ref{exmp:well-behaved-no-deriv} in Section~\ref{sec:dynamics}, provide some intuition on this.

\begin{exmp}\label{exmp:well-behaved}
Suppose that a stochastic process $P\in\processes$ is such that, at some time $t\in\realsnonneg$ and for some history $x_u\in\states^u$, $u<t$, for some $x\in\states$ it satisfies for all $\Delta\in[0,1]$,
\begin{equation*}
P(X_{t+\Delta}=x\,\vert\,X_{t}=x,X_u=x_u) = 1-\sqrt{\Delta}\,.
\end{equation*}
This is a probability that is continuous in $\Delta$, and $P(X_{t}=x\,\vert\,X_{t}=x,X_u=x_u) = 1$. However, we clearly have that
\begin{equation*}
\frac{1}{\Delta}\abs{P(X_{t+\Delta}=x\,\vert\,X_{t}=x,X_u=x_u) - \ind{x}(x)} \to +\infty\,,\quad\text{as}\quad \Delta\to0^+\,,
\end{equation*}
and hence $P$ is not well-behaved.
\exampleend
\end{exmp}

\subsection{Process Dynamics}\label{sec:dynamics}

We will now introduce some tools to describe the behavior of stochastic processes. Rather than work with the individual probabilities, it will be convenient to jointly consider probabilities that are related by the same conditioning event. To this end, we will next introduce the notion of transition matrices corresponding to a given stochastic process.

\begin{definition}[Corresponding Transition Matrix]\label{def:trans_matrix}
Consider any stochastic process $P\in\processes$. Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, the \emph{corresponding transition matrix} $T_t^s$ is a matrix that is defined by
\begin{equation*}
T_t^s(x_t, x_s) \coloneqq P(X_s=x_s\,\vert X_t=x_t)\quad\text{for all $x_s,x_t\in\states$}\,.
\end{equation*}
We denote this family of matrices by $\mathcal{T}_P$.%, and call it the \emph{system of transition matrices} that corresponds to $P$.
\end{definition}

Because we will also want to work with conditioning events that contain more than a single time point, we furthermore introduce the following generalization.

\begin{definition}[History-Dependent Corresponding Transition Matrix]
Let $P\in\processes$ be any stochastic process. Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, any sequence of time points $u\in\mathcal{U}_{<t}$, and any state assignment $x_u\in\states^u$, the corresponding \emph{history-dependent} transition matrix $T_{t,\,x_u}^s$ is a matrix that is defined by
\begin{equation*}
T^s_{t,\,x_u}(x_t,x_s)
\coloneqq
P(X_s=x_s\vert X_t=x_t, X_u=x_u)\quad\text{for all $x_s,x_t\in\states$}\,.
\end{equation*}
For notational convenience, we allow $u$ to be empty, in which case $T_{t,\,x_u}^s=T_t^s$.
\end{definition}

The following proposition establishes some simple properties of these corresponding (history-dependent) transition matrices.

\begin{proposition}\label{prop:stochasticprocess:simpleproperties}
Let $P\in\processes$ be a stochastic process.  %Then for any $t,s\in\realsnonneg$ such that $t\leq s$, $T_t^s$ is a transition matrix and $T_t^t=I$ and, if $P$ is well-behaved, then also
%\begin{equation}\label{eq:wellbehavedtransitionmatrix}
%\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_t^{t+\Delta}-I}<+\infty
%\text{~~~~and~~~~}
%\lim_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta}^t-I}<+\infty.
%\end{equation}
%Similarly, 
Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, any sequence of time points $u\in\mathcal{U}_{<t}$, and any state assignment $x_u\in\states^u$, the corresponding (history dependent) transition matrix $T_{t,\,x_u}^s$ is---as its name suggests---a transition matrix, and $T_{t,\,x_u}^t=I$. Furthermore, $P$ is well-behaved if and only if
\begin{equation}\label{eq:wellbehavedtransitionmatrix}%\label{eq:wellbehavedhistorictransitionmatrix}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t,x_u}^{t+\Delta}-I}<+\infty\,,
\text{~~~~and,~~~~}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta,x_u}^t-I}<+\infty\,,
\end{equation}
for all $u\in\mathcal{U}$, $x_u\in\states^u$ and $t\in\reals_{\geq0}$ such that $t>u$.
\end{proposition}

\begin{remark}\label{remark:expectationT}
Note that for any $P\in\processes$, a corresponding transition matrix $T_{t, x_u}^s$ is a map from $\gamblesX$ to $\gamblesX$, that can therefore be applied to any $f\in\gamblesX$. Observe that for all $x_t\in\states$, we have that
%\begin{align*}
%\left[T_t^sf\right](x_t) &= \sum_{x_s\in\states}f(x_s)P(X_s=x_s\,\vert\,X_t=x_t)
%= \mathbb{E}\left[f(X_s)\,\vert\,X_t=x_t\right]\,,
%\end{align*}
\begin{align*}
\left[T_{t,x_u}^sf\right](x_t) &= \sum_{x_s\in\states}f(x_s)P(X_s=x_s\,\vert\,X_t=x_t,X_u=x_u)= \mathbb{E}\left[f(X_s)\,\vert\,X_t=x_t, X_u=x_u\right]\,,
\end{align*}
where the expectation is taken with respect to $P$. This result is useful when we later focus on expectations with respect to stochastic processes; for functions $f\in\gamblesX$, their corresponding transition matrices serve as an alternative representation for the expectation operator.  %Similarly, for any history-dependent transition matrix $T_{t,x_u}^s$, we have that
\exampleend
\end{remark}

Because a stochastic process is defined on a continuous time line, and because its corresponding transition matrices $T_{t,x_u}^s$ only describe the behavior of this process on a fixed point in time, we will furthermore require some tools to capture the dynamics of a given process. That is, we will be interested in how these transition matrices change over time.

One seemingly obvious way to describe these dynamics is to use the derivatives of a process' corresponding transition matrices. However, because we do not impose differentiability assumptions on these processes, such derivatives need not exist. We will therefore instead introduce \emph{outer partial derivatives} below. It will be instructive, however, to first consider ordinary \emph{directional partial derivatives}.

%Unfortunately, for general stochastic processes, these derivatives do not necessarily exist. In fact, also the slightly weaker directional derivatives---which we define below---are not guaranteed to exist. Worse still, they are not even guaranteed to exist for well-behaved processes.

\begin{definition}[Directional Partial Derivatives]\label{def:direc_partial_deriv}
For any stochastic process $P\in\processes$, any $t\in\realsnonneg$, any sequence of time points $u\in\mathcal{U}_{<t}$, and any state assignment $x_u\in\states^u$, the \emph{right-} and \emph{left-sided partial derivatives} of $T_{t,x_u}^t$ are defined, respectively, as
\begin{equation*}
\partial_{+}{T_{t,\,x_u}^t}
\coloneqq
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-T^t_{t,\,x_u})
=
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)
\end{equation*}

\begin{equation*}
\partial_{-}{T_{t,\,x_u}^t}
\coloneqq
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-T^t_{t,\,x_u})
=
\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-I)
\end{equation*}
\noindent If these partial derivatives exist, then because of Proposition~\ref{prop:rate_from_stochastic_matrix}, they are guaranteed to belong to the set of rate matrices  $\mathcal{R}$. If they both exist and coincide, we write $\partial{T_{t,\,x_u}^t}$ to denote their common value.
\end{definition}

The following example established that these directional partial derivatives need not exist. In particular, they need not exist even for well-behaved processes.
\begin{exmp}\label{exmp:well-behaved-no-deriv}
Suppose that, for some well-behaved stochastic process $P\in\wprocesses$, for some time $t$ and for some history $x_u$ such that $u<t$, there exist two different sequences $\{\Delta_i\}_{i\in\nats}$ and $\{\Delta_i'\}_{i\in\nats}$ such that $\{\Delta_i\}_{i\in\nats}\to0^+$ and $\{\Delta_i'\}_{i\in\nats}\to0^+$, for which
\begin{equation*}
\lim_{i\to\infty}~~\frac{1}{\Delta_i}\left(T_{t,x_u}^{t+\Delta_i} - I\right) = Q_1\,,~~\text{and,}~~ \lim_{i\to\infty}~~ \frac{1}{\Delta_i'}\left(T_{t,x_u}^{t+\Delta_i'} - I\right) = Q_2\,,
\end{equation*}
with $Q_1,Q_2\in\mathcal{R}$ such that $Q_1\neq Q_2$. Then, clearly, the directional partial derivative $\partial_{+}{T_{t,\,x_u}^t}=\lim_{\Delta\to 0^{+}}
\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)$ does not exist. 

However, $\lim_{i\to\infty}\nicefrac{1}{\Delta_i}\lVert T_{t,x_u}^{t+\Delta_i} - I\rVert = \norm{Q_1}$ and $\lim_{i\to\infty}\nicefrac{1}{\Delta_i'}\lVert T_{t,x_u}^{t+\Delta_i'} - I\rVert = \norm{Q_2}$, and because $\norm{Q_1},\norm{Q_2}<+\infty$, this is not problematic for the well-behavedness of $P$.

Of course, one might wonder whether it is possible for such a $P\in\wprocesses$ to exist in the first place. Example~\ref{exmp:markov_with_no_deriv} in Section~\ref{sec:nonhomogen_markov} will show that this is indeed the case.
%**** verschillende $\{\Delta_i\}_{i\in\nats}\to0^+$ geven verschillende $Q$.
%***** Meh, werkt ook niet echt. Wil eigenlijk accumulatie punt van exponentiele, maar hebben we hier nogniet geintroduceerd.
%
%Consider a sequence $\{\Delta_i\}_{i\in\nats}$ in $\realspos$ such that $\Delta_i\coloneqq \nicefrac{1}{2^i}$, for all $i\in\nats$. Consider a well-behaved stochastic process $P\in\wmprocesses$ at some time $t\in\realsnonneg$ and for some history $x_u\in\states^u$, $u<t$. Suppose that the corresponding transition matrix $T_{t,x_u}^{t+\Delta}$ satisfies for all $\Delta\in\realspos$, that if $\Delta\in[\Delta_{i+1},\Delta_i)$ for some $i\in\nats$, then
%\begin{equation*}
%\frac{1}{\Delta}(T_{t,x_u}^{t+\Delta} - I) = \left\{\begin{array}{cl}
%Q_1 & \quad \text{if $i$ is odd, and} \\
%Q_2 & \quad \text{otherwise,}
%\end{array}\right.
%\end{equation*}
%for some rate matrices $Q_1,Q_2\in\mathcal{R}$ with $Q_1\neq Q_2$. Clearly, the norm of this quantity is uniformly bounded for all $\Delta\in\realspos$, and hence this does not pose a problem for the well-behavedness of $P$. On the other hand, $\partial_{+}{T_{t,\,x_u}^t}$ clearly does not exist.
%
%Of course, although this construction does not interfere with well-behavedness, one might wonder whether it is possible for such a $P$ to exist in the first place. We will find that this is indeed possible when we consider Example~{\bf REF} in Section~{\bf REF}.
\exampleend
\end{exmp}

Observe, therefore, that the problem is essentially that the finite-difference expressions $\nicefrac{1}{\Delta}(T_{t,x_u}^{t+\Delta} - I)$ and $\nicefrac{1}{\Delta}(T_{t-\Delta,x_u}^{t} - I)$, parameterized in $\Delta$, can have multiple accumulation points as we take $\Delta$ to $0$. Therefore, it will be more convenient to instead work with \emph{outer partial derivatives} {\bf (REF)}. These can be seen as a kind of set-valued derivatives, containing all these accumulation points obtained as $\Delta\to0^+$.

\begin{definition}[Outer Partial Derivatives]\label{def:outerpartialderivatives}
For any stochastic process $P\in\processes$, any $t\in\realsnonneg$, any sequence of time points $u\in\mathcal{U}_{<t}$, and any state assignment $x_u\in\states^u$, the \emph{right-} and \emph{left-sided} \emph{outer partial derivatives} of $T_{t,x_u}^t$ are defined, respectively, as
\begin{align}
\overline{\partial}_{+}
{T^t_{t,\,x_u}}
&\coloneqq
\left\{
Q\in\mathcal{R}
\colon
\left(\exists \,\{\Delta_i\}_{i\in\nats}\to0^+\,:\,
~
%\lim_{i\to+\infty}\Delta_i=0
%\text{~~and~}
\lim_{i\to+\infty}
\frac{1}{\Delta_i}
(T^{t+\Delta_i}_{t,\,x_u}-I)
=Q
\right)
\right\}\label{eq:rightouterderivative}\\
\overline{\partial}_{-}
{T^t_{t,\,x_u}}
&\coloneqq
\left\{
Q\in\mathcal{R}
\colon
\left(\exists\, \{\Delta_i\}_{i\in\nats}\to0^+\,:\,
~
%\lim_{i\to+\infty}\Delta_i=0
%\text{~~and~}
\lim_{i\to+\infty}
\frac{1}{\Delta_i}
(T^{t}_{t-\Delta_i,\,x_u}-I)
=Q
\right)\label{eq:leftouterderivative}
\right\}
\end{align}
Furthermore, the \emph{outer partial derivative} of $T_{t,x_u}^t$ is defined as
\begin{equation*}
\overline{\partial}
{T^t_{t,\,x_u}}
\coloneqq
\overline{\partial}_{+}
{T^t_{t,\,x_u}}
\cup
\overline{\partial}_{-}
{T^t_{t,\,x_u}}\,.
\end{equation*}
\end{definition}

The next result shows that, at least for well-behaved processes $P\in\wprocesses$, these outer partial derivatives always exist; in particular, that they are non-empty and bounded.

\begin{proposition}\label{prop:boundednon-emptyandclosed}
For any $P\in\wprocesses$, $\overline{\partial}_{+}
{T^t_{t,\,x_u}}$, $\overline{\partial}_{-}
{T^t_{t,\,x_u}}$ and $\overline{\partial}
{T^t_{t,\,x_u}}$ are non-empty, bounded and closed subsets of $\mathcal{R}$.
\end{proposition}

\begin{exmp}
The process from Example~\ref{exmp:well-behaved-no-deriv} above will satisfy $\overline{\partial}_{+}
{T^t_{t,\,x_u}}\supseteq \{Q_1,Q_2\}$.
\exampleend
\end{exmp}

\noindent Furthermore, these right- and left-sided outer partial derivatives are clearly related to the directional partial derivatives. The next statement makes this explicit for well-behaved stochastic processes, using an $\epsilon-\delta$ expression for the limits in Definition~\ref{def:direc_partial_deriv}.

\begin{proposition}\label{prop:outerderivativebehaveslikelimit}
Consider any well-behaved stochastic process $P\in\wprocesses$. Then, for any $t\in\realsnonneg$, any sequence of time points $u\in\mathcal{U}_{<t}$, any state assignment $x_u\in\states^u$, and any $\epsilon>0$, there is some $\delta>0$ such that, for all $0<\Delta<\delta$:
\begin{equation}
\label{eq:outerderivativebehaveslikelimit1}
(\exists Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}<\epsilon
\end{equation}
and
\begin{equation}
\label{eq:outerderivativebehaveslikelimit2}
(\exists Q\in\overline{\partial}_{-}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t}_{t-\Delta,\,x_u}-I)-Q}<\epsilon
\end{equation}
\end{proposition}

We conclude this section by establishing that, for well-behaved processes, the outer partial derivatives are a proper generalization of the directional partial derivatives. In particular, if the latter exist, their values correspond exactly to the elements of the former.

\begin{corollary}\label{corol:outersingleton}
Consider any $P\in\wprocesses$. Then $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is a singleton if and only if $\smash{\partial_{+}
{T^t_{t,\,x_u}}}$ exists and, in that case, $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}=\{\partial_{+}
{T^t_{t,\,x_u}}\}$. Analogous results hold for $\smash{\overline{\partial}_{-}
{T^t_{t,\,x_u}}}$ and $\smash{\partial_{-}
{T^t_{t,\,x_u}}}$, and for $\smash{\overline{\partial}
{T^t_{t,\,x_u}}}$ and $\smash{\partial
{T^t_{t,\,x_u}}}$.
\end{corollary}


\section{Continuous-Time Markov Chains}\label{sec:cont_time_markov_chains}

Having introduced continuous-time stochastic processes in Section~\ref{sec:stochastic_processes}, we will in this section focus on a specific class of such processes: the \emph{continuous-time Markov chains}.

\begin{definition}[Markov Property, Markov Chain]\label{def:markov_property}
A stochastic process $P\in\processes$ satisfies the \emph{Markov property} if for any $t,s\in\realsnonneg$ such that $t\leq s$, any time sequence $u\in\mathcal{U}_{<t}$, any $x_u\in\states^u$, and any states $x,y\in\states$:
\begin{equation*}
P(X_s=y\,\vert\,X_t=x,X_u=x_u) = P(X_s=y\,\vert\, X_{t}=x)\,.
\end{equation*}
A stochastic process that satisfies this property is called a \emph{Markov chain}. We denote the set of all Markov chains by $\mprocesses$ and use $\wmprocesses$ to refer to the subset that only contains the well-behaved Markov chains.
\end{definition}

%\subsection{Transition Matrix Systems}\label{sec:trans_mat_systems}

We already know from Proposition~\ref{prop:stochasticprocess:simpleproperties} that the transition matrices of a stochastic process---and therefore also, in particular, of a Markov chain---satisfy a number of simple properties. For the specific case of a Markov chain $P\in\mprocesses$, the family of transition matrices $\mathcal{T}_P$ also satisfies an additional property. In particular, for any $t,r,s\in\realsnonneg$ such that $t\leq r\leq s$, these transition matrices satisfy
\begin{equation}\label{eq:markovintermsofmatrices}
T_t^s = T_t^rT_r^s\,.
\end{equation}
In this context, this property is known as the \emph{Chapman-Kolmogorov equation}~{\bf CITE}, or alternatively as the semi-group property~{\bf CITE}. Indeed, this is the same semi-group property that we defined in Section~\ref{sec:systems} to hold for transition matrix systems $\mathcal{T}$. We therefore have the following result.

\begin{proposition}\label{prop:Markovhassystem}
Consider a Markov chain $P\in\mprocesses$ and let $\mathcal{T}_P$ be the corresponding family of transition matrices. Then $\mathcal{T}_P$ is a transition matrix system. Furthermore, $\mathcal{T}_P$ is well-behaved if and only if $P$ is well-behaved.
\end{proposition}

Hence, at this point we know that every (well-behaved) Markov chain has a corresponding (well-behaved) transition matrix system. Our next result establishes that the converse is true as well: every (well-behaved) transition matrix system has a corresponding (well-behaved) Markov chain, and for a given initial distribution, this Markov chain is even unique.

\begin{theorem}\label{theo:uniqueMarkovchain}
Let $p$ be an arbitrary probability mass function on $\states$ and let $\mathcal{T}$ be a transition matrix system. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}$ and, for all $y\in\states$, $P(X_0=y)=p(y)$. Furthermore, if $\mathcal{T}$ is well-behaved, then $P$ is also well-behaved.
\end{theorem}

Hence, Markov chains---and well-behaved Markov chains in particular---are completely characterized by their transition matrices and their initial distribution. 

As a final note, observe that not only does the Markov property simplify the conditional probabilities of Markov chains, it also simplifies their dynamics. In particular, for any $t\in\realsnonneg$, any $u\in\mathcal{U}_{<t}$, and any $x_u\in\states^u$, it holds that $\smash{\overline{\partial}}T_{t,x_u}^t=\smash{\overline{\partial}}T_{t}^t$. We now focus on a number of special cases.

\subsection{Homogeneous Markov chains}\label{sec:homogen_markov_chain}

\begin{definition}[Homogeneous Markov chain]\label{def:homogeneousMarkov}
A Markov chain $P\in\mprocesses$ is called \emph{homogeneous} if its transition matrices $T_t^s$ do not depend on the absolute value of $t$ and $s$, but only on the time-difference $s-t$:
\begin{equation}\label{eq:homogeneousMarkov}
T_t^s=T_0^{s-t}
\text{~~for all $t,s\in\realsnonneg$ such that $t\leq s$.}
\end{equation}
We denote the set of all homogeneous Markov chains by $\hmprocesses$ and use $\whmprocesses$ to refer to the subset that consists of the well-behaved homogeneous Markov chains.
\end{definition}

Recall now from Section~\ref{sec:systems} the exponential transition matrix system $\mathcal{T}_Q$ corresponding to some $Q\in\mathcal{R}$. In particular, the transition matrices of such a system were defined by $T_t^s = e^{Q(s-t)}$. This family $\mathcal{T}_Q$ therefore clearly satisfies Equation~\ref{eq:homogeneousMarkov}. Furthermore, by Proposition~\ref{prop:systemQ}, $\mathcal{T}_Q$ is well-behaved. Hence, we have the following result.

\begin{corollary}\label{cor:rate_has_unique_homogen_markov_process}
Consider any rate matrix $Q\in\mathcal{R}$ and let $p$ be an arbitrary probability mass function on $\states$. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$ and, furthermore, this unique Markov chain is well-behaved and homogeneous.
\end{corollary}

%As our next result shows, the converse is true as well: not only does each $\mathcal{T}_Q$ characterize%: every well-behaved homogeneous Markov chain can be characterized---up to its initial distribution---by the family $\mathcal{T}_Q$, for some unique $Q\in\mathcal{R}$.
The next result strengthens this connection between well-behaved homogeneous Markov chains and exponential transition matrix systems.

\begin{theorem}\label{theo:homogeneoushasQ}
For any well-behaved homogeneous Markov chain $P\in\whmprocesses$, there is a unique rate matrix $Q\in\mathcal{R}$ such that $\mathcal{T}_P=\mathcal{T}_Q$.
\end{theorem}

Hence, any well-behaved homogeneous Markov chain $P\in\whmprocesses$ is completely characterized by its initial distribution and a rate matrix $Q\in\mathcal{R}$. We will denote this rate matrix by $Q_P$.

*** I think this also follows from Theorem II.4 and II.5 in Chung's book on Markov chains ***
**** maybe some notes that this result is known, and that in that sense our definition of stochastic processes overlaps/agrees with the normal characterizations in the literature. ****

The dynamic behavior of well-behaved homogeneous Markov chains is furthermore particularly easy to describe, as shown by the next result.
\begin{proposition}\label{prop:Q_is_singleton_deriv_for_homogen}
Consider any well-behaved homogeneous Markov chain $P\in\whmprocesses$, and let $Q_P\in\mathcal{R}$ be its corresponding rate matrix. Then, for all $t\in\realsnonneg$, it holds that the outer partial derivative satisfies $\smash{\overline{\partial}}T_t^t=\{Q_P\}$.
\end{proposition}
\begin{proof}
*** still need to check this *** {\bf TODO} (I THINK THAT I HAVE ALREADY USED THIS PROPERTY IN SOME PROOFS WITHOUT REFERRING TO IT)
\end{proof}

\subsection{Non-homogeneous Markov chains}\label{sec:nonhomogen_markov}

In contrast to homogeneous Markov chains, a Markov chain for which Equation~\eqref{eq:homogeneousMarkov} does not hold is called---rather obviously---\emph{non-homogeneous}. While we know from Theorem~\ref{theo:homogeneoushasQ} that homogeneous Markov chains can be characterized (up to an initial distribution) by a fixed rate matrix $Q\in\mathcal{R}$, this does not hold for non-homogeneous Markov chains. 

Instead, such systems are typically described by a function $Q_t$ that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$. For any such function $Q_t$, the existence and uniqueness of a corresponding non-homogeneous Markov chain then depend on the specific properties of $Q_t$. Rather than attempt to treat all these different cases here, we instead refer to some examples from the literature. 

Typically, some kind of continuity of $Q_t$ in terms of $t$ is assumed. The specifics of these assumptions may then depend on the intended generality of the results, computational considerations, the domain of application, and so forth. For example,~\cite{aalen1978empirical} assumes that $Q_t$ is left-continuous and has bounded right-hand limits. As a stronger restriction,~\cite{johnson1989nonhomogeneous} uses a collection $Q_1,\ldots,Q_n$ of commuting rate matrices, and defines $Q_t$ as a weighted linear combination of these component rate matrices wherein the weights vary continuously with $t$. In~\cite{rindos1995exact}, a right-continuous and piecewise-constant $Q_t$ is used, meaning that $Q_t$ takes different values on various (half-open) intervals of $\realsnonneg$, but fixed values within those intervals.

Also for our present work, we will require some tools to describe non-homogeneous Markov chains for which the function $Q_t$ differs on at most a finite number of intervals. We use to this end the \emph{restricted} transition matrix systems defined in Section~\ref{sec:systems}.

%However, it will be convenient to define these tools in a slightly more general way, so that we can re-use them later. The basic idea in what follows is to construct a transition matrix system $\mathcal{T}$ out of various ``restricted transition matrix systems'' defined on given intervals. Theorem~\ref{theo:uniqueMarkovchain} then guarantees that there is a Markov chain that corresponds to such a $\mathcal{T}$.

%Recall from Section~\ref{sec:trans_mat_systems} that a transition matrix system $\mathcal{T}$ is a family of transition matrices $T_t^s$, defined for all $t,s\in\realsnonneg$ for which $t\leq s$. In particular, such a family of matrices satisfies $T_t^t=I$ for all $t\in\realsnonneg$, and $T_t^s=T_t^rT_r^s$ for all $t,r,s\in\realsnonneg$ with $t\leq r\leq s$.

%**** blabla

%We now have the required tools for the construction of the non-homogeneous Markov chains in which we will be mostly interested. 

In particular, if for any interval $\mathbf{I}\subseteq\realsnonneg$ and any rate matrix $Q\in\mathcal{R}$ we now let $\mathcal{T}_Q^{\mathbf{I}}$ be the restriction of $\mathcal{T}_Q$ to $\mathbf{I}$, we immediately obtain the following result.

\begin{proposition}\label{prop:finite_different_rate_matrix_has_process}
Consider any non-empty finite sequence of time points $u=t_0,\ldots,t_n$, and a corresponding collection of rate matrices $Q_0,\ldots,Q_{n+1}\in\mathcal{R}$. Then, there exists a well-behaved continuous-time Markov chain $P\in\wmprocesses$ with transition matrix system $\mathcal{T}_P$, such that
\begin{equation*}
\mathcal{T}_P = \mathcal{T}_{Q_0}^{[0,t_0]}\otimes \mathcal{T}_{Q_1}^{[t_0,t_1]} \otimes \cdots \otimes \mathcal{T}_{Q_n}^{[t_{n-1},t_n]} \otimes \mathcal{T}_{Q_{n+1}}^{[t_n,\infty)}\,.
\end{equation*}
\end{proposition}
\begin{proof}
*** still need to check this *** Trivial consequence of Propositions~\ref{prop:systemQ} and~\ref{prop:concat_restr_trans_mat_systems_is_system}, and Theorem~\ref{theo:uniqueMarkovchain}.
\end{proof}

The dynamic behavior of Markov chains constructed as in Proposition~\ref{prop:finite_different_rate_matrix_has_process} is conveniently described as follows. Taking some small liberties with the index notation, the outer partial derivatives of such a process will satisfy, for all $t\in\realsnonneg$,
\begin{equation*}
\smash{\overline{\partial}} T_t^t = \{Q_i\}\quad\text{if $t\in(t_{i-1},t_i)$,~~~~and}\quad \smash{\overline{\partial}} T_t^t = \{Q_i,Q_{i+1}\}\quad\text{if $t=t_i$.}
\end{equation*}
The first equality, where $t\in(t_{i-1},t_i)$, is explained by Proposition~\ref{prop:Q_is_singleton_deriv_for_homogen}. In particular, the behavior on the interval $[t_{i-1},t_i]$ is homogeneous, and fully characterized by $\mathcal{T}_{Q_i}^{[t_{i-1},t_i]}$.

The second equality, where $t=t_i$, is due to the definition $\smash{\overline{\partial}}T_t^t\coloneqq \smash{\overline{\partial}}_-T_t^t\cup \smash{\overline{\partial}}_+T_t^t$. To see this, note that $\smash{\overline{\partial}}_-T_t^t$ will here correspond to the interval $[t_{i-1}, t_i]$, and hence satisfies $\smash{\overline{\partial}}_-T_t^t=\{Q_i\}$. On the other hand, $\smash{\overline{\partial}}_+T_t^t$ corresponds to $[t_i,t_{i+1}]$, and is therefore given by $\smash{\overline{\partial}}_+T_t^t=\{Q_{i+1}\}$.

Note that in the above, the directional outer partial derivatives $\smash{\overline{\partial}}_-T_t^t$ and $\smash{\overline{\partial}}_+T_t^t$ are always singletons. Due to Corollary~\ref{corol:outersingleton}, this implies that the ordinary directional partial derivatives also exist. The next example shows that this is not the case in general.

\begin{exmp}\label{exmp:markov_with_no_deriv}
Recall from Example~\ref{exmp:limit_trans_mat_system} the well-behaved transition matrix system $\mathcal{T}_*$ that was constructed as the limit of a sequence $\{\mathcal{T}_i\}_{i\in\nats}$ of (well-behaved) transition matrix systems. Theorem~\ref{theo:uniqueMarkovchain} now implies that there is some $P\in\wmprocesses$ such that $\mathcal{T}_P = \mathcal{T}_*$. Let $T_t^s$ denote the transition matrices corresponding to this $\mathcal{T}_P = \mathcal{T}_*$.

Recall furthermore from Examples~\ref{exmp:limit_trans_mat_system} and~\ref{exmp:limit_trans_mat_system_matrices} that $\mathcal{T}_*$ was constructed using a sequence $\{\Delta_i\}_{i\in\nats}=\{\nicefrac{1}{2^i}\}_{i\in\nats}$, and that, for all $i\in\nats$,
\begin{equation*}
T_0^{\Delta_i} = \begin{cases}
e^{\Delta_i(Q_1(\nicefrac{2}{3}) + Q_2(\nicefrac{1}{3}))} & \text{if $i$ is odd, and} \\
e^{\Delta_i(Q_1(\nicefrac{1}{3}) + Q_2(\nicefrac{2}{3}))} & \text{otherwise,}
\end{cases}
\end{equation*}
with $Q_1,Q_2\in\mathcal{R}$ and $Q_1\neq Q_2$. 

Let now $Q_o\coloneqq Q_1(\nicefrac{2}{3}) + Q_2(\nicefrac{1}{3})$ and $Q_e\coloneqq Q_1(\nicefrac{1}{3}) + Q_2(\nicefrac{2}{3})$. Then, it clearly holds that $Q_o\neq Q_e$, and, for all $i\in\nats$, that $T_0^{\Delta_i}=e^{\Delta_i Q_o}$ if $i$ is odd, and $T_0^{\Delta_i}=e^{\Delta_iQ_e}$, otherwise.

Let $\{\Delta_{i_j}\}_{j\in\nats}$ be the subsequence of $\{\Delta_i\}_{i\in\nats}$ corresponding to the odd terms. Then,
\begin{equation*}
\lim_{j\to\infty}\frac{1}{\Delta_{i_j}}\left(T_0^{\Delta_{i_j}}-I\right) = \lim_{j\to\infty}\frac{1}{\Delta_{i_j}}\left(e^{\Delta_{i_j}Q_o}-I\right) = Q_o\,.
\end{equation*}
Similarly, for the subsequence $\{\Delta_{i_k}\}_{k\in\nats}$ corresponding to the even terms, we have $\lim_{k\to\infty}\nicefrac{1}{\Delta_{i_k}}(T_0^{\Delta_{i_k}}-I)=Q_e$. It follows that for the directional outer partial derivative $\smash{\overline{\partial}}_+T_t^t$ corresponding to $P$, at $t=0$, it holds that $\{Q_o,Q_e\}\subseteq \smash{\overline{\partial}}_+T_t^t$.
%**** Misschien voorbeeld dat we hiermee het bestaan van het tegenvoorbeeld in Example~\ref{exmp:well-behaved-no-deriv} kunnen bewijzen?
\exampleend
\end{exmp}

Thus, we have found that in general, the directional outer partial derivatives $\smash{\overline{\partial}}_-T_t^t$ and $\smash{\overline{\partial}}_+T_t^t$ can be non-singleton sets of rate matrices. Furthermore, we know from Proposition~\ref{prop:boundednon-emptyandclosed} that, for well-behaved stochastic processes, these sets are always non-empty, bounded, and closed. 

********** DO WE WANT TO KEEP THE BELOW EXAMPLE? COULD ALSO REPLACE WITH JUST MENTIONING THAT THEY WILL BE CONNECTED, ALTHOUGH WE DONT HAVE TO PROVIDE A PROOF.

The next example establishes that these sets might furthermore be convex.

\begin{exmp}
Consider again the Markov chain $P\in\wmprocesses$ from Example~\ref{exmp:markov_with_no_deriv}, whose transition matrix system $\mathcal{T}_P$ is given by $\mathcal{T}_P=\mathcal{T}^*$, the transition matrix system from Example~\ref{exmp:limit_trans_mat_system}. We already established in Example~\ref{exmp:markov_with_no_deriv} that this Markov chain's outer partial derivative satisfies $\{Q_o,Q_e\}\subseteq \smash{\overline{\partial}}_+T_0^0$. Consider now any $\lambda\in[0,1]$, and let $Q_\lambda\coloneqq \lambda Q_o + (1-\lambda)Q_e$. We will show that $Q_\lambda \in \smash{\overline{\partial}}_+T_0^0$.

**** TODO: FINISH THIS
\exampleend
\end{exmp}

%\begin{proposition}\label{prop:continuous_rate_matrix_has_process}
%Consider any left-continuous, piecewise-constant function $Q_t$ that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$, such that $Q_t$ takes different values on at most a finite number of intervals. Then, there exists a well-behaved continuous-time Markov chain $P\in\wmprocesses$ such that $\mathcal{T}_P=\mathcal{T}_{Q_t}$, where $\mathcal{T}_{Q_t}$ is defined as in Lemma~\ref{lemma:nonhomogen_trans_mat_system}.
%\end{proposition}

\section{Imprecise Continuous-Time Markov chains}
\label{sec:iCTMC}

In Sections~\ref{sec:stochastic_processes} and~\ref{sec:cont_time_markov_chains}, we formalized stochastic processes, and provided ways to characterize them. We now turn to the field of \emph{imprecise probability}~\cite{Walley:1991vk} {\bf CITE CITE}, and will formalize in this section the notion of \emph{imprecise continuous-time Markov chains}. Here, rather than look at a single stochastic process $P\in\processes$, we instead consider jointly some \emph{set} of processes $\mathcal{P}\subseteq\processes$. Our main object of consideration will be the \emph{lower expectation} with respect to such a set $\mathcal{P}$.

\begin{definition}[Lower Expectation]\label{def:lower_exp}
For any set of stochastic processes $\mathcal{P}\subseteq\processes$, the \emph{(conditional) lower expectation with respect to $\mathcal{P}$} is defined as
\begin{equation*}
\underline{\mathbb{E}}[\cdot\,\vert\,\cdot] \coloneqq \inf\left\{\mathbb{E}_P[\cdot\,\vert\,\cdot]\,:\,P\in\mathcal{P}\right\}\,,
\end{equation*}
where $\mathbb{E}_P[\cdot\,\vert\,\cdot]$ denotes the (conditional) expectation taken with respect to $P$.
\vspace{0pt}
\end{definition}

\noindent The corresponding \emph{upper expectation}, $\overline{\mathbb{E}}[\cdot\,\vert\,\cdot]\coloneqq \sup\{\mathbb{E}_P[\cdot\,\vert\,\cdot]\,:\,P\in\mathcal{P}\}$ can, as is well-known~{\bf CITE}, always be defined through the conjugacy relation $\overline{\mathbb{E}}[\cdot\,\vert\,\cdot]=-\underline{\mathbb{E}}[-\cdot\,\vert\,\cdot]$. Therefore, we will in the remainder of this work focus only on the lower expectation with respect to some set $\mathcal{P}$.

Because these lower expectations are linked to a given set $\mathcal{P}\subseteq\processes$ of stochastic processes, we will first look at how we can describe such a set $\mathcal{P}$.
Recall from Section~\ref{sec:dynamics} that for a given stochastic process $P\in\processes$, its behavior can be described using the outer partial derivatives $\smash{\overline{\partial}}T_{t,\,x_u}^t$ of its transition matrices $T_{t,\,x_u}^t$ at each time $t\in\realsnonneg$ and for each history $x_u\in\states_u$. There, we found that---at least for well-behaved processes---these outer partial derivatives are non-empty bounded sets of rate matrices. Hence, because individual stochastic processes can be described using sets of rate matrices, this is also a natural starting point for describing sets of processes.

Therefore, consider some non-empty bounded set $\rateset\subset\mathcal{R}$ of rate matrices. We can then ask if there are any processes that are described by this set $\rateset$. Indeed, we found in Section~\ref{sec:homogen_markov_chain} that for any $Q\in\rateset$, there is a well-behaved homogeneous Markov chain that is characterized---up to its initial distribution---by this $Q$. Hence, an obvious way to construct a set $\mathcal{P}$ from a given set $\rateset$ is as the set of well-behaved homogeneous Markov chains characterized by elements of $\rateset$. This idea will be formalized in Section~\ref{subsec:types_ictmc}.

For now, we next consider whether we can also describe sets of non-homogeneous, (non-)Markov processes using a given set $\rateset$ of rate matrices. However, it should be clear that this is less straightforward than in the homogeneous, Markovian case. In particular, because such a set $\rateset$ is not explicitly connected to any time $t$ or history $x_u$, asking if a (non-homogeneous, non-Markov) process is described by this set is in some sense an ill-posed question.
Therefore, rather than asking if a process can be described by some set of rate matrices, we will now introduce the following weaker notion of \emph{consistency} of a stochastic process with a given set of rate matrices.

\begin{definition}[Consistent Process]\label{def:consistent_process}
Consider any non-empty bounded set $\rateset$ of rate matrices, and any stochastic process $P\in\processes$. Then $P$ is said to be \emph{consistent} with $\rateset$, if
\begin{equation*}
(\forall t\in\realsnonneg)(\forall u\in\mathcal{U}_{<t})(\forall x_u\in\states_u)\,:\, \smash{\overline{\partial}}T_{t,x_u}^t \subseteq \rateset\,.
\end{equation*}
If $P$ is consistent with $\rateset$, we will write $P\sim\rateset$.
\end{definition}

Thus, when a process $P\in\processes$ is consistent with some $\rateset\subset\mathcal{R}$, we know that its behavior over time can always be described using the elements of $\rateset$. However, we do not know which of these elements $Q\in\rateset$ describe this behavior at any given time  $t\in\realsnonneg$ or for any given history $x_u\in\states_u$. Furthermore, consistency of a process with some set $\rateset$ only asserts ``consistency in behavior"; in particular, it does not tell us anything about the initial distribution of the process. It should therefore be clear that there will be many processes consistent with any non-empty bounded set $\rateset\subset\mathcal{R}$. The next example shows that this is already the case when $\rateset$ is singleton.

\begin{exmp}\label{example:singleton_infinite_consistent}
Suppose that $\rateset=\{Q\}$ for some $Q\in\mathcal{R}$. Then, there are an infinite number of stochastic processes consistent with $\rateset$. In particular, recall from Section~\ref{sec:homogen_markov_chain} that there exists a well-behaved homogeneous Markov chain that is characterized uniquely by the rate matrix $Q$, \emph{up to} its initial distribution $P(X_0)$. Therefore, and due to Proposition~\ref{prop:Q_is_singleton_deriv_for_homogen}, for each choice of probability mass function $p$ on $\states$, there is a well-behaved homogeneous Markov chain $P$ that is consistent with $\rateset$, and for which $P(X_0=x)=p(x)$ for all $x\in\states$.
\exampleend
\end{exmp}

Hence, if we consider the set $\mathcal{P}$ of all stochastic processes consistent with some $\rateset$, this set will contain (infinitely) many processes that differ only in their initial distributions. Of course, this should hardly be surprising; even individual homogeneous Markov chains require both a rate matrix $Q$ and an initial distribution $p$ to be uniquely characterized. We will return to this observation shortly.

For now, we first consider the case where $\rateset$ is not singleton. The example below illustrates that there are many different processes consistent with such a $\rateset$; in particular, these processes not only differ in their initial distributions, but they also differ in ``behavior''.

%Put differently, even though different processes may be consistent with the same set $\rateset$, their difference need not be relevant. In particular, this is the case whenever different processes have the same conditional expectation. However, when $\rateset$ is not singleton, the collection of processes consistent with $\rateset$ actually starts to exhibit differences that lead to different conditional expectations, and that are therefore relevant.

\begin{exmp}\label{example:rateset_not_singleton}
Suppose that $\rateset=\{Q_1,Q_2\}$ for some $Q_1,Q_2\in\mathcal{R}$ with $Q_1\neq Q_2$. Then, clearly, all well-behaved homogeneous Markov chains described by either $Q_1$ or $Q_2$ are consistent with $\rateset$. Furthermore, there will also be different non-homogeneous Markov chains consistent with $\rateset$. In particular, not only can such processes differ in their initial distribution, there will be non-homogeneous Markov chains with the same initial distribution but which differ in the choice of $Q_1$ or $Q_2$ for any given time $t\in\realsnonneg$. 

Furthermore, various non-homogeneous, non-Markov processes will be consistent with $\rateset$ as well. For example, for any $t\in\realsnonneg$ and any $x_u,y_u\in\states_u$ such that $x_u\neq y_u$, there will be a process that satisfies $\smash{\overline{\partial}}T_{t,x_u}^t=\{Q_1\}$ and $\smash{\overline{\partial}}T_{t,y_u}^t=\{Q_2\}$.
\exampleend
\end{exmp}

In summary, we conclude that for a given non-empty bounded set $\rateset$ of rate matrices, there are many stochastic processes consistent with this $\rateset$. We will in the sequel focus on such sets of processes. 

However, as noted above, consistency with some $\rateset$ does not allow us to specify the initial distributions of the processes in such a set. Therefore, we first briefly introduce the notion of consistency of initial distributions. To this end, let $\mathcal{M}$ be an arbitrary set of probability mass functions on $\states$. We will then say that a process $P$ is \emph{consistent with $\mathcal{M}$ in initial distribution}, whenever $P(X_0)\in\mathcal{M}$. We now focus on sets of processes that are jointly consistent with some given $\rateset$ and $\mathcal{M}$.

However, rather than look at the set of \emph{all} processes consistent with such $\rateset$ and $\mathcal{M}$, we will for the sake of generality and ease of notation consider the consistent subset of a given set of processes $\mathcal{P}\subseteq\processes$.

%*** dit leidt dan voor een gegeven $\rateset$ naar het idee om naar de verzameling van alle $P\in\processes$ te kijken die consistent zijn met $\rateset$. Voor notationeel gemak en generiekheid kijken we echter liever naar de subset van een gegeven $\mathcal{P}\subseteq\processes$ die consistent is met $\rateset$:

\begin{definition}[Consistent Subset of Processes]\label{def:consistent_process_set}
Consider any non-empty bounded set of rate matrices $\rateset$, any non-empty set $\mathcal{M}$ of probability mass functions on $\states$, and any set of stochastic processes $\mathcal{P}\subseteq\processes$. Then, the \emph{subset of $\mathcal{P}$ consistent with} $\rateset$ \emph{and} $\mathcal{M}$ is denoted $\mathcal{P}_{\rateset,\mathcal{M}}$, and defined as
\begin{equation*}
\mathcal{P}_{\rateset,\mathcal{M}} \coloneqq \left\{P\in\mathcal{P}\,:\,P\sim\rateset,\,P(X_0)\in\mathcal{M}\right\}\,.
\end{equation*}
When $\mathcal{M}$ is the set of \emph{all} probability mass functions on $\states$, we will write $\mathcal{P}_{\rateset}$ for the sake of brevity.
\end{definition}
Such a set $\mathcal{P}_{\rateset,\mathcal{M}}$ will also be called an \emph{imprecise continuous-time Markov chain}. We will next focus on some specific cases.

\subsection{Types of Imprecise Continuous-Time Markov Chains}\label{subsec:types_ictmc}

It is clear from Example~\ref{example:rateset_not_singleton} that, for arbitrary $\rateset$, the entire consistent set $\processes_\rateset$ contains processes that differ not only quantitatively---differing in which $Q\in\rateset$ they use---but also qualitatively: differing in whether they satisfy homogeneity or Markov properties. We will therefore find it convenient to focus on the following sets of processes, which make explicit this qualitative difference.

\begin{definition}\label{def:process_sets}
For any non-empty bounded set of rate matrices $\rateset$,
\begin{itemize}
\item $\wprocesses_\rateset\subset\wprocesses$ is the set of all well-behaved stochastic processes consistent with $\rateset$.
\item $\wmprocesses_\rateset\subset\wmprocesses$ is the set of all well-behaved Markov chains consistent with $\rateset$;
\item $\whmprocesses_\rateset\subset\whmprocesses$ is the set of all well-behaved homogeneous Markov chains consistent with $\rateset$;
\end{itemize}
We will denote the lower expectation with respect to $\wprocesses_\rateset$ as $\underline{\mathbb{E}}_\rateset^{\mathrm{W}}$, with respect to $\wmprocesses_\rateset$ as $\underline{\mathbb{E}}_\rateset^{\mathrm{WM}}$, and with respect to  $\whmprocesses_\rateset$ as $\underline{\mathbb{E}}_\rateset^{\mathrm{WHM}}$.
\end{definition}
Following Definition~\ref{def:consistent_process_set}, we will write $\wprocesses_{\rateset,\mathcal{M}}$, with lower expectation $\underline{\mathbb{E}}_{\rateset,\mathcal{M}}^{\mathrm{W}}$, when we want to consider a specific set $\mathcal{M}$ of initial distributions (and similarly for the other sets above). However, we will in the sequel focus almost exclusively on sets of processes consistent only with a given set $\rateset$, and \emph{conditional} lower expectations with respect to those sets. Restrictions to specific sets $\mathcal{M}$ of initial distributions are then revisited in Section~{\bf REF}, where we will show how to compute (conditional) lower expectations when we do take into consideration such sets $\mathcal{M}$.

%\begin{proposition}\label{prop:process_sets_simplify}
%For any non-empty bounded set of rate matrices $\rateset$, it holds that
%\begin{align*}
%\whmprocesses_\rateset &= \left\{P\in\whmprocesses\,:\,P\sim\rateset\right\} \\
% &= \left\{P\in\whmprocesses\,:\,\left((\forall t\in\realsnonneg):~
%\overline{\partial}
%{T^t_{t}}\subseteq\rateset
%\right) \right\} \\
% &= \left\{P\in\whmprocesses\,:\,Q_P\in\rateset\right\}\,.
%\end{align*}
%Furthermore, it holds that
%\begin{align*}
%\wmprocesses_\rateset &= \left\{P\in\wmprocesses\,:\,P\sim\rateset\right\} \\
% &= \left\{P\in\wmprocesses\,:\,\left((\forall t\in\realsnonneg):~
%\overline{\partial}
%{T^t_{t}}\subseteq\rateset
%\right) \right\}\,.
%\end{align*}
%\end{proposition}
%\begin{proof}
%*** {\bf TODO}, but trivial.
%\end{proof}
Returning to the qualitative differences between the sets mentioned above, observe that these differences simplify the notion of consistency from Definition~\ref{def:consistent_process}. In particular, it holds that
\begin{equation*}
\wmprocesses_\rateset = \left\{P\in\wmprocesses\,:\,(\forall t\in\realsnonneg)~~ \smash{\overline{\partial}}T_t^t\subseteq\rateset\right\}\,,
\end{equation*}
and
\begin{equation*}
\whmprocesses_\rateset = \left\{P\in\whmprocesses\,:\,Q_P\in\rateset\right\}\,.
\end{equation*}
This first equality follows from the Markov property of the elements of $\wmprocesses$, ensuring that $\smash{\overline{\partial}}T_{t,x_u}^t=\smash{\overline{\partial}}T_t^t$ for all $u\in\mathcal{U}_{<t}$ and all $x_u\in\states^u$. The second equality follows from the Markov property and the homogeneity of the elements $P\in\whmprocesses$, which, by Proposition~\ref{prop:Q_is_singleton_deriv_for_homogen}, ensures that $\smash{\overline{\partial}}T_t^t=\{Q_P\}$ for all $t\in\realsnonneg$.

It should furthermore be clear that these different sets are nested, and, as an immediate result, that their corresponding lower expectations provide bounds for each other.

\begin{proposition}\label{prop:markov_set_subset_of_nonmarkov_set}
Consider any bounded set of rate matrices $\rateset$. Then,
\begin{equation*}
\whmprocesses_\rateset \subseteq \wmprocesses_\rateset \subseteq \wprocesses_\rateset\,.
\end{equation*}
\end{proposition}
\begin{proof}
*** still need to check this *** This is immediate from Definition~\ref{def:process_sets} and the fact that $\whmprocesses\subset\wmprocesses\subset\wprocesses$.
\end{proof}

\begin{proposition}\label{prop:lower_exp_markov_bounded_by_nonmarkov}
Consider any bounded set of rate matrices $\rateset$. Then,
\begin{equation*}
\underline{\mathbb{E}}_\rateset^\mathrm{W}[\cdot\,\vert\,\cdot] \leq
\underline{\mathbb{E}}_\rateset^\mathrm{WM}[\cdot\,\vert\,\cdot] \leq
\underline{\mathbb{E}}_\rateset^\mathrm{WHM}[\cdot\,\vert\,\cdot]\,.
\end{equation*}
\end{proposition}
\begin{proof}
*** still need to check this *** This is immediate from Definition~\ref{def:lower_exp} and Proposition \ref{prop:markov_set_subset_of_nonmarkov_set}.
\end{proof}

Now, ironically, it turns out that computing lower expectations for the set $\whmprocesses_\rateset$ is in general much harder than for the sets $\wmprocesses_\rateset$ or $\wprocesses_\rateset$. In fact, we will show in Section~\ref{sec:funcs_multi_time_points} that $\wprocesses_\rateset$ is in some sense the easiest set to do this for, for arbitrary functions. 

The problem is, essentially, that while homogeneous Markov chains are easy to work with numerically, the set $\whmprocesses_\rateset$ does not provide enough ``degrees of freedom'' to make the optimization problem involved in computing $\underline{\mathbb{E}}^\mathrm{WHM}_\rateset$ tractable. The following example serves as an illustration. 

*** Known to be difficult: even finding an $\epsilon$-approximation takes at least exponential time~\cite[Theorem 20.1]{kreinovich2013computational}.

\begin{exmp}
**** example that homogeneous case is harder than non-homogeneous case.
\exampleend
\end{exmp}

Because of these observations, we will in the sequel focus on the sets $\wmprocesses_\rateset$ and $\wprocesses_\rateset$, having introduced $\whmprocesses_\rateset$ only for the sake of completeness. Note that Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov} guarantees that the lower expectation for either $\wmprocesses_\rateset$ or $\wprocesses_\rateset$ suffices to compute a lower bound for $\underline{\mathbb{E}}_\rateset^\mathrm{WHM}$. Of course, this bound will in general not be tight if one insists that any analysis should be done with respect to the set $\whmprocesses_\rateset$. However, because this latter set does make the strictest---and, we would argue, most unrealistic---assumptions about the domain of interest, we do not consider this particularly problematic.

\subsection{Closure Properties for Sets of Consistent Processes}

We have in the previous section defined imprecise continuous-time Markov chains as sets of stochastic processes consistent with some set $\rateset$ of rate matrices. An obvious question, therefore, is to see if we can express properties of these sets $\wmprocesses_\rateset$ and $\wprocesses_\rateset$ in relation to properties of the set $\rateset$. We have by assumption stated that such a $\rateset$ must be non-empty and bounded, and will here consider the cases where $\rateset$ is additionally closed or convex.

We start with considering the case where $\rateset$ is convex, in relation to the set $\wprocesses_\rateset$. It turns out that this leads to a powerful construction tool for stochastic processes. In particular, this allows us to construct a new stochastic process by ``piecing together'' other processes from the set $\wprocesses_\rateset$. The next result ensures that this new process will then also belong to $\wprocesses_\rateset$.
\begin{theorem}\label{theo:aanelkaarplakken}
Consider a non-empty convex set of rate matrices $\rateset\subseteq\mathcal{R}$.
Fix a finite sequence of time points $u$. Choose any $P_\emptyset\in\wprocesses_\rateset$ and, for all $x_u\in\states^u$, choose some $P_{x_u}\in\wprocesses_\rateset$. Then there is a stochastic process $P\in\wprocesses_\rateset$ such that, for all $u_1,u_2\subseteq u$ such that $u_1<u_2$, all $x_u\in\states^u$ and all $A\in\mathcal{A}_u$:
\begin{equation}\label{eq:theo:aanelkaarplakken:equalsfirst}
P(X_{u_2}=x_{u_2}\vert X_{u_1}=x_{u_1})=P_{\emptyset}(X_{u_2}=x_{u_2}\vert X_{u_1}=x_{u_1})
%\text{~for all $x_u\in\states^u$}
\vspace{-7pt}
\end{equation}
and
\begin{equation}\label{eq:theo:aanelkaarplakken:equalssecond}
P(A\vert X_u=x_u)=P_{x_u}(A\vert X_u=x_u).
%\text{~for all $x_u\in\states^u$ and $A\in\mathcal{A}_u$.}
\vspace{7pt}
\end{equation}
\end{theorem}
This result is particularly useful when we consider functions $f\in\gambles(\states^{u\cup v})$ defined on an arbitrary number of time points $u$ and $v$. Specifically, it follows from the basic properties of expectation that for any process $P\in\wprocesses_\rateset$, the expectation $\mathbb{E}$ of such a function satisfies, for any $w\subseteq v$,
\begin{equation*}
\mathbb{E}[f(X_u,X_v)\,\vert\,X_u] = \mathbb{E}\bigl[\mathbb{E}[f(X_u,X_v)\,\vert\,X_u,X_{v\setminus w}]\,\big\vert\,X_u\bigr]\,.
\end{equation*}
As shown by our next result, it follows from Theorem~\ref{theo:aanelkaarplakken} that, if $\rateset$ is convex, the same decomposition property holds for lower expectations $\underline{\mathbb{E}}^{\mathrm{W}}_\rateset$.

\begin{theorem}\label{theorem:decomposition_multivar}
Let $\rateset$ be an arbitrary non-empty, bounded, and convex set of rate matrices. Then for any $u,v,w\in\mathcal{U}$ such that $u<v<w$ and any $f\in\gambles(\states^{u\cup v\cup w})$:
\begin{equation}\label{eq:lower_exp_factorizes}
\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\,\vert\,X_u\right] = \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\Bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\,\vert\,X_u,X_v\right] \Big\vert\,X_u\Bigr]\,. 
\end{equation}
\end{theorem}
The reason that this result is useful, is that it essentially allows us to compute the lower expectation of such functions recursively. In particular, instead of computing the lower expectation on all time-points simultaneously, we can focus on subsets of the time-points separately. We will revisit this idea in Section~\ref{sec:funcs_multi_time_points}.

We next consider the case where $\rateset$ is both convex and closed, and consider the set $\wmprocesses_\rateset$. Recall from Section~\ref{sec:dynamics} that for any function $f\in\gamblesX$, its expectation with respect to a process $P\in\wmprocesses_\rateset$ can alternatively be expressed using the transition matrix corresponding to this process. Specifically, it holds that $\mathbb{E}[f(X_s)\,\vert\,X_t]=T_t^sf$. Note furthermore that a lower expectation $\underline{\mathbb{E}}_{\,\rateset}^{\mathrm{WM}}$ is an infimum over the set of expectations $\mathbb{E}$ corresponding to all $P\in\wmprocesses_\rateset$. It should therefore be clear that the set of transition matrices $T_t^s$ corresponding to all $P\in\wmprocesses_\rateset$ is also of interest to us.

Recall from Section~\ref{sec:cont_time_markov_chains} that for each $P\in\wmprocesses_\rateset$, there is a transition matrix system $\mathcal{T}_P$ specifying all transition matrices corresponding to $P$. Furthermore, this $\mathcal{T}_P$ has a restriction $\mathcal{T}_P^{[t,s]}$ to the interval $[t,s]$, and this restriction also specifies $T_t^s$. 

We have already seen in Section~\ref{sec:systems} that the space $\mathbb{T}^{[t,s]}$ of \emph{all} restricted transition matrix systems on $[t,s]$ is complete. We will now look at the subspace of $\mathbb{T}^{[t,s]}$ corresponding to all $P\in\wmprocesses_\rateset$. To this end, for any non-empty bounded set of rate matrices $\rateset$ and any $t,s\in\realsnonneg$ such that $t\leq s$, let
\begin{equation*}
\mathbb{T}_\rateset^{[t,s]} \coloneqq \left\{\mathcal{T}_P^{[t,s]}\,:\,P\in\wmprocesses_\rateset\right\}\,.
\end{equation*}

Then, if $\rateset$ is closed and convex, we obtain the following result.

\begin{theorem}\label{theorem:restricted_transmatsystem_space_compact_if_Q_closed}
Consider a non-empty, bounded, convex and closed set of rate matrices $\rateset$ and any $t,s\in\realsnonneg$ such that $t\leq s$. Let $d$ be the metric that is defined in Equation~\eqref{eq:trans_mat_system_metric}. The metric space $\smash{(\mathbb{T}_\rateset^{[t,s]},d)}$ is then compact.
\end{theorem}

The reason that this result is of interest to us, is that it allows us to prove the existence of certain processes $P\in\wmprocesses_\rateset$. In particular, it allows us to establish the existence of limits of sequences $\left\{P_i\right\}_{i\in\nats}$ in $\wmprocesses_\rateset$ that satisfy certain behavior on a specified interval $[t,s]$. This works because, since the space $\mathbb{T}^{[t,s]}_\rateset$ is compact, not only do convergent sequences converge within this set, but furthermore, any sequence in this set will always have a convergent subsequence.

For instance, the next result follows immediately from Theorem~\ref{theorem:restricted_transmatsystem_space_compact_if_Q_closed}. This result establishes that, if $\rateset$ is closed and convex, the lower expectation $\underline{\mathbb{E}}_{\,\rateset}^{\mathrm{WM}}$ is always reached by some $P\in\wmprocesses_\rateset$, and hence is in fact a minimum.
\begin{proposition}\label{prop:lower_expectation_reached_if_Q_closed}
Let $\rateset$ be an arbitrary non-empty, bounded, convex and closed set of rate matrices. Then, for all $t,s\in\realsnonneg$ such that $t\leq s$, all $f\in\gamblesX$, and all $x\in\states$, there is some $P\in\wmprocesses_\rateset$ with expectation operator $\mathbb{E}$, such that
\begin{equation*}
\underline{\mathbb{E}}^{\mathrm{WM}}_\rateset[f(X_s)\,\vert\,X_t=x] = \mathbb{E}[f(X_s)\,\vert\,X_t=x]\,.
\end{equation*}
\end{proposition}
%In summary, then, closedness of $\rateset$ allows us to construct processes $P\in\wmprocesses_\rateset$ as limits of sequences of processes.

\section{An Important Lower Transition Operator}
\label{sec:lowertrans}

Having introduced the notion of lower expectations with respect to sets of (non-)Markov processes, one might wonder how to compute these quantities, either numerically or for analytical purposes. 

Obviously, one way to go about doing this is to work directly with the definitions. That is, explicitly generate the entire set $\wmprocesses_\rateset$ (or $\wprocesses_\rateset$) for a given $\rateset$, compute expectations of a function $f$ for each element of this set, and then find the infimum of these expectations. It should be clear that this approach is fairly unwieldy, not in the least because for arbitrary $\rateset$ the corresponding set of processes may be infinite. Therefore, we will instead provide an alternative characterization of these lower expectations.

We have in the previous sections seen that the transition matrix $T_t^s$ corresponding to a stochastic process serves as an alternative representation of the expectation operator $\mathbb{E}[f(X_s)\,\vert\,X_t]$ for this process. We will in this section introduce a generalization of these transition matrices, so-called \emph{lower transition operators}. Specifically, we will introduce a lower transition operator corresponding to some \emph{lower transition rate operator}, which in turn generalizes the notion of a transition rate matrix. We will then show in Sections~\ref{sec:connections} and~\ref{sec:funcs_multi_time_points} that this lower transition operator serves as an alternative representation for the lower expectations of imprecise continuous-time Markov chains.

We focus in this section on introducing the relevant concepts, and deriving this operator of interest. We end in Section~\ref{sec:properties_lower_trans} by showing that this operator satisfies a number of convenient properties.
%We will in this section introduce a \emph{lower transition operator}, which is a map from $\gamblesX$ to $\gamblesX$ that generalizes the notion of a transition matrix. We will here focus on introducing the relevant concepts, and showing that this operator of interest is well-defined. We end this section by establishing that this (family) of operators is in many ways intuitively comparable to the transition matrix system of a well-behaved homogeneous Markov chain. In Section~\ref{sec:connections} we will then establish the relation between this operator and lower expectations, and show that we can indeed use it to compute the quantities of interest. In Section~\ref{sec:prev_work} we will show how this operator is related to previous work from the literature.

\subsection{Lower Transition (Rate) Operators}

We start in this section by generalizing transition matrices $T$ to lower transition operators $\lt$, as follows.

\begin{definition}[Lower Transition Operator]\label{def:coh_low_trans}
We will call a map $\lt$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, and all $x\in\states$:

*** why did you put these componentwise??? ***

%\vspace{5pt}
\begin{enumerate}[label=LT\arabic*:,ref=LT\arabic*]
\item
$\left[\lt\,f\right](x)\geq\min\left\{f(y)\,\vert\,y\in\states\right\}$ \label{LT:bounded_min}
\item
$\left[\lt(f+g)\right](x)\geq \left[\lt\,f\right](x)+\left[\lt\,g\right](x)$; \label{LT:super_additive}
\item
$\left[\lt(\lambda f)\right](x)=\lambda\left[\lt\,f\right](x)$. \label{LT:homo}
\end{enumerate}
%\vspace{5pt}
\noindent We use $\underline{\mathcal{T}}$ to denote the set of all lower transition operators.
\vspace{5pt}
\end{definition}
The first thing to note is that any transition matrix $T$ will also satisfy properties~\ref{LT:bounded_min}-\ref{LT:homo}, and hence is also a lower transition operator. It is therefore clear that lower transition operators are a proper generalization of transition matrices.

A first way to motivate this specific generalization is to note the following. For any lower transition operator $\lt$ and any $x\in\states$, consider the map $\lt_x:\gamblesX\to\reals$, defined for all $f\in\gamblesX$ as
\begin{equation}\label{eq:lowerprevisionfromlt}
\lt_xf \coloneqq \left[\lt f\right](x)\,.
\end{equation}
Due to properties~\ref{LT:bounded_min}-\ref{LT:homo}, it follows that $\lt_x$ is a map from $\gamblesX$ to $\reals$ that is super-additive, non-negatively homogeneous, and bounded below by the minimum operator. By definition~\cite[Definition~2.3.3]{Walley:1991vk}, that means that $\lt_x$ is a \emph{coherent lower prevision} on $\gamblesX$. 

For the unfamiliar reader, this notion of coherent lower previsions is perhaps best clarified as follows. Consider some arbitrary set $\mathcal{M}$ of probability mass functions on $\states$, and consider a map $\underline{\mathbb{E}}:\gamblesX\to\reals$, defined for all $f\in\gamblesX$ as
\begin{equation*}
\underline{\mathbb{E}}f \coloneqq \inf\left\{\sum_{x\in\states} p(x)f(x)\,:\,p\in\mathcal{M}\right\}\,.
\end{equation*}
It should be clear that this map $\underline{\mathbb{E}}$ essentially computes a lower expectation with respect to the set $\mathcal{M}$. Furthermore, note that for any $p\in\mathcal{M}$, the quantity $\sum_{x\in\states}p(x)f(x)$ is bounded from below by $\min\{f(y):y\in\states\}$. Because this holds for any $p\in\mathcal{M}$, it follows that $\underline{\mathbb{E}}$ is also bounded below by this minimum operator. Furthermore, it is easily verified that $\underline{\mathbb{E}}$ is super-additive and non-negatively homogeneous. Hence, by the definition cited above, this lower expectation operator corresponding to $\mathcal{M}$ is a coherent lower prevision on $\gamblesX$. 

Our consideration of the coherent lower prevision $\lt_x$  above is essentially the same idea, but \emph{without} considering explicitly any link to sets $\mathcal{M}$ of probability mass functions. Suffice it to say that, for any coherent lower prevision, such a set $\mathcal{M}$ will always exist. It should furthermore be noted that the qualifier ``coherent'' here has connections to the notions of \emph{coherence} and \emph{avoiding sure loss} that we explained in Section~\ref{sec:cond_prob}. We refer to {\bf CITE CITE CITE} for further discussion on this.

Because for any lower transition operator $\lt$ and any $x\in\states$, the map $\lt_x$ is a coherent lower prevision, it follows that the lower transition operator $\lt$ is essentially a vector of coherent lower previsions. It should be clear from the above that such operators $\lt$ are an intuitive starting point for alternative representations of lower expectations. We next focus on the generalization of transition rate matrices $Q$ to lower transition rate operators $\lrate$, as follows.
\begin{definition}[Lower Transition Rate Operator]\label{def:coh_low_trans_rate}
We will call a map $\lrate$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition rate operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, all constant functions $\mu\in\gamblesX$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=LR\arabic*:,ref=LR\arabic*]
\item\label{LR:constantzero}
$\left[\lrate\mu\right](x)=0$;
\item\label{LR:nondiagpos}
$\left[\lrate\ind{y}\right](x)\geq0$ for all $y\in\states$ such that $x\neq y$;
\item\label{LR:subadditive}
$\left[\lrate(f+g)\right](x)\geq\left[\lrate f\right](x)+\left[\lrate g\right](x)$;
\item\label{LR:homo}
$\left[\lrate(\lambda f)\right](x)= \lambda\left[\lrate f\right](x)$.
\end{enumerate}
%\vspace{5pt}
\end{definition}

Note that properties~\ref{LR:constantzero} and~\ref{LR:nondiagpos} essentially preserve properties~\ref{def:Q:sumzero} and~\ref{def:Q:nonnegoffdiagonal} from Definition~\ref{def:rate_matrix}. However, where any rate matrix $Q$ represents a linear map, properties~\ref{LR:subadditive} and~\ref{LR:homo} merely require that a lower transition rate operator $\lrate$ is super-additive and non-negatively homogeneous. Hence, any rate matrix is in fact also a lower transition rate operator, with the latter concept providing a proper generalization of the former.

%\begin{proposition}\label{lem:normlratefinite}
%For any lower transition rate operator $\lrate$, we have that $0\leq\norm{\lrate}<+\infty$.
%\end{proposition}
%
%\begin{proposition}\label{lemma:normofcoherenttrans}
%For any lower transition operator $\lt$, we have that $0\leq \norm{\lt}\leq 1$.
%\end{proposition}
The reason that this specific generalization is of interest, is that it preserves the relation between transition matrices and rate matrices, as stated by Propositions~\ref{prop:stochastic_from_rate_matrix} and~\ref{prop:rate_from_stochastic_matrix}. The following two propositions make this explicit in our current setting.

%**** We next establish that there is a correspondence between lower transition rate operators and lower transition operators that is analogous to the one found in Section~\ref{sec:trans_rate_matrices}.

\begin{proposition}[Reference \cite{DeBock:2016}]\label{lemma:normQsmallenough}
Consider any lower transition rate operator $\lrate$, and any $\Delta\in\realsnonneg$ such that $\Delta\norm{\lrate}\leq 1$. Then, the operator $(I+\Delta\lrate)$ is a lower transition operator.
\end{proposition}

\begin{proposition}[Reference \cite{DeBock:2016}]\label{lemma:lower_trans_to_lower_rate}
Consider any lower transition operator $\lt$, and any $\Delta\in\realspos$. Then, the operator $\nicefrac{1}{\Delta}(\lt - I)$ is a lower transition rate operator.
\end{proposition}

\noindent We next give some additional useful properties of the norm of these operators.

\begin{proposition}[Reference \cite{DeBock:2016}]\label{prop:norm_properties_lrate_lt}
For any lower transition operator $\lt$, any lower transition rate operator $\lrate$, and any two non-negatively homogeneous operators $A,B$ from $\gamblesX$ to $\gamblesX$,
\begin{multicols}{2}
\begin{enumerate}[label=LT\arabic*:,ref=LT\arabic*,start=4]
\item
$\norm{\lt} \leq 1$; \label{LT:norm_at_most_one}
\item
$\norm{\lt A - \lt B} \leq \norm{A - B}$. \label{LT:differencenorm}
\end{enumerate}
\begin{enumerate}[label=LR\arabic*:,ref=LR\arabic*,start=5]
\item
$\norm{\lrate} < +\infty$ \label{LR:normlratefinite};
\item
$\norm{\lrate A - \lrate B} \leq 2\norm{\lrate}\norm{A - B}.$ \label{LR:differenceofnorm}
\end{enumerate}
\end{multicols}
\end{proposition}

We now have two results about the set $\underline{\mathcal{T}}$ of all lower transition operators. As the first result shows, this set is closed under composition.
\begin{proposition}\label{lemma:compositioncoherence}
For any $\lt,\underline{S}\in\underline{\mathcal{T}}$, it holds that $\left(\lt\,\underline{S}\right)\in\underline{\mathcal{T}}$.
\end{proposition}
\begin{proof}
*** still need to check this *** Simply check each of the properties.
\end{proof}

\noindent Furthermore, this set is a complete metric space under our usual norm.

\begin{proposition}\label{lemma:completemetricspace}
The metric space $(\underline{\mathcal{T}},d)$ is complete under the metric $d$ induced by our usual norm $\norm{\cdot}$.
\end{proposition}

% \noindent We conclude this section with the following result.
% \begin{proposition}\label{lemma:productiscoherent}
% Consider any $t,s\in\realsnonneg$ such that $t<s$, any lower transition rate operator $\lrate$, and any sequence $u\in\mathcal{U}_{[t,s]}$ of time points such that $\sigma(u)\leq\nicefrac{1}{\norm{\lrate}}$. Then
% \begin{equation*}
% \prod_{k=1}^n(I+\Delta_k\lrate)\coloneqq (I+\Delta_1\lrate)(I+\Delta_2\lrate)\cdots (I+\Delta_n\lrate)
% \end{equation*}
% is a lower transition operator.
% \end{proposition}
% \begin{proof}
% Trivial consequence of Propositions~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}.
% \end{proof}

%*** I WILL TURN ALL THESE LITLE PROPETIES INTO ONE PROPOSITION AND REFER TO MY CONVERGENCE PAPER FOR THEIR PROOF ***
%
%\begin{lemma}\label{lemma:differencenormofcoherenttransrate}
%Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lrate$ be an arbitrary lower transition rate operator. Then, it holds that $\norm{\lrate A-\lrate B}\leq 2\norm{\lrate}\norm{A-B}$.
%\end{lemma}
%\begin{proof}
%*** {\bf TODO } *** This can be shown to follow from the definition of the norm and the properties of $\lrate$.
%\end{proof}

% \begin{lemma}\label{lemma:differencenormofcoherenttrans}
% Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lt$ be an arbitrary lower transition operator. Then, it holds that $\norm{\lt A-\lt B}\leq \norm{A-B}$.
% \end{lemma}
% \begin{proof}
% *** {\bf TODO } *** This can be shown to follow from coherence.
% \end{proof}

\subsection{The Operator of Interest}

We will in this section introduce a specific lower transition operator on which we will focus for the remainder of this work. We will assume here that we are given some arbitrary lower transition rate operator $\lrate$, and define for any $u\in\mathcal{U}_{[t,s]}$ the auxiliary operator
\begin{equation}\label{eq:aux_lower_trans}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate)\,.
\end{equation}
Before defining the operator of interest, we first give some preliminary results to provide intuition on how this operator will be constructed. We start with a bound on the distance between two operators $\Phi_u$ and $\Phi_{u*}$.

\begin{proposition}\label{prop:differencebetweenu}
Consider any $t,s\in\realsnonneg$ with $t\leq s$, any $\delta\in\realspos$ such that $\delta\norm{\lrate}\leq1$, and any $u,u^*\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$ and $\sigma(u^*)<\delta$. Let $C\coloneqq s-t$. Then
\begin{equation*}
\norm{\Phi_u-\Phi_{u^*}}\leq 2\delta C\norm{\lrate}^2\,.
\end{equation*}
\end{proposition}

Using this bound established by Proposition~\ref{prop:differencebetweenu}, we find that for any sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$, the corresponding sequence $\{\Phi_{u_i}\}_{i\in\nats}$ is Cauchy.

\begin{corollary}\label{corol:cauchy}
For every sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$, the corresponding sequence $\{\Phi_{u_i}\}_{i\in\nats}$ is Cauchy.
\end{corollary}

Furthermore, because of Proposition~\ref{lemma:completemetricspace}, this Cauchy sequence converges to a limit, and this limit is a lower transition operator.

\begin{corollary}\label{corol:limitexistsandiscoherent}
For every sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$, the corresponding sequence $\{\Phi_{u_i}\}_{i\in\nats}$ converges to a lower transition operator.
\end{corollary}

Finally, as our next result establishes, this limit is also unique, in the sense that it is independent of the choice of $\{u_i\}_{i\in\nats}$.

\begin{theorem}\label{theo:convergencelowerbound}
For any $t,s\in\realsnonneg$ such that $t\leq s$ and any lower transition rate operator $\lrate$, there is a unique lower transition operator $\lt\in\underline{\mathcal{T}}$ such that 
\begin{equation}\label{eq:theo:convergencelowerbound}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lt - \Phi_u}<\epsilon.
\end{equation}
\end{theorem}

Note that the $\epsilon-\delta$ expression in Theorem~\ref{theo:convergencelowerbound} is a kind of limit statement. Specifically, it is a limit of operators $\Phi_{u}$ corresponding to increasingly finer partitions $u$ of the interval $[t,s]$. In the sequel, whenever such a unique limit exists and equals some lower transition operator $\lt$, we will denote it as
\begin{equation}\label{eq:net_limit_lower_trans}
\lim_{\sigma(u)\to0}\left\{\Phi_u\,\big\vert\,u\in\mathcal{U}_{[t,s]}\right\} = \lt\,.
\end{equation}
Here, the notation is understood to indicate that the limit of these operators $\Phi_{u}$ is independent of the exact choice of $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$, so long as $\lim_{i\to\infty}\sigma(u_i)=0$.

We are now ready to define the \emph{lower transition operator corresponding to $\lrate$}, which is the operator in which we will be interested for the remainder of this work.

\begin{definition}[Corresponding Lower Transition Operator]\label{def:low_trans}
Consider any $t,s\in\realsnonneg$ such that $t\leq s$ and let $\lrate$ be an arbitrary lower transition rate operator. The \emph{corresponding lower transition operator} $\lbound_t^s$ is a map from $\gamblesX$ to $\gamblesX$, defined by
\begin{equation}\label{eq:lowerbound}
\lbound_t^s\coloneqq\lim_{\sigma(u)\to0}\left\{ \Phi_u\,\big\vert\,u\in\mathcal{U}_{[t,s]}\right\},
\end{equation}
where the limit is understood as in Equation~\eqref{eq:net_limit_lower_trans}.
\end{definition}

\subsection{Properties of the Operator of Interest}\label{sec:properties_lower_trans}

We will next establish that this operator $L_t^s$ satisfies a number of convenient properties. In particular, we will focus on the family $\underline{\mathcal{T}}_{\lrate}$ of lower transition operators corresponding to a given lower transition rate operator $\lrate$.

\begin{definition}[Lower Transition Operator System]
Let $\lrate$ be an arbitrary lower transition rate operator. Then, the \emph{lower transition operator system} corresponding to $\lrate$ is the family $\underline{\mathcal{T}}_{\lrate}$ of lower transition operators $L_t^s$ corresponding to $\lrate$, defined for all $t,s\in\realsnonneg$, with $t\leq s$, as in Definition~\ref{def:low_trans}.
\end{definition}

Our first result is that this family $\underline{\mathcal{T}}_{\lrate}$ satisfies the same semi-group property that was in Section~\ref{sec:cont_time_markov_chains} found to hold for the transition matrix systems $\mathcal{T}_P$ corresponding to Markov chains $P\in\mprocesses$.

\begin{proposition}\label{prop:lower_trans_system_is_system}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,r,s\in\realsnonneg$ such that $t\leq r\leq s$, it holds that
\begin{equation*}
L_t^s = L_t^rL_r^s\,.
\end{equation*}
Furthermore, for all $t\in\realsnonneg$, we have that $L_t^t=I$.
\end{proposition}

Our next result is that this family $\underline{\mathcal{T}}_{\lrate}$ is also time-homogeneous:

\begin{proposition}\label{prop:lower_transition_is_homogeneous}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t\leq s$, we have that $L_t^s=L_0^{s-t}$.
%\begin{equation*}
%L_t^s = L_{t+\Delta}^{s+\Delta}\,.
%\end{equation*}
\end{proposition}

Finally, we note that the derivatives of these lower transition operators always exist.

\begin{proposition}\label{prop:lower_transition_has_deriv}
Let $\lrate$ be an arbitrary lower transition rate operator, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t\leq s$, it holds that\footnote{If $0=t<s$, the derivative with respect to $t$ is taken to be a right derivative. If $t=s$, the derivative with respect to $s$ is taken to be a right derivative and the derivative with respect to $t$ is taken to be a left derivative (or becomes meaningless if $t=0$).}
\begin{equation*}
\frac{\partial}{\partial t}\lbound_t^s=-\lrate\lbound_t^s\,,\quad\text{and}\quad\frac{\partial}{\partial s}\lbound_t^s=\lrate\lbound_t^s.\vspace{7pt}
\end{equation*}
% Then $\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s$ and $\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate$, meaning that
\end{proposition}

One conclusion that we can draw from this, is that $L_t^s$ can alternatively be expressed as the solution of the differential equation
\begin{equation*}
\frac{\partial}{\partial s}L_t^s=\lrate L_t^s\,,\quad\quad L_t^t=I\,.
\end{equation*}
Observe, therefore, the strong correspondence between the operator $L_t^s$ corresponding to some $\lrate$, and the matrix exponential $e^{Q(s-t)}$ of any rate matrix $Q\in\mathcal{R}$. In particular, it is well known {\bf REF} that this matrix exponential is the solution of the differential equation
\begin{equation*}
\frac{\partial}{\partial s}e^{Q(s-t)}=Qe^{Q(s-t)}\,,\quad\quad e^{Q(t-t)}=I\,.
\end{equation*}
Now, recall that any rate matrix $Q$ is also a lower transition rate operator. It therefore follows from the above that the lower transition operator $L_t^s$ corresponding to this $\lrate=Q$ will satisfy $L_t^s=e^{Q(s-t)}$. 

Furthermore, if $\lrate=Q$ for some $Q\in\mathcal{R}$, we find that the family $\underline{\mathcal{T}}_{\lrate}$ is equal to $\mathcal{T}_Q$, the exponential transition matrix system from Definition~\ref{def:systemfromQ} that, by Corollary~\ref{cor:rate_has_unique_homogen_markov_process}, corresponds to some well-behaved homogeneous Markov chain $P\in\whmprocesses$. We can conclude from this that Equation~\eqref{eq:lowerbound} expresses a proper generalization of the matrix exponential, that can be used when working with lower transition rate operators.

Interestingly, then, the family $\underline{\mathcal{T}}_{\lrate}$ maintains the convenient properties of differentiability, time-homogeneity, and ``Markovian-like'' factorization, when instead of some rate matrix $Q$ we take an arbitrary lower transition rate operator $\lrate$. And, mathematical niceties aside, we are of course not really interested in the trivial case where $\lrate=Q$. Instead, we wish to use our lower transition operator to compute lower expectations with respect to imprecise continuous-time Markov chains. We will find in the next section that this is possible by properly choosing $\lrate$.

\section{Connections Between Lower Transition (Rate) Operators and Imprecise Continuous-Time Markov Chains}\label{sec:connections}

One of the objectives of this paper is to establish a connection between the operator $\lbound_t^s$ that we have just introduced, and the different types of imprecise continous-time Markov chains that were discussed in Section~\ref{sec:iCTMC}. Since the former is derived from a lower transition rate operator $\lrate$ and the latter are derived from a non-empty bounded set of rate matrices $\rateset$, an obvious first step is to investigate the connection between lower transition rate operators and non-empty bounded sets of rate matrices.

\subsection{Connections Between Lower Transition Rate Operators and Sets of Rate Matrices}\label{sec:connections_rate}

% $\rateset$ and $\lrate$. 
%In order to do that, we start by discussing some properties of sets of rate matrices.

We start by considering a non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices. For any $f\in\gamblesX$,
\begin{equation}\label{eq:correspondinglowertrans}
\lrate f\coloneqq\inf\{Qf\colon Q\in\rateset\}\\[2mm]
\end{equation}
is then again an element of $\gamblesX$.\footnote{%Since $\rateset$ is non-empty, the components of $\lrate f$ cannot be $+\infty$.
Since $\rateset$ is bounded,~\ref{N:normAf} implies that, for all $Q\in\rateset$, $\norm{Qf}\leq\norm{Q}\norm{f}\leq\norm{\rateset}\norm{f}<+\infty$. Therefore, and since $\rateset$ is non-empty, the components of $\lrate f$ are bounded below by $-\norm{\rateset}\norm{f}$, which implies that $\lrate f$ is a real-valued function on $\states$.}
Therefore, $\lrate$ is a map from $\gamblesX$ to $\gamblesX$. We call this operator $\lrate$, as defined by Equation~\eqref{eq:correspondinglowertrans}, the \emph{lower envelope} of $\rateset$. It is a matter of straightforward verification to see that $\lrate$ is a lower transition rate operator.

\begin{proposition}\label{prop:lowerenvelopeislowertrans}
For any non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices, the corresponding operator $\lrate\colon\gamblesX\to\gamblesX$, as defined by Equation~\eqref{eq:correspondinglowertrans}, is a lower transition rate operator.
\end{proposition}

\noindent
Inspired by this result, we will also refer to the lower envelope of $\rateset$ as the \emph{lower transition rate operator that corresponds to $\rateset$}. %As we have just seen, every non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices has such a corresponding lower transition rate operator $\lrate$. 
However, this correspondence is not one-to-one. As the following example establishes, different non-empty bounded sets of rate matrices may have the same corresponding lower transition rate operator.

\begin{exmp}\label{example:different_sets_same_lower_rate}
For the sake of simplicity, we assume that the state space $\states\coloneqq\{1,2\}$ has only two elements, which allows us to work with $2\times 2$ matrices. Consider now two rate matrices
\begin{equation*}
A\coloneqq\left[\begin{array}{rr}-1 & 1 \\2 & -2\end{array}\right]\quad\text{and}\quad
B\coloneqq\left[\begin{array}{rr}-3 & 3 \\1 & -1\end{array}\right]\,,
\vspace{7pt}
\end{equation*}
let $C\coloneqq \nicefrac{1}{2}(A+B)$ be their convex mixture, which is clearly also a rate matrix, and use these matrices to define the sets $\rateset_1\coloneqq\{A,B\}$ and $\rateset_2\coloneqq\{A,B,C\}$. Then clearly, $\rateset_1$ and $\rateset_2$ are two different, non-empty and bounded sets of rate matrices. Nevertheless, as we are about to show, the corresponding lower transition rate operators are identical.

Let $\lrate_1$ and $\lrate_2$ be the lower transition rate operators that correspond to $\rateset_1$ and $\rateset_2$, respectively, %We will show that $\lrate_1=\lrate_2$.
and consider any $f\in\gamblesX$. Equation~\eqref{eq:correspondinglowertrans} then implies that $\lrate_1f\leq Af$ and $\lrate_1f\leq Bf$, which in turn implies that $\lrate_1f\leq Cf$. Therefore, by applying Equation~\eqref{eq:correspondinglowertrans} once more, we find that $\lrate_1 f\leq\lrate_2 f$. Hence, since Equation~\eqref{eq:correspondinglowertrans} also trivially implies that $\lrate_2 f\leq\lrate_1 f$, we find that $\lrate_1 f=\lrate_2 f$. Since this is true for any $f\in\gamblesX$, we conclude that $\lrate_1=\lrate_2$.
\exampleend
\end{exmp}
Now, it is not necessarily problematic that different sets can have the same corresponding lower transition rate operator, but it does lead to a natural question: what do these different sets have in common?

Therefore, we next consider some fixed lower transition rate operator $\lrate$.
All the non-empty bounded sets $\rateset$ of rate matrices that have $\lrate$ as their lower envelope then share this common property: they consist of rate matrices $Q$ that dominate $\lrate$, in the sense that $Qf\geq\lrate f$ for all $f\in\gamblesX$. Therefore, each of these sets $\rateset$ is contained in the following set of dominating rate matrices:
\begin{equation}\label{eq:dominatingratematrices}
\rateset_{\lrate}\coloneqq
\left\{
Q\in\mathcal{R}
\colon
Qf\geq\lrate f\text{ for all $f\in\gamblesX$}
\right\}.
\end{equation}
As our next result shows, this set $\rateset_{\lrate}$ is non-empty and bounded, and has $\lrate$ as its lower envelope. Even stronger, the infimum in Equation~\eqref{eq:correspondinglowertrans} is reached---can be replaced by a minimum.

\begin{proposition}\label{prop:dominating_nonempty_bounded}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is non-empty and bounded and, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $\lrate f=Qf$.
\end{proposition}

\noindent
Because of this result, and since---as discussed above---every non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope is a subset of $\rateset_{\lrate}$, it follows that $\rateset_{\lrate}$ is the largest non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope.
Furthermore, as we will show in Proposition~\ref{prop:dominatingproperties} below, this set $\rateset_{\lrate}$ is also closed and convex, and has \emph{separately specified rows}. We say that a set of rate matrices has separately specified rows if it is closed under taking arbitrary combinations of rows from its elements.

\begin{definition}\label{def:separatelyspecifiedrows}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ has separately specified rows if
\begin{equation*}
\rateset=\left\{
Q\in\mathcal{R}
\colon
(\forall x\in\states)~Q(x,\cdot)\in\rateset_x\right\},
\end{equation*}
where, for every $x\in\states$, $\rateset_x\coloneqq\{Q(x,\cdot)\colon Q\in\rateset\}$ is some set of rows from which the $x$-th row of the rate matrices in $\rateset$ are taken.
\end{definition}

\begin{proposition}\label{prop:dominatingproperties}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is closed and convex, and has separately specified rows.
\end{proposition}

\noindent
These additional properties characterize $\rateset_{\lrate}$ completely, in the sense that no other set satisfies them.

\begin{proposition}\label{prop:dominating_unique_characterization}
Consider any non-empty, bounded, closed and convex set of rate matrices $\rateset\subseteq\mathcal{R}$ with separately specified rows that has $\lrate$ as its lower envelope. Then $\rateset=\rateset_{\lrate}$.
\end{proposition}

\begin{exmp}\label{ex:dominatingset}
Let $\rateset_1$ and $\rateset_2$ be constructed as in Example~\ref{example:different_sets_same_lower_rate}, and let $\lrate\coloneqq\lrate_1=\lrate_2$ be their common lower transition rate operator. As we are about to show, the corresponding set of dominating rate matrices $\rateset_{\lrate}$ is then equal to
\begin{equation}\label{eq:ex:dominatingset:globaldef}
\rateset^* \coloneqq \{Q\in\mathcal{R}\,:\,(\forall x\in\states)\, Q(x,\cdot)\in\rateset_x\},
\end{equation}
where, for all $x\in\states$, $\rateset_x$ is given by
\begin{equation*}
\rateset_x \coloneqq \left\{\lambda A(x,\cdot)+(1-\lambda)B(x,\cdot)\,:\,\lambda\in[0,1]\right\}.
\end{equation*}

First of all, for any $Q\in\rateset^*$, $f\in\gamblesX$ and $x\in\states$, it follows from Equation~\eqref{eq:ex:dominatingset:globaldef} that there is some $\lambda\in[0,1]$ such that
\begin{equation*}
[Qf](x)=\lambda[Af](x)+(1-\lambda)[Bf](x)
\geq\lambda[\lrate f](x)+(1-\lambda)[\lrate f](x)
=[\lrate f](x),
\end{equation*}
where the inequality holds because $\lrate$ is the lower envelope of $\rateset_1$. Since this is true for all $x\in\states$, we find that $Qf\geq\lrate f$. Since this is true for all $f\in\gamblesX$ and all $Q\in\rateset^*$, it follows that $\rateset^*\subseteq\rateset_{\lrate}$. 


Let now $\lrate^*$ be the lower transition rate operator that corresponds to $\rateset^*$. Consider any $f\in\gamblesX$. Then, because $\rateset^*\subseteq\rateset_{\lrate}$, we find that $\lrate^*f\geq\lrate f$. Similarly, since $\rateset_1$ is clearly a subset of $\rateset^*$, we find that $\lrate^*f\leq\lrate_1f$. Since $\lrate_1=\lrate$, it follows that $\lrate f\leq \lrate^*f\leq \lrate f$, and because this holds for any $f\in\gamblesX$, we conclude that $\lrate^*=\lrate$.


% Recall from Example~\ref{example:different_sets_same_lower_rate} that for all $f\in\gamblesX$, it holds that either $\lrate f=Af$, or $\lrate f=Bf$. Using a similar argument as we used there for the rate matrix $C$, it is clear that any convex combination $Q_\lambda\coloneqq \lambda A+(1-\lambda)B$, with $\lambda\in[0,1]$, is a rate matrix that dominates $\lrate$. Furthermore, because we there found that $[Af](1)\leq [Bf](1)$ if and only if $[Af](2)\leq[Bf](2)$, any matrix constructed by combining rows from $A$ and $B$ will dominate $\lrate$. For example, let $D$ be a rate matrix such that $D(1,\cdot)\coloneqq A(1,\cdot)$ and $D(2,\cdot)\coloneqq B(2,\cdot)$. Then, if $Af\leq Bf$, it also holds that $Af\leq Df$. Similarly, if $Bf\leq Af$, then also $Bf\leq Df$. A similar argument shows that the same is true for any matrix constructed by combining rows from different convex combinations of $A$ and $B$.

% From these ideas, we first construct two sets of rows. For all $x\in\states$, let
% \begin{equation*}
% \rateset_x \coloneqq \left\{\lambda A(x,\cdot)+(1-\lambda)B(x,\cdot)\,:\,\lambda\in[0,1]\right\}\,.
% \end{equation*}
% Let now $\rateset \coloneqq \{Q\in\mathcal{R}\,:\,(\forall x\in\states)\, Q(x,\cdot)\in\rateset_x\}$. Then, for all $Q\in\rateset$, $Q$ is such that
% \begin{equation*}
% Q=\left[\begin{array}{rr}-a & a \\ b& -b\end{array}\right]\,,\quad\text{where $a\in[1,3]$ and $b\in[1,2]$.}
% \end{equation*}
% The converse also holds; for all $a\in[1,3]$ and $b\in[1,2]$, there is a $Q\in\rateset$ that takes the above form. From this, it is clear that for all $Q\in\rateset$, it holds that $\lrate f\leq Qf$, for all $f\in\gamblesX$. 
%Furthermore, $\rateset^1\subset\rateset$ and $\rateset^2\subset\rateset$. Hence, $\rateset$ has $\lrate$ as its corresponding lower transition rate operator. 

It remains to determine that $\rateset^*=\rateset_{\lrate}$. One way to verify this is to consider any $Q'\notin\rateset^*$ and show that there is some $f\in\gamblesX$ and some $x\in\states$ such that $[Q'f](x)<[\lrate f](x)$. We leave this method as an exercise for the reader, and note instead the following. $\rateset^*$ is clearly non-empty, bounded, closed, convex, has separately specified rows and, as we have just shown, has $\lrate$ as its lower transition rate operator. Therefore, by Proposition~\ref{prop:dominating_unique_characterization}, we find that indeed $\rateset=\rateset_{\lrate}$.
\exampleend
\end{exmp}

We conclude from all of this that non-empty bounded sets of rate matrices are more informative than lower transition rate operators. Different non-empty bounded sets of rate matrices $\rateset$ may have the same lower transition rate operator $\lrate$ and therefore, in general, knowledge of $\lrate$ does not suffice to reconstruct $\rateset$; we can only reconstruct an outer approximation $\rateset_{\lrate}$, which is guaranteed to include $\rateset$. This changes if, besides non-empty and bounded, $\rateset$ is also closed and convex and has separately specified rows. In that case, $\lrate$ serves as an alternative representation for $\rateset$ because, since $\rateset=\rateset_{\lrate}$, we can use $\lrate$ to reconstruct $\rateset$. In other words: there is a one-to-one correspondence between lower transition rate operators and non-empty, bounded, closed and convex sets of rate matrices that have separately specified rows.

%\section{Imprecise Continuous-Time Markov Chains}\label{sec:imp_markov}

%\subsection{New Version}

%This section contains the new, simplified proofs.

\subsection{Computing Lower Expectations with Lower Transition Operators}\label{sec:single_var_lower_exp}

Having established a strong connection between the operator $\lrate$ and non-empty bounded sets of rate matrices $\rateset$, we will now turn to the connection between the operator $L_t^s$ and lower expectations $\underline{\mathbb{E}}$ with respect to sets of (non-)Markov processes. Specifically, we will in this section focus on the lower expectation of functions defined on the state space at a single point in time. In Section~\ref{sec:funcs_multi_time_points} we will then use and generalize these results when we consider functions defined on the state space at multiple time points.

As the following result shows, the operator $L_t^s$ corresponding to a lower transition rate operator $\lrate$ computes a lower bound on the expectation of a function $f\in\gamblesX$, with respect to the set $\wprocesses_\rateset$ of well-behaved stochastic processes consistent with any set $\rateset$ of which $\lrate$ is the lower envelope.

\begin{proposition}\label{theorem:nonmarkov_single_var_lower_bounded}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$, and let $\smash{\underline{\mathcal{T}}_{\lrate}}$ be the corresponding lower transition operator system. Then, for any $P\in\wprocesses_\rateset$, any $t,s\in\realsnonneg$ such that $t\leq s$, any $u\in\mathcal{U}_{<t}$, any $x_t\in\states$ and $x_u\in\states^u$, and any $f\in\gamblesX$,
\begin{equation*}
[L_{t}^s f](x_t) \leq \mathbb{E}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u]\,.
\end{equation*}
\end{proposition}

As this result shows, $L_t^sf$ is a lower bound on the expectation of a function $f\in\gamblesX$, with respect to a set of stochastic processes $\wprocesses_\rateset$ induced by some non-empty bounded set of rate matrices $\rateset$. Our next result establishes that this bound is tight if $\rateset$ also has separately specified rows. Specifically, we show that $L_t^sf$ can then be approximated to arbitrary precision by carefully choosing a Markov process $P$ from the set $\wmprocesses_\rateset$.

\begin{proposition}\label{theorem:lower_markov_bound_is_tight}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then for all $t,s\in\realsnonneg$ such that $t\leq s$, all $f\in\gamblesX$, and all $\epsilon\in\realspos$, there is a $P\in\wmprocesses_{\rateset}$ such that
\begin{equation*}
\norm{\lbound_t^sf-\mathbb{E}[f(X_s)\,\vert\,X_t]} < \epsilon\,.
\end{equation*}
\end{proposition}

Together, Propositions~\ref{theorem:nonmarkov_single_var_lower_bounded} and~\ref{theorem:lower_markov_bound_is_tight} establish a strong connection between $L_t^s$ and lower expectations $\underline{\mathbb{E}}$. In particular, for functions defined on a single point in time, they turn out to be equivalent, as shown by the result below.

\begin{corollary}\label{cor:lower_operator_is_infimum}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $t,s\in\realsnonneg$ such that $t\leq s$, all $u\in\mathcal{U}_{<t}$, $x_u\in\states^u$ and $x_t\in\states$, and all $f\in\gamblesX$:
\begin{align*}
\left[L_t^sf\right](x_t) = \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u] 
=
\underline{\mathbb{E}}^{\mathrm{W}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u].
 %&= \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x_t]\,,
\end{align*}\\[-25pt]
% and furthermore,
% \begin{equation*}
% \left[L_t^sf\right](x) = \underline{\mathbb{E}}^{\mathrm{W}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x,X_u=x_u]\,.
% \end{equation*}
\end{corollary}

Thus, we find that there is a correspondence between the operator $L_t^s$ and lower expectations with respect to sets of stochastic processes. However, we also find that this correspondence is not one-to-one. Clearly, Corollary~\ref{cor:lower_operator_is_infimum} shows that $L_t^s$ computes the lower expectation for the different sets $\wmprocesses_\rateset$ and $\wprocesses_\rateset$. Furthermore, we know from Section~\ref{sec:connections_rate} that different sets $\rateset_1$ and $\rateset_2$ may have the same corresponding lower transition rate operator $\lrate$. Hence, whenever this is the case, $L_t^s$ would---assuming the conditions in Corollary~\ref{cor:lower_operator_is_infimum} are met by both $\rateset_1$ and $\rateset_2$---then compute the lower expectation with respect to both $\wmprocesses_{\rateset_1}$ and $\wmprocesses_{\rateset_2}$ (and also, furthermore, for $\wprocesses_{\rateset_1}$ and $\wprocesses_{\rateset_2}$). An obvious question, therefore, would be what the largest set of stochastic processes is for which $L_t^s$ computes the lower expectation.

%*** This needs some rewording ***
%
%One somewhat unsurprising result is therefore that $L_t^s$ computes the lower expectation of functions $f\in\gamblesX$ with respect to sets of Markov processes $\mprocesses_\rateset$. The reason that this is to be expected is the previously established correspondence between $L_t^s$ and the solution of the differential equation introduced in {\bf DAMJANREF}, which was there shown to compute exactly this quantity.
%
%A rather more surprising result, perhaps, is that this \emph{same} operator also computes lower expectations with respect to sets $\processes_\rateset$ of non-Markov processes. Our next result strengthens this connection between $L_t^s$ and sets of non-Markov processes.

Recall from Section~\ref{sec:connections_rate} that for a given lower transition rate operator $\lrate$, its set of dominating rate matrices $\rateset_{\lrate}$ is---by definition---the largest set of rate matrices which has $\lrate$ as its lower envelope. The following result shows that the set $\wprocesses_{\rateset_{\lrate}}$ of well-behaved stochastic processes consistent with $\rateset_{\lrate}$ is the largest set of stochastic processes for which $L_t^s$ computes the lower expectation.

\begin{theorem}\label{theo:dominating_rate_processes_max_set}
Let $\lrate$ be an arbitrary lower transition rate operator, with $\rateset_{\lrate}$ its set of dominating rate matrices, and let  $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, the set $\wprocesses_{\rateset_{\lrate}}$ is the largest set of stochastic processes for which, for all $t,s\in\realsnonneg$ such that $t\leq s$, all $u\in\mathcal{U}_{<t}$, all $x_t\in\states$ and $x_u\in\states^u$, and all $f\in\gamblesX$,
\begin{equation*}
[L_t^sf](x_t) = \underline{\mathbb{E}}_{\,\rateset_{\lrate}}^{\mathrm{W}}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u]\,.
\end{equation*}
\end{theorem}

One conclusion that we can draw from this is that if $\rateset$ is any non-empty and bounded set of rate matrices that has separately specified rows, and lower transition rate operator $\lrate$, that then
\begin{equation*}
L_{t}^sf = \underline{\mathbb{E}}_{\,\rateset}^{\mathrm{WM}}[f(X_s)\,\vert\,X_{t}] = \underline{\mathbb{E}}_{\,\rateset}^{\mathrm{W}}[f(X_s)\,\vert\,X_t,X_u=x_u] = \underline{\mathbb{E}}_{\,\rateset_{\lrate}}^{\mathrm{W}}[f(X_s)\,\vert\,X_t,X_u=x_u]\,.
\end{equation*}
In other words, for functions defined on a single time point, the lower expectations with respect to the sets $\wmprocesses_{\rateset}$, $\wprocesses_{\rateset}$, and $\wprocesses_{\rateset_{\lrate}}$ are all the same! Consequently, the lower expectation will also be the same for any set $\mathcal{P}\subset\processes$ that satisfies $\wmprocesses_\rateset \subseteq \mathcal{P} \subseteq \wprocesses_{\rateset_{\lrate}}$.

Another way of putting this is that, if one is interested in the lower expectation with respect to, say, $\wprocesses_{\rateset}$, and if $\rateset$ is non-empty and bounded with separately specified rows, that convexifying or closing $\rateset$ will not influence the lower expectation for functions defined on a single time point. This is due to Proposition~\ref{prop:dominating_unique_characterization}, which states that if you do this, you obtain $\rateset=\rateset_{\lrate}$.

%Similar to our results from Section~\ref{sec:connections_rate}, we conclude from this that sets of stochastic processes are more informative than their corresponding lower expectations. In general, different sets $\wprocesses_\rateset$ of stochastic processes can have their lower expectations computed by the same lower transition operator $L_t^s$, and hence they necessarily have the same lower expectation. 
%
%Therefore, even if $L_t^s$ computes the lower expectation with respect to $\wprocesses_\rateset$, knowledge of $L_t^s$ does not in general suffice to reconstruct $\wprocesses_\rateset$. We can, however, construct an outer approximation which is guaranteed to contain $\wprocesses_\rateset$, as follows. From Proposition~\ref{prop:lower_transition_has_deriv}, we can find the lower transition rate operator $\lrate$ that characterizes $L_t^s$, using $\lim_{\Delta\to0^+}\nicefrac{1}{\Delta}(L_t^{t+\Delta}-I)=\lrate$. Using this operator $\lrate$, we can construct the set $\rateset_{\lrate}$ of rate matrices that dominate $\lrate$. Finally, we can construct the set $\wprocesses_{\rateset_{\lrate}}$. Because $L_t^s$ computes the lower expectation with respect to $\wprocesses_\rateset$, Theorem~\ref{theo:dominating_rate_processes_max_set} now guarantees that $\wprocesses_\rateset\subseteq\wprocesses_{\rateset_{\lrate}}$.
%
%This changes if the set $\wprocesses_\rateset$ is not only well-behaved, includes non-Markov processes, and is such that $\rateset$ is non-empty and bounded with separately specified rows, but furthermore is such that $\rateset$ is closed and convex. In that case, $L_t^s$ serves as an alternative characterization of $\wprocesses_\rateset$, because then, by Proposition~\ref{prop:dominating_unique_characterization}, it holds that $\wprocesses_\rateset=\wprocesses_{\rateset_{\lrate}}$.
%
%**** ergens voelt het wel nice om die karakterisatie in termen van de set $\wprocesses_\rateset$ zelf te doen, in plaats van in termen van $\rateset$. een resultaat zoals hieronder staat (in commentaar, in de latex file) zou daarvoor kunnen helpen.

%**** misschien nog zoiets toevoegen ergens? voor elke $\mathcal{P}\subset\processes$, (misschien rekening houden met $t,s,x_u$ om geldig te laten zijn),
%\begin{equation*}
%\mathcal{T}_{\mathcal{P}} \coloneqq \left\{T_{t,\,x_u}^s\,:\,P\in\mathcal{P}\right\}
%\end{equation*}
%
%dan, voor $\mathcal{T}_{\wprocesses_\rateset}$:
%\begin{align*}
%\rateset\neq \emptyset &\Leftrightarrow \mathcal{T}_{\wprocesses_\rateset}\neq\emptyset \\
%\rateset~\text{is closed} &\Leftrightarrow \mathcal{T}_{\wprocesses_\rateset}~\text{is closed} \\
%\rateset~\text{is convex} &\Leftrightarrow \mathcal{T}_{\wprocesses_\rateset}~\text{is convex} \\
%\rateset~\text{has s.s.r.} &\Leftrightarrow \mathcal{T}_{\wprocesses_\rateset}~\text{has s.s.r.}
%\end{align*}
%
%vervolgens laten zien dat als voor $\mathcal{P}\subset\wprocesses$ de set $\mathcal{T}_{\mathcal{P}}$ al die eigenschappen heeft, dat dan $\mathcal{P}=\wprocesses_{\rateset}$, waarbij $\rateset$ al die eigenschappen heeft en verder ook bounded is.
%
%dan als voor $\mathcal{P}\subset\wprocesses$ de set $\mathcal{T}_{\mathcal{P}}$ al die eigenschappen heeft, zijn $\mathcal{P}$, $L_t^s$ (en $\lrate$) allemaal even sterk, in de zin dat
%\begin{align*}
%\mathcal{P}&=\wprocesses_{\rateset} \Rightarrow \rateset \Rightarrow \lrate \Rightarrow \underline{\mathcal{T}}_{\lrate} \Rightarrow L_t^s \\
%L_t^s &\Rightarrow \lim_{\Delta\to0^+}\frac{1}{\Delta}(L_t^{t+\Delta} - I)=\lrate \Rightarrow \rateset_{\lrate}=\rateset \Rightarrow \wprocesses_{\rateset} = \mathcal{P} \\
%\lrate &\Rightarrow \underline{\mathcal{T}}_{\lrate} \Rightarrow L_t^s,\quad\text{and,}\quad \lrate\Rightarrow \rateset_{\lrate} \Rightarrow \wprocesses_{\rateset_{\lrate}} = \mathcal{P}
%\end{align*}

\section{Functions Defined on Multiple Time Points}\label{sec:funcs_multi_time_points}

*** I removed this from Section~\ref{sec:multivar_notation} because $X_t$ was not yet defined at that point: ``For any such function $f\in\gambles(\states^{u\cup\{s\}})$, we will write $f(x_u,X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ specific to the joint state assignment $x_u\in\states^u$. Thus, $f(x_u,X_s)$ corresponds to a $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_u,x_s)$ for all $x_s\in\states^{\{s\}}$.'' I guess that this section here would be a good place to incorporate it in... ***

Having shown in Section~\ref{sec:single_var_lower_exp} that the operator $L_t^s$ computes lower expectations for functions $f\in\gamblesX$ defined on a single point in time, we will now turn our attention to functions defined on multiple time points. Because we are considering \emph{conditional} expectations, where the conditioning is done with respect to states in a (non-)Markov chain's history, it makes sense to distinguish between two different classes of functions defined at multiple time points. 

In Section~\ref{sec:function_single_future_multiple_past}, we first consider functions $f\in\gambles(\states^{u\cup\{s\}})$, and lower expectations of the form $\underline{\mathbb{E}}[f(x_u,X_s)\,\vert\,X_u=x_u]$. Thus, although the function $f$ depends on multiple time points, all but one of these time points are taken to be fixed due to the conditioning of the lower expectation.

This is then generalized in Section~\ref{sec:decomposition} to arbitrary functions $f\in\gambles(\states^{u\cup v})$ and lower expectations $\underline{\mathbb{E}}[f(x_u,X_v)\,\vert\,X_u=x_u]$, where the expectation is taken over an arbitrary number of time points.

We finally apply these results with some worked examples in Section~\ref{sec:tractability}, where we also provide some comments on tractability and computational aspects.

%First, in Section~\ref{sec:function_single_future_multiple_past}, we will consider functions defined on a single time point in a chain's future, and multiple time points in the chain's history. Thus, we will consider functions $f\in\gambles(\states^{u\cup\{s\}})$, and lower expectations of the form
%\begin{equation*}
%\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
%\end{equation*}
%We will see that it is a straightforward implication of our results from Section~\ref{sec:single_var_lower_exp} that such lower expectations are computable using $L_t^s$, both with respect to sets of Markov chains and with respect to sets of non-Markov chains.
%
%In Section~\ref{sec:decomposition} we will generalize this to functions defined on multiple time points in a chain's future and history, considering functions $f\in\gambles(\states^{u\cup v})$ and lower expectations of the form
%\begin{equation*}
%\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
%\end{equation*}
%As we will see, for functions of this kind the lower expectations with respect to sets of non-Markov chains no longer correspond to those taken with respect to sets of Markov chains. One of the main results of this paper, however, is that we can still use the operator $L_t^s$ to compute lower expectations of this form if taken with respect to sets of non-Markov chains. We will see that this is because the optimization problem involved in computing lower expectations in some sense becomes simpler when we drop the Markov assumption.
%
%Finally, in Section~\ref{sec:tractability}, we will show that although $L_t^s$ provides us with a way to compute such lower expectations, doing so for general functions $f\in\gambles(\states^{u\cup v})$ is still computationally intractable. However, we then provide algorithms to tractably compute lower expectations for large and practically useful subclasses of $\gambles(\states^{u\cup v})$.

\subsection{Multi-Variable Functions on a Single Point in the Future}\label{sec:function_single_future_multiple_past}

We start by considering functions $f\in\gambles(\states^{u\cup\{s\}})$ defined on a single time point $s$ in a (non-)Markov chain's future, and multiple time points $u=t_0,\ldots,t_n$ in a chain's history. Observe that for any stochastic process $P$, the conditional expectation of such a function satisfies, for any $x_u\in\states^u$,
\begin{equation*}
\mathbb{E}[f(X_u,X_s)\,\vert\,X_u=x_u] = \mathbb{E}[f(x_u,X_s)\,\vert\,X_u=x_u]\,.
\end{equation*}
Therefore, and because a lower expectation is an infimum over such expectations, we find that also
\begin{equation*}
\underline{\mathbb{E}}[f(X_u,X_s)\,\vert\,X_u=x_u] = \underline{\mathbb{E}}[f(x_u,X_s)\,\vert\,X_u=x_u]\,.
\end{equation*}

Now, recall our notation from Section~\ref{sec:multivar_notation}; the function $f(x_u,X_s)$ is the restriction of a function $f\in\gambles(\states^{u\cup\{s\}})$ to the state $\states^{\{s\}}$, specific to the state assignment $x_u\in\states^u$. In other words, for all $x_u\in\states^u$, there is some function $g_{x_u}\in\gambles(\states^{\{s\}})$, such that $g_{x_u}(x_s)=f(x_u,x_s)$ for all $x_s\in\states^{\{s\}}$.
Hence, the problem of computing $\underline{\mathbb{E}}[f(x_u,X_s)\,\vert\,X_u=x_u]$ is in fact equivalent to computing $\underline{\mathbb{E}}[g_{x_u}(X_s)\,\vert\,X_u=x_u]$. 

Furthermore, because $g_{x_u}\in\gambles(\states^{\{s\}})=\gamblesX$, we already know from Section~\ref{sec:single_var_lower_exp} how to do this! In particular, we can do this by computing 
\begin{equation*}
\left[L_{t_n}^sg_{x_u}\right](x_{t_n}) = \underline{\mathbb{E}}_\rateset^{\mathrm{WM}}[g_{x_u}(X_s)\,\vert\,X_{t_n}=x_{t_n}] = \underline{\mathbb{E}}_\rateset^{\mathrm{W}}[g_{x_u}(X_s)\,\vert\,X_u=x_u]\,.
\end{equation*}
%recall from Section~\ref{sec:multivar_notation} that for operators from $\gamblesX$ to $\gamblesX$, and functions $f\in\gambles(\states^{u\cup\{s\}})$, we write
%\begin{equation*}
%\left[L_{t_n}^sf\right](x_u) \equiv \left[L_{t_n}^sf(x_u,X_s)\right](x_{t_n}) = \left[L_{t_n}^sg_{x_u}(X_s)\right](x_{t_n})\,.
%\end{equation*}
In order to unify our notation, we therefore stipulate the following convention. For any $s\in\realspos$ and any $u\in\mathcal{U}_{<s}$ with $u\neq\emptyset$ such that $u=t_0,\ldots,t_n$, we allow $L_{t_n}^s$ to be applied to any $f\in\gambles(\states^{u\cup\{s\}})$, by applying it to the restriction of $f$ to the \emph{latest} time point at which it is defined---the time point $s$, in this case. Because this restriction depends on the state assignment $x_u$ at the other time points, the result is a function $[L_{t_n}^sf]\in\gambles(\states^u)$. In short, we stipulate for any $f\in\gambles(\states^{u\cup\{s\}})$ and any $x_u\in\states^u$, that
\begin{equation*}
[L_{t_n}^sf](x_u) \coloneqq [L_{t_n}^sg_{x_u}](x_{t_n}) \coloneqq [L_{t_n}^sf(x_u,X_s)](x_{t_n})\,.
\end{equation*}

The following now formalizes the fact that our previous results also apply to functions in $\gambles(\states^{u\cup\{s\}})$.

\begin{corollary}\label{cor:inf_works_for_single_future_var}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. Then, for all $s\in\realspos$, all $u\in\mathcal{U}_{<s}$ such that $u\neq\emptyset$, all $x_u\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_{t_n}^s f\right](x_u) = \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(x_u,X_s)\,\vert\,X_u=x_u]\,,
\end{equation*}
and furthermore,
\begin{equation*}
\left[L_{t_n}^s f\right](x_u) = \underline{\mathbb{E}}^\mathrm{W}_{\,\rateset}[f(x_u,X_s)\,\vert\,X_u=x_u]\,.
\end{equation*}
\end{corollary}
\begin{proof}
*** still need to check this *** Immediate consequence of Corollary~\ref{cor:lower_operator_is_infimum}.
\end{proof}

%Recall our notation from Section~\ref{sec:multivar_notation}; we write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ for a specific state assignment $(x_{t_0},\ldots,x_{t_n})$, and have defined
%\begin{equation*}
%\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n}) \equiv \left[L_{t_n}^sf(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
%\end{equation*}
%The following results are now direct implications of, and analogies to, our results from Section~\ref{sec:single_var_lower_exp}.
%
%\begin{proposition}\label{prop:multi_var_single_future_bounded}
%Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for any $P\in\wprocesses_\rateset$, any $s\in\realsnonneg$, any $u\in\mathcal{U}_{<s}$, any $x_u\in\states^u$, and any $f\in\gambles(\states^{u\cup\{s\}})$,
%\begin{equation*}
%\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]\,.
%\end{equation*}
%\end{proposition}
%
%\begin{proposition}\label{prop:multi_var_single_future_tight}
%Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $s\in\realsnonneg$, all $u\in\mathcal{U}_{<s}$, all $x_u\in\states^u$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $\epsilon\in\realspos$, there is a $P\in\wmprocesses_\rateset$ such that
%\begin{equation*}
%\abs{\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n}) - \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right]} < \epsilon\,.
%\end{equation*}
%\end{proposition}
%
%Note that this result is weaker than the corresponding Theorem~\ref{theorem:lower_markov_bound_is_tight} in Section~\ref{sec:single_var_lower_exp}. Specifically, this says that for a given history $x_u\in\states^u$, there is a $P\in\wmprocesses_\rateset$ that approaches $\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n})$. This does not imply that there is a $P\in\wmprocesses_\rateset$ that approaches $\left[L_{t_n}^sf\right](x_{t_0},\ldots,x_{t_n})$ for \emph{all} histories! 
%
%We will see in Section~\ref{sec:decomposition} that this is exactly the reason that computing lower expectations with respect to sets of non-Markov processes is ``easy''; by dropping the Markov assumption, the corresponding optimization problems become solvable locally with respect to a given history. In contrast, the optimization over sets of Markov processes must there be done globally with respect to all possible histories, because they do not allow for minimizing selections specific to a given trajectory.
%
%For our present purposes, Proposition~\ref{prop:multi_var_single_future_tight} is still strong enough to imply the following result.
%
%\begin{proposition}
%In essentie, voor alle $f\in\gambles(\states^{u\cup\{s\}})$ en alle $\epsilon\in\realspos$, is er een $P\in\mprocesses_\rateset$ zodat
%\begin{equation*}
%\norm{L_t^s f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%{\bf TODO} This is immediate.
%\end{proof}
%
%\begin{corollary}\label{cor:inf_works_for_single_future_var}
%Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $s\in\realsnonneg$, all $u\in\mathcal{U}_{<s}$, all $x_u\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
%\begin{equation*}
%\left[L_{t_n}^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,,
%\end{equation*}
%and furthermore,
%\begin{equation*}
%\left[L_{t_n}^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^\mathrm{W}_{\,\rateset}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}]\,.
%\end{equation*}
%\end{corollary}
%
%Thus, we see that the operator $L_t^s$ can also be used to compute lower expectations of functions $f\in\gambles(\states^{u\cup\{s\}})$, both with respect to sets of Markov processes and with respect to sets of non-Markov processes. We will now turn to functions defined on multiple time points in a process' future, where we will find this correspondence to no longer hold.
%

\subsection{Multi-Variable Functions on Multiple Points in the Future}\label{sec:decomposition}

We now consider functions $f\in\gambles(\states^{u\cup v})$, where $u=t_0,\ldots,t_n$ is a sequence of time points in a process' history, and $v=s_0,\ldots,s_m$ is a sequence of time points in a process' future; hence, we assume $s_0>t_n$. For functions of this kind, it---surprisingly---turns out that computing the lower expectation with respect to $\wprocesses_\rateset$ is easier than for $\wmprocesses_\rateset$. We will show in this section how to do this for the former set, and provide in the next section a counter example for the latter.

We have seen in the previous sections that we can use the operator $L_t^s$ to compute lower expectations of functions $f\in\gamblesX$ and $f\in\gambles(\states^{u\cup\{s\}})$, whenever $\rateset$ is non-empty, bounded, and has separately specified rows. We here show that if $\rateset$ is additionally convex, then we can do the same for functions $f\in\gambles(\states^{u\cup v})$.

Hence, we are now interested in computing the quantity
\begin{equation*}
\underline{\mathbb{E}}_\rateset^{\mathrm{W}}[f(x_u,X_v)\,\vert\,X_u=x_u]\,.
\end{equation*}
Recall now from Theorem~\ref{theorem:decomposition_multivar} that if $\rateset$ is convex, we can decompose this lower expectation as
\begin{equation}\label{eq:nested_lower_exp_single_step}
\underline{\mathbb{E}}_\rateset^{\mathrm{W}}[f(x_u,X_v)\,\vert\,X_u=x_u] = \underline{\mathbb{E}}_\rateset^{\mathrm{W}}\bigl[ \underline{\mathbb{E}}_\rateset^{\mathrm{W}}[f(x_u,X_v)\,\vert\,X_u=x_u,X_{v\setminus\{s_m\}}] \,\big\vert\,X_u=x_u\bigr]\,,
\end{equation}
where we use the notation $v=s_0,\ldots,s_m$. Observe that the inner lower expectation on the right-hand side of this equality is conditioned on all the time points $u$ and $v\setminus\{s_m\}$. Therefore, this lower expectation is only taken at a single time $s_m$. It therefore follows from Corollary~\ref{cor:inf_works_for_single_future_var} that, for all $x_{v\setminus\{s_m\}}\in\states^{v\setminus\{s_m\}}$,
\begin{equation*}
\left[L_{s_{m-1}}^{s_m}f\right]\left(x_u,x_{v\setminus\{s_m\}}\right) = \underline{\mathbb{E}}_\rateset^{\mathrm{W}}[f(x_u,X_v)\,\vert\,X_u=x_u,X_{v\setminus\{s_m\}}=x_{v\setminus\{s_m\}}]\,.
\end{equation*}
Because this holds for all $x_{v\setminus\{s_m\}}\in\states^{v\setminus\{s_m\}}$, we can now replace the inner lower expectation in Equation~\eqref{eq:nested_lower_exp_single_step}, to obtain
\begin{equation*}
\underline{\mathbb{E}}_\rateset^{\mathrm{W}}[f(x_u,X_v)\,\vert\,X_u=x_u] = \underline{\mathbb{E}}_\rateset^{\mathrm{W}}\left[ \left[L_{s_{m-1}}^{s_m}f\right]\left(x_u,X_{v\setminus\{s_m\}}\right) \,\big\vert\,X_u=x_u\right]\,.
\end{equation*}
If we now recursively apply this process, we can repeatedly factor out the latest remaining time point $s_{m-1}, s_{m-2},\ldots,s_{0}$, and at each step replace the inner lower expectation with the operator $L_{s_{m-2}}^{s_{m-1}},L_{s_{m-3}}^{s_{m-2}},\ldots,L_{t_n}^{s_0}$. Because $v$ is finite, this process eventually stops, and we then obtain the following result.

%As the next example shows, the lower expectation of such functions, when taken with respect to a set $\wmprocesses_\rateset$ of Markov processes, no longer necessarily corresponds to the lower expectation taken with respect to a set $\wprocesses_\rateset$ of non-Markov processes.
%
%\begin{exmp}
%{\bf TODO} Example that sometimes $\underline{\mathbb{E}}^\mathrm{M}\neq \underline{\mathbb{E}}$ for functions $f\in\gambles(\states^{u\cup v})$.
%\exampleend
%\end{exmp}


%*** blabla, uitleg dat dit niet werkt over set van Markov chains

%*** blabla, dit kunnen we recursief doen totdat elke lower expectation nog maar een tijdpunt bevat. ergo, als $\rateset$ ook s.s.r. heeft, dan

\begin{corollary}\label{cor:composition_lower_trans}
Let $\rateset$ be an arbitrary non-empty, bounded, and convex set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the corresponding lower transition operator system. 

Consider any $t\in\realsnonneg$, any $u\in\mathcal{U}_{<t}$ and $v\in\mathcal{U}_{\geq t}$ with $u=t_0,\ldots,t_n$ and $v={s_0,\ldots,s_m}$. Then, for any $f\in\gambles(\states^{u\cup v})$ and any $x_u\in\states^u$,
\begin{equation}\label{eq:composition_lower_trans}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_u) = \underline{\mathbb{E}}^{\mathrm{W}}_{\,\rateset}[f(x_u,X_v)\,\vert\,X_u=x_u]\,.
\end{equation}
\end{corollary}
\begin{proof}
Immediate consequence of Theorem~\ref{theorem:decomposition_multivar} and Corollary~\ref{cor:inf_works_for_single_future_var}.
\end{proof}

Observe, therefore, that we can use our lower transition operator to compute lower expectations of arbitrary functions $f\in\gambles(\states^{u\cup v})$ with respect to the set $\wprocesses_\rateset$, whenever $\rateset$ is at least non-empty, bounded, convex, and has separately specified rows. Due to Proposition~\ref{prop:dominating_unique_characterization}, this requires that $\rateset\supseteq\text{int}(\rateset_{\lrate})$, the interior of the set of rate matrices $\rateset_{\lrate}$ that dominate the lower envelope $\lrate$ of $\rateset$. In other words, for arbitrary functions, the smallest set of stochastic processes for which our lower transition operator can compute the lower expectation is given by $\wprocesses_{\text{int}(\rateset_{\lrate})}$.

If $\rateset$ is furthermore closed, then by Proposition~\ref{prop:dominating_unique_characterization}, we have that $\rateset=\rateset_{\lrate}$. In that case, the lower expectation is taken with respect to $\wprocesses_\rateset=\wprocesses_{\rateset_{\lrate}}$ which, by Theorem~\ref{theo:dominating_rate_processes_max_set}, is exactly the largest set of stochastic processes for which our lower transition operator computes the lower expectation.

The difference between $\wprocesses_{\text{int}(\rateset_{\lrate})}$ and $\wprocesses_{\rateset_{\lrate}}$ is that for the latter set, due to Proposition~\ref{prop:lower_expectation_reached_if_Q_closed}, Corollary~\ref{cor:inf_works_for_single_future_var} and Theorem~\ref{theo:aanelkaarplakken}, the left-hand side of Equation~\eqref{eq:composition_lower_trans} is always reached by some $P\in\wprocesses_{\rateset_{\lrate}}$, and hence is in fact a minimum.

%**** rewrite most things (everything) below
%
%An obvious question is therefore what the operator $L_t^s$ computes for functions of this form, as it clearly cannot compute both $\underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}$ and $\underline{\mathbb{E}}^\mathrm{W}_{\,\rateset}$. As we will see below, it turns out that we can use $L_t^s$ to compute the lower expectation of such functions with respect to sets of non-Markov processes. We start by showing that, using a composition of these operators, we can compute a lower bound with respect to a set $\wprocesses_\rateset$.
%\begin{proposition}\label{prop:multivar_bounded}
%Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for any $P\in\wprocesses_\rateset$, any $s,s'\in\realsnonneg$ such that $s<s'$, any $u\in\mathcal{U}_{<s}$ and $x_u\in\states^u$, any $v\in\mathcal{U}_{[s,s']}$, and any $f\in\gambles(\states^{u\cup v})$,
%\begin{equation*}
%\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%\end{proposition}
%
%Note that a direct implication of this, together with Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, is that $L_t^s$ can also be used to compute lower bounds on the expectation with respect to a set $\wmprocesses_\rateset$ of Markov processes. However, this bound will then in general not be tight, and hence will not correspond to the lower expectation.
%
%To see why, observe that in the term $[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n})$, the operators $L_{s_{i-1}}^{s_i}$ can take on different values depending on the choice of $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$. Hence, to approach these values of $L_{s_{i-1}}^{s_i}$ from within a set $\wmprocesses$ of Markov processes, we have to be able to pick the approximating values such that they depend on the specific trajectory $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$, and this is exactly what the Markov condition prevents us from doing. As the next result shows, we can however approach this quantity from within a set $\wprocesses_\rateset$ of non-Markov processes.
%
%\begin{proposition}\label{prop:multivar_bound_tight}
%Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $s,s'\in\realsnonneg$ such that $s<s'$, all $u\in\mathcal{U}_{<s}$, all $v\in\mathcal{U}_{[s,s']}$, all $f\in\gambles(\states^{u\cup v})$, and all $\epsilon\in\realspos$, there is a $P\in\wprocesses_\rateset$ such that
%\begin{equation*}
%\norm{L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{proposition}
%
%\begin{corollary}\label{cor:inf_works_for_multivar}
%Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$, and let $\underline{\mathcal{T}}_{\lrate}$ be the family of lower transition operators corresponding to $\lrate$. Then, for all $s,s'\in\realsnonneg$ such that $s<s'$, all $u\in\mathcal{U}_{<s}$ and $x_u\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
%\begin{equation*}
%\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^{\mathrm{W}}_{\,\rateset}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%\end{corollary}

\subsection{Computational Aspects}\label{sec:tractability}

We will in this section provide some results on computational aspects, along with a number of worked numerical examples. To this end, we consider again the two-state model from Example~\ref{ex:health_sick_exmp} in Section~\ref{sec:introduction}, describing how a person periodically becomes ill. We start by setting some (arbitrarily chosen) numerical parameters that we will use throughout this section.

\begin{exmp}\label{exmp:example_rateset_simple_model}
Consider again the binary-state disease model from Example~\ref{ex:health_sick_exmp}, given by
\begin{equation*}
(\text{{\tt healthy}}) \rightleftarrows (\text{{\tt sick}})
\end{equation*}
Thus, the state-space here is of the form $\states=\{\text{{\tt healthy}},\text{{\tt sick}}\}$. This models a person periodically becoming sick and recovering after some time. 

We now need to assign numerical values to the rate with which the process modeling this behavior moves between these states. If we were using a precise, time-homogeneous Markov chain $P\in\whmprocesses$, it would suffice to select a single rate matrix $Q\in\mathcal{R}$. In that case, and if we---arbitrarily---assume that the person becomes sick once a year, with illness lasting for one week, this rate matrix would be of the form
\begin{equation*}
Q = \left[ \begin{array}{rr}
-\frac{1}{52} & \frac{1}{52} \\
1 & -1
\end{array}\right]\,,
\end{equation*}
with the unit-time being one week. That is, the person on average becomes sick once every 52 weeks, and recovers after one week on average.

However, instead of using a precise model, we now wish to use an imprecise continuous-time Markov chain to model this behavior. Hence, we need to select a set of rate matrices $\rateset$. Suppose therefore that we feel confident in saying that the person becomes sick anywhere between one and three times per year, with sickness lasting anywhere from half a week to two weeks on average. This set of rate matrices can then be expressed as
\begin{equation}\label{eq:num_example_rateset_params}
\rateset = \left\{\left[\begin{array}{rr}
-a & a \\
b & -b
\end{array}\right]\,:\,a\in\left[\frac{1}{52},\frac{3}{52}\right], b\in\left[\frac{1}{2},2\right]\right\}\,,
\end{equation}
with the unit-time again being one week. Note that $\rateset$ here is clearly non-empty, bounded, closed, convex, and has separately specified rows. As such, this set will satisfy all the preconditions necessary to be used in the examples that follow.
\exampleend
\end{exmp}

We will now focus on how to compute lower expectations with respect to imprecise continuous-time Markov chains induced by the set $\rateset$ from Example~\ref{exmp:example_rateset_simple_model}. To this end, consider the lower transition rate operator $\lrate$ corresponding to this $\rateset$. It should be clear that, for any function $f\in\gamblesX$, computing the quantity $\lrate f$ is relatively easy.

In general, for any set $\rateset$ with corresponding lower transition rate operator $\lrate$, and any function $f\in\gamblesX$, computing $\lrate f$ is a constrained linear optimization problem. The exact form of these constraints---and hence the difficulty of solving this optimization problem---clearly depends on the set $\rateset$.

In the case of the set $\rateset$ from Example~\ref{exmp:example_rateset_simple_model}, as given by Equation~\eqref{eq:num_example_rateset_params}, these constraints are simple box-constraints. Furthermore, because the state-space $\states$ contains only two states in this specific example model, computing $\lrate f$ is particularly easy, regardless of the choice of $f\in\gamblesX$. This is illustrated by the following example.

\begin{exmp}\label{exmp:numerical_lrate}
Consider the set $\rateset$ from Example~\ref{exmp:example_rateset_simple_model}, given by Equation~\eqref{eq:num_example_rateset_params}. Let $\lrate$ be the lower transition rate operator corresponding to $\rateset$, and consider any $f\in\gamblesX$. We now want to solve
\begin{align*}
\lrate f &= \inf\{Qf\,:\,Q\in\rateset\} \\
 &= \inf\left\{\left[\begin{array}{rr}-a & a \\b & -b\end{array}\right]\left[\begin{array}{r} f(0) \\ f(1) \end{array}\right]\,:\,a\in\left[\frac{1}{52}, \frac{3}{52} \right],\,b\in\left[\frac{1}{2}, 2 \right]\right\} \\
 &= \inf\left\{ \left[\begin{array}{r} a\bigl(f(1) - f(0)\bigr) \\ b\bigl(f(0) - f(1) \bigr)\end{array} \right]\,:\,a\in\left[\frac{1}{52}, \frac{3}{52} \right],\,b\in\left[\frac{1}{2}, 2 \right]\right\}\,.
\end{align*}
From the above, it is readily seen that $\lrate f$ is always given by some $Q f$, with $Q\in\rateset$ such that $a\in\{\nicefrac{1}{52}, \nicefrac{3}{52}\}$ and $b\in\{\nicefrac{1}{2},2\}$. Hence, we only need to consider four possible rate matrices $Q\in\rateset$ to find $\lrate f$ for any given $f\in\gamblesX$.

Using these observations, we will next compute $\norm{\lrate}$. We have that
\begin{align*}
\norm{\lrate} &= \sup\left\{ \norm{\lrate f}\,:\,f\in\gamblesX,\, \norm{f}=1 \right\} \\
 &= \sup\left\{ \max\left\{\abs{\left[\lrate f\right](x)}\,:\,x\in\states\right\}\,:\,f\in\gamblesX,\, \norm{f}=1 \right\}\,.
\end{align*}
Combining with the above expression for $\lrate f$, it should be clear that the supremum is in this case always reached by an $f\in\gamblesX$ such that either $f=[-1\,\,1]^\top$ or $f=[1\,\,-1]^\top$. Some arithmetic then reveals that $\norm{\lrate}=4$.
\exampleend
\end{exmp}

*** herschrijf bruggetje een beetje
The reason that this is of interest to us, is the following result. This result states how we can approximate the quantity $L_t^sf$ to an arbitrary precision $\epsilon$, using a fine enough partition of the interval $[t,s]$.
%*** het is duidelijk dat, gegeven $\rateset$, we voor elke $f\in\gamblesX$ de vector $\lrate f$ makkelijk kunnen uitrekenen.

%*** we beginnen met een approximatie methode van de $L$ operator.

\begin{proposition}\label{prop:approximation_error_bound}
Let $\lrate$ be an arbitrary lower transition rate operator. Choose any $t,s\in\realsnonneg$ such that $t\leq s$, any $f\in\gamblesX$, and any $\epsilon\in\realspos$. Let $L_t^s$ be the lower transition operator corresponding to $\lrate$. Choose any $n\in\nats$ such that $n\geq\nicefrac{\left((s-t)^2\norm{\lrate}^2\norm{f}\right)}{\epsilon}$,
%\begin{equation*}
%n > \frac{(s-t)^2\norm{\lrate}^2\norm{f}}{\epsilon}\,,
%\end{equation*}
and let $\Delta\coloneqq \nicefrac{(s-t)}{n}$. Then,
\begin{equation*}
\norm{L_t^sf - \prod_{i=1}^n(I + \Delta\lrate)f} \leq \epsilon\,.
\end{equation*}
\end{proposition}

Simply put, this result tells us that if we can compute the quantity $\lrate g$ for any $g\in\gamblesX$, then we can also approximate the quantity $L_t^sf$ to an arbitrary precision for any given $f\in\gamblesX$. Due to the correspondence between $L_t^sf$ and lower expectations $\underline{\mathbb{E}}_\rateset^\mathrm{WM}[f(X_s)\vert X_t]$ and $\underline{\mathbb{E}}_\rateset^\mathrm{W}[f(X_s)\vert X_t]$, this therefore tells us how to compute---or at least approximate to arbitrary precision---these latter quantities. The following provides a worked numerical example.

\begin{exmp}\label{exmp:single_time_numerical}
Consider again the model and the set of rate matrices $\rateset$ from Example~\ref{exmp:example_rateset_simple_model}. We will here compute the lower probability of a person being sick one week from now, given that they are currently also sick. That is, we will be computing
\begin{equation*}
\underline{P}(X_1 = \text{{\tt sick}}\,\vert\,X_0=\text{{\tt sick}}) \equiv \underline{\mathbb{E}}[\ind{\text{{\tt sick}}}(X_1)\,\vert\,X_0=\text{{\tt sick}}] \equiv \left[L_0^1\ind{\text{{\tt sick}}}\right](\text{{\tt sick}})\,.
\end{equation*}
Proposition~\ref{prop:approximation_error_bound} tells us that we can compute this quantity using a fine enough partition of the time interval $[0,1]$. We will be using a maximum error for this computation of $\epsilon\coloneqq 10^{-3}$. Furthermore, the length of the time interval $(s-t)=1$, the function norm $\norm{\ind{\text{Sick}}}=1$, and due to our choice of $\rateset$, we have $\norm{\lrate} = 4$, as in Example~\ref{exmp:numerical_lrate}. In order to reach the desired maximum error, we therefore need to subdivide the time interval into at least $n$ steps, where
\begin{equation*}
n = \frac{(s-t)^2\norm{\lrate}^2\norm{\ind{\text{Sick}}}}{\epsilon} = \frac{1^2\cdot 4^2\cdot 1}{10^{-3}} = 16\cdot 10^3\,,
\end{equation*}
resulting in a step size of $\Delta=\nicefrac{(s-t)}{n}=\nicefrac{1}{(16\cdot 10^{-3})}$. We now compute the quantity
\begin{equation*}
\prod_{i=1}^n(I + \Delta\lrate)\ind{\text{{\tt sick}}} = \prod_{i=1}^{16000}(I + \Delta\lrate)\ind{\text{{\tt sick}}} = \left(\prod_{i=1}^{15999}(I + \Delta\lrate)\right)\left((I+\Delta\lrate)\ind{\text{{\tt sick}}}\right)\,.
\end{equation*}
We start by computing the right-most factor on the right-hand side of this equation. First compute $\lrate\ind{\text{{\tt sick}}}$, as in Example~\ref{exmp:numerical_lrate}:
\begin{align*}
\lrate\ind{\text{{\tt sick}}} &= \inf\left\{ Q\ind{\text{{\tt sick}}}\,:\,Q\in\rateset \right\} = \inf\left\{ Q \left[\begin{array}{c} 0\\ 1 \end{array}\right]\,:\,Q\in\rateset \right\} \\
 &= \inf\left\{ \left[\begin{array}{rr}-a&a\\b&-b \end{array}\right] \left[\begin{array}{c} 0\\ 1 \end{array}\right]\,:\, a\in[\nicefrac{1}{52},\nicefrac{3}{52}],\,b\in[\nicefrac{1}{2},2]\right\} = \left[\begin{array}{c}\nicefrac{1}{52}\\2\end{array}\right]\,.
\end{align*}
Let now
\begin{equation*}
g_1 \coloneqq (I + \Delta\lrate)\ind{\text{{\tt sick}}} = \ind{\text{{\tt sick}}} + \Delta\lrate\ind{\text{{\tt sick}}} = \left[\begin{array}{c}0\\1\end{array}\right] + 16\cdot 10^{-3}\cdot\left[\begin{array}{c}\nicefrac{1}{52}\\2\end{array}\right] = \left[\begin{array}{l} 0 + \nicefrac{1}{52}\cdot 16\cdot 10^{-3}\\ 1 + 2\cdot 16\cdot 10^{-3}\end{array}\right]\,.
\end{equation*}
Having completed this first step, we have
\begin{equation*}
\prod_{i=1}^n(I + \Delta\lrate)\ind{\text{{\tt sick}}} = \prod_{i=1}^{15999}(I + \Delta\lrate)g_1 = \left(\prod_{i=1}^{15998}(I + \Delta\lrate)\right)\left((I+\Delta\lrate)g_1\right)\,.
\end{equation*}
We now repeat this process, computing the term $g_2\coloneqq(I+\Delta\lrate)g_1$. After $j$ steps, we then have
\begin{equation*}
\prod_{i=1}^n(I + \Delta\lrate)\ind{\text{{\tt sick}}} = \prod_{i=1}^{16000-j}(I + \Delta\lrate)g_j\,.
\end{equation*}
After repeating this process for $n$ steps, we have computed the term
\begin{equation*}
\prod_{i=1}^n(I + \Delta\lrate)\ind{\text{{\tt sick}}} = g_{16000} \approx \left[\begin{array}{c}0.0083 \\ 0.1410\end{array}\right]\,,
\end{equation*}
from which we conclude that
\begin{align*}
\underline{P}(X_1 = \text{{\tt sick}}\,\vert\,X_0=\text{{\tt sick}}) &\equiv \left[L_0^1\ind{\text{{\tt sick}}}\right](\text{{\tt sick}})
= g_{16000}(\text{{\tt sick}}) \pm \epsilon \approx 0.141 \pm 10^{-3}\,,
\end{align*}
where the second equality is due to the error bound guaranteed by Proposition~\ref{prop:approximation_error_bound}.
\exampleend
\end{exmp}

\begin{algorithm}[H]
  \caption{Numerically compute $L_t^sf$ for any $f\in\gamblesX$.}
    \label{alg:compute_singlevar}
  \begin{algorithmic}[1]
    \Require{A non-empty, bounded set of rate matrices $\rateset$ with separately specified rows, two time points $t,s\in\realsnonneg$ such that $t\leq s$, a function $f\in\gamblesX$, and a maximum numerical error $\epsilon\in\realspos$.}    
\Ensure{A function $L_t^sf\pm \epsilon$ in $\gamblesX$.}
    \Statex
    \Function{ComputeLf}{$\rateset, t,s, f,\epsilon$}
      \State $n\gets \lceil\nicefrac{((s-t)^2\norm{\lrate}^2\norm{f})}{\epsilon}\rceil$
		\State $\Delta\gets \nicefrac{(s-t)}{n}$
		\State $g_0 \gets f$
		\For{$i\in\{1,\ldots,n\}$}
		\State $g_i\gets g_{(i-1)} + \Delta\lrate g_{(i-1)}$
		\EndFor
      \State \Return{$g_n$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}
{\bf TODO:} some explanation for algorithm.

Observe that in the previous example, we computed the term $\left[L_0^1\ind{\text{{\tt sick}}}\right](\text{{\tt sick}})$ which, due to Corollary~\ref{cor:lower_operator_is_infimum}, corresponds to the lower expectation taken with respect to both $\wmprocesses_\rateset$ and $\wprocesses_\rateset$. In Section~\ref{sec:decomposition}, we noted that we can also compute lower expectations of more complex functions with respect to the set $\wprocesses_\rateset$. The following provides a worked numerical example, which is followed by Example~\ref{exmp:num_counterexample_markov} below which shows that this approach does not work for the set of Markov models $\wmprocesses_\rateset$.

\begin{exmp}\label{exmp:num_multivar_func_nonmarkov}
Consider again the model and set of rate matrices $\rateset$ from Example~\ref{exmp:example_rateset_simple_model}. Suppose that we are now interested in the lower probability of the model being in the same state at two different time points. That is, we will be computing
\begin{equation*}
\underline{P}_{\rateset}^{\mathrm{W}}(X_1 = X_2\,\vert\, X_0) \equiv \underline{\mathbb{E}}_{\rateset}^{\mathrm{W}}[f(X_1,X_2)\,\vert\,X_0]\,,
\end{equation*}
where,
\begin{equation*}
f(X_1,X_2) \coloneqq \left\{\begin{array}{ll}
1 & \text{if $X_1 = X_2$, and} \\
0 & \text{otherwise.}
\end{array}\right.
\end{equation*}
Corollary~\ref{cor:composition_lower_trans} tells us that we can compute this lower expectation as
\begin{equation}\label{eq:num_example_composition}
\underline{\mathbb{E}}_{\rateset}^\mathrm{W}[f(X_1,X_2)\,\vert\,X_0] \equiv \left[L_0^1L_1^2f\right](X_0)\,.
\end{equation}
In order to compute this quantity, we now resolve this composition of operators $L_0^1L_1^2$ by starting from the latest time point, and working back to the earliest time point. Hence, we start by looking at the quantity $L_1^2f$. 

Note that $f$ is here a function defined on multiple time points. Due to our convention stipulated in Section~\ref{sec:function_single_future_multiple_past}, the resulting function $L_1^2f$ will be a function in $\gambles(\states^{\{1\}})$, which is given by looking at all restrictions of $f$ to the state $X_2$, dependent on the corresponding values at $X_1$. Due to the definition of $f$, we therefore have that
\begin{align*}
\left[L_1^2f\right](\text{{\tt healthy}}) &= \left[L_1^2f(\text{{\tt healthy}}, X_2)\right](\text{{\tt healthy}}) \\
 &= \left[L_1^2\ind{\text{{\tt healthy}}}(X_2)\right](\text{{\tt healthy}})\,,\\
 &\text{and,} \\
\left[L_1^2f\right](\text{{\tt sick}}) &= \left[L_1^2f(\text{{\tt sick}}, X_2)\right](\text{{\tt sick}}) \\ 
 &= \left[L_1^2\ind{\text{{\tt sick}}}(X_2)\right](\text{{\tt sick}})\,.
\end{align*}
Using Proposition~\ref{prop:approximation_error_bound}, the quantities $L_1^2\ind{\text{{\tt healthy}}}$ and $L_1^2\ind{\text{{\tt sick}}}$ can now be computed using the same procedure outlined in Example~\ref{exmp:single_time_numerical}, i.e., using Algorithm~\ref{alg:compute_singlevar}. We find that
\begin{equation*}
L_1^2 f = \left[\begin{array}{l}
\left[L_1^2\ind{\text{{\tt healthy}}}(X_2)\right](\text{{\tt healthy}}) \\
\left[L_1^2\ind{\text{{\tt sick}}}(X_2)\right](\text{{\tt sick}})
\end{array}\right] 
\approx \left[\begin{array}{c}
0.956 \\
0.141
\end{array}\right]\,.
\end{equation*}
Substituting this result into Equation~\eqref{eq:num_example_composition}, we have
\begin{equation*}
\underline{\mathbb{E}}_{\rateset}^\mathrm{W}[f(X_1,X_2)\,\vert\,X_0] \equiv \left[L_0^1L_1^2f\right](X_0) \approx \left[L_0^1 \left[\begin{array}{c}
0.956 \\
0.141
\end{array}\right]\right](X_0)\,.
\end{equation*}
In order to compute the lower expectation of interest, we apply Algorithm~\ref{alg:compute_singlevar} once more, finding that
\begin{equation*}
L_0^1\left[\begin{array}{c}
0.956 \\
0.141
\end{array}\right] \approx \left[\begin{array}{c}
0.920 \\
0.453
\end{array}\right]\,.
\end{equation*}
We therefore conclude that
\begin{align*}
\underline{P}_{\rateset}^{\mathrm{W}}(X_1 = X_2\,\vert\,X_0=\text{{\tt healthy}}) \approx 0.920,\quad\text{and,}\quad \underline{P}_{\rateset}^{\mathrm{W}}(X_1 = X_2\,\vert\,X_0=\text{{\tt sick}}) \approx 0.453\,.
\end{align*}
\exampleend
\end{exmp}

\begin{algorithm}[H]
  \caption{Numerically compute $\underline{\mathbb{E}}_\rateset^\mathrm{W}[f(X_u,X_v)\,\vert\,X_u]$ for any $f\in\gambles(\states^{u\cup v})$.}
    \label{alg:compute_multivar}
  \begin{algorithmic}[1]
    \Require{A non-empty, bounded, and convex set of rate matrices $\rateset$ with separately specified rows, two sets of time points $u,v\in\mathcal{U}_\emptyset$ such that $u\leq v$, a function $f\in\gambles(\states^{u\cup v})$, and a maximum numerical error per step of $\epsilon\in\realspos$.}
\Ensure{An approximation of the function $\underline{\mathbb{E}}_\rateset^\mathrm{W}[f(X_u,X_v)\,\vert\,X_u]$ in $\gambles(\states^u)$.}
	 \Statex
    \Function{ComputeLowerExp}{$\rateset, u,v, f,\epsilon$}
		\State $m\gets \vert v\vert$ \Comment{We use the notation $v=s_0,s_1,\ldots,s_m$, and $\vert v\vert \coloneqq m$.}
		\State $w \gets v$ \Comment{Buffer for remaining time points.}
		\State $g_{m}\gets f$ \Comment{Buffer for function.}
		\Statex
		\For{$i\in\{m,m-1,\ldots,0\}$}	\Comment{Running index of latest remaining time point.}
			\State $w \gets w\setminus s_i$ \Comment{Remove latest time point $s_i$ from $w$. If $i=0$ then $w=\emptyset$.}
		
			\State $r \gets s_{(i-1)}$ \Comment{Latest time point before $s_i$.}
			\If{$i=0$} \Comment{If only $s_0$ remains, get latest time before that.}
				\State $r \gets t_n$ \Comment{We use the notation $u=t_0,t_1,\ldots,t_n$.}
			\EndIf
			\Statex
			\State $g_{(i-1)} \gets g_{(i-1)} \in \gambles(\states^{u\cup w})$ \Comment{Create new function $g_{(i-1)}\in\gambles(\states^{u\cup w})$.}
			\For{$x_{u\cup w}\in\states^{u\cup w}$} \Comment{Compute all values for $g_{(i-1)}$.}
				\Statex \Comment{Set $g_{(i-1)}(x_{u\cup w})\coloneqq \left[L_r^{s_i}g_i(x_{u\cup w},X_{s_i})\right](x_r)$, where $x_r\in x_{u\cup w}$.}
				\State $h \gets$ {\tt ComputeLf}$\bigl(\rateset, r, s_i, g_i(x_{u\cup w}, X_{s_i}),\epsilon\bigr)$ %\Comment{Compute $L_r^sf(x_{u\cup w},X_s)\pm\epsilon$.}
				\State $g_{(i-1)}(x_{u\cup w}) \gets h(x_r)$ 
			\EndFor
%			\Statex
		\EndFor
		\Statex
		\State \Return{$g_{(-1)}$}\Comment{Result is a function in $\gambles(\states^u)$.}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

{\bf TODO:} some explanation for algorithm. Maybe rewrite middle part into more general version of {\tt ComputeLf}. Also, the total computational error is not entirely trivial to compute (and rather outside the scope of this paper).

As noted above, the procedure outlined in Example~\ref{exmp:num_multivar_func_nonmarkov} does not work for computing lower expectations with respect to a set of Markov chains $\wmprocesses_\rateset$.

** some intuition first.

The example below will show that, indeed, the lower expectation of the function from Example~\ref{exmp:num_multivar_func_nonmarkov}, with respect to $\wmprocesses_\rateset$, is different from the one that was found for $\wprocesses_\rateset$. This therefore proves by counterexample that Algorithm~\ref{alg:compute_multivar} is not applicable when working with sets of Markov chains. Put differently, a composition of lower transition operators does not in general correspond to the lower expectation for such a set, i.e.,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_u) \neq \underline{\mathbb{E}}_\rateset^{\mathrm{WM}}[f(X_u,X_v)\,\vert\,X_u=x_u]\,,
\end{equation*}
in contrast to our results from Section~\ref{sec:decomposition} for the set $\wprocesses_\rateset$.

\begin{exmp} \label{exmp:num_counterexample_markov}
Consider again the model and set of rate matrices $\rateset$ from Example~\ref{exmp:example_rateset_simple_model}, and the function $f\in\gambles(\states^{\{1,2\}})$ from Example~\ref{exmp:num_multivar_func_nonmarkov} given by
\begin{equation*}
f(X_1,X_2) \coloneqq \left\{\begin{array}{ll}
1 & \text{if $X_1 = X_2$, and} \\
0 & \text{otherwise.}
\end{array}\right.
\end{equation*}
We will here compute the lower expectation $\underline{\mathbb{E}}_\rateset^{\mathrm{WM}}[f(X_1,X_2)\,\vert\,X_0]$ of this function with respect to the set $\wmprocesses_\rateset$ of Markov chains consistent with $\rateset$. We will then see that this lower expectation differs from the one found in Example~\ref{exmp:num_multivar_func_nonmarkov} for the set $\wprocesses_\rateset$.

One problem in doing this, is that we do not have a general method for computing such lower expectations for sets of Markov chains. Fortunately, in the very specific case of a binary-state model, as we use here, we can actually numerically solve the optimization problem given by the right-hand side of
\begin{equation*}
\underline{\mathbb{E}}_\rateset^{\mathrm{WM}}[f(X_1,X_2)\,\vert\,X_0] = \inf\left\{ \mathbb{E}[f(X_1,X_2)\,\vert\,X_0]\,:\,P\in\wmprocesses_\rateset\right\}.
\end{equation*}

This numerical approach was verified to work for simple functions $g\in\gamblesX$, i.e. functions defined on a single time point, for which Corollary~\ref{cor:lower_operator_is_infimum} tells us that the lower expectation should correspond to the quantity $L_t^sg$. Because Algorithm~\ref{alg:compute_singlevar} tells us how to compute this latter quantity, we could verify the precision of the numerical optimization employed here. In doing so, we found a numerical correspondence between the ``brute-force'' optimization over $\wmprocesses_\rateset$ and the computation using $L_t^s$, with deviations less than the $\epsilon$ error-bound used to compute the quantity $L_t^sg$.

Employing this ``brute-force'' optimization to the function $f$ that we are interested in here, we found that
\begin{align*}
\underline{P}_{\rateset}^{\mathrm{WM}}(X_1 = X_2\,\vert\,X_0=\text{{\tt healthy}}) \approx 0.939,\quad\text{and,}\quad \underline{P}_{\rateset}^{\mathrm{WM}}(X_1 = X_2\,\vert\,X_0=\text{{\tt sick}}) \approx 0.467\,.
\end{align*}

Comparing to the lower expectation for the set $\wprocesses_\rateset$ that was found in Example~\ref{exmp:num_multivar_func_nonmarkov}, we see that the lower expectations are indeed different for these two sets of processes. Furthermore, as guaranteed by Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, the lower expectation for $\wprocesses_\rateset$ provides a lower bound on the lower expectation for $\wmprocesses_\rateset$.
\exampleend
\end{exmp}

**** notes about computational aspects/tractability, observation that decomposition reduces the problem to discrete-time computations, reference earlier work containing algorithms to compute this

**** computation can also be brute-forced if state space and number of time points allow for it. Algorithm~\ref{alg:compute_multivar} does this, but this approach is exponential in the size of $v$; specifically the runtime is $O\left({\vert\states\vert}^{m+1}(s_m-t_0)^2\norm{\lrate}^2\norm{f}\epsilon^{-1}\right)$, with $u=t_0,t_1,\ldots,t_n$ and $v=s_0,s_1,\ldots,s_m$.

\section{Relation to Previous Work}\label{sec:prev_work}

**** other related work: Markov Decision Processes, controlled Markov processes, differential inclusions, ....

To the best of our knowledge, the concept of imprecise continuous-time Markov chains was first introduced in the literature by the work of {\v{S}}kulj~\cite{Skulj:2015cq}. There, the idea was used to define $\lrate$ as a lower envelope of a set of rate matrices $\rateset$ that has separately specified rows. It was shown that then, for a given $f\in\gamblesX$, the differential equation
\begin{align}\label{eq:damjans_diff}
\begin{split}
\frac{d \underline{f}_{\,s}}{d s} &\coloneqq \lrate\,\underline{f}_{\,s}\,,\quad
\underline{f}_{\,0} \coloneqq f\,,
\end{split}
\end{align}
has a unique solution, and that this solution satisfies
\begin{equation}\label{eq:damjans_lower}
\underline{f}_{\,s} = \underline{\mathbb{E}}_{\rateset}^{\mathrm{WM}}[f(X_s)\,\vert\,X_0]\,.
\end{equation}
Our work extends these results in several ways. First, as shown by Proposition~\ref{prop:lower_transition_has_deriv}, the differential equation~\eqref{eq:damjans_diff} has a uniform solution, i.e. one that is independent of $f$, and this solution is given by the operator $L_0^s$ corresponding to $\lrate$. Second, Corollary~\ref{cor:lower_operator_is_infimum} confirms Equation~\eqref{eq:damjans_lower}, but furthermore shows that the solution $\underline{f}_{\,s}$ also corresponds to the lower expectation with respect to a set of non-Markov processes.

This earlier work also contains several results that are useful to us. In particular, {\v{S}}kulj discusses several practical techniques to numerically compute, or approximate to arbitrary precision, the quantity $\underline{f}_{\,s}$ for a given $f\in\gamblesX$. Hence, because of the correspondence between $\underline{f}_{\,s}$ and $L_0^sf$, these results can be applied when numerically working with our lower transition operator.

**** our proof in Section~\ref{sec:lowertrans} shows that we can numerically approximate to arbitrary precision using a fine enough partition of the intervals *** (Lemma~\ref{lemma:limitboundonL} and Proposition~\ref{prop:approximation_error_bound} make this precision specific) (we should mention that this bound is better than the one that is provided by Damjan! For example, for the numerical example at the end of Section 4.1 in his paper, he finds that $N$ should be more than $6000$, whereas our bound only needs $N=100$)

Our present work furthermore adds to the literature in several ways. In~\cite{Skulj:2015cq}, the arguments were cast purely in terms of lower envelopes of expectation functionals; but side-stepped the question of which sets of processes these envelopes correspond to. In contrast, our work in Section~\ref{sec:iCTMC} makes explicit the sets of processes that we are dealing with, and Theorem~\ref{theo:dominating_rate_processes_max_set} exactly characterizes the largest set of processes for which our lower transition operator---and indeed the differential equation~\eqref{eq:damjans_diff}---computes a lower envelope.

Finally, previous work only focused on functions $f\in\gamblesX$ defined at a single point in time. In contrast, our work in Section~\ref{sec:funcs_multi_time_points} extends these results to functions $f\in\gambles(\states^{u\cup v})$ defined at an arbitrary number of time points. We there also found that it was, in fact, crucial to be aware of the exact set of processes with respect to which one is taking the lower expectation.

\section{Conclusions \& Future Work}\label{sec:conclusions}

*** kijk, dit hebben we besproken

*** wat hebben we daarvan geleerd?

*** wat kunnen we nog meer doen? Nou, bijvoorbeeld:

*** I would discuss sigma-additivity things here: explain that this is possible with our approach by using Kolmogorovs extension theorem, say that this would allow us to consider for example the lower and upper expected time till absorbtion. ***

*** mogelijk nog opmerking over dat dit Hidden-ICTMCs mogelijk maakt omdat GBR in subklasse van berekenbare functies zit

*** mogelijk iets over dat het handig kan zijn om voor berekenbaarheid nieuwe methodes te vinden om $\hat{f}_t^s$ numeriek uit te rekenen.


\bibliographystyle{plain} 
\bibliography{general}

\appendix

%\section{Proofs of results in Section~\ref{sec:prelim}}\label{app:prelim}

\section{Proofs of results in Section~\ref{sec:systems}}\label{app:systems}

\begin{proof}[Proof of Proposition~\ref{lemma:compositiontransitionmatrix}]
Simply check each of the properties.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:stochastic_from_rate_matrix}]
\ref{def:T:sumone} follows from \ref{def:Q:sumzero}: for all $x\in\states$, \ref{def:Q:sumzero} implies that
\begin{equation*}
%\sum_{y\in\states} T(x,y) = 
\sum_{y\in\states} [I + \Delta Q](x,y) = \sum_{y\in\states}I(x,y) + \Delta \sum_{y\in\states}Q(x,y) = 1+\Delta 0=1.
\end{equation*}
\ref{def:T:nonneg} follows from~\ref{def:Q:nonnegoffdiagonal} and because $0\leq \Delta\norm{Q} \leq 1$: for all $x,y\in\states$ such that $x\neq y$, $0\leq\Delta\norm{Q} \leq 1$ implies that $[I+\Delta Q](x,x)=1+\Delta Q(x,x)\geq 1-\Delta\norm{Q}\geq0$, and \ref{def:Q:nonnegoffdiagonal} and $\Delta\geq0$ imply that $[I+\Delta Q](x,y)=\Delta Q(x,y)\geq0$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:rate_from_stochastic_matrix}]
This proof is analogous to that of Proposition~\ref{prop:stochastic_from_rate_matrix}; simply verify each of the properties in Definition~\ref{def:rate_matrix}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:alternativedefforbounded}]
We start by proving that Equation~\eqref{eq:alternative_bounded} implies $\norm{\rateset}<+\infty$. %To this end, we first observe that for all $x\in\states$, Equation~\eqref{eq:alternative_bounded} implies that
% \begin{equation*}
% c_x\coloneqq\inf
% \end{equation*}
% For any $x\in\states$ and $Q\in\rateset$, it follows from Definition~\ref{def:rate_matrix} that
% \begin{equation*}
% \sum_{y\in\states\setminus\{x\}}\abs{Q(x,y)}
% =\sum_{y\in\states\setminus\{x\}}Q(x,y)
% =-Q(x,x)=\abs{Q(x,x)},
% \end{equation*}
% which implies that
% \begin{equation*}
% \sum_{y\in\states}\abs{Q(x,y)}=2\abs{Q(x,x)}\leq 2c.
% \end{equation*}
 To this end, assume that Equation~\eqref{eq:alternative_bounded} holds. Since $\states$ is finite, it then follows that
\begin{align*}
\norm{\rateset} = \sup\{\norm{Q}\,:\,Q\in\rateset\}
 &= \sup\left\{\max\left\{\sum_{y\in\states}\abs{Q(x,y)}\,:\,x\in\states\right\}\,:\,Q\in\rateset\right\} \\
 &= \max\left\{\sup\left\{\sum_{y\in\states}\abs{Q(x,y)}\,:\,Q\in\rateset\right\} \,:\,x\in\states\right\}.
\end{align*}Hence, there is some $x'\in\states$ such that
% \begin{equation*}
% \norm{\rateset} = \sup\left\{\sum_{y\in\states}\abs{Q(x',y)}\,:\,Q\in\rateset\right\}\,.
% \end{equation*}
% Consider this $x'$. Then,
\begin{align*}
\norm{\rateset} = \sup_{Q\in\rateset}\sum_{y\in\states}\abs{Q(x',y)}
&=\sup_{Q\in\rateset}\left(2\abs{Q(x',x')}\right)
 = 2\sup_{Q\in\rateset}\abs{Q(x',x')}\\
 &= -2\inf_{Q\in\rateset}\left(-\abs{Q(x',x')}\right)
 = -2\inf_{Q\in\rateset}Q(x',x')< +\infty\,,
\end{align*}
where the second and last equality follows from Definition~\ref{def:rate_matrix}, and the final inequality follows from Equation~\eqref{eq:alternative_bounded}.

We next show that $\norm{\rateset}<+\infty$ implies Equation~\eqref{eq:alternative_bounded}. To this end, consider any set of rate matrices $\rateset\subseteq\mathcal{R}$ such that $\norm{\rateset}<+\infty$, and assume \emph{ex absurdo} that Equation~\eqref{eq:alternative_bounded} is not true. Then clearly, there is some $x'\in\states$ and $Q'\in\rateset$ such that $Q'(x,x)<-\norm{\rateset}$. However, this implies that $\norm{\rateset}\geq\norm{Q'}\geq\abs{Q'(x,x)}>\norm{\rateset}$, which is a contradiction. Hence, it follows that Equation~\eqref{eq:alternative_bounded} must be true.
% $\norm{\rateset}<+\infty$, and consider any $x\in\states$. Then,
% \begin{align*}
% \inf\{Q(x,x)\,:\,Q\in\rateset\} &\geq \inf\left\{ \min\{Q(y,y)\,:\,y\in\states \}\,:\,Q\in\rateset \right\} \\
%  &\geq \inf\left\{ \min\{2\cdot Q(y,y)\,:\,y\in\states \}\,:\,Q\in\rateset \right\} \\
%  &= \inf\left\{ -\max\{-2\cdot Q(y,y)\,:\,y\in\states \}\,:\,Q\in\rateset \right\} \\
%  &= \inf\left\{ -\max\{2\cdot \abs{Q(y,y)}\,:\,y\in\states \}\,:\,Q\in\rateset \right\} \\
%  &= \inf\left\{ -\max\left\{ \sum_{z\in\states}\abs{Q(y,z)}\,:\,y\in\states \right\}\,:\,Q\in\rateset \right\} \\
%  &= \inf\left\{ -\norm{Q}\,:\,Q\in\rateset \right\} \\
%  &= -\sup\left\{ \norm{Q}\,:\,Q\in\rateset \right\} \\
%  &= -\norm{\rateset} > -\infty\,,
% \end{align*}
% where the first and second inequalities follow from the fact that, by Definition~\ref{def:rate_matrix}, $Q(y,y)$ is non-positive, the third equality also follows from Definition~\ref{def:rate_matrix}, and the final inequality follows from the fact that $\rateset$ is bounded. Because the $x\in\states$ was arbitrary, Equation~\eqref{eq:alternative_bounded} now follows.
\end{proof}

\begin{lemma}{\cite[Theorem 2.1.1]{norris1998markov}}\label{lemma:deriv_exponential_trans}
For any $Q\in\mathcal{R}$, we have that
\begin{equation*}
{\frac{d}{d \Delta}e^{Q\Delta}}\big\vert_{\Delta=0} \coloneqq \lim_{\Delta\to 0}\frac{1}{\Delta}(e^{Q\Delta}-I)= Q.
\end{equation*}
\end{lemma}

\begin{proof}[Proof of Proposition~\ref{prop:systemQ}]
We start by showing that $\mathcal{T}_Q$ is a transition matrix system. Because of Proposition~\ref{prop:stochastic_from_exponential}, $\mathcal{T}_Q$ is clearly a family of transition matrices. Consider now any $t,r,s\in\realsnonneg$ such that $t\leq r\leq s$. It then follows from the definition of $\mathcal{T}_Q$ and~\cite[Theorem 2.1.1]{norris1998markov} that $T_t^s=T_t^rT_r^s$, and $T_t^t=I$. Because the $t,r,s$ are arbitrary, it follows from Definition~\ref{def:trans_mat_system} that $\mathcal{T}_Q$ is a transition matrix system.

To prove that $\mathcal{T}_Q$ is well-behaved, note that for any $t\in\realsnonneg$, because of Definition~\ref{def:systemfromQ},
\begin{align*}
\limsup_{\Delta\to 0^+}\frac{1}{\Delta}\norm{T_t^{t+\Delta}-I} 
%= \norm{\frac{\partial}{\partial s}\left[T_t^s\right]\Big\vert_{s=t}} 
&=\limsup_{\Delta\to 0^+}\norm{\frac{1}{\Delta}(T_t^{t+\Delta}-I)-Q+Q}\\
&\leq\limsup_{\Delta\to 0^+}\norm{\frac{1}{\Delta}(T_t^{t+\Delta}-I)-Q}+\norm{Q}  
= \norm{Q} < \infty\,,
\end{align*}
where the first inequality follows from Proposition~\ref{prop:norm_properties}, the second equality follows from Lemma~\ref{lemma:deriv_exponential_trans} and the final inequality follows from the fact that $Q$ is real-valued. Because this holds for any $t\in\realsnonneg$, the first condition in Equation~\eqref{eq:wellbehavedtransitionmatrixsystem} is satisfied. A similar argument shows that also the second condition is satisfied for all $t\in\realspos$, and hence $\mathcal{T}_Q$ is well-behaved.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:restr_trans_mat_system_if_semigroup}]
If $\mathcal{T}^{\mathbf{I}}$ is a restricted transition matrix system, then, by definition, it is the restriction to $\mathbf{I}$ of some transition matrix system $\mathcal{T}$. Therefore, the `only if' part of this result follows trivially from Definition~\ref{def:trans_mat_system}.

For the `if' part, we need to prove that for any family of transition matrices $\mathcal{T}^{\mathbf{I}}$ such that, for all $t,r,s\in\mathbf{I}$ with $t\leq r\leq s$, it holds that $T_t^s = T_t^rT_r^s$ and $T_t^t=I$, there is a transition matrix system $\mathcal{T}$ that coincides with $\mathcal{T}^{\mathbf{I}}$ on $\mathbf{I}$. In order to prove this, it suffices to show that the unique family of transition matrices $\mathcal{T}$ that coincides with $\mathcal{T}^{\mathbf{I}}$ on $\mathbf{I}$ and that is otherwise defined by
\begin{equation}\label{eq:prop:restr_trans_mat_system_if_semigroup}
T_t^s
\coloneqq
\begin{cases}
I &\text{ if $s<\min \mathbf{I}$}\\
T_{\min\textbf{I}}^s &\text{ if $t<\min\textbf{I}$ and $s\in\textbf{I}$}\\
T_{\min\textbf{I}}^{\sup\textbf{I}} &\text{ if $t<\min\textbf{I}$ and $\sup\textbf{I}<s$}\\
T_t^{\sup\textbf{I}}
&\text{ if $t\in\textbf{I}$ and $\sup\textbf{I}<s$}\\
I &\text{ if $\sup\mathbf{I}<t$}
\end{cases}
~~~\text{ for $t,s\in\realsnonneg$ with $t\leq s$ and $[t,s]\not\subseteq\mathbf{I}$,}
\end{equation}
is a transition matrix system. This is a matter of straightforward verification.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:well_restr_trans_mat_system_if_limsup}]
% Consider any closed interval $\mathbf{I}\subseteq\realsnonneg$, and let $\mathcal{T}^{\mathbf{I}}$ be a restricted transition matrix system defined on $\mathbf{I}$. Then $\mathcal{T}^{\mathbf{I}}$ is well-behaved if and only if
% \begin{equation*}%\label{eq:wellbehavedrestrictedtransitionmatrixsystem}%\label{eq:wellbehavedhistorictransitionmatrix}
% (\forall t\in\mathbf{I}^+)~\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t}^{t+\Delta}-I}<+\infty\,
% \text{~and~}
% (\forall t\in\mathbf{I}^-)~\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta}^t-I}<+\infty\,,
% \end{equation*}
% where $\mathbf{I}^+\coloneqq\mathbf{I}\setminus\{\sup\mathbf{I}\}$ and $\mathbf{I}^-\coloneqq\mathbf{I}\setminus\{\min\mathbf{I}\}$.%\footnote{Since $\mathbf{I}$ may not have a maximum $\max\mathbf{I}$, for example if $\mathbf{I}=\realsnonneg$, we consider its supremum $\sup\mathbf{I}$.}
If $\mathcal{T}^{\mathbf{I}}$ is well-behaved, then, by definition, it is the restriction to $\mathbf{I}$ of a well-behaved transition matrix system $\mathcal{T}$. Therefore, the `only if' part of this result follows trivially from Definition~\ref{def:well_behaved_trans_mat_system}.

For the `if' part, we need to show that for any restricted transition matrix system $\mathcal{T}^\mathbf{I}$ on $\mathbf{I}$ that satisfies Equation~\eqref{eq:wellbehavedrestrictedtransitionmatrixsystem}, there is a well-behaved transition matrix system $\mathcal{T}$ that coincides with $\mathcal{T}^\mathbf{I}$ on $\mathbf{I}$. Let $\mathcal{T}$ be constructed as in the proof of Proposition~\ref{prop:restr_trans_mat_system_if_semigroup}. Then, as explained in that proof, $\mathcal{T}$ is a transition matrix system that coincides with $\mathcal{T}^{\mathbf{I}}$ on $\mathbf{I}$. Therefore, it suffices to prove that $\mathcal{T}$ is well-behaved. We start by proving the first part of Equation~\eqref{eq:wellbehavedtransitionmatrixsystem}. So consider any $t\in\realsnonneg$. If $t\in\mathbf{I}^+$, then the desired inequality follows from Equation~\eqref{eq:wellbehavedrestrictedtransitionmatrixsystem}. If $t\notin\mathbf{I}^+$, then either $t<\min\mathbf{I}$ or $t\geq\sup\mathbf{I}$, and therefore, for sufficiently small $\Delta>0$, it follows from Equation~\eqref{eq:prop:restr_trans_mat_system_if_semigroup} that $T_t^{t+\Delta}=I$, thereby making the desired inequality trivially true. We second part of Equation~\eqref{eq:wellbehavedtransitionmatrixsystem} can be proved similarly.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:concat_restr_trans_mat_systems_is_system}]
For all $t,s\in\mathbf{I}\cup\mathbf{J}$ such that $t\leq s$, it follows from Proposition~\ref{lemma:compositiontransitionmatrix} that the matrix $T_t^s$ that corresponds to $\mathcal{T}^{\mathbf{I}\cup\mathbf{J}}$ is a transition matrix. Furthermore, for all $t\in\mathbf{I}\cup\mathbf{J}$, we have that either $t\in\mathbf{I}$ or $t\in\mathbf{J}$. In either case, we have that $T_t^t=I$, because either $T_t^t=\presuper{i}T_t^t=I$ or $T_t^t=\presuper{j}T_t^t=I$, with $\presuper{i}T_t^t$ and $\presuper{j}T_t^t$ corresponding to $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$, respectively. It remains to show that for all $t,q,s\in\mathbf{I}\cup\mathbf{J}$ with $t\leq q\leq s$, it holds that
\begin{equation*}
T_t^s = T_t^qT_q^s\,.
\end{equation*}
If both $t,s\in\mathbf{I}$ or if both $t,s\in\mathbf{J}$, this clearly holds. Therefore, suppose that $t\in\mathbf{I}$ and $s\in\mathbf{J}$. Suppose furthermore that $q\in\mathbf{I}$. Then, from the definition of the concatenation operator $\otimes$, we have that $T_q^s=T_q^rT_r^s$, with $r=\max\mathbf{I}=\min\mathbf{J}$. Because $t,q,r\in\mathbf{I}$, we know that $T_t^qT_q^r=T_t^r$, and hence, by the definition of the concatenation operator,
\begin{equation*}
T_t^qT_q^s = T_t^qT_q^rT_r^s = T_t^rT_r^s = T_t^s\,.
\end{equation*}
An exactly analogous argument proves the case for $q\in\mathbf{J}$. Therefore, it follows from Proposition~\ref{prop:restr_trans_mat_system_if_semigroup} that $\mathcal{T}^{\mathbf{I}\cup\mathbf{J}}$ is a restricted transition matrix system.

It remains to show that if $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$ are both well-behaved, that then $\mathcal{T}^{\mathbf{I}\cup\mathbf{J}}$ is also well-behaved. We need to show that for all $t\in\mathbf{I}\cup\mathbf{J}$, it holds that
\begin{equation*}
\limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}\left(T_t^{t+\Delta} - I\right)} < +\infty,\quad\text{and,}\quad \limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}\left(T_{t-\Delta}^{t} - I\right)} < +\infty\,.
\end{equation*}
Because this clearly holds for all $t\in\mathbf{I}\setminus\{r\}$ and all $t\in\mathbf{J}\setminus\{r\}$, the interesting case is to show that this holds for $t=r$. However, for all $\Delta\in\realspos$ we have that $T_r^{r+\Delta}$ corresponds to $\mathcal{T}^{\mathbf{J}}$, and $T_{r-\Delta}^{r}$ corresponds to $\mathcal{T}^{\mathbf{I}}$. Because both $\mathcal{T}^{\mathbf{I}}$ and $\mathcal{T}^{\mathbf{J}}$ are well-behaved, we find that $\mathcal{T}^{\mathbf{I}\cup\mathbf{J}}$ is also well-behaved at $t=r$. Hence, $\mathcal{T}^{\mathbf{I}\cup\mathbf{J}}$ is well-behaved.
\end{proof}

\begin{lemma}\label{lemma:recursive}
Consider two finite sequences $T_1,\ldots,T_n$ and $S_1,\ldots,S_n$ of transition matrices. Then
\begin{equation*}
\norm{\prod_{i=1}^nT_i - \prod_{i=1}^nS_i} \leq \sum_{i=1}^n \norm{T_i - S_i}.
\end{equation*}
\end{lemma}
\begin{proof}
This is a special case of Lemma~\ref{lemma:recursive_lower_trans}, which states a more general version. We have included this version separately because Lemma~\ref{lemma:recursive_lower_trans} uses concepts that, on a chronological reading of this paper, would be undefined at this point.
%We provide a proof by induction. Clearly, Equation~\eqref{eq:lemma_recursive_inequality} holds for $n=1$. Suppose that it holds for $n=k-1$. We show that it then also holds for $n=k$.
%\begin{align*}
%\norm{\prod_{i=1}^nA_i - \prod_{i=1}^nB_i} &= \norm{\prod_{i=1}^nA_i - \left(\prod_{i=1}^{n-1}A_i\right)B_n + \left(\prod_{i=1}^{n-1}A_i\right)B_n - \prod_{i=1}^nB_i} \\
% &\leq \norm{\prod_{i=1}^nA_i - \left(\prod_{i=1}^{n-1}A_i\right)B_n} + \norm{\left(\prod_{i=1}^{n-1}A_i\right)B_n - \prod_{i=1}^nB_i} \\
% &= \norm{\left(\prod_{i=1}^{n-1}A_i\right)(A_n - B_n)} + \norm{\left(\prod_{i=1}^{n-1}A_i - \prod_{i=1}^{n-1}B_i\right)B_n} \\
% &\leq \norm{\prod_{i=1}^{n-1}A_i}\norm{A_n - B_n} + \norm{\prod_{i=1}^{n-1}A_i - \prod_{i=1}^{n-1}B_i}\norm{B_n} \\
% &\leq \norm{A_n - B_n} + \norm{\prod_{i=1}^{n-1}A_i - \prod_{i=1}^{n-1}B_i} \\
% &\leq \norm{A_n - B_n} + \sum_{i=1}^{n-1}\norm{A_i - B_i} = \sum_{i=1}^{n}\norm{A_i - B_i}\,.
%\end{align*}
%Here, in the second-to-last inequality, we used the fact that $\norm{A_i}\leq 1$ and $\norm{B_i}\leq 1$ for all $i\in\{1,\ldots,n\}$. In the final inequality, we used the induction hypothesis.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{lemma:restricted_trans_mat_system_cauchy_converges}]
Consider any sequence $\{\mathcal{T}_i^{\mathbf{I}}\}_{i\in\nats}$ in $\mathbb{T}^{\mathbf{I}}$ that is Cauchy. We will prove that this sequence converges to a limit that belongs to $\mathbb{T}^{\mathbf{I}}$. For all $i\in\nats$ and any $t,s\in\mathbf{I}$ such that $t\leq s$, we will use $\presuper{i}T_t^s$ to denote the transition matrix that corresponds to $\mathcal{T}_i^{\mathbf{I}}$. 

Since $\{\mathcal{T}_i^{\mathbf{I}}\}_{i\in\nats}$ is Cauchy, %we know that
% \begin{equation*}
% (\forall\epsilon\in\realspos)\,(\exists n\in\nats)\,(\forall k,\ell > n)~d(\mathcal{T}_k^{\mathbf{I}},\mathcal{T}_\ell^{\mathbf{I}}) < \epsilon,
% \end{equation*}
%and therefore, 
it follows from Equation~\eqref{eq:trans_mat_system_metric} that
\begin{equation}\label{eq:lemma:restricted_trans_mat_system_cauchy_converges:consequenceofCauchy}
(\forall\epsilon\in\realspos)\,(\exists n_\epsilon\in\nats)\,(\forall k,\ell > n_\epsilon)\,(\forall t,s\in\mathbf{I}\colon t\leq s)
~\norm{\presuper{k}T_t^s-\presuper{\ell}T_t^s} < \epsilon.
\end{equation}
Clearly, for any $t,s\in\mathbf{I}$ such that $t\leq s$, this implies that the sequence $\{\presuper{i}T_t^s\}_{i\in\nats}$ is Cauchy. Since the set of all transition matrices---of the same finite dimension---is trivially complete, this implies that the sequence $\{\presuper{i}T_t^s\}_{i\in\nats}$ has a limit $\presuper{*}T_t^s$, and that this limit is a transition matrix. We use $\mathcal{T}_*^{\mathbf{I}}$ to denote the family of transition matrices that consists of these limits. %We will now prove that $\mathcal{T}_*^{\mathbf{I}}$ is a restricted transition matrix system.

Fix any $t,r,s\in \mathbf{I}$ such that $t\leq r\leq s$. Then for any $i\in\nats$, because $\mathcal{T}_i^{\mathbf{I}}$ is a restricted transition matrix system, we know  that $\presuper{i}T_t^t=I$ and $\presuper{i}T_t^s=\presuper{i}T_t^r\presuper{i}T_r^s$, which implies that $\norm{\presuper{*}T_t^t-I}=\norm{\presuper{*}T_t^t-\presuper{i}T_t^t}$ and, due to Lemma~\ref{lemma:recursive}, that
\begin{align*}
\norm{\presuper{*}T_t^s-\presuper{*}T_t^r\presuper{*}T_r^s}
&\leq
\norm{\presuper{*}T_t^s-\presuper{i}T_t^s}+\norm{\presuper{i}T_t^r\presuper{i}T_r^s-\presuper{*}T_t^r\presuper{*}T_r^s}\\
&\leq
\norm{\presuper{*}T_t^s-\presuper{i}T_t^s}
+
\norm{\presuper{i}T_t^r-\presuper{*}T_t^r}
+
\norm{\presuper{i}T_r^s-\presuper{*}T_r^s}.
\end{align*}
Since we know that $\lim_{i\to+\infty}\presuper{i}T_t^t=\presuper{*}T_t^t$, $\lim_{i\to+\infty}\presuper{i}T_t^s=\presuper{*}T_t^s$, $\lim_{i\to+\infty}\presuper{i}T_t^r=\presuper{*}T_t^r$ and $\lim_{i\to+\infty}\presuper{i}T_r^s=\presuper{*}T_r^s$, this implies that $\norm{\presuper{*}T_t^t-I}=0$ and $\norm{\presuper{*}T_t^s-\presuper{*}T_t^r\presuper{*}T_r^s}=0$, or equivalently, that $\presuper{*}T_t^t=I$ and $\presuper{*}T_t^s=\presuper{*}T_t^r\presuper{*}T_r^s$. Since this is true for any $t,r,s\in I$ such that $t\leq r\leq s$, and because we already know that that the family $\mathcal{T}_*^{\mathbf{I}}$ consists of transition matrices, it follows from Proposition~\ref{prop:restr_trans_mat_system_if_semigroup} that $\mathcal{T}_*^{\mathbf{I}}$ is a restricted transition matrix system.
In the remainder of this proof, we will show that $\mathcal{T}_*^{\mathbf{I}}=\lim_{i\to\infty}\mathcal{T}_i^{\mathbf{I}}$. 

Fix any $\epsilon>0$ and consider the corresponding $n_\epsilon\in\nats$ whose existence is guaranteed by Equation~\eqref{eq:lemma:restricted_trans_mat_system_cauchy_converges:consequenceofCauchy}. Fix any $k>n_\epsilon$. For any $t,s\in\mathbf{I}$ such that $t\leq s$, it then follows from Equation~\eqref{eq:lemma:restricted_trans_mat_system_cauchy_converges:consequenceofCauchy} that, for all $\ell>n_\epsilon$:
\begin{equation*}
\norm{\presuper{k}T_t^s-\presuper{*}T_t^s}
\leq
\norm{\presuper{k}T_t^s-\presuper{\ell}T_t^s}
+
\norm{\presuper{\ell}T_t^s-\presuper{*}T_t^s} < \epsilon+\norm{\presuper{\ell}T_t^s-\presuper{*}T_t^s}.
\end{equation*}
Since $\lim_{\ell\to+\infty}\presuper{\ell}T_t^s=\presuper{*}T_t^s$, this implies that $\norm{\presuper{k}T_t^s-\presuper{*}T_t^s}\leq\epsilon$. Since this is true for all $t,s\in\mathbf{I}$ such that $t\leq s$, it follows from Equation~\eqref{eq:trans_mat_system_metric} that $d(\mathcal{T}_k^{\mathbf{I}},\mathcal{T}_*^{\mathbf{I}})\leq\epsilon$. Since $\epsilon>0$ was arbitrary, we conclude that
\begin{equation*}
(\forall\epsilon\in\realspos)\,(\exists n_\epsilon\in\nats)\,(\forall k > n_\epsilon)~d(\mathcal{T}_k^{\mathbf{I}},\mathcal{T}_*^{\mathbf{I}})\leq\epsilon,
\end{equation*}
which implies that $\mathcal{T}_*^{\mathbf{I}}=\lim_{i\to\infty}\mathcal{T}_i^{\mathbf{I}}$.
\end{proof}

% \begin{proof}[Proof of Proposition~\ref{prop:fullcoherent}]
% Since $\power$ is clearly an algebra of events, it follows from Reference~\cite{berti1991coherent} that $P$ is a coherent conditional probability if and only if it satisfies the conditions~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3} in Lemma~\ref{lemma:alt_axiom_prob}. The result now follows directly from Lemma~\ref{lemma:alt_axiom_prob}.
% \end{proof}


% \begin{lemma}\label{lemma:alt_axiom_prob}
% Let $P$ be any map from $\power\times\nonemptypower$ to $\reals$. Then, $P$ is a full conditional probability if and only if it satisfies
% \begin{enumerate}[label=P\arabic*:,ref=P\arabic*]
% \item $P(\cdot\,\vert\,C)$ is a non-negative, additive function on $\power$, for all $C\in\nonemptypower$;\label{lem:alt_axiom_prob_1}
% \item $P(A\,\vert\,C)=1$ for all $A\in\power$ and $C\in\nonemptypower$ such that $C\subseteq A$;\label{lem:alt_axiom_prob_2}
% \item $P(A\cap D\,\vert\,C)=P(A\,\vert\,D\cap C)P(D\,\vert\,C)$ for all $A\in\power$ and $C,D\cap C\in\nonemptypower$.\label{lem:alt_axiom_prob_3}
% \end{enumerate}
% Note that because $\power$ and $\nonemptypower$ both contain subsets of $\Omega$, the subset and intersection relations in~\ref{lem:alt_axiom_prob_2} and~\ref{lem:alt_axiom_prob_3} are properly defined.
% \end{lemma}
% \begin{proof}
% We start by proving the ``if" direction; if $P$ satisfies~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3}, it is a full conditional probability, i.e., it then satisfies~\ref{def:coh_prob_1}-\ref{def:coh_prob_4}.

% Property~\ref{def:coh_prob_1} is immediate from~\ref{lem:alt_axiom_prob_2}. Furthermore, property~\ref{def:coh_prob_3} is the formal definition of (finite-)additivity from~\ref{lem:alt_axiom_prob_1}. The non-negativity in~\ref{def:coh_prob_2} follows from the non-negativity in~\ref{lem:alt_axiom_prob_1}. To fully prove~\ref{def:coh_prob_2}, it remains to show that for all $A\in\power$ and all $C\in\nonemptypower$, it holds that $P(A\,\vert\,C)\leq 1$. Note that $A\cap A^c=\emptyset$, where $A^c$ denotes the complement of $A$ in $\Omega$. Hence, by~\ref{def:coh_prob_3}, we have $P(A\,\vert\,C)+P(A^c\,\vert\,C)=P(A\cup A^c\,\vert\,C)=P(\Omega\,\vert\,C)=1$, where the last equality follows from~\ref{lem:alt_axiom_prob_2}, since $C\subseteq \Omega$. Hence, by non-negativity of $P(A\,\vert\,C)$ and $P(A^c\,\vert\,C)$, we find that $P(A\,\vert\,C)\leq 1$.

% For property~\ref{def:coh_prob_4}, take any $A\in\power$ and any $C,D\in\nonemptypower$ such that $A\subseteq D\subseteq C$. Then clearly, $A\cap D=A$ and $D\cap C=D$. Hence, it also holds that $D\cap C\in\nonemptypower$. Therefore, by property~\ref{lem:alt_axiom_prob_3}, we have that $P(A\,\vert\,C)=P(A\cap D\,\vert\,C)=P(A\,\vert\,D\cap C)P(D\,\vert\,C)=P(A\,\vert\,D)P(D\,\vert\,C)$, which confirms~\ref{def:coh_prob_4}.

% We now prove the other direction; if $P$ satisfies~\ref{def:coh_prob_1}-\ref{def:coh_prob_4}, it also satisfies~\ref{lem:alt_axiom_prob_1}-\ref{lem:alt_axiom_prob_3}.

% The non-negativity in~\ref{lem:alt_axiom_prob_1} follows from~\ref{def:coh_prob_2}. The (finite-)additivity is equal to~\ref{def:coh_prob_3}, which concludes the proof for~\ref{lem:alt_axiom_prob_1}.

% For property~\ref{lem:alt_axiom_prob_2}, take any $A\in\power$ and any $C\in\nonemptypower$ such that $C\subseteq A$. Let $B\coloneqq A\setminus C$. Then clearly, $B\in\power$, $B\cup C=A$, and $B\cap C=\emptyset$. Hence, we have $P(A\,\vert\,C)=P(B\cup C\,\vert\,C)$, and by~\ref{def:coh_prob_3}, we have $P(B\cup C\,\vert\,C)=P(B\,\vert\,C)+P(C\,\vert\,C)$. Using properties~\ref{def:coh_prob_1} and~\ref{def:coh_prob_2}, it then follows that $P(B\,\vert\,C)=0$, and furthermore, $P(A\,\vert\,C)=P(B\cup C\,\vert\,C)=1$, confirming~\ref{lem:alt_axiom_prob_2}. 

% For property~\ref{lem:alt_axiom_prob_3}, we first show that $P(A\cap C\,\vert\,C) = P(A\,\vert\,C)$ for all $A\in\power$ and all $C\in\nonemptypower$. To show this, take any $A\in\power$, any $C\in\nonemptypower$, and let $B\coloneqq A\setminus(A\cap C)$. Then clearly, $A=B\cup(A\cap C)$ and $B\cap (A\cap C)=\emptyset$. Hence, by~\ref{def:coh_prob_3}, we have
% \begin{align*}
% P(A\,\vert\,C) = P(B\cup(A\cap C)\,\vert\,C)= P(B\,\vert\,C) + P(A\cap C\,\vert\,C).
% \end{align*}
% It remains to show that $P(B\vert\,C)=0$. Note that because $B=A\setminus(A\cap C)$, it follows that $B\cap C=\emptyset$. Therefore, by~\ref{def:coh_prob_3},
% \begin{align*}
% P(B\cup C\,\vert\,C)=P(B\,\vert\,C) + P(C\,\vert\,C) \\
% P(B\cup C\,\vert\,C)=P(B\,\vert\,C) + 1\,,
% \end{align*}
% using~\ref{def:coh_prob_1}. Since $0\leq P(B\,\vert\,C)$ and $P(B\cup C\,\vert\,C)\leq 1$ by~\ref{def:coh_prob_2}, it follows that $P(B\,\vert\,C)=0$. Hence,
% \begin{align*}
% P(A\,\vert\,C) = P(B\cup(A\cap C)\,\vert\,C)= P(B\,\vert\,C) + P(A\cap C\,\vert\,C) = P(A\cap C\,\vert\,C)\,.
% \end{align*}

% Now, to prove property~\ref{lem:alt_axiom_prob_3}, take any $A\in\power$ and any $C,D\cap C\in\nonemptypower$. Let $E\coloneqq A\cap D\cap C$, and let $G\coloneqq D\cap C$. Then clearly, $E\subseteq G\subseteq C$. Hence, by~\ref{def:coh_prob_4},
% \begin{align*}
% P(E\,\vert\,C) &= P(E\,\vert\,G)P(G\,\vert\,C) \\
% P(A\cap D\cap C\,\vert\,C) &= P(A\cap D\cap C\,\vert\,D\cap C)P(D\cap C\,\vert\,C)\,.
% \end{align*}
% Using the previously established property, we have that $P(A\cap D\cap C\,\vert\,C)=P(A\cap D\,\vert\,C)$, $P(A\cap D\cap C\,\vert\,D\cap C)=P(A\,\vert\,D\cap C)$, and $P(D\cap C\,\vert\,C)=P(D\,\vert\,C)$. Hence, we find
% \begin{align*}
% P(A\cap D\cap C\,\vert\,C) &= P(A\cap D\cap C\,\vert\,D\cap C)P(D\cap C\,\vert\,C) \\
% P(A\cap D\,\vert\,C) &= P(A\,\vert\,D\cap C)P(D\,\vert\,C)\,,
% \end{align*}
% which proves~\ref{lem:alt_axiom_prob_3}.
% \end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:stochastic_processes}}\label{app:stoch_proc}

\begin{proof}[Proof of \ref{def:coh_prob_2b}-\ref{def:coh_prob_5}]
Consider any $A\in\power$ and $C\in\nonemptypower$. It then follows from \ref{def:coh_prob_1} and \ref{def:coh_prob_3} that
\vspace{-7pt}
\begin{equation}\label{eq:extracohprop1}
P(A\vert C)
=P(A\cup C\vert C)-P(C\setminus A\vert C)
=1-P(C\setminus A\vert C)
\end{equation}\\[-20pt]
and
\begin{equation}\label{eq:extracohprop2}
P(A\cap C\vert C)
=P(C\vert C)-P(C\setminus A\vert C)
=1-P(C\setminus A\vert C).
\vspace{6pt}
\end{equation}
\ref{def:coh_prob_2b} follows from Equation~\eqref{eq:extracohprop1} and~\ref{def:coh_prob_2}. \ref{def:coh_prob_7} follows from Equations~\eqref{eq:extracohprop1} and~\eqref{eq:extracohprop2}. \ref{def:coh_prob_5} follows from \ref{def:coh_prob_8}, by letting $B\coloneqq\emptyset$. \ref{def:coh_prob_5} follows trivially from \ref{def:coh_prob_1}.
\end{proof}


%*** Next 2 Lemmas use old definitions, ignore for now please
%
%\begin{lemma}
%For any $t\in\realsnonneg$, let $\mathcal{A}_{>t}$ be the algebra of subsets of $\Omega$ generated by all the elementary events $(X_s=x)= \{\omega\in\Omega\,:\,\omega(s)=x\}$, for all $x\in\states$ and all $s>t$. Consider any pairwise-disjoint sequence $\{A_i\}_{i\in\nats}$ such that $A_i\in\mathcal{A}_{>t}$ for all $i\in\nats$ and $A_i\cap A_j=\emptyset$ for all $i,j\in\nats$ with $i\neq j$, and for which $\cup_{i=1}^\infty A_i\in\mathcal{A}_{>t}$. Then, there is some $n\in\nats$ such that, for all $j>n$, it holds that $A_j=\emptyset$.
%\end{lemma}
%\begin{proof}
%{\bf TODO}, but fairly trivial since $\mathcal{A}_{>t}$ is the algebra generated by all elementary events.
%\end{proof}
%
%\begin{lemma}\label{lem:stoch_process_sigma_add_on_algebra}
%Let $P:\mathcal{C}^{\mathrm{SP}}\to\reals$ be a stochastic process, where $\mathcal{C}^{\mathrm{SP}}$ is defined as in Definition~\ref{def:stoch_process}. Then, for all $t\in\realsnonneg$ and all $C\in\mathcal{F}_{\leq t}$, the map $P(\cdot\,\vert\,C):\mathcal{A}_{>t}\to\reals$ is $\sigma$-additive on $\mathcal{A}_{>t}$, meaning that for every pairwise-disjoint sequence $\{A_i\}_{i\in\nats}$ such that $A_i\in\mathcal{A}_{>t}$ for all $i\in\nats$ and $A_i\cap A_j=\emptyset$ for all $i,j\in\nats$ with $i\neq j$, and for which $\cup_{i=1}^\infty A_i\in\mathcal{A}_{>t}$, it holds that
%\begin{equation*}
%P\left(\bigcup_{i=1}^\infty A_i\,\Big\vert\,C\right) = \sum_{i=1}^\infty P(A_i\,\vert\,C)\,.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%Consider any $t\in\realsnonneg$, any $C\in\mathcal{F}_{\leq t}$, and any pairwise-disjoint sequence $\{A_i\}_{i\in\nats}$ in $\mathcal{A}_{>t}$ such that $\cup_{i=1}^\infty A_i\in\mathcal{A}_{>t}$. Then, by Lemma {\bf REF}, there is some $n\in\nats$ such that, for all $j>n$, it holds that $A_j=\emptyset$. Hence, we find that the claim reduces to
%\begin{align*}
%P\left(\bigcup_{i=1}^\infty A_i\,\Big\vert\,C\right) &= \sum_{i=1}^\infty P(A_i\,\vert\,C) \\
%P\left(\bigcup_{i=1}^n A_i\,\Big\vert\,C\right) &= \sum_{i=1}^n P(A_i\,\vert\,C) + \sum_{i=n+1}^\infty P(A_i\,\vert\,C) \\
%P\left(\bigcup_{i=1}^n A_i\,\Big\vert\,C\right) &= \sum_{i=1}^n P(A_i\,\vert\,C) + \sum_{i=n+1}^\infty P(\emptyset\,\vert\,C) \\
%P\left(\bigcup_{i=1}^n A_i\,\Big\vert\,C\right) &= \sum_{i=1}^n P(A_i\,\vert\,C)\,,
%\end{align*}
%which we know to be true from finite-additivity of $P(\cdot\,\vert\,C)$.
%
%**** Maybe expand this last argument a bit by linking it to~\ref{def:coh_prob_3}.
%\end{proof}

\begin{proof}[Proof of Corollary~\ref{corol:coherentextendable}]
First assume that $P$ can be extended to a full conditional probability $P^*$. Theorem~\ref{theo:fullcoherent} then implies that $P^*$ is a coherent conditional probability, and therefore, since $P$ is the restriction of $P^*$ to $\mathcal{C}$, it clearly follows from Definition~\ref{def:coherence} that $P$ is a coherent conditional probability.

Conversely, if $P$ is a coherent conditional probability on $\mathcal{C}$, it follows from Theorem~\ref{theo:largerdomain} that $P$ can be extended to a coherent conditional probability $P^*$ on $\power\times\nonemptypower$, which, because of Theorem~\ref{theo:fullcoherent}, is a full conditional probability.
\end{proof}
\begin{proof}[Proof of Corollary~\ref{corol:processiffcoherent}]
Trivial consequence of Corollary~\ref{corol:coherentextendable}.
\end{proof}
\begin{proof}[Proof of Proposition~\ref{prop:stochasticprocess:simpleproperties}]
Trivial consequence of Definitions~\ref{def:stoch_matrix}, \ref{def:cond_prob}, and \ref{def:well-behaved}.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:boundednon-emptyandclosed}]
We only give the proof for $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. The proof for $\smash{\overline{\partial}_{-}
{T^t_{t,\,x_u}}}$ is completely analogous. The proof for $\smash{\overline{\partial}
{T^t_{t,\,x_u}}}$ then follows trivially because a union of two bounded, non-empty and closed sets is always bounded, non-empty and closed itself.

We start by establishing the boundedness of $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Since $P$ is well-behaved, it follows from Definition~\ref{def:well-behaved} that there is some $B>0$ and $\delta>0$ such that
\begin{equation}\label{eq:boundedbyB}
(\forall 0<\Delta<\delta)
~
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)}\leq B.
\end{equation}
Consider now any $Q\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Because of Equation~\eqref{eq:rightouterderivative}, $Q$ is the limit of a sequence of matrices $\{Q_k\}_{k\in\nats}$, defined by
\begin{equation}\label{eq:sequenceofQsinproof}
Q_k\coloneqq\frac{1}{\Delta_k}
(T^{t+\Delta_k}_{t,\,x_u}-I)
\text{~~for all $k\in\nats$}.
\end{equation}
Because of Equation~\eqref{eq:boundedbyB}, the norms $\norm{Q_k}$ of these matrices are eventually (for large enough $k$) bounded above by $B$. Hence, it follows that $\norm{Q}\leq B$. Since this is true for any $Q\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, we find that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is bounded.


In order to prove that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is non-empty, we consider any sequence $\{\Delta_k\}_{k\in\nats}\to0^+$. The corresponding sequence of matrices $\{Q_k\}_{k\in\nats}$, as defined by Equation~\eqref{eq:sequenceofQsinproof}, is then bounded because $P$ is well-behaved---see Definition~\ref{def:well-behaved}---and therefore, it follows from the Bolzano-Weierstrass theorem that it has a convergent subsequence $\{Q_{k_i}\}_{i\in\nats}$ of which we denote the limit by $Q^*$. Hence, we have found a sequence $\{\Delta_{k_i}\}_{i\in\nats}\to0^+$ such that $\{Q_{k_i}\}_{i\in\nats}\to Q^*$.
Since we know from Lemma~\ref{prop:rate_from_stochastic_matrix} that each of the matrices in $\{Q_{k_i}\}_{i\in\nats}$ is a rate matrix, the limit $Q^*$ is also a rate matrix, which therefore clearly belongs to $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$.

We end by showing that $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$ is closed, or equivalently, that for any converging sequence $\{Q^*_k\}_{k\in\nats}$, of rate matrices in $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, the limit point $Q^*\coloneqq\lim_{k\to+\infty}Q^*_k$ is again an element of $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$. Since the rate matrices $Q^*_k$ belong to the bounded set $\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$, their limit $Q^*$ is a (real-valued) rate matrix. For any $k\in\nats$, it now follows from Equation~\eqref{eq:rightouterderivative} that there is some $0<\Delta_k<\nicefrac{1}{k}$ such that $\norm{Q_k-Q^*_k}\leq\nicefrac{1}{k}$, with $Q_k$ defined as in Equation~\eqref{eq:sequenceofQsinproof}.
Since
\begin{equation*}
0\leq\lim_{k\to+\infty}\norm{Q^*-Q_k}\leq\lim_{k\to+\infty}\norm{Q^*-Q^*_k}+\lim_{k\to+\infty}\norm{Q^*_k-Q_k}=0,
\end{equation*}
we find that the sequence $\{Q_k\}_{k\in\nats}$ converges to $Q^*$. Hence, because $\lim_{k\to+\infty}\Delta_k=0$, we conclude that $Q^*\in\smash{\overline{\partial}_{+}
{T^t_{t,\,x_u}}}$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:outerderivativebehaveslikelimit}]
Fix any $\epsilon>0$.
Assume \emph{ex absurdo} that
\begin{equation*}
(\forall\delta>0)(\exists0<\Delta<\delta)
(\forall Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}})
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}\geq\epsilon.
\end{equation*}
Clearly, this implies the existence of a sequence $\{\Delta_k\}_{k\in\nats}\to0^+$ such that
\begin{equation}\label{eq:boundednonelement}
\norm{Q_k-Q}\geq\epsilon
\text{~~for all $k\in\nats$ and all $Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}}$},
\end{equation}
with $Q_k$ defined as in Equation~\eqref{eq:sequenceofQsinproof}. As we know from the proof of Proposition~\ref{prop:boundednon-emptyandclosed}, the sequence $\{Q_k\}_{k\in\nats}$ has a convergent subsequence $\{Q_{k_i}\}_{i\in\nats}$ of which the limit $Q^*$ belongs to $\overline{\partial}_{+}
{T^t_{t,\,x_u}}$. Hence, since Equation~\eqref{eq:boundednonelement} implies that $\norm{Q^*-Q}\geq\epsilon$ for all $Q\in\overline{\partial}_{+}
{T^t_{t,\,x_u}}$, we find that $0=\norm{Q^*-Q^*}\geq\epsilon$. From this contradiction, it follows that there must be some $\delta_1>0$ such that Equation~\eqref{eq:outerderivativebehaveslikelimit1} holds for all $0<\Delta<\delta_1$. Similarly, using a completely analogous argument, we infer that there must be some $\delta_2>0$ such that Equation~\eqref{eq:outerderivativebehaveslikelimit2} holds for all $0<\Delta<\delta_1$. Now let $\delta\coloneqq\min\{\delta_1,\delta_2\}$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{corol:outersingleton}]
This follows trivially from Proposition~\ref{prop:outerderivativebehaveslikelimit}.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:cont_time_markov_chains}}

\begin{proof}[Proof of Proposition~\ref{prop:Markovhassystem}]
Consider any Markov chain $P\in\mprocesses$, with $\mathcal{T}_P$ its corresponding family of transition matrices. Then, because $P$ is a stochastic process, it follows from Proposition~\ref{prop:stochasticprocess:simpleproperties} that $T_t^t=I$ for all $t\in\realsnonneg$. 

Consider now any $t,r,s\in\realsnonneg$ with $t\leq r\leq s$. We need to show that $T_t^s=T_t^rT_r^s$. If $t=r$, we have that $T_t^s=T_t^rT_r^s=IT_r^s=T_t^s$, and hence the result follows trivially. Similarly, the claim is trivial for $r=s$. Hence, it remains to show that the claim holds for $t < r < s$. It follows from Definition~\ref{def:markov_property} that for all $x_t,x_r,x_s\in\states$,
\begin{equation*}
P(X_s=x_s\,\vert\,X_r=x_r,X_t=x_t) = P(X_s=x_s\,\vert\,X_r=x_r)\,.
\end{equation*}
Furthermore, because $P$ is a stochastic process, it follows from~\ref{def:coh_prob_6} that
\begin{align*}
P(X_s=x_s,X_r=x_r\,\vert\,X_t=x_t) &= P(X_s=x_s\,\vert\,X_r=x_r,X_t=x_t)P(X_r=x_r\,\vert\,X_t=x_t) \\
 &= P(X_s=x_s\,\vert\,X_r=x_r)P(X_r=x_r\,\vert\,X_t=x_t)\,,
\end{align*}
where the second equality used the Markov property. From~\ref{def:coh_prob_3}, it then follows that
\begin{equation*}
P(X_s=x_s\,\vert\,X_t=x_t) = \sum_{x_r\in\states} P(X_s=x_s,X_r=x_r\,\vert\,X_t=x_t)\,.
\end{equation*}
From the definition of $\mathcal{T}_P$ in Definition~\ref{def:trans_matrix}, it now follows that, for any $x_t,x_s\in\states$,
\begin{align*}
T_t^s(x_t,x_s) &= P(X_s=x_s\,\vert\,X_t=x_t) = \sum_{x_r\in\states} P(X_s=x_s,X_r=x_r\,\vert\,X_t=x_t) \\
 &= \sum_{x_r\in\states} P(X_s=x_s\,\vert\,X_r=x_r)P(X_r=x_r\,\vert\,X_t=x_t) = \sum_{x_r\in\states} T_t^r(x_t,x_r) T_r^s(x_r,x_s)\,,
\end{align*}
and hence, by the rules of matrix multiplication, we find that $T_t^s=T_t^rT_r^s$. Therefore, and because the $t,r,s\in\realsnonneg$ were arbitrary, $\mathcal{T}_P$ is a transition matrix system.

The fact that $\mathcal{T}_P$ is well-behaved if and only if $P$ is well-behaved follows immediately from Definition~\ref{def:trans_matrix} and Proposition~\ref{prop:stochasticprocess:simpleproperties}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:uniqueMarkovchain}]
Let
\begin{multline*}
\mathcal{C}\coloneqq\{
(X_s=y,X_u=x_u)\in\mathcal{C}^{\mathrm{SP}}
\colon 
u\in\mathcal{U}_\emptyset,~s>u,~x_u\in\states^u,~y\in\states
\}\\
\cup
\{
(X_0=y,X_\emptyset=x_\emptyset)\in\mathcal{C}^{\mathrm{SP}}\colon y\in\states
\}
\end{multline*}
and consider a real-valued function $\tilde{P}$ on $\mathcal{C}$ that is defined by 
\begin{equation}\label{eq:theo:uniqueMarkovchain:prob_func}
\tilde{P}(X_s=y\vert X_u=x_u)
%=
%\tilde{P}(X_s=y\vert X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})
\coloneqq
\begin{cases}
p(y)&\text{~if $u=\emptyset$}\\
T_{\max u}^s(x_{\max u},y)&\text{~otherwise}
\end{cases}
\text{~~~for all $(X_s=y,X_u=x_u)\in\mathcal{C}$.}
\end{equation}


We first prove that $\tilde{P}$ is a coherent conditional probability on $\mathcal{C}$. So consider any $n\in\nats$ and, for all $i\in\{1,\dots,n\}$, choose $(A_i,C_i)=(X_{s_i}=y_i,X_{u_i}=x_{u_i})\in\mathcal{C}$ and $\lambda_i\in\reals$. We need to show that
\begin{equation}\label{eq:theo:uniqueMarkovchain:coh1}
\max\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation}
with $C_0\coloneqq\cup_{i=1}^nC_i$.
Since every sequence $u_i$ is finite, there is some finite set $w=\{w_0,w_1,\dots,w_m\}\subset\reals_{\geq0}$ of time points, with $m\in\nats$, such that $0=w_0<w_1<\dots<w_m$ and, for all $i\in\{1,\dots,n\}$, $u_i\subseteq w$ and $s_i\in w$.
Let $P_w$ be the restriction of $\tilde{P}$ to $\mathcal{C}_w$, with $\mathcal{C}_w$ defined as in Lemma~\ref{lemma:simplechaincoherence}. Then since $P_w$ clearly satisfies the conditions of Lemma~\ref{lemma:simplechaincoherence}, it follows that $P_w$ is a coherent conditional probability. Because of Theorem~\ref{theo:largerdomain}, this implies that $P_w$ is the restriction to $\mathcal{C}_w$ of a coherent conditional probability $\tilde{P}_w$ on $\power\times\nonemptypower$. It now follows from Definition~\ref{def:coherence} that
\begin{equation}\label{eq:theo:uniqueMarkovchain:coh2}
\max\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(\tilde{P}_w(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0.
\end{equation}
By comparing Equations~\eqref{eq:theo:uniqueMarkovchain:coh1} and~\eqref{eq:theo:uniqueMarkovchain:coh2}, we see that in order to prove that $\tilde{P}$ is coherent, it suffices to show that, for all $i\in\{1,\dots,n\}$, $\tilde{P}_w(A_i\vert C_i)=\tilde{P}(A_i\vert C_i)$. So fix any $i\in\{1,\dots,n\}$. If $u_i=\emptyset$, then $s_i=0=w_0$ and therefore $(A_i,C_i)\in\mathcal{C}_w$, which implies that $\tilde{P}_w(A_i\vert C_i)=P_w(A_i\vert C_i)=\tilde{P}(A_i\vert C_i)$. If $u_i\neq\emptyset$, then since $u_i\subseteq w$, $s_i\in w$ and $s_i>u_i$, it follows from Lemma~\ref{lemma:simplechainextend} that $\tilde{P}_w(A_i\vert C_i)=\tilde{P}(A_i\vert C_i)$. Hence, $\tilde{P}$ is coherent.

% \begin{equation*}
% \tilde{P}_w(A_i\vert C_i)
% =\tilde{P}_w(X_{s_i}=y_i\vert X_{u_i}=x_{u_i})
% =\tilde{P}_w(X_{s_i}=y_i\vert X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})
% =T_{t_n}^s(x_n,y)
% =
% =P_w(A_i\vert C_i)
% =\tilde{P}(A_i\vert C_i)
% \end{equation*}


Therefore, and because of Corollary~\ref{corol:coherentextendable}, $\tilde{P}$ can be extended to a full conditional probability $\tilde{P}^*$. If we now let $P$ be the restriction of $\tilde{P}^*$ to $\mathcal{C}^\mathrm{SP}$, then clearly, because $\mathcal{C}\subseteq\mathcal{C}^\mathrm{SP}$, $P$ is a stochastic process that extends $\tilde{P}$. Due to Equation~\eqref{eq:theo:uniqueMarkovchain:prob_func}, this implies that $P$ is a Markov chain such that $\mathcal{T}_P=\mathcal{T}$ and, for all $y\in\states$, $P(X_0=y)=p(y)$. Lemma~\ref{lemma:samepandTissameP} implies that this Markov chain is unique. If $\mathcal{T}$ is well-behaved, then since $\mathcal{T}_P=\mathcal{T}$, it follows from Proposition~\ref{prop:Markovhassystem} that $P$ is well-behaved.
\end{proof}

\begin{lemma}\label{lemma:simplechaincoherence}
Let $w=\{w_0,w_1,\dots,w_m\}\subset\reals_{\geq0}$ be a finite set of time points, with $m\in\nats_0$, such that $w_0<w_1<\dots<w_m$.
Let $P_w$ be a real-valued function on
\begin{equation*}
\mathcal{C}_w\coloneqq
\left\{
(X_{w_j}=y,X_u=x_u)
\colon 
j\in\{0,\dots,m\},~
u=\{w_0,\dots,w_{j-1}\},~
y\in\states,~
x_u\in\states^u
\right\}
\end{equation*}
such that, for any $j\in\{0,\dots,m\}$, $u=\{w_0,\dots,w_{j-1}\}$ and $x_u\in\states^u$, $P_w(X_{w_j}=\cdot\,\vert X_u=x_u)$ is a probability mass function on $\states$. Then $P_w$ is a coherent conditional probability on $\mathcal{C}_w$.
\end{lemma}
\begin{proof}
We provide a proof by induction. Assume that this statement is true for any $m<k$, with $k\in\nats_\emptyset$. We will show that this implies that it is also true for $m=k$.

Consider any $n\in\nats$ and, for all $i\in\{1,\dots,n\}$, choose $(A_i,C_i)\in\mathcal{C}_w$ and $\lambda_i\in\reals$. We need to show that
\begin{equation}\label{eq:lemma:simplechaincoherence:TB}
\max\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(P_w(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation}
with $C_0\coloneqq\cup_{i=1}^nC_i$. %Clearly, without loss of generality, we may assume that for any $i_1,i_2\in\{1,\dots,n\}$ such that $i_1\neq i_2$, $(A_{i_1},C_{i_1})\neq(A_{i_2},C_{i_2})$.

For any $i\in\{1,\dots,n\}$, since $(A_i,C_i)\in\mathcal{C}_w$, there is some $j_i\in\{0,\dots,m\}$ and, for all $\ell\in\{0,\dots,j_i\}$, some $z_{\ell,i}\in\states$ such that
\begin{equation*}
A_i=(X_{w_{j_i}}=z_{j_i,i})
\text{~~and~~}
C_i=(X_{w_{0}}=z_{0,i}, \dots, X_{w_{j_i-1}}=z_{j_i-1,i}).
\end{equation*}
Let $S=\left\{i\in\{1,\dots,n\}\colon j_i<m\right\}$. If $S\neq\emptyset$, then by the induction hypothesis, we know that
\begin{equation*}
\max\left\{\sum_{i\in S}\lambda_i\ind{C_i}(\omega)\bigl(P_w(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0^*\right\}\geq0,
\end{equation*}
with $C_0^*\coloneqq\cup_{i\in S}C_i$. It follows that there is some $\omega^*\in C_0^*$ such that
\begin{equation}\label{eq:lemma:simplechaincoherence:supremumreached}
\sum_{i\in S}\lambda_i\ind{C_i}(\omega^*)\bigl(P_w(A_i\vert C_i)-\ind{A_i}(\omega^*)\bigr)\geq0.
\end{equation}
If $S=\emptyset$, then let $\omega^*$ be any element of $C_0$. Equation~\eqref{eq:lemma:simplechaincoherence:supremumreached} is then trivially satisfied. Hence, in all cases, we have found some $\omega^*\in C_0$ that satisfies Equation~\eqref{eq:lemma:simplechaincoherence:supremumreached}.


Let $C^*\coloneqq\cap_{1\leq \ell<m}(X_{w_\ell}=\omega^*(w_\ell))$ and $S^*\coloneqq\{i\in\{1,\dots,n\}\colon C_i=C^*\}$. Then by the assumptions of this lemma, there is some probability mass function $p$ on $\states$ such that, for all $x\in\states$, $P_w(X_{w_m}=x\vert C^*)=p(x)$. 
For all $x\in\states$, let $\lambda_x\coloneqq\sum_{\{i\in S^*\colon z_{m,i}=x\}}\lambda_i$.
Since $p$ is a probability mass function, it then follows that
\begin{equation*}
\sum_{i\in S^*}\lambda_i P_w(A_i\vert C^*)
=
\sum_{x\in\states}
\lambda_x p(x)
\geq
\sum_{x\in\states} \left(\min_{y\in\states}\lambda_y\right) p(x)
=
\left(\min_{y\in\states}\lambda_y\right)\sum_{x\in\states}p(x)
=\min_{y\in\states}\lambda_y.
\end{equation*}
Now let $y^*$ be any element of $\states$ such that $\min_{y\in\states}\lambda_y=\lambda_{y^*}$ (since $\states$ is finite, this is always possible). Let $\omega^{**}$ be any path in $\Omega$ such that $\omega^{**}\in C^*$ and $\omega^{**}(w_m)=y^*$; Equation~\eqref{eq:path_exists_for_finite_points} guarantees that this $\omega^{**}\in\Omega$ exists. Then
\begin{equation*}
\sum_{i\in S^*}\lambda_i\bigl(P_w(A_i\vert C_i)-\ind{A_i}(\omega^{**})\bigr)
\geq
\min_{y\in\states}\lambda_y
-\sum_{i\in S^*}\lambda_i\ind{A_i}(\omega^{**})
=\lambda_{y^*}-\lambda_{y^*}=0.
\end{equation*}

Let $S^{**}\coloneqq\{1,\dots,n\}\setminus(S\cup S^*)$. Since $\omega^{**}\in C^*$, we find that $\ind{C_i}(\omega^{**})=\ind{C_i}(\omega^{*})$ and $\ind{A_i}(\omega^{**})=\ind{A_i}(\omega^{*})$ for all $i\in S$, that $\ind{C_i}(\omega^{**})=1$ for all $i\in S^{*}$, and that $\ind{C_i}(\omega^{**})=0$ for all $i\in S^{**}$. Hence, it follows from Equation~\eqref{eq:lemma:simplechaincoherence:supremumreached} that
\begin{equation*}
\sum_{i=1}^n\lambda_i\ind{C_i}(\omega^{**})\bigl(P_w(A_i\vert C_i)-\ind{A_i}(\omega^{**})\bigr)
\geq
\sum_{i\in S^*}\lambda_i\bigl(P_w(A_i\vert C^*)-\ind{A_i}(\omega^{**})\bigr).
\end{equation*}
By combining this inequality with the previous one, we find that in order to show that Equation~\eqref{eq:lemma:simplechaincoherence:TB} holds, it suffices to prove that $\omega^{**}\in C_0$. 

In order to prove this, it suffices to notice that the question of whether or not a path $\omega\in\Omega$ belongs to $C_0$, only depends on the values $\omega(t)$ of $\omega$ at time points $t\in\{w_0,\dots,w_{m-1}\}$. Indeed, since we infer from $\omega^{**}\in C^*$ that the value of $\omega^*$ and $\omega^{**}$ at these time points is the same, and because $\omega^*\in C_0$, this implies that $\omega^{**}\in C_0$.
\end{proof}

\begin{lemma}\label{lemma:simplechainextend}
Let $w=\{w_0,w_1,\dots,w_m\}\subset\reals_{\geq0}$ be a finite set of time points, with $m\in\nats_0$, such that $w_0<w_1<\dots<w_m$. Let $\mathcal{T}$ be a transition matrix system and let $\tilde{P}_w$ be any full conditional probability such that for all $j\in\{1,\dots,m\}$ and $x_{w_{\ell}}\in\states$, $\ell\in\{0,\dots,j\}$:
\begin{equation*}
\tilde{P}_w(X_{w_j}=x_{w_j}\vert X_{w_0}=x_{w_0},\dots,X_{w_{j-1}}=x_{w_{j-1}})=T_{w_{j-1}}^{w_j}(x_{w_{j-1}},x_{w_j}).
\end{equation*}
Then for any $s\in w$ and $u\subseteq w$ such that $s>u$ and $u\neq\emptyset$, any $y\in\states$ and any $x_u\in\states^u$, we have that
\begin{equation*}
\tilde{P}_w(X_s=y\vert X_u=x_u)
=T_{\max u}^s(x_{\max u},y).
\end{equation*}
\end{lemma}
\begin{proof}
Since $\emptyset\neq u\subseteq w$, $s\in w$ and $s>u$, it follows that there is some $j\in\{1,\dots,m\}$ such that $s=w_j$ and $u\subseteq\{w_0,\dots,w_{j-1}\}$.

We provide a proof by induction. If $s=w_1$, then $u=\{w_0\}$, and therefore, the result follows trivially from the assumptions in this lemma. Assume now that the result is true for $s=w_j$, with $1\leq j<m$. We will prove that this implies that it is also true for $s=w_{j+1}$. We consider two cases: $\max u=w_j$ and $\max u<w_j$.

If $\max u=w_j$, then with $v\coloneqq\{w_0,\dots,w_j\}\setminus u$:
\begin{align*}
\tilde{P}_w(X_{w_{j+1}}=y\vert X_u=x_u)
&=\sum_{z_{v}\in\states^{v}}\tilde{P}_w(X_{w_{j+1}}=y, X_{v}=z_{v}\vert X_u=x_u)\\
&=\sum_{z_{v}\in\states^{v}}
\tilde{P}_w(X_{w_{j+1}}=y\vert X_u=x_u, X_{v}=z_{v})
\tilde{P}_w(X_{v}=z_{v}\vert X_u=x_u)\\
&=\sum_{z_{v}\in\states^{v}}
T_{\max u}^{w_{j+1}}(x_{\max u},y)
\tilde{P}_w(X_{v}=z_{v}\vert X_u=x_u)\\[-1mm]
&\quad\quad\quad\quad\quad\quad
=T_{\max u}^{w_{j+1}}(x_{\max u},y)
\sum_{z_{v}\in\states^{v}}
\tilde{P}_w(X_{v}=z_{v}\vert X_u=x_u)\\[-1mm]
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad~~~
=T_{\max u}^{w_{j+1}}(x_{\max u},y),
\end{align*}
where the first equality follows from~\ref{def:coh_prob_3}, the second equality follows from \ref{def:coh_prob_6}, the third equality follows from the assumptions in this lemma and the fact that $\max u=w_j$, and the last equality follows from \ref{def:coh_prob_3} and~\ref{def:coh_prob_5}.

If $\max u<w_j$, then with $v\coloneqq\{w_0,\dots,w_{j-1}\}\setminus u$:
\begin{align*}
&\tilde{P}_w(X_{w_{j+1}}=y\vert X_u=x_u)\\[1,5mm]
&=\sum_{z_{w_j}\in\states}
\sum_{z_{v}\in\states^{v}}
\tilde{P}_w(X_{w_{j+1}}=y, X_{w_j}=z_{w_j}, X_v=z_v\vert X_u=x_u)\\
&=\sum_{z_{w_j}\in\states}
\sum_{z_{v}\in\states^{v}}
\tilde{P}_w(X_{w_{j+1}}=y\vert X_u=x_u, X_{w_j}=z_{w_j}, X_v=z_v)\\[-4mm]
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad~\,
\tilde{P}_w(X_v=z_v\vert X_u=x_u, X_{w_j}=z_{w_j})
\tilde{P}_w(X_{w_j}=z_{w_j}\vert X_u=x_u)\\[4mm]
&=\sum_{z_{w_j}\in\states}
\sum_{z_{v}\in\states^{v}}
T_{w_j}^{w_{j+1}}(z_{w_j},y)
\tilde{P}_w(X_v=z_v\vert X_u=x_u, X_{w_j}=z_{w_j})
\tilde{P}_w(X_{w_j}=z_{w_j}\vert X_u=x_u)\\
&=\sum_{z_{w_j}\in\states}
\sum_{z_{v}\in\states^{v}}
T_{w_j}^{w_{j+1}}(z_{w_j},y)
\tilde{P}_w(X_v=z_v\vert X_u=x_u, X_{w_j}=z_{w_j})
T_{\max u}^{w_{j}}(x_{\max u},z_{w_j})\\
&=\sum_{z_{w_j}\in\states}
T_{w_j}^{w_{j+1}}(z_{w_j},y)
T_{\max u}^{w_{j}}(x_{\max u},z_{w_j})
\sum_{z_{v}\in\states^{v}}
\tilde{P}_w(X_v=z_v\vert X_u=x_u, X_{w_j}=z_{w_j})
\\
&=\sum_{z_{w_j}\in\states}
T_{w_j}^{w_{j+1}}(z_{w_j},y)
T_{\max u}^{w_{j}}(x_{\max u},z_{w_j})
=T_{\max u}^{w_{j+1}}(x_{\max u},y),
\end{align*}
where the first equality follows from~\ref{def:coh_prob_3}, the second equality follows from \ref{def:coh_prob_6}, the third equality follows from the assumptions in this lemma, the fourth equality follows from the induction hypothesis, the sixth equality follows from \ref{def:coh_prob_3} and~\ref{def:coh_prob_5}, and the last equality follows from Equation~\eqref{eq:transmatrixproduct}.
\end{proof}

\begin{lemma}\label{lemma:samepandTissameP}
Consider two Markov chains $P_1,P_2\in\mprocesses$ such that $\mathcal{T}_{P_1}=\mathcal{T}_{P_2}$ and, for all $y\in\states$, $P_1(X_0=y)=P_2(X_0=y)$. Then $P_1=P_2$.
\end{lemma}
\begin{proof}
Let $\mathcal{T}\coloneqq\mathcal{T}_{P_1}=\mathcal{T}_{P_2}$ be the common transition matrix system of $P_1$ and $P_2$ and let $p$ be their common initial probability mass function, as defined by $p(y)\coloneqq P_1(X_0=y)=P_2(X_0=y)$ for all $y\in\states$. Let $\tilde{P}$ be a real-valued function on $\mathcal{C}$, with $\mathcal{C}$ and $\tilde{P}$ defined as in the proof of Theorem~\ref{theo:uniqueMarkovchain}. It then follows from Definition~\ref{def:markov_property} that the restriction of $P_1$ and $P_2$ to $\mathcal{C}$ is equal to $\tilde{P}$. Furthermore, for any $s>0$, $y\in\states$ and $j\in\{1,2\}$, we find that
\begin{align*}
P_j(X_s=y)
=\sum_{x\in\states}P_j(X_s=y, X_0=x)
&=\sum_{x\in\states}P_j(X_s=y\vert X_0=x)P_j(X_0=x)\\
&=\sum_{x\in\states}\tilde{P}(X_s=y\vert X_0=x)\tilde{P}(X_0=x).
\end{align*}
Hence, the restrictions of $P_1$ and $P_2$ to
\begin{align*}
\mathcal{C}^*
\coloneqq&\mathcal{C}\cup
\{
(X_s=y,X_\emptyset=x_\emptyset)
\colon 
s\in\reals_{>0},~y\in\states
\}\\
=&\{
(X_s=y,X_u=x_u)
\colon 
u\in\mathcal{U},~s\in\reals_{\geq0},~s>u,~x_u\in\states^u,~y\in\states
\}
\end{align*}
are identical. We denote this common restriction by $\tilde{P}^*$.

Consider now any $(A,X_u=x_u)\in\mathcal{C}^\mathrm{SP}$. Then since $A\in\mathcal{A}_u$, there is some finite set of time points $v=\{v_1,v_2,\dots,v_n\}\subseteq\reals_{\geq0}$, with $n\in\nats_0$, such that $\max u<v_1<v_2<\dots<v_n$, and some set $S\subseteq\states^{u\cup v}$ such that $A=\cup_{z_{u\cup v}\in S}(X_{u\cup v}=z_{u\cup v})$. 
Let $S_v\coloneqq\{s_v\in\states^v\colon (x_u,z_v)\in S\}$.
For any $j\in\{1,2\}$, we then find that
\begin{align*}
P_j(A\vert X_u=x_u)
&=\sum_{z_{u\cup v}\in S}
P_j(X_{u\cup v}=z_{u\cup v}\vert X_u=x_u)\\
%&=\sum_{z_{v}\in S_v}
%P_1(X_{u}=x_{u}, X_v=z_v\vert X_u=x_u)\\
%&=\sum_{z_{v}\in S_v}
%P_1(X_v=z_v\vert X_u=x_u)\\
&=\sum_{z_{v}\in S_v}
P_j(X_{v_1}=z_{v_1}, X_{v_2}=z_{v_2}, \dots, X_{v_n}=z_{v_n}\vert X_u=x_u)\\[-1mm]
&=\sum_{z_{v}\in S_v}
\,\,
\prod_{i=1}^n
\,\,
P_j(X_{v_i}=z_{v_i}\vert X_u=x_u, X_{v_1}=z_{v_1}, \dots, X_{v_{i-1}}=z_{v_{i-1}})\\
&=\sum_{z_{v}\in S_v}
\,\,
\prod_{i=1}^n
\,\,
\tilde{P}^*(X_{v_i}=z_{v_i}\vert X_u=x_u, X_{v_1}=z_{v_1}, \dots, X_{v_{i-1}}=z_{v_{i-1}}),
\end{align*}
which implies that $P_1(A\vert X_u=x_u)=P_2(A\vert X_u=x_u)$. Since this is the case for any $(A,X_u=x_u)\in\mathcal{C}^\mathrm{SP}$, it follows that $P_1=P_2$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:rate_has_unique_homogen_markov_process}]
Since we know from Proposition~\ref{prop:systemQ} that $\mathcal{T}_Q$ is a well-behaved transition matrix system, it follows from Theorem~\ref{theo:uniqueMarkovchain} that there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$, and that this Markov chain is furthermore well-behaved. Since it---trivially---follows from Definition~\ref{def:systemfromQ} that $\mathcal{T}_Q$ satisfies Equation~\eqref{eq:homogeneousMarkov}, Definition~\ref{def:homogeneousMarkov} implies that $P$ is homogeneous.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:homogeneoushasQ}]
Because of Proposition~\ref{prop:boundednon-emptyandclosed}, we know that $\overline{\partial}_{+}
{T^0_{0}}$ is a non-empty bounded set of rate matrices, which implies that there is some real $B>0$ such that $\norm{Q'}\leq B$ for all $Q'\in\overline{\partial}_{+}
{T^0_{0}}$. Let $Q$ be any element of $\overline{\partial}_{+}
{T^0_{0}}$.


Fix any $c\geq0$, $\epsilon>0$ and $\delta>0$. 
It then follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and~\ref{N:homogeneous} that there is some $\delta^*>0$ such that
\begin{equation}
\label{eq:homogeneoushasQ1}
(\forall 0<\Delta^*<\delta^*)
~
(\exists Q^*\in\overline{\partial}_{+}
{T^0_{0}})
~
\norm{T_0^{\Delta^*}-(I+\Delta^*Q^*)}<\Delta^*\epsilon.
\end{equation}
Furthermore, because of Equation~\eqref{eq:rightouterderivative} and~\ref{N:homogeneous}, there is some $0<\Delta<\min\{\delta,\delta^*\}$ such that
\begin{equation}
\label{eq:homogeneoushasQ2}
\norm{T^{\Delta}_{0}-(I+\Delta Q)}<\Delta\epsilon.
\end{equation}
If we now define $n\coloneqq\lfloor\nicefrac{c}{\Delta}\rfloor$ and $d\coloneqq c-n\Delta$, then $n\Delta\leq c<(n+1)\Delta$ and therefore also $0\leq d<\Delta$. Because of Proposition~\ref{prop:Markovhassystem}, Equation~\eqref{eq:transmatrixproduct} and Definition~\ref{def:homogeneousMarkov}, we know that
\begin{equation*}
T_0^c=\left(
\prod_{j=1}^{n}
T_{(j-1)\Delta}^{j\Delta}
\right)
T_{n\Delta}^c
=\left(T_0^{\Delta}\right)^{n}
T_0^{d}
\end{equation*}
and therefore, it follows from Lemma~\ref{lemma:recursive} that
\begin{equation}
\label{eq:homogeneoushasQ3}
\norm{
	e^{Qc}-T_0^c
}
=
\norm{
\left(T_0^{\Delta}\right)^{n}
T_0^{d}
-
\left(
e^{Q\Delta}
\right)^{n}
e^{Qd}
}
\leq
n\norm{T_0^{\Delta}-e^{Q\Delta}}
+\norm{T_0^{d}-e^{Qd}}.
\end{equation}
From Equation~\eqref{eq:homogeneoushasQ2} and Lemma~\ref{lemma:linearpartofexponential}, we infer that
\begin{equation}
\label{eq:homogeneoushasQ4}
\norm{T_0^{\Delta}-e^{Q\Delta}}
\leq
\norm{T_0^{\Delta}-(I+\Delta Q)}
+
\norm{(I+\Delta Q)-e^{Q\Delta}}
\leq
\Delta\epsilon
+
\Delta^2\norm{Q}^2.
\end{equation}
Since $d<\Delta<\delta^*$, we infer from Equation~\eqref{eq:homogeneoushasQ1} that there is some $Q^*\in\overline{\partial}_{+}
{T^0_{0}}$ such that $\norm{T_0^{d}-(I+d Q^*)}<d\epsilon$. Hence, also using Lemma~\ref{lemma:linearpartofexponential}, we find that
\begin{align}
\norm{T_0^{d}-e^{Qd}}
&\leq
\norm{T_0^{d}-(I+d Q^*)}
+
\norm{(I+d Q^*)-(I+d Q)}
+
\norm{(I+d Q)-e^{Qd}}\notag\\
&\leq
d\epsilon+d\norm{Q^*-Q}
+d^2\norm{Q}^2
\leq
d\epsilon+d\norm{Q^*}
+d\norm{Q}
+d^2\norm{Q}^2.\label{eq:homogeneoushasQ5}
\end{align}
By combining Equations~\eqref{eq:homogeneoushasQ3}, \eqref{eq:homogeneoushasQ4} and~\eqref{eq:homogeneoushasQ5}, it follows that
\begin{equation*}
\norm{
	e^{Qc}-T_0^c
}
\leq
n\Delta\epsilon
+
n\Delta^2\norm{Q}^2
+
d\epsilon
+d\norm{Q^*}
+d\norm{Q}
+d^2\norm{Q}^2.
\end{equation*}
Taking into account that $\norm{Q}\leq B$, $\norm{Q^*}\leq B$, $n\Delta\leq c$ and $d<\Delta<\delta$, this implies that
\begin{equation*}
\norm{
	e^{Qc}-T_0^c
}
\leq
c\epsilon
+
c\delta B^2
+
\delta\epsilon
+2\delta B
+\delta^2 B^2.
\end{equation*}
Since this is true for any $\epsilon>0$ and $\delta>0$, it follows that $\norm{e^{Qc}-T_0^c}\leq0$, which implies that $T_0^c=e^{Qc}$. Since this is true for all $c\geq0$, it follows from Definition~\ref{def:homogeneousMarkov} that
\begin{equation}\label{eq:homogeneoushasQ6}
T_t^s=T_0^{s-t}=e^{Q(s-t)}
\text{~~for all $0\leq t\leq s$,}
\end{equation}
or equivalently, that $\mathcal{T}_P=\mathcal{T}_Q$.

Finally, we prove that $Q$ is unique. Assume \emph{ex absurdo} that this is not the case, or equivalently, that there are rate matrices $Q_1$ and $Q_2$, with $Q_1\neq Q_2$, such that $\mathcal{T}_P=\mathcal{T}_{Q_1}$ and $\mathcal{T}_P=\mathcal{T}_{Q_2}$. It then follows from Definition~\ref{def:systemfromQ} that $\partial_{+}{T^0_{0}}=Q_1$ and $\partial_{+}{T^0_{0}}=Q_2$, which implies that $Q_1=Q_2$. From this contradiction, it follows that $Q$ is indeed unique.
\end{proof}


\begin{lemma}\label{lemma:linearpartofexponential}
Consider any $Q\in\mathcal{R}$ and any $\Delta\geq0$. Then,
\begin{equation*}
\norm{e^{Q\Delta}-(I+\Delta Q)}\leq
\Delta^2\norm{Q}^2.
\end{equation*}
\end{lemma}
\begin{proof}
This is a special case of a more general lemma that we prove later. To see this, refer to our comments in Section~\ref{sec:properties_lower_trans}, where we note that the rate matrix $Q$ is also a lower transition rate operator $\lrate=Q$, for which the corresponding lower transition operator $L_0^{\Delta}$ satisfies $L_0^{\Delta}=e^{Q\Delta}$. This lemma then follows by combining Theorem~\ref{theo:convergencelowerbound} with Lemma~\ref{lemma:justthelinearpart}.
%To avoid trivialities, we assume that $\Delta\neq 0$ and $\norm{Q}\neq 0$. 
%
%Recall the $\epsilon-\delta$ limit expression for the matrix exponential, given by Equation~\eqref{eq:matrix_exp_limit}. Now, consider any $\epsilon\in\realspos$, and let $\delta^*\coloneqq\min\{\nicefrac{1}{\norm{Q}},\delta\}$, where $\delta$ satisfies Equation~\eqref{eq:matrix_exp_limit}. Take any $u\in\mathcal{U}_{[0,\Delta]}$ such that $\sigma(u)<\delta^*$. Then, clearly,
%\begin{align*}
%\norm{e^{Q\Delta}-(I+\Delta Q)} &\leq \norm{e^{Q\Delta} - \prod_{i=1}^n(I+\Delta_iQ)} + \norm{\prod_{i=1}^n(I+\Delta_iQ) - (I+\Delta Q)} \\
% &< \norm{\prod_{i=1}^n(I+\Delta_iQ) - (I+\Delta Q)} + \epsilon\,.
%\end{align*}
%Furthermore, because $Q$ is a rate matrix, it is easily verified that $Q$ satisfies~\ref{LR:constantzero}--\ref{LR:nondiagpos}. Hence, $Q$ is also a lower transition rate operator. Therefore, and because $\sigma(u)<\delta^*\leq\nicefrac{1}{\norm{Q}}$, it follows from Lemma~\ref{lemma:justthelinearpart} that
%\begin{equation*}
%\norm{\prod_{i=1}^n(I+\Delta_iQ) - (I+\Delta Q)} \leq \Delta^2\norm{Q}^2\,.
%\end{equation*}
%Hence, we find
%\begin{equation*}
%\norm{e^{Q\Delta}-(I+\Delta Q)} < \Delta^2\norm{Q}^2 + \epsilon\,.
%\end{equation*}
%Because this holds for any $\epsilon\in\realspos$, the statement in the lemma  follows.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:iCTMC}}


\begin{proof}[Proof of Theorem~\ref{theo:aanelkaarplakken}]
Let $\mathcal{C}\coloneqq\mathcal{C}_\emptyset\cup(\bigcup_{x_u\in\states^u}\mathcal{C}_{x_u})$, with
\begin{multline}\label{eq:theo:aanelkaarplakken:firstpartofdomain}
\mathcal{C}_\emptyset\coloneqq
\{(A,X_v=x_v)\in\mathcal{C}^{\mathrm{SP}}\colon \max v<\max u\text{~and~}\\A\in\left\langle
\left\{
(X_t=x)
\colon
x\in\states,t\in[0,\max u]
\right\}
\right\rangle\}
\end{multline}
and
\begin{equation}\label{eq:theo:aanelkaarplakken:secondpartofdomain}
\mathcal{C}_{x_u}\coloneqq\{
(A,X_v=x_v)\in\mathcal{C}^\mathrm{SP}
\colon
u\subseteq v\in\mathcal{U},\,
x_{v\setminus u}\in\states^{v\setminus u},\,
 A\in\mathcal{A}_{u\cup(v\setminus[0,\max u])}
\}
\end{equation}
for all $x_u\in\states^u$, and consider a real-valued function $\tilde{P}$ on $\mathcal{C}$ that is defined by 
\begin{equation}\label{eq:theo:aanelkaarplakken:defPtilde}
\tilde{P}(A\vert X_v=x_v)
\coloneqq
\begin{cases}
P_\emptyset(A\vert X_v=x_v)&\text{~if $(A,X_v=x_v)\in\mathcal{C}_\emptyset$}\\
P_{x_u}(A\vert 
X_{u\cup(v\setminus[0,\max u])}=x_{u\cup(v\setminus[0,\max u])})&\text{~if $(A,X_v=x_v)\in\mathcal{C}_{x_u}$}
\end{cases}
\end{equation}
for all $(A,X_v=x_v)\in\mathcal{C}$.

We first prove that $\tilde{P}$ is a coherent conditional probability on $\mathcal{C}$. So consider any $n\in\nats$ and, for all $i\in\{1,\dots,n\}$, choose $(A_i,C_i)\in\mathcal{C}$ and $\lambda_i\in\reals$. We need to show that
\begin{equation}\label{eq:theo:aanelkaarplakken:coh}
\max\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation}
with $C_0\coloneqq\cup_{i=1}^nC_i$.

Let $S^*\coloneqq\left\{i\in\{1,\dots,n\}\colon(A_i,C_i)\in\mathcal{C}_\emptyset\right\}$. We first consider the case $S^*\neq\emptyset$. Then since $P_\emptyset$ is a stochastic process, it follows from Corollary~\ref{corol:processiffcoherent} and Equation~\eqref{eq:theo:aanelkaarplakken:defPtilde} that
\begin{equation*}%\label{eq:theo:aanelkaarplakken:cohfirstpart}
\max\left\{\sum_{i\in S^*}\lambda_i\ind{C_i}(\omega)\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C^*\right\}\geq0,
\end{equation*}
with $C^*\coloneqq\cup_{i\in S^*}C_i$. Therefore, there is some $\omega^*\in C^*$ such that
\begin{equation}\label{eq:theo:aanelkaarplakken:geqfirstpart}
\sum_{i\in S^*}\lambda_i\ind{C_i}(\omega^*)\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega^*)\bigr)\geq0.
\end{equation}
If $S^*=\emptyset$, we let $\omega^*$ be any element of $C_0$ (this is always possible, because $C_0\neq\emptyset$). Clearly, this path $\omega^*$ will then also satisfy Equation~\eqref{eq:theo:aanelkaarplakken:geqfirstpart}.


Let $x_u^*\in\states^u$ be defined by $x_u^*\coloneqq \omega^*\vert_u$.
Then for all $i\in\{1,\dots,n\}$ such that $(A_i,C_i)\in\mathcal{C}_{x_u^*}$, we know from Equation~\eqref{eq:theo:aanelkaarplakken:secondpartofdomain} that there are $u\subseteq v_i\in\mathcal{U}$ and $x_{v_i\setminus u}\in\states^{v_i\setminus u}$ such that
\begin{equation}\label{eq:theo:aanelkaarplakken:Cisplit}
C_i=(X_u=x_u^*)\cap(X_{v_i\setminus u}=x_{v_i\setminus u})=C_i^*\cap C_i^{**},\vspace{2mm}
\end{equation}
with $C_i^{*}\coloneqq(X_{v_i\cap [0,\max u]}=x_{v_i\cap [0,\max u]})$, and
\begin{equation}\label{eq:theo:aanelkaarplakken:Cistarstar}
C_i^{**}\coloneqq(X_u=x_u^*)\cap(X_{v_i\setminus [0,\max u]}=x_{v_i\setminus [0,\max u]})\,.
\end{equation}
Using this notation, we define
\begin{equation}\label{eq:theo:aanelkaarplakken:Sstarstardef}
S^{**}\coloneqq
\{
i\in\{1,\dots,n\}
\colon
(A_i,C_i)\in\mathcal{C}_{x_u^*}
\text{~and~}
\ind{C_i^*}(\omega^*)=1
\}.
\end{equation}


We first consider the case $S^{**}\neq\emptyset$. Since $P_{x_u^*}$ is a stochastic process, it then follows from Corollary~\ref{corol:processiffcoherent} that
\begin{equation*}%\label{eq:theo:aanelkaarplakken:secondpart}
\max\left\{\sum_{i\in S^{**}}\lambda_i\ind{C_i^{**}}(\omega)\bigl(P_{x_u^*}(A_i\vert C_i^{**})-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C^{**}\right\}\geq0,
\end{equation*}
with $C^{**}\coloneqq\cup_{i\in S^{**}}C_i^{**}$. Because of Equation~\eqref{eq:theo:aanelkaarplakken:defPtilde}, this implies that
\begin{equation*}%\label{eq:theo:aanelkaarplakken:secondpart}
\max\left\{\sum_{i\in S^{**}}\lambda_i\ind{C_i^{**}}(\omega)\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C^{**}\right\}\geq0,
\end{equation*}
which allows us to infer that there is some $\omega^{**}\in C^{**}$ such that
\begin{equation}\label{eq:theo:aanelkaarplakken:geqsecondpart}
\sum_{i\in S^{**}}\lambda_i\ind{C_i^{**}}(\omega^{**})\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega^{**})\bigr)\geq0.
\end{equation}
Furthermore, since $\omega^{**}\in C^{**}$, Equation~\eqref{eq:theo:aanelkaarplakken:Cistarstar} implies that
\begin{equation}\label{eq:theo:aanelkaarplakken:starstarniceonu}
\omega^{**}\vert_u=x_u^* = \omega^{*}\vert_u\,.
%\text{~~for all $t\in u$.}
\end{equation}
If $S^{**}=\emptyset$, we let $\omega^{**}=\omega^{*}$. Clearly, also in this case, $\omega^{**}$ satisfies Equations~\eqref{eq:theo:aanelkaarplakken:geqsecondpart} and~\eqref{eq:theo:aanelkaarplakken:starstarniceonu}.

For any $i\in\{1,\ldots,n\}$, because $(A_i,C_i)\in\mathcal{C}$, there exists some finite sequence of time points $w_{C_i}\in\mathcal{U}$ such that $C_i$ only depends on the time points in $w_{C_i}$. Furthermore, it follows from Equations~\eqref{eq:theo:aanelkaarplakken:firstpartofdomain} and~\eqref{eq:theo:aanelkaarplakken:secondpartofdomain} that $A_i$ is an element of some algebra $\mathcal{A}$ that is generated by a set of events that only depend on a finite number of time points. Therefore, there is also some finite sequence of time points $w_{A_i}\in\mathcal{U}$, such that $A_i$ only depends on the time points in $w_{A_i}$. If we now let $w_i\coloneqq w_{A_i}\cup w_{C_i}$, then $(A_i,C_i)$ only depends on the (finite) sequence of time points $w_i$.

Because this holds for any $i\in\{1,\ldots,n\}$, this implies the existence of some finite sequence $w\in\mathcal{U}$ such that $w_i\subseteq w$ for all $i\in\{1,\ldots,n\}$.

Now let $\omega^{***}\in\Omega$ be any path such that, for all $s\in w$,
\begin{equation*}%\label{eq:theo:aanelkaarplakken:omegatriplestar}
\omega^{***}(s)\coloneqq
\begin{cases}
\omega^{*}(s) & \text{if $s<\max u$}\\
\omega^{**}(s) & \text{if $s\geq \max u$}
\end{cases}
%\text{~~~for all $s\in w$.}
\end{equation*}
Equation~\eqref{eq:path_exists_for_finite_points} guarantees that this $\omega^{***}\in\Omega$ exists. Furthermore, because of Equation~\eqref{eq:theo:aanelkaarplakken:starstarniceonu}, we know that, for all $s\in w$,
\begin{equation}\label{eq:theo:aanelkaarplakken:triplestarpartone}
\omega^{***}(s)=\omega^*(s)
\text{~~if $s\in [0,\max u]$}\vspace{-3mm}
\end{equation}
and
\begin{equation}\label{eq:theo:aanelkaarplakken:triplestarparttwo}
\omega^{***}(s)=\omega^{**}(s)
\text{~~if $s\in u\cup [\max u,+\infty)$}
\vspace{2mm}
\end{equation}
and therefore, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:Cisplit} that
\begin{equation}\label{eq:theo:aanelkaarplakken:triplestarequivalence}
\omega^{***}\in C_i
\Leftrightarrow
(\omega^{***}\in C_i^*
\text{~and~}
\omega^{***}\in C_i^{**})
\Leftrightarrow
(\omega^{*}\in C_i^*
\text{~and~}
\omega^{**}\in C_i^{**})
\end{equation}
for all $i\in\{1,\dots,n\}$ such that $(A_i,C_i)\in\mathcal{C}_{x_u^*}$.

%*** still need to finish the stuff below ***

Next, for any $i\in S^*$, we infer from Equation~\eqref{eq:theo:aanelkaarplakken:firstpartofdomain} that the value of $\ind{A_i}(\omega^{***})$ and $\ind{C_i}(\omega^{***})$ is completely determined by $\omega^{***}(t)$, $t\in(w\cap[0,\max u])$. Therefore, it follows from Equations~\eqref{eq:theo:aanelkaarplakken:geqfirstpart} and ~\eqref{eq:theo:aanelkaarplakken:triplestarpartone} that 
\begin{equation}\label{eq:theo:aanelkaarplakken:geqfirstparttriplestar}
\sum_{i\in S^*}\lambda_i\ind{C_i}(\omega^{***})\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega^{***})\bigr)
%=
%\sum_{i\in S^*}\lambda_i\ind{C_i}(\omega^{*})\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega^{*})\bigr)
\geq0.
\end{equation}

Similarly, for any $i\in S^{**}$, 
Equations~\eqref{eq:theo:aanelkaarplakken:triplestarequivalence} and~\eqref{eq:theo:aanelkaarplakken:Sstarstardef} imply that $\ind{C_i}(\omega^{***})=\ind{C_i^{**}}(\omega^{**})$,
and Equations~\eqref{eq:theo:aanelkaarplakken:secondpartofdomain} and~\eqref{eq:theo:aanelkaarplakken:triplestarparttwo} imply that $\ind{A_i}(\omega^{***})=\ind{A_i}(\omega^{**})$. Therefore, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:geqsecondpart} that
\begin{equation}\label{eq:theo:aanelkaarplakken:geqsecondparttriplestar}
\sum_{i\in S^{**}}\lambda_i\ind{C_i}(\omega^{***})\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega^{***})\bigr)
\geq0.
\end{equation}

Consider now any $i\in\{1,\dots,n\}$ such that $i\notin S^*$ and $i\notin S^{**}$. Since $i\notin S^*$, there is some $x_u\in\states^u$ such that $(A_i,C_i)\in\mathcal{C}_{x_u}$. If $x_u= x_u^*$, then since $i\notin S^{**}$, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:Sstarstardef} that $\ind{C_i^*}(\omega^*)=0$, and therefore, Equation~\eqref{eq:theo:aanelkaarplakken:triplestarequivalence} implies that $\ind{C_i}(\omega^{***})=0$. 
If $x_u\neq x_u^*$, then $(X_u=x_u)\cap(X_u=x_u^*)=\emptyset$, and therefore, since $(A_i,C_i)\in\mathcal{C}_{x_u}$ implies that $C_i\subseteq (X_u=x_u)$, it follows that $C_i\cap (X_u=x_u^*)=\emptyset$. Since it follows from Equations~\eqref{eq:theo:aanelkaarplakken:starstarniceonu} and~\eqref{eq:theo:aanelkaarplakken:triplestarparttwo} that $\omega^{***}(t)=x_t^*$ for all $t\in u$, this implies that $\omega^{***}\notin C_i$, and therefore, we find that $\ind{C_i}(\omega^{***})=0$.
Hence, in all cases, we find that $\ind{C_i}(\omega^{***})=0$. Since this is true for any $i\in\{1,\dots,n\}$ such that $i\notin S^*$ and $i\notin S^{**}$, it follows from Equations~\eqref{eq:theo:aanelkaarplakken:geqfirstparttriplestar} and~\eqref{eq:theo:aanelkaarplakken:geqsecondparttriplestar} that
\begin{equation}\label{eq:theo:aanelkaarplakken:geqtotal}
\sum_{i=1}^n\lambda_i\ind{C_i}(\omega^{***})\bigl(\tilde{P}(A_i\vert C_i)-\ind{A_i}(\omega^{***})\geq0.
\end{equation}

We will now prove that $\omega^{***}\in C_0$. We consider two cases: $S^*\neq\emptyset$ and $S^*=\emptyset$. First assume that $S^*\neq\emptyset$. In that case, we have that $\omega^*\in C^*$, which implies that there is some $i\in S^*$ such that $\omega^*\in C_i$. It then follows from Equations~\eqref{eq:theo:aanelkaarplakken:firstpartofdomain} and~\eqref{eq:theo:aanelkaarplakken:triplestarpartone} that $\omega^{***}\in C_i\subseteq C_0$. 
Next, assume that $S^*=\emptyset$. In that case, we have that $\omega^*\in C_0$, which implies that there is some $i\in\{1,\dots,n\}$ such that $\omega^*\in C_i$. Since $(A_i,C_i)\in\mathcal{C}$ and $S^*=\emptyset$, there is some $x_u\in\states^u$ such that $(A_i,C_i)\in\mathcal{C}_{x_u}$ and, since Equation~\eqref{eq:theo:aanelkaarplakken:secondpartofdomain} implies that $x_t=\omega^*(t)$ for all $t\in u$, it follows that $x_u=x_u^*$. We conclude from this that $(A_i,C_i)\in\mathcal{C}_{x_u^*}$. Furthermore, since $\omega^*\in C_i\subseteq C_i^*$, we know that $\ind{C_i^*}(\omega^*)=1$. Therefore, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:Sstarstardef} that $S^{**}\neq\emptyset$, which implies that $\omega^{**}\in C^{**}$. Hence, there is some $j\in S^{**}$ such that $\omega^{**}\in C_j^{**}$ and, since $j\in S^{**}$, we also know that $\ind{C_j^*}(\omega^*)=1$, or equivalently, that $\omega^*\in C_j^*$. By combining this with Equation~\eqref{eq:theo:aanelkaarplakken:triplestarequivalence}, it follows that $\omega^{***}\in C_j\subseteq C_0$.
So, in all cases, we find that $\omega^{***}\in C_0$. By combining this with Equation~\eqref{eq:theo:aanelkaarplakken:geqtotal}, it follows that Equation~\eqref{eq:theo:aanelkaarplakken:coh} holds, and therefore, that $\tilde{P}$ is coherent.


Since $\tilde{P}$ is coherent, and because of Corollary~\ref{corol:coherentextendable}, $\tilde{P}$ can be extended to a full conditional probability $\tilde{P}^*$. If we now let $P$ be the restriction of $\tilde{P}^*$ to $\mathcal{C}^\mathrm{SP}$, then $P$ is a stochastic process that, because $\mathcal{C}\subseteq\mathcal{C}^\mathrm{SP}$, extends $\tilde{P}$. Due to Equation~\eqref{eq:theo:aanelkaarplakken:defPtilde}, this implies that $P$ satisfies Equations~\eqref{eq:theo:aanelkaarplakken:equalsfirst} and~\eqref{eq:theo:aanelkaarplakken:equalssecond}. In the remainder of this proof, we will show that $P\in\wprocesses_\rateset$.

In order to do this, we start by establishing an important equality.
Consider any $w\in\mathcal{U}$ and $s\in\reals_{\geq0}$ such that $w<s$ and $u<s$. Then for all $x_w\in\states^w$ and $y\in\states$, we have that
\begin{align}\label{eq:theo:aanelkaarplakken:convexcombo}
P&(X_s=y\vert X_w=x_w)
=
\sum_{x_{u\setminus w}\in\states^{u\setminus w}}
P(X_s=y,
X_{u\setminus w}=x_{u\setminus w}
\vert X_w=x_w)\notag\\
&=
\sum_{x_{u\setminus w}\in\states^{u\setminus w}}
P(X_s=y\vert
X_{u\setminus w}=x_{u\setminus w}, X_w=x_w)
P(X_{u\setminus w}=x_{u\setminus w}
\vert X_w=x_w)\notag\\
&=
\sum_{x_{u\setminus w}\in\states^{u\setminus w}}
P(X_s=y\vert
X_{u}=x_{u}, X_{w\setminus u}=x_{w\setminus u})
P(X_{u\setminus w}=x_{u\setminus w}
\vert X_w=x_w)\notag\\
&=
\sum_{x_{u\setminus w}\in\states^{u\setminus w}}
\tilde{P}(X_s=y\vert
X_{u}=x_{u}, X_{w\setminus u}=x_{w\setminus u})
P(X_{u\setminus w}=x_{u\setminus w}
\vert X_w=x_w)\notag\\
&=
\sum_{x_{u\setminus w}\in\states^{u\setminus w}}
P_{x_u}(X_s=y\vert
X_{u}=x_{u}, X_{w\setminus [0,\max u]}=x_{w\setminus [0,\max u]})
P(X_{u\setminus w}=x_{u\setminus w}
\vert X_w=x_w)\notag\\[-3mm]
%&=
%\sum_{x_{u\setminus w}\in\states^{u\setminus w}}
%P_{x_u}(X_s=y\vert
%X_{u\cup(w\setminus[0,\max u])}=x_{u\cup(w\setminus[0,\max u])})
%P(X_{u\setminus w}=x_{u\setminus w}
%\vert X_w=x_w)
\end{align}
Using this equality, we will now show that, for any $t\geq0$, $v\in\mathcal{U}_{<t}$ and $x_v\in\states^v$, there is some finite index set $I$, some $v^*\in\mathcal{U}_{<t}$, a set of non-negative coefficients $(\lambda_i)_{i\in I}$ that sum to one, a set of stochastic processes $({}^iP\in\wprocesses_\rateset)_{i\in I}$ and a set of state instantiations $({}^ix_{v^*}\in\states^{v^*})_{i\in I}$ such that
\begin{equation}\label{eq:theo:aanelkaarplakken:convexTright}
(\exists \delta>0)~(\forall 0<\Delta<\delta)~~
T_{t,x_v}^{t+\Delta}
=\sum_{i\in I}\lambda_i
{}^iT_{t,{}^ix_{v^*}}^{t+\Delta}
\end{equation}
and, similarly, that for any $t>0$, $v\in\mathcal{U}_{<t}$ and $x_v\in\states^v$, there is some finite index set $I$, some $v^*\in\mathcal{U}_{<t}$, a set of non-negative coefficients $(\lambda_i)_{i\in I}$ that sum to one, a set of stochastic processes $({}^iP\in\wprocesses_\rateset)_{i\in I}$ and a set of state instantiations $({}^ix_{v^*}\in\states^{v^*})_{i\in I}$ such that
\begin{equation}\label{eq:theo:aanelkaarplakken:convexTleft}
(\exists \delta>0)~(\forall 0<\Delta<\delta)~~
T_{t-\Delta,x_v}^{t}
=\sum_{i\in I}\lambda_i
{}^iT_{t-\Delta,{}^ix_{v^*}}^{t}.
\end{equation}

We start by proving Equation~\eqref{eq:theo:aanelkaarplakken:convexTright}. So consider any $t\geq0$, $v\in\mathcal{U}_{<t}$ and $x_v\in\states^v$.
We distinguish between two cases: $t<\max u$ and $t\geq\max u$. If $t<\max u$, then for all $\Delta\in(0,\max u-\max v)$ and $x,y\in\states$, we see that $(X_{t+\Delta}=y,(X_t=x, X_v=x_v))\in\mathcal{C}_\emptyset$, and therefore, since $P$ is an extension of $\tilde{P}$, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:defPtilde} that
\begin{equation*}
P(X_{t+\Delta}=y\vert X_t=x, X_v=x_v)
=P_\emptyset(X_{t+\Delta}=y\vert X_t=x, X_v=x_v).
\end{equation*}
Hence, if we let $I\coloneqq\{i\}$, $v^*\coloneqq v$, $\lambda_i\coloneqq 1$, ${}^iP\coloneqq P_\emptyset$ and ${}^ix_{v^*}\coloneqq x_\emptyset$, Equation~\eqref{eq:theo:aanelkaarplakken:convexTright} is satisfied by choosing $\delta\coloneqq \max u-\max v$.
If $t\geq \max u$, then for all $\Delta>0$, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:convexcombo} (with $s\coloneqq t+\Delta$ and $w\coloneqq v\cup t$) that
\begin{align*}
&P(X_{t+\Delta}=y\vert X_t=x, X_v=x_v)\\
&~~~~=
\sum_{x_{u\setminus(v\cup t)}\in\states^{u\setminus(v\cup t)}}
P_{x_u}(X_{t+\Delta}=y\vert X_t=x, 
X_{(u\setminus t)\cup(v\setminus [0,\max u])}= 
x_{(u\setminus t)\cup(v\setminus [0,\max u])})\\[-4mm]
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
P(X_{u\setminus(v\cup t)}=x_{u\setminus(v\cup t)}
\vert X_t=x, X_v=x_v).
\end{align*}
Therefore, if we let $I\coloneqq\states^{u\setminus(v\cup t)}$, $v^*\coloneqq (u\setminus t)\cup(v\setminus [0,\max u])$ and, for all $x_{u\setminus(v\cup t)}\in I$,
\begin{equation*}
\lambda_{x_{u\setminus(v\cup t)}}
\coloneqq P(X_{u\setminus(v\cup t)}=x_{u\setminus(v\cup t)}
\vert X_t=x, X_v=x_v),
\end{equation*}
${}^{x_{u\setminus(v\cup t)}}P=P_{x_u}$ and ${}^{x_{u\setminus(v\cup t)}}x_{v^*}\coloneqq
x_{(u\setminus t)\cup(v\setminus [0,\max u])}$, Equation~\eqref{eq:theo:aanelkaarplakken:convexTright} is satisfied for any $\delta>0$.
Hence, Equation~\eqref{eq:theo:aanelkaarplakken:convexTright} is satisfied in both cases.

We will now prove Equation~\eqref{eq:theo:aanelkaarplakken:convexTleft}. So, consider any $t>0$, $v\in\mathcal{U}_{<t}$ and $x_v\in\states^v$.
We distinguish between two cases: $t\leq\max u$ and $t>\max u$. If $t\leq\max u$, then for all $\Delta\in(0,t-\max v)$ and $x,y\in\states$, we see that $(X_{t}=y,(X_{t-\Delta}=x, X_v=x_v))\in\mathcal{C}_\emptyset$, and therefore, since $P$ is an extension of $\tilde{P}$, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:defPtilde} that
\begin{equation*}
P(X_{t}=y\vert X_{t-\Delta}=x, X_v=x_v)
=P_\emptyset(X_{t}=y\vert X_{t-\Delta}=x, X_v=x_v).
\end{equation*}
Hence, if we let $I\coloneqq\{i\}$, $v^*\coloneqq v$, $\lambda_i\coloneqq 1$, ${}^iP\coloneqq P_\emptyset$ and ${}^ix_{v^*}\coloneqq x_\emptyset$, Equation~\eqref{eq:theo:aanelkaarplakken:convexTright} is satisfied by choosing $\delta\coloneqq t-\max v$.
If $t>\max u$, then for all $\Delta\in(0,t-\max(v\cup u))$, it follows from Equation~\eqref{eq:theo:aanelkaarplakken:convexcombo} (with $s\coloneqq t$ and $w\coloneqq v\cup {t-\Delta}$) that
\begin{align*}
&P(X_{t}=y\vert X_{t-\Delta}=x, X_v=x_v)\\
&~~~~=
\sum_{x_{u\setminus v}\in\states^{u\setminus v}}
P_{x_u}(X_{t}=y\vert X_{t-\Delta}=x, 
X_{u\cup(v\setminus [0,\max u])}= 
x_{u\cup(v\setminus [0,\max u])})\\[-4mm]
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
P(X_{u\setminus v}=x_{u\setminus v}
\vert X_{t-\Delta}=x, X_v=x_v).
\end{align*}
Therefore, if we let $I\coloneqq\states^{u\setminus v}$, $v^*\coloneqq u\cup(v\setminus [0,\max u])$ and, for all $x_{u\setminus v}\in I$,
\begin{equation*}
\lambda_{x_{u\setminus v}}
\coloneqq P(X_{u\setminus v}=x_{u\setminus v}
\vert X_{t-\Delta}=x, X_v=x_v),
\end{equation*}
${}^{x_{u\setminus v}}P=P_{x_u}$ and ${}^{x_{u\setminus v}}x_{v^*}\coloneqq
x_{ u\cup(v\setminus [0,\max u])}$, Equation~\eqref{eq:theo:aanelkaarplakken:convexTleft} is satisfied by choosing $\delta\coloneqq t-\max(v\cup u)$.
Hence, Equation~\eqref{eq:theo:aanelkaarplakken:convexTleft} is satisfied in both cases.


Hence, indeed, as claimed before, Equations~\eqref{eq:theo:aanelkaarplakken:convexTright} and~\eqref{eq:theo:aanelkaarplakken:convexTleft} are both true.
We are now ready to prove that $P$ is well-behaved and consistent with $\mathcal{Q}$.

We start by proving that $P$ is well-behaved. First consider any $t\geq0$, $v\in\mathcal{U}_{<t}$ and $x_v\in\states^v$. 
Then due to Equation~\eqref{eq:theo:aanelkaarplakken:convexTright}, and because every process ${}^iP$ that appears in these equations is well-behaved, it follows from Proposition~\ref{prop:stochasticprocess:simpleproperties} that
\begin{equation*}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t,x_u}^{t+\Delta}-I}
\leq\sum_{i\in I}\lambda_i\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{{}^iT_{t,x_u}^{t+\Delta}-I}
<+\infty.
\end{equation*}
Similarly, for any $t>0$, $v\in\mathcal{U}_{<t}$ and $x_v\in\states^v$, Equation~\eqref{eq:theo:aanelkaarplakken:convexTleft} implies that
\begin{equation*}
\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta,x_u}^t-I}
\leq\sum_{i\in I}\lambda_i\limsup_{\Delta\to 0^{+}}\frac{1}{\Delta}\norm{T_{t-\Delta,x_u}^t-I}
<+\infty.
\end{equation*}
Hence, by invoking Proposition~\ref{prop:stochasticprocess:simpleproperties}, it follows that $P$ is well-behaved.

We end by proving that $P$ is consistent with $\rateset$. Assume \emph{ex absurdo} that $P$ is not consistent with $\rateset$, or equivalently, that there are $t\geq0$, $v\in\mathcal{U}_{<t}$, $x_v\in\states^v$ and $Q^*\in\mathcal{R}$ such that $Q^*\in\overline{\partial}
{T^t_{t,\,x_v}}$ and $Q^*\notin\rateset$. We will show that this leads to a contradiction. 
We consider two (possibly overlapping) cases: $\smash{Q^*\in\overline{\partial}_{+}
{T^t_{t,\,x_v}}}$ and $\smash{Q^*\in\overline{\partial}_{-}
{T^t_{t,\,x_v}}}$.

If $\smash{Q^*\in\overline{\partial}_{+}
{T^t_{t,\,x_v}}}$, it follows from Equation~\eqref{eq:rightouterderivative} that there is a sequence $\{\Delta_j>0\}_{j\in\nats}$ such that $\lim_{j\to\infty}\Delta_j=0$ and 
\begin{equation}\label{eq:theo:aanelkaarplakken:Qstar}
\lim_{j\to+\infty}
\frac{1}{\Delta_j}
(T^{t+\Delta_j}_{t,\,x_v}-I)
=Q^*
\end{equation}
Consider $I$, $\{\lambda_i\}_{i\in I}$, $\{{}^iP\in\wprocesses_\rateset\}_{i\in I}$ and $\delta$  as in Equation~\eqref{eq:theo:aanelkaarplakken:convexTright}. Fix any $i\in I$. Since ${}^iP$ is well-behaved, the sequence
\begin{equation*}%\label{eq:theo:aanelkaarplakken:fixingsubsequence}
\left\{\frac{1}{\Delta_j}
({}^iT^{t+\Delta_j}_{t,\,x_v}-I)\right\}_{j\in\nats}
\end{equation*}
is bounded, and therefore, the Bolzano-Weierstra{\ss} theorem implies that it has a convergent subsequence, of which we denote the limit by $Q_i$. Hence, without loss of generality---simply remove the indexes $j$ that do not correspond to the subsequence---we may assume that
\begin{equation}\label{eq:theo:aanelkaarplakken:fixingsubsequence}
\lim_{j\to+\infty}\frac{1}{\Delta_j}
({}^iT^{t+\Delta_j}_{t,\,x_v}-I)=Q_i.
\end{equation}
Since we know from Proposition~\ref{prop:rate_from_stochastic_matrix} that $Q_i$ is a limit of rate matrices, $Q_i$ is also a rate matrix, and therefore, it follows from Equation~\eqref{eq:rightouterderivative} that $Q_i\in\smash{\overline{\partial}_{+}
{{}^iT^t_{t,\,x_v}}}$, which, since ${}^iP$ is consistent with $\rateset$, implies that $Q_i\in\rateset$. By repeating this argument for every other $i\in I$, we obtain a set of rate matrices $\{Q_i\in\rateset\}_{i\in I}$ such that, without loss of generality, Equation~\eqref{eq:theo:aanelkaarplakken:fixingsubsequence} holds for every $i\in I$. Additionally, since $\lim_{j\to\infty}\Delta_j=0$, we may assume without loss of generality that $0<\Delta_j<\delta$ for all $j\in\nats$. Equations~\eqref{eq:theo:aanelkaarplakken:convexTright} and~\eqref{eq:theo:aanelkaarplakken:fixingsubsequence} now imply that
\begin{equation*}
\lim_{j\to+\infty}
\frac{1}{\Delta_j}
(T_{t,x_u}^{t+\Delta_j}-I)
=\lim_{j\to+\infty}
\frac{1}{\Delta_j}
(\sum_{i\in I}\lambda_i
{}^iT_{t,x_u}^{t+\Delta_j}-I)
%=
%\sum_{i\in I}\lambda_i
%\lim_{j\to+\infty}
%\frac{1}{\Delta_j}
%({}^iT_{t,x_u}^{t+\Delta_j}-I)
=\sum_{i\in I}\lambda_i Q_i,
\end{equation*}
which, because of Equation~\eqref{eq:theo:aanelkaarplakken:Qstar}, implies that $Q^*=\sum_{i\in I}\lambda_i Q_i$. Since $\rateset$ is convex, this implies that $Q^*\in\rateset$, a contradiction.

If $\smash{Q^*\in\overline{\partial}_{-}
{T^t_{t,\,x_v}}}$, a completely analogous argument leads to the same contradiction: simply use Equations~\eqref{eq:leftouterderivative} and~\eqref{eq:theo:aanelkaarplakken:convexTleft} instead of Equations~\eqref{eq:rightouterderivative} and~\eqref{eq:theo:aanelkaarplakken:convexTright}, respectively, and adapt the rest of the argument accordingly.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theorem:decomposition_multivar}]
Let $g(X_u,X_v)\coloneqq\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,X_u,X_v\right]$ and fix any $x_u\in\states^u$.
For any $P\in\wprocesses_\rateset$, it then follows from the basic properties of expectation (which follow from~\ref{def:coh_prob_2}--\ref{def:coh_prob_6}) and Definition~\ref{def:lower_exp} that
\begin{align*}
\mathbb{E}\left[f(X_u,X_v,X_w)\vert\,x_u\right] = \mathbb{E}\bigl[\mathbb{E}\left[f(X_u,X_v,X_w)\vert\,X_u,X_v\right]\vert\,x_u\bigr] 
 &\geq \mathbb{E}\bigl[g(X_u,X_v)\vert\,x_u\bigr] \\
 &\geq \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\vert\,x_u\bigr].
\end{align*}\\[-20pt]
Since $P\in\wprocesses_\rateset$ is arbitrary, this implies that
\begin{equation}\label{eq:theorem:decomposition_multivar:easyinequality}
\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,x_u\right] \geq \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\vert\,x_u\bigr].
\end{equation}

% To prove Equation~\eqref{eq:lower_exp_factorizes}, it therefore remains to show that, for all $\epsilon\in\realspos$, there is some $P\in\wprocesses_\rateset$ such that
% \begin{equation*}
% \norm{\mathbb{E}\left[f(X_u,X_v)\,\vert\,X_u\right] - \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v)\,\vert\,X_u,X_{v\setminus w}\right]\,\vert\,X_u\bigr]} < \epsilon\,.
% \end{equation*}
Fix $\epsilon\in\realspos$.
% Due to the definition of the norm, we have to show that
% \begin{equation*}
% \max\left\{\abs{\mathbb{E}\left[f(x_u,X_v)\,\vert\,x_u\right] - \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr]}\,:\,x_u\in\states^u\right\} < \epsilon\,.
% \end{equation*}
% Let now $z\coloneqq v\setminus w$. For any $x_u\in\states^u$, consider
% \begin{equation*}
% \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset[f(x_u,X_z,X_w)\,\vert\,x_u,X_z]\,\big\vert\,x_u\bigr] = \inf\{\mathbb{E}\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset[f(x_u,X_z,X_w)\,\vert\,x_u,X_z]\,\big\vert\,x_u\bigr]\,:\,P\in\wprocesses_\rateset\}\,.
% \end{equation*}
Because of Definition~\ref{def:lower_exp}, there is some $P_\emptyset\in\wprocesses_\rateset$ such that
\begin{equation}\label{eq:theorem:decomposition_multivar:firsthalfprob}
\mathbb{E}_\emptyset\bigl[g(x_u,X_v)\vert\,x_u\bigr]
=\mathbb{E}_\emptyset\bigl[g(X_u,X_v)\vert\,x_u\bigr] < \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\big\vert\,x_u\bigr]
+\nicefrac{\epsilon}{2}
\end{equation}
and, for all $x_{v}\in\states^v$, there is some $P_{x_v}\in\wprocesses_\rateset$ such that
\begin{equation}\label{eq:theorem:decomposition_multivar:secondhalfprob}
\mathbb{E}_{x_v}\left[f(X_u,X_v,X_w)\vert\,x_u,x_v\right] < \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,x_u,x_v\right]+\nicefrac{\epsilon}{2}=g(x_u,x_v)+\nicefrac{\epsilon}{2}.
\end{equation}
Since $\rateset$ is non-empty, bounded, and convex, Theorem~\ref{theo:aanelkaarplakken} now implies the existence of a process $P\in\wprocesses_\rateset$ such that for all $x_v\in\states^v$ and $x_w\in\states^w$:
\begin{equation*}
P(X_v=x_v\,\vert\,X_u=x_u) = P_\emptyset(X_v=x_v\,\vert\,X_u=x_u)
\end{equation*}\\[-20pt]
and
\begin{equation*}
P(X_w=x_w\,\vert\,X_u=x_u,X_v=x_v) = P_{x_v}(X_w=x_w\,\vert\,X_u=x_u,X_v=x_v)\vspace{6pt}
\end{equation*}
and therefore also, due to Equations~\eqref{eq:theorem:decomposition_multivar:firsthalfprob} and~\eqref{eq:theorem:decomposition_multivar:secondhalfprob},
\begin{equation*}%\label{eq:theorem:decomposition_multivar:firsthalfexp}
\mathbb{E}[g(x_u,X_v)\vert\,x_u]
%&=\sum_{x_v\in\states^v}g(x_u,x_v)P(X_v=x_v\vert X_u=x_u)\\
%&=\sum_{x_v\in\states^v}g(x_u,x_v)P_\emptyset(X_v=x_v\vert X_u=x_u)
=\mathbb{E}_{\emptyset}[g(x_u,X_v)\vert\,x_u]
< \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\big\vert\,x_u\bigr]
+\nicefrac{\epsilon}{2}
\end{equation*}\\[-20pt]
and
\begin{equation*}%\label{theorem:decomposition_multivar:secondhalfexp}
\mathbb{E}[f(X_u,X_v,X_w)\vert\,x_u,x_v]
%&=\sum_{x_w\in\states^w}f(x_u,x_v,x_w)P(X_w=x_w\vert\,X_u=x_u, X_v=x_v)\\
%&=\sum_{x_w\in\states^w}f(x_u,x_v,x_w)P_{x_v}(X_w=x_w\vert\, X_u=x_u, X_v=x_v)\\
=\mathbb{E}_{x_v}[f(X_u,X_v,X_w)\vert\, x_u,x_v]
<g(x_u,x_v)+\nicefrac{\epsilon}{2}.\vspace{6pt}
\end{equation*}
Hence, we find that
\begin{align*}
\mathbb{E}\bigl[\mathbb{E}\left[f(X_u,X_v,X_w)\vert\,x_u,X_v\right]\vert\,x_u\bigr]
%&\leq
% \mathbb{E}\bigl[g(x_u,X_v)+\nicefrac{\epsilon}{2}\,\big\vert\,x_u\bigr]\\
\leq
\mathbb{E}\bigl[g(x_u,X_v)\,\big\vert\,x_u\bigr]+\nicefrac{\epsilon}{2}
< \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\big\vert\,x_u\bigr]
+\epsilon.
\end{align*}
Since $\epsilon$ was arbitrary, and because
\begin{equation*}
\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,x_u\right]
\leq
\mathbb{E}\left[f(X_u,X_v,X_w)\vert\,x_u\right] = \mathbb{E}\bigl[\mathbb{E}\left[f(X_u,X_v,X_w)\vert\,x_u,X_v\right]\vert\,x_u\bigr],
\end{equation*}
this implies that
$\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,x_u\right]\leq\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\big\vert\,x_u\bigr]$ and therefore, because of Equation~\eqref{eq:theorem:decomposition_multivar:easyinequality}, that $\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,x_u\right]=\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\big\vert\,x_u\bigr]$. Since $x_u$ was arbitrary, this implies that $\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,X_u\right]=\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[g(X_u,X_v)\big\vert\,X_u\bigr]$. The result is now immediate because $g(X_u,X_v)=\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v,X_w)\vert\,X_u,X_v\right]$. 
% Finally, consider an arbitrary $P_{X_u}\in\wprocesses_\rateset$, and the processes $P_{X_w,X_z,x_u}$ for all $x_u\in\states^u$. Theorem~{\bf REF} then implies the existence of some $P_{X_w,X_z,X_u}$ such that, for all $x_u\in\states^u$, all $x_z\in\states^z$, and all $x_w\in\states^w$,
% \begin{align*}
% P_{X_w,X_z,X_u}(X_z=x_z\,\vert\,X_u=x_u) &= P_{X_w,X_z,x_u}(X_z=x_z\,\vert\,X_u=x_u) \\
%  &= P_{X_z,x_u}(X_z=x_z\,\vert\,X_u=x_u)\,, \\
% \text{and,} \\
% P_{X_w,X_z,X_u}(X_w=x_w\,\vert\,X_u=x_u,X_z=x_z) &= P_{X_w,X_z,x_u}(X_w=x_w\,\vert\,X_u=x_u,X_z=x_z) \\
%  &= P_{X_w,x_z,x_u}(X_w=x_w\,\vert\,X_u=x_u,X_z=x_z)\,.
% \end{align*}
% For notational convenience, denote $P\coloneqq P_{X_w,X_z,X_u}$.
% It remains to show that $P$ satisfies equation REF. To this end, assume \emph{ex absurdo} that it does not. This implies the existence of some $x_u\in\states^u$, for which
% \begin{equation*}
% \epsilon \leq \abs{\mathbb{E}\left[f(x_u,X_v)\,\vert\,x_u\right] - \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr]}\,.
% \end{equation*}
% This implies that
% \begin{align*}
%  &\quad \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr] \\
%  &\leq \mathbb{E}\left[f(X_u,X_v)\,\vert\,x_u\right] - \epsilon \\
%  &= \mathbb{E}\bigl[\mathbb{E}[f(x_u,X_z,X_w)\,\vert\,x_u,X_z]\,\big\vert\,x_u\bigr] - \epsilon \\
%  &= \sum_{x_z\in\states^z}P(X_z=x_z\,\vert\,X_u=x_u)\mathbb{E}[f(x_u,x_z,X_w)\,\vert\,x_u,x_z] - \epsilon \\
%  &= \sum_{x_z\in\states^z}P_{X_z,x_u}(X_z=x_z\,\vert\,X_u=x_u)\mathbb{E}_{X_w,x_z,x_u}[f(x_u,x_z,X_w)\,\vert\,x_u,x_z] - \epsilon\,,
% \end{align*}
% where in the last step we applied Equation REF. Using Equation REF, we therefore find that
% \begin{align*}
%  &\quad \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr] \\
%  &\leq \sum_{x_z\in\states^z}P_{X_z,x_u}(X_z=x_z\,\vert\,X_u=x_u)\mathbb{E}_{X_w,x_z,x_u}[f(x_u,x_z,X_w)\,\vert\,x_u,x_z] - \epsilon \\
%  &= \sum_{x_z\in\states^z}P_{X_z,x_u}(X_z=x_z\,\vert\,X_u=x_u)\left(\mathbb{E}_{X_w,x_z,x_u}[f(x_u,x_z,X_w)\,\vert\,x_u,x_z] - \frac{\epsilon}{2}\right) - \frac{\epsilon}{2} \\
%  &< \sum_{x_z\in\states^z}P_{X_z,x_u}(X_z=x_z\,\vert\,X_u=x_u)\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,x_z,X_w)\,\vert\,x_u,x_z\right] - \frac{\epsilon}{2} \\
%  &= \mathbb{E}_{X_z,x_u}\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_z,X_w)\,\vert\,x_u,X_z\right] \,\vert\,x_u\bigr] - \frac{\epsilon}{2}\,.
% \end{align*}
% Therefore, and due to Equation REF, we finally find
% \begin{align*}
% \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr] &< \mathbb{E}_{X_z,x_u}\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_z,X_w)\,\vert\,x_u,X_z\right] \,\vert\,x_u\bigr] - \frac{\epsilon}{2} \\
%  &< \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_z,X_w)\,\vert\,x_u,X_z\right] \,\vert\,x_u\bigr] \\
%  &= \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr]\,,
% \end{align*}
% a contradiction. Hence, it follows that indeed
% \begin{equation*}
% \norm{\mathbb{E}\left[f(X_u,X_v)\,\vert\,X_u\right] - \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\bigl[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v)\,\vert\,X_u,X_{v\setminus w}\right]\,\vert\,X_u\bigr]} < \epsilon\,.
% \end{equation*}
% Because the $\epsilon$ was arbitrary, together with Equation REF, it then follows that
% \begin{align*}
% \underline{\mathbb{E}}^{\mathrm{W}}_\rateset[\underline{\mathbb{E}}^{\mathrm{W}}_\rateset [f(x_u,X_v)\,\vert\,x_u,X_{v\setminus w}]\,\vert\,x_u] %&= \inf\{\mathbb{E}\bigl[\mathbb{E}\left[f(X_u,X_v)\,\vert\,X_u,X_{v\setminus w}\right]\,\vert\,x_u\bigr]\,:\,P\in\wprocesses_\rateset\} \\
%  &= \inf\{\mathbb{E}\left[f(X_u,X_v)\,\vert\,x_u\right]\,:\,P\in\wprocesses_\rateset\} \\
%  &= \underline{\mathbb{E}}^{\mathrm{W}}_\rateset\left[f(X_u,X_v)\,\vert\,x_u\right]\,.
% \end{align*}
\end{proof}

\begin{lemma}\label{lemma:bound_on_linear_approx_partition}
Consider any $P\in\wprocesses_\rateset$, any $0\leq t<s$, any $u\in\mathcal{U}_{<t}$ and any $x_u\in\states^u$. Then for all $\epsilon>0$ and $\delta>0$, there is some $v\in\mathcal{U}_{[t,s]}$ such that $\sigma(v)<\delta$ and, for all $i\in\{0,\dots,n-1\}$:
\begin{equation*}
(\exists Q\in\rateset)
~
\norm{
T^{t_{i+1}}_{t_i,\,x_u}-(I+\Delta_{i+1}Q)
}<\Delta_{i+1}\epsilon
\end{equation*}
\end{lemma}
\begin{proof}
Fix any $\epsilon>0$ and $\delta>0$. It then follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and Definition~\ref{def:consistent_process} that there is some $0<\delta^*<\min\{\delta,\nicefrac{1}{2}(s-t)\}$ such that, for all $0<\Delta<\delta^*$:
\begin{equation}\label{eq:epsilonboundsforboundsinlemma}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{t+\Delta}_{t,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{s}_{s-\Delta,\,x_u}-I)-Q}<\epsilon.
\end{equation}
Let $t^*\coloneqq t+\epsilon^*$ and $s^*\coloneqq s-\epsilon^*$. Then clearly, $t<t^*<s^*<s$.
For any $r\in[t^*,s^*]$, it follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} and Definition~\ref{def:consistent_process} that there is some $0<\delta_r<\delta^*$ such that, for all $0<\Delta<\delta_r$:
\begin{equation}\label{eq:epsilonboundsforlemma}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{r+\Delta}_{r,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta}
(T^{r}_{r-\Delta,\,x_u}-I)-Q}<\epsilon.
\end{equation}
Let $U_r\coloneqq(r-\delta_r,r+\delta_r)$. Then the set $C\coloneqq\{U_r\colon r\in[t^*,s^*]\}$ is an open cover of $[t^*,s^*]$. By the Heine-Borel theorem, $C$ contains a finite subcover $C^*$ of $[t^*,s^*]$. Without loss of generality, we can take this subcover to be minimal, in the sense that if we remove any of its elements, it is no longer a cover. Let $m$ be the cardinality of $C^*$ and let $r_1<r_2<\dots<r_m$ be the ordered sequence of the midpoints of the intervals in $C^*$.

We will now prove that
\begin{equation}\label{eq:orderingofbounds}
r_i-\delta_{r_i}<r_j-\delta_{r_j}
\text{~~and~~}
r_i+\delta_{r_i}<r_j+\delta_{r_j}
\text{~~for all $1\leq i<j\leq m$.}
\end{equation}
Assume \emph{ex absurdo} that this statement is not true. Then this implies that there are $1\leq i<j\leq m$ such that either $r_i-\delta_{r_i}\geq r_j-\delta_{r_j}$ or $r_i+\delta_{r_i}\geq r_j+\delta_{r_j}$. If $r_i-\delta_{r_i}\geq r_j-\delta_{r_j}$, then since $i<j$ implies that $r_i<r_j$, it follows that $\delta_{r_j}\geq\delta_{r_i}+r_j-r_i>\delta_{r_i}$ and therefore, that $r_j+\delta_{r_j}>r_i+\delta_{r_i}$. 
Hence, we find that $U_{r_i}\subseteq U_{r_j}$. Since $C^*$ was taken to be a minimal cover, this is a contradiction.
Similarly, if $r_i+\delta_{r_i}\geq r_j+\delta_{r_j}$, then since $i<j$ implies that $r_i<r_j$, it follows that $\delta_{r_i}\geq\delta_{r_j}+r_j-r_i>\delta_{r_j}$ and therefore, that $r_i-\delta_{r_i}<r_j-\delta_{r_i}$. Hence, we find that $U_{r_j}\subseteq U_{r_i}$. Since $C^*$ was taken to be a minimal cover, this is again a contradiction. From these two contradictions, it follows that Equation~\eqref{eq:orderingofbounds} is indeed true.

Next, we prove that
\begin{equation}\label{eq:overlapasyouwantit}
r_{k+1}-\delta_{r_{k+1}}<r_k+\delta_{r_k}
\text{~~for all $k\in\{1,\dots,m-1\}$.}
\end{equation}
Assume \emph{ex absurdo} that this statement is not true or, equivalently, that there is some $k\in\{1,\dots,m-1\}$ such that $r_k+\delta_{r_k}\leq r_{k+1}-\delta_{r_{k+1}}$. For all $i\in\{k+1,\dots, m\}$, it then follows from Equation~\eqref{eq:orderingofbounds} that $r_k+\delta_{r_k}\leq r_i-\delta_{r_i}$, which implies that $r_k+\delta_{r_k}\notin U_{r_i}$. Similarly, for all $i\in\{1,\dots,k\}$, it follows from Equation~\eqref{eq:orderingofbounds} that $r_i+\delta_{r_i}\leq r_k+\delta_{r_k}$, which again implies that $r_k+\delta_{r_k}\notin U_{r_i}$. 
Hence, for all $i\in\{1,\dots,m\}$, we have found that $r_k+\delta_{r_k}\notin U_{r_i}$. 
Since $C^*$ is a cover of $[t^*,s^*]$, this implies that $r_k+\delta_{r_k}\notin[t^*,s^*]$, which, since $r_k\in[t^*,s^*]$, implies that $r_k+\delta_{r_k}>s^*$. Hence, since we know from Equation~\eqref{eq:orderingofbounds} that $r_k-\delta_{r_k}<r_m-\delta_{r_m}$, it follows that $U_{r_m}\cap[t^*,s^*]\subseteq U_{r_k}\cap[t^*,s^*]$. This contradicts the fact that $C^*$ was taken to be a minimal cover.

For all $k\in\{1,\dots,m-1\}$, we now define $q_k\coloneqq\nicefrac{1}{2}(r_k+\delta_{r_k}+r_{k+1}-\delta_{r_{k+1}})$.
Using Equation~\eqref{eq:orderingofbounds}, it then follows that
\begin{equation*}
q_k<\frac{r_{k+1}+\delta_{r_{k+1}}+r_{k+1}-\delta_{r_{k+1}}}{2}=r_{k+1}
\text{~~and~~}
q_k>\frac{r_{k}+\delta_{r_{k}}+r_{k}-\delta_{r_{k}}}{2}=r_{k},
\end{equation*}
and Equation~\eqref{eq:overlapasyouwantit} trivially implies that $r_{k+1}-\delta_{r_{k+1}}<q_k<r_k+\delta_{r_k}$. Hence,
\begin{equation*}
r_k<q_k<r_k+\delta_{r_k}
\text{~~and~~}
r_{k+1}-\delta_{r_{k+1}}<q_k<r_{k+1}.
\end{equation*}
Due to Equation~\eqref{eq:epsilonboundsforlemma}, and with $\Delta^*_k\coloneqq q_k-r_k$ and $\Delta^{**}_k\coloneqq r_{k+1}-q_k$, this implies that
\begin{equation}\label{eq:epsilonboundsforqk}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta^*_k}
(T^{q_k}_{r_k,\,x_u}-I)-Q}<\epsilon
\text{~~and~~}
(\exists Q\in\rateset)
\norm{\frac{1}{\Delta^{**}_k}
(T^{r_{k+1}}_{q_k,\,x_u}-I)-Q}<\epsilon.
\end{equation}

For all $k\in\{1,\dots,m\}$, we now let $t_{2k}\coloneqq r_k$ and, for all $k\in\{1,\dots,m-1\}$, we let $t_{2k+1}\coloneqq q_k$. For the resulting sequence $t_2<t_3<\dots<t_{2m-1}<t_{2m}$, it then follows from Equation~\eqref{eq:epsilonboundsforqk} and~\ref{N:homogeneous} that, for all $i\in\{2,\dots,2m-1\}$:
\begin{equation}\label{eq:opencoverproofresult} 
(\exists Q\in\rateset)
~
\norm{
T^{t_{i+1}}_{t_i,\,x_u}-(I+\Delta_{i+1}Q)
}<\Delta_{i+1}\epsilon,
\end{equation}
with $\Delta_{i+1}\coloneqq t_{i+1}-t_i<\delta$.  

Next, since $C^*$ is a minimal cover, and because of Equation~\eqref{eq:orderingofbounds}, we know that $r_1-\delta_{r_1}<t^*\leq r_1=t_2$ and, since $\delta_{r_1}<\delta^*$, we also know that $r_1-\delta_{r_1}>t$. Therefore, it follows that there is some $t_1\in\reals$ such that $t<r_1-\delta_{r_1}<t_1<t^*\leq r_1$. If we now let $t_0\coloneqq t$, then $\Delta_1\coloneqq t_1-t_0<\delta^*$ and $\Delta_2\coloneqq t_2-t_1=r_1-t_1<\delta_{r_1}$, and therefore, it follows from Equations~\eqref{eq:epsilonboundsforboundsinlemma} and~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous} that Equation~\eqref{eq:opencoverproofresult} is also true for $i=0$ and $i=1$.
% \begin{equation*}
% (\exists Q\in\rateset)
% ~
% \norm{
% T^{t_1}_{t_0,\,x_u}-(I+\Delta_{1}Q)
% }<\Delta_{1}\epsilon
% \text{~~and~~}
% (\exists Q\in\rateset)
% ~
% \norm{
% T^{t_2}_{t_1,\,x_u}-(I+\Delta_{2}Q)
% }<\Delta_{2}\epsilon
% \end{equation*}

Finally, again since $C^*$ is a minimal cover and because of Equation~\eqref{eq:orderingofbounds}, we know that $t_{2m}=r_m\leq s^*<r_m+\delta_{r_m}$ and, since $\delta_{r_m}<\delta^*$, we also know that $r_m+\delta_{r_m}<s$. Therefore, it follows that there is some $t_{2m+1}\in\reals$ such that $t_{2m}=r_m\leq s^*<t_{2m+1}<r_m+\delta_{r_m}<s$. If we now let $t_{2m+2}\coloneqq s$, then $\Delta_{2m+2}\coloneqq t_{2m+2}-t_{2m+1}<\delta^*$ and $\Delta_{2m+1}\coloneqq t_{2m+1}-t_{2m}=t_{2m+1}-r_m<\delta_{r_m}$, and therefore, it follows from Equations~\eqref{eq:epsilonboundsforboundsinlemma} and~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous} that Equation~\eqref{eq:opencoverproofresult} is also true for $i=2m$ and $i=2m+1$.

Hence, we conclude that Equation~\eqref{eq:opencoverproofresult} holds for all $i\in\{0,1,\dots,2m,2m+1\}$. The result now follows by letting $n\coloneqq2m+2$.
% \begin{equation*}
% (\exists Q\in\rateset)
% ~
% \norm{
% T^{t_1}_{t_0,\,x_u}-(I+\Delta_{1}Q)
% }<\Delta_{1}\epsilon
% \text{~~and~~}
% (\exists Q\in\rateset)
% ~
% \norm{
% T^{t_2}_{t_1,\,x_u}-(I+\Delta_{2}Q)
% }<\Delta_{2}\epsilon
% \end{equation*}
% If $s_{2m-1}\neq s$, we also define $s_{2m}\coloneqq s$ and $\Delta'_{2m}\coloneqq s_{2m}-s_{2m-1}=s-r_m>0$. Since $C^*$ is a minimal cover, and because of Equation~\eqref{eq:orderingofbounds}, it follows that $r_m<s<r_m+\delta_{r_m}$, which, because of Equation~\eqref{eq:epsilonboundsforlemma} and~\ref{N:homogeneous}, implies that
% \begin{equation*}
% (\exists Q\in\rateset)
% ~
% \norm{
% T^{s_{2m}}_{s_{2m-1},\,x_u}-(I+\Delta'_{2m}Q)
% }<\Delta'_{2m}\epsilon.
% \end{equation*}
% We now consider four cases.
% If $s_1=t$ and $s_{2m-1}=s$, the result follows by letting $n\coloneqq 2m-2$ and defining $t_i\coloneqq s_{i+1}$ for all $i\in\{0,\dots,n\}$. 
% If $s_1=t$ and $s_{2m-1}\neq s$, the result follows by letting $n\coloneqq 2m-1$ and defining $t_i\coloneqq s_{i+1}$ for all $i\in\{0,\dots,n\}$.
% If $s_1\neq t$ and $s_{2m-1}=s$, the result follows by letting $n\coloneqq 2m-1$ and defining $t_i\coloneqq s_{i}$ for all $i\in\{0,\dots,n\}$.
% If $s_1\neq t$ and $s_{2m-1}\neq s$, the result follows by letting $n\coloneqq 2m$ and defining $t_i\coloneqq s_{i}$ for all $i\in\{0,\dots,n\}$.
\end{proof}


\begin{lemma}\label{lemma:convex_rate_set_contains_approximation}
Consider a non-empty, bounded, convex set of rate matrices $\rateset$ and, for all $i\in\{1,\ldots,n\}$, some $\Delta_i\geq0$ and $Q_i\in\rateset$ such that $\Delta_i\norm{Q_i}\leq1$. Let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_iQ_i) - (I+\sum_{i=1}^n\Delta_iQ_i)} \leq \Delta^2\norm{\rateset}^2.
\end{equation*}
\end{lemma}
\begin{proof}
We provide a proof by induction. For $n=1$, the result is trivial. So consider the case $n\geq2$ and assume that the result is true for $n-1$. 

For all $i\in\{2,\dots,n\}$, since $\Delta_i\norm{Q_i}\leq 1$, it follows from Proposition~\ref{prop:stochastic_from_rate_matrix} that $(I+\Delta_iQ_i)$ and $I$ are transition matrices. Therefore,
\begin{multline*}
\norm{\prod_{i=1}^n(I+\Delta_iQ_i)-(I+\sum_{i=1}^n\Delta_iQ_i)}\\
\begin{aligned}
&=\norm{\prod_{i=2}^n(I+\Delta_iQ_i)+\Delta_1Q_1\prod_{i=2}^n(I-\Delta_iQ_i)-(I+\sum_{i=2}^n\Delta_iQ_i)-\Delta_1Q_1}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_iQ_i)-(I+\sum_{i=2}^n\Delta_iQ_i)}+\norm{\Delta_1Q_1\prod_{i=2}^n(I-\Delta_iQ_i)-\Delta_1Q_1}\\
&\leq(\sum_{i=2}^n\Delta_i)^2\norm{\rateset}^2
+\Delta_1\norm{Q_1}
\norm{\prod_{i=2}^n(I+\Delta_iQ_i)-I}\\
&\leq(\sum_{i=2}^n\Delta_i)^2\norm{\rateset}^2
+\Delta_1\norm{Q_1}
\sum_{i=2}^n
\norm{(I+\Delta_iQ_i)-I}\\
&=(\sum_{i=2}^n\Delta_i)^2\norm{\rateset}^2
+\Delta_1\norm{Q_1}
\sum_{i=2}^n
\Delta_i\norm{Q_i}\\
&\leq(\sum_{i=2}^n\Delta_i)^2\norm{\rateset}^2
+\Big(\Delta_1
\sum_{i=2}^n
\Delta_i\Big)\norm{\rateset}^2
\leq\Big(
\Delta_1+\sum_{i=2}^n\Delta_i
\Big)^2\norm{\rateset}^2
=\Delta^2\norm{\rateset}^2,
\end{aligned}
\end{multline*}\\[3pt]
where the second inequality follows from the induction hypothesis and the third inequality follows from Lemma~\ref{lemma:recursive}.
\end{proof}



% \begin{proof}
% The claim that $Q\in\rateset$ follows trivially from the fact that $\rateset$ is convex.

% Now, consider the following product expansion:
% \begin{align*}
% \prod_{i=1}^n(I+\Delta_iQ_i) &= I + \sum_{i=1}^n\Delta_iQ_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_jQ_iQ_j + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_kQ_iQ_jQ_k + \cdots \\
% % &= I + \sum_{i=1}^n\Delta_iQ_i + \sum_{k=1}^n\left[\sum_{i_1=1}^{n-k}\cdots\sum_{i_k=i_{k-1}+1}^n XXX \right]
% \end{align*}
% It follows that
% \begin{align*}
%  &\quad \norm{\prod_{i=1}^n(I+\Delta_iQ_i)} \\
%  &= \norm{I + \sum_{i=1}^n\Delta_iQ_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_jQ_iQ_j + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_kQ_iQ_jQ_k + \cdots} \\
%  &\leq \norm{I} + \norm{\sum_{i=1}^n\Delta_iQ_i} + \norm{\sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_jQ_iQ_j} + \norm{\sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_kQ_iQ_jQ_k} + \cdots \\
%  &\leq \norm{I} + \sum_{i=1}^n\Delta_i\norm{Q_i} + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_j\norm{Q_iQ_j} + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_k\norm{Q_iQ_jQ_k} + \cdots \\
%  &\leq \norm{I} + \sum_{i=1}^n\Delta_i\norm{Q_i} + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_j\norm{Q_i}\norm{Q_j} + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_k\norm{Q_i}\norm{Q_j}\norm{Q_k} + \cdots \\
%  &\leq 1 + \sum_{i=1}^n\Delta_i\norm{\rateset} + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_j\norm{\rateset}^2 + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_k\norm{\rateset}^3 + \cdots \\
%  &= 1 + \Delta\norm{\rateset} + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_j\norm{\rateset}^2 + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_k\norm{\rateset}^3 + \cdots
% \end{align*}
% Consider next the product expansion of the scalar product:
% \begin{align*}
% \prod_{i=1}^n(1+\Delta_i\norm{\rateset}) = 1 + \sum_{i=1}^n\Delta_i\norm{\rateset} + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_j\norm{\rateset}^2 + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_k\norm{\rateset}^3 + \cdots
% \end{align*}
% Clearly, these have the same form. Hence, we find that
% \begin{equation*}
% \norm{\prod_{i=1}^n(I+\Delta_iQ_i)} \leq \prod_{i=1}^n(1+\Delta_i\norm{\rateset})\,.
% \end{equation*}

% We are now ready to start proving the inequality of interest. We start by expanding the product term:
% \begin{align*}
%  &\quad \norm{\prod_{i=1}^n(I+\Delta_iQ_i) - (I+\Delta Q)} \\
%  &= \norm{I + \sum_{i=1}^n\Delta_iQ_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_jQ_iQ_j + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_kQ_iQ_jQ_k + \cdots - (I+\Delta Q)} \\
%  &\leq \norm{I + \sum_{i=1}^n\Delta_iQ_i - (I+\Delta Q)} + \norm{\sum_{i=1}^{n-1}\sum_{j=i+1}^n\Delta_i\Delta_jQ_iQ_j + \sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}\sum_{k=j+1}^n\Delta_i\Delta_j\Delta_kQ_iQ_jQ_k + \cdots} \\
%  &\leq \norm{I + \sum_{i=1}^n\Delta_iQ_i - (I+\Delta Q)} + \prod_{i=1}^n(1+\Delta_i\norm{\rateset}) - (1 + \Delta\norm{\rateset}) \\
%  &= \norm{\sum_{i=1}^n\Delta_iQ_i - \Delta Q} + \prod_{i=1}^n(1+\Delta_i\norm{\rateset}) - (1 + \Delta\norm{\rateset}) \\
%  &= \norm{\sum_{i=1}^n\Delta_iQ_i - \Delta \sum_{i=1}^n\frac{\Delta_i}{\Delta}Q_i} + \prod_{i=1}^n(1+\Delta_i\norm{\rateset}) - (1 + \Delta\norm{\rateset}) \\
%  &= \prod_{i=1}^n(1+\Delta_i\norm{\rateset}) - (1 + \Delta\norm{\rateset})\,.
% \end{align*}

% We next give an upper bound on the remaining product term. Consider the exponential function $e^a$. It is well known that this function has the series representation
% \begin{equation*}
% e^a = \sum_{i=0}^\infty\frac{a^i}{i!} = 1 + a + \frac{a^2}{2!} + \frac{a^3}{3!} + \cdots
% \end{equation*}
% Hence, it follows that $(1+a)<e^a$ for all $a\in\realspos$. Therefore in particular, we find that $(1+\Delta_i\norm{\rateset})<e^{\Delta_i\norm{\rateset}}$ for all $i\in\{1,\ldots,n\}$. Hence,
% \begin{align*}
% \norm{\prod_{i=1}^n(I+\Delta_iQ_i) - (I+\Delta Q)} &\leq \prod_{i=1}^n(1+\Delta_i\norm{\rateset}) - (1 + \Delta\norm{\rateset}) \\
% % &< e^{\Delta_1\norm{\rateset}}\prod_{i=2}^n(1+\Delta_i\norm{\rateset}) - (1 + \Delta\norm{\rateset}) \\
% % &\quad\vdots \\
%  &< \prod_{i=1}^n e^{\Delta_i\norm{\rateset}} - (1 + \Delta\norm{\rateset}) \\
%  &= e^{\sum_{i=1}^n\Delta_i\norm{\rateset}} - (1 + \Delta\norm{\rateset}) \\
%  &= e^{\Delta\norm{\rateset}} - (1 + \Delta\norm{\rateset})\,.
% \end{align*}
% We now use the following claim.
% \begin{claim}\label{claim:bound_on_exp}
% For all $a\in\realspos$ with $a<1$, it holds that $e^a - (1 + a) < a^2$.
% \end{claim}
% Hence, because $\Delta<\nicefrac{1}{\norm{\rateset}}$, we find that
% \begin{align*}
% \norm{\prod_{i=1}^n(I+\Delta_iQ_i) - (I+\Delta Q)} &< e^{\Delta\norm{\rateset}} - (1 + \Delta\norm{\rateset}) \\
%  &< \Delta^2\norm{\rateset}^2\,.
% \end{align*}
% \end{proof}

% \begin{proof}[Proof of Claim~\ref{claim:bound_on_exp}]
% We have to show that $e^a-(1+a)<a^2$ for all $a\in\realspos$ such that $a<1$. To this end, consider the function
% \begin{equation*}
% f(a) \coloneqq 1 + a + a^2 - e^a\,.
% \end{equation*}
% Clearly, the inequality of interest holds whenever $f(a)>0$. Now, we clearly have that $f(0)=0$. Furthermore, if we consider the first and second derivatives,
% \begin{equation*}
% \frac{d f(a)}{d a}=1+2a-e^a\,,\quad\text{and,}\quad\frac{d^2 f(a)}{da^2}=2-e^a\,,
% \end{equation*}
% we see that at $a=0$, the function is in a local minimum, because $\nicefrac{df(a)}{da}=0$ and $\nicefrac{d^2f(a)}{da^2}>0$. Hence, $f(a)>0$ will hold close to $a=0$. Furthermore, inspection of the second derivative shows that the first derivative increases until $a=\ln(2)$, after which it remains decreasing. Therefore, $f(a)$ is monotonically increasing on $(0,b)$, for some value $b>\ln(2)$ at which the first derivative becomes negative. To prove the claim, we therefore only have to note that $\nicefrac{df(a)}{da} > 0$ still holds at $a=1$.
% \end{proof}

\begin{lemma}\label{lemma:uniform_delta_for_convex_markov_set}
Let $\rateset$ be any non-empty, bounded and convex set of rate matrices, and choose any $\epsilon\in\realspos$. Then there is some $\delta\in\realspos$ such that for all $P\in\wmprocesses_\rateset$, all $t\in\realsnonneg$, and all $\Delta\in\realsnonneg$ such that $\Delta<\delta$:
\begin{equation*}
(\exists Q\in\rateset)\,\norm{T_{t}^{t+\Delta} - (I+\Delta Q)} \leq \Delta\epsilon.
\end{equation*}
\end{lemma}
\begin{proof}
Choose $\delta\in\realspos$ such that $\delta\norm{\rateset}<1$ and $\delta\norm{\rateset}^2<\nicefrac{\epsilon}{2}$ [this is clearly always possible] and consider any $P\in\wmprocesses_\rateset$, any $t\in\realsnonneg$ and any $\Delta<\delta$. If $\Delta=0$, the result is trivial because we know from Proposition~\ref{prop:Markovhassystem} and Definition~\ref{def:trans_mat_system} that $T_t^t=I$. Hence, without loss of generality, we may assume that $\Delta>0$.

It now follows from Lemma~\ref{lemma:bound_on_linear_approx_partition} that there is some $v\in\mathcal{U}_{[t,t+\Delta]}$ with $v=t_0,\ldots,t_n$ and $t=t_0<t_1<\dots<t_n=t+\Delta$ and, for all $i\in\{1,\ldots,n\}$, some $Q_i\in\rateset$ such that
\begin{equation*}
\norm{T_{t_{i-1}}^{t_i} - (I+\Delta_iQ_i)} < \Delta_i\frac{\epsilon}{2}
\text{~~and~~}
\Delta_i\norm{Q_i}\leq \Delta\norm{\rateset}\leq \delta\norm{\rateset}<1.
\end{equation*}
Therefore, it follows from Propositions~\ref{prop:stochastic_from_rate_matrix} and~\ref{prop:Markovhassystem}, Definition~\ref{def:trans_mat_system} and Lemma~\ref{lemma:recursive} that
\begin{equation*}%\label{lemma:uniform_delta_for_convex_markov_set:firstinequality}
\norm{T_t^{t+\Delta} - \prod_{i=1}^n(I+\Delta_iQ_i)} = \norm{\prod_{i=1}^nT_{t_{i-1}}^{t_i} - \prod_{i=1}^n(I+\Delta_iQ_i)}
 < \sum_{i=1}^n\Delta_i\frac{\epsilon}{2} 
 = \Delta\frac{\epsilon}{2}.
\end{equation*}
and, with $Q\coloneqq\nicefrac{1}{\Delta}\sum_{i=1}^n \Delta_i Q_i$, it follows from Lemma~\ref{lemma:convex_rate_set_contains_approximation} that
\begin{equation*}%\label{lemma:uniform_delta_for_convex_markov_set:secondinequality}
\norm{\prod_{i=1}^n(I+\Delta_iQ_i)-(I+\Delta Q)}=
\norm{\prod_{i=1}^n(I+\Delta_iQ_i) - (I+\sum_{i=1}^n\Delta_i Q_i)} \leq \Delta^2\norm{\rateset}^2.
\end{equation*}
%It then follows from Equations~\eqref{lemma:uniform_delta_for_convex_markov_set:firstinequality} and~\eqref{lemma:uniform_delta_for_convex_markov_set:secondinequality} that
By combining these two inequalities, we find that
\begin{equation*}
\norm{T_t^{t+\Delta} - (I+\Delta Q)} %\leq \norm{T_t^{t+\Delta} - \prod_{i=1}^n(I+\Delta_iQ_i)} + \norm{\prod_{i=1}^n(I+\Delta_iQ_i) - (I+\sum_{i=1}^n\Delta_i Q_i)}
<\Delta\frac{\epsilon}{2}+\Delta^2\norm{\rateset}^2
\leq\Delta\frac{\epsilon}{2}+\Delta\delta\norm{\rateset}^2
<\Delta\frac{\epsilon}{2}+\Delta\frac{\epsilon}{2}=\Delta\epsilon.
\end{equation*}
The result now follows because the convexity of $\rateset$ implies that $Q\in\rateset$.
\end{proof}


\begin{lemma}\label{lemma:uniformelywellbehaved}
Consider a non-empty bounded set $\rateset$ of rate matrices and let $P\in\wmprocesses_{\rateset}$ be a well-behaved Markov chain that is consistent with $\rateset$. 
Then, for any $t,s\in\realsnonneg$ such that $t\leq s$, we have that $\norm{T_t^s-I}\leq(s-t)\norm{\rateset}$.
\end{lemma}
\begin{proof}
If $t=s$, the result is trivial because we know from Proposition~\ref{prop:Markovhassystem} and Definition~\ref{def:trans_mat_system} that $T_t^s=I$. Hence, without loss of generality, we may assume that $t<s$. Fix any $\epsilon>0$. Since $t<s$, it follows from Lemma~\ref{lemma:bound_on_linear_approx_partition} that there is some $u\in\mathcal{U}_{[t,t+\Delta]}$ with $u=t_0,\ldots,t_n$ and $t=t_0<\ldots< t_n=s$ such that, for all $i\in\{1,\ldots,n\}$, there is some $Q_i\in\rateset$ such that $\norm{T_{t_{i-1}}^{t_i} - (I+\Delta_iQ_i)} < \Delta_i\epsilon$, and therefore also
\begin{equation*}
\norm{T_{t_{i-1}}^{t_i} - I}
\leq\norm{T_{t_{i-1}}^{t_i} - (I+\Delta_iQ_i)}+\Delta_i\norm{Q_i} < \Delta_i\epsilon+\Delta_i\norm{\rateset}=\Delta_i(\epsilon+\norm{\rateset}),
\end{equation*}
with $\Delta_i\coloneqq t_{i}-t_{i-1}$. Since it follows from Proposition~\ref{prop:Markovhassystem} and Definition~\ref{def:trans_mat_system} that $T_t^s=\prod_{i=1}^n T_{t_{i-1}}^{t_i}$ and that, for all $i\in\{1,\dots,n\}$, $T_{t_{i-1}}^{t_i}$ and $I$ are transition matrices, we infer from Lemma~\ref{lemma:recursive} that
\begin{align*}
\norm{T_t^s-I}
=\norm{\prod_{i=1}^nT_{t_{i-1}}^{t_i}-\prod_{i=1}^n I}
\leq\sum_{i=1}^n\norm{T_{t_{i-1}}^{t_i}-I}
<
\sum_{i=1}^n
\Delta_i(\epsilon+\norm{\rateset})
%=\left(\sum_{i=1}^n\Delta_i\right)
%(\epsilon+\norm{\rateset})
=(s-t)(\epsilon+\norm{\rateset}).
\end{align*}
Since $\epsilon>0$ is arbitrary, it follows that $\norm{T_t^s-I}\leq(s-t)\norm{\rateset}$.
\end{proof}

\begin{lemma}\label{lemma:restricted_trans_mat_system_complete_if_Q_closed}
Consider a non-empty, bounded, convex and closed set of rate matrices $\rateset$ and any $t,s\in\realsnonneg$ such that $t\leq s$. Let $d$ be the metric that is defined in Equation~\eqref{eq:trans_mat_system_metric}. The metric space $\smash{(\mathbb{T}_\rateset^{[t,s]},d)}$ is then complete.
\end{lemma}
\begin{proof}
Consider any sequence $\{\mathcal{T}_i^{[t,s]}\}_{i\in\nats}$ in $\mathbb{T}_{\rateset}^{[t,s]}$ that is Cauchy. We need to prove that this sequence converges to a limit $\mathcal{T}_*^{[t,s]}$ that belongs to $\mathbb{T}_{\rateset}^{[t,s]}$. Since we already know from Lemma~\ref{lemma:restricted_trans_mat_system_cauchy_converges} this limit exists and belongs to $\mathbb{T}^{[t,s]}$, the only thing that remains to be shown is that $\mathcal{T}_*^{[t,s]}\in\mathbb{T}_{\rateset}^{[t,s]}$. 
Throughout this proof, for any $q,r\in[t,s]$ such that $q\leq r$, we will use $\smash{\presuper{*}T_q^r}$ to denote the transition matrix that corresponds to $\mathcal{T}_*^{[t,s]}$ and, for all $i\in\nats$, we will use $\presuper{i}T_q^r$ to denote the transition matrix that corresponds to $\mathcal{T}_i^{[t,s]}$. 

Fix any $r\in[t,s]$ and consider any $\Delta>0$. Then due to Lemma~\ref{lemma:uniformelywellbehaved}, for all $i\in\nats$:
\begin{equation*}
\norm{\presuper{*}T_r^{r+\Delta}-I}
\leq\norm{\presuper{*}T_r^{r+\Delta}-\presuper{i}T_r^{r+\Delta}}+\norm{\presuper{i}T_r^{r+\Delta}-I}
\leq
\norm{\presuper{*}T_r^{r+\Delta}-\presuper{i}T_r^{r+\Delta}}+\Delta\norm{\rateset}.
\end{equation*}
Since $\lim_{i\to+\infty}\presuper{i}T_r^{r+\Delta}=\presuper{*}T_r^{r+\Delta}$, this implies that $\norm{\presuper{*}T_r^{r+\Delta}-I}\leq\Delta\norm{\rateset}$. Similarly, if $r-\Delta\geq0$, we also find that $\norm{\presuper{*}T_{r-\Delta}^{r}-I}\leq\Delta\norm{\rateset}$. Hence, it follows that
\begin{equation*}
\limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}\left(\presuper{*}T_r^{r+\Delta}-I\right)}\leq\norm{\rateset}\text{~~~and~~~}
\limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}\left(\presuper{*}T_{r-\Delta}^{r}-I\right)}\leq\norm{\rateset},
\end{equation*}
which implies that $\mathcal{T}_*^{[t,s]}$ is well-behaved.

It remains to be show that $\mathcal{T}_*^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}$. To this end, consider any $Q\in\rateset$, and let $\mathcal{T}_Q$ be the corresponding transition matrix system, as given by Definition~\ref{def:systemfromQ}. Let 
\begin{equation*}
\mathcal{T}_*\coloneqq \mathcal{T}_Q^{[0,t]}\otimes \mathcal{T}_*^{[t,s]} \otimes \mathcal{T}_Q^{[s,\infty)}\,.
\end{equation*}
Then clearly, $\mathcal{T}_*$ is a transition matrix system and, since $\mathcal{T}_*^{[t,s]}$ is well-behaved, it follows from Proposition~\ref{prop:systemQ} and~\ref{prop:concat_restr_trans_mat_systems_is_system} that $\mathcal{T}_*$ is also well-behaved. Therefore, and because of Theorem~\ref{theo:uniqueMarkovchain}, there is some $P_*\in\wmprocesses$ such that $\mathcal{T}_{P_*}=\mathcal{T}_*$. In the remainder of this proof, we will show that $P_*\in\wmprocesses_\rateset$, which then implies that $\mathcal{T}_*^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}$.

Fix any $r\in\realsnonneg$. We need to show that $\smash{\overline{\partial}}\presuper{*}T_r^r\subseteq\rateset$, or equivalently, that $\smash{\overline{\partial}_{+}}\hspace{-3pt}\presuper{*}T_r^r\subseteq\rateset$ and $\smash{\overline{\partial}_{-}}\hspace{-3pt}\presuper{*}T_r^r\subseteq\rateset$. We will only prove that $\smash{\overline{\partial}_{+}}\hspace{-3pt}\presuper{*}T_r^r\subseteq\rateset$. The proof for $\smash{\overline{\partial}_{-}}\hspace{-3pt}\presuper{*}T_r^r\subseteq\rateset$ is completely analogous. 
So fix any $Q_*\in\smash{\overline{\partial}_{+}}\hspace{-3pt}\presuper{*}T_r^r$. We will show that $Q_*\in\rateset$. If $r<t$ or $s\leq r$, this holds trivially because in that case, we know that $\smash{\partial}_+\presuper{*}T_r^r=Q$, which, because of Corollary~\ref{corol:outersingleton}, implies that $\smash{\overline{\partial}}_+\presuper{*}T_r^r=\{Q\}\subseteq\rateset$.
Therefore, without loss of generality, we may assume that $r\in[t,s)$. 

Fix any $m\in\nats$. Lemma~\ref{lemma:uniform_delta_for_convex_markov_set} then implies the existence of some $\delta\in\realspos$ such that
\begin{equation}\label{eq:lemma:restricted_trans_mat_system_complete_if_Q_closed:uniformdelta}
(\forall 0<\Delta<\delta)\,
(\forall i\in\nats)\,
(\exists Q\in\rateset)~\norm{\frac{\presuper{i}T_{r}^{r+\Delta} - I}{\Delta}- Q} \leq\frac{1}{m}.
\end{equation}
Since $Q_*\in \smash{\overline{\partial}}_+\hspace{-3pt}\presuper{*}T_r^r$, Equation~\eqref{eq:rightouterderivative} implies the existence of some $0<\Delta_m<\delta$ such that
\begin{equation*}%\label{eq:lemma:restricted_trans_mat_system_complete_if_Q_closed:deltam}
\norm{\frac{\presuper{*}T_r^{r+\Delta_m} - I}{\Delta_m}-Q_*}<\frac{1}{m}
\end{equation*}
and, since $\lim_{i\to+\infty}\presuper{i}T_r^{r+\Delta_m}=\presuper{*}T_r^{r+\Delta_m}$, there is some $i_m\in\nats$ such that
\begin{equation*}%\label{eq:lemma:restricted_trans_mat_system_complete_if_Q_closed:im}
\norm{\frac{\presuper{*}T_r^{r+\Delta_m}-\presuper{i_m}T_r^{r+\Delta_m}}{\Delta_m}}\leq\frac{1}{m}.
\end{equation*}
Therefore, we can apply Equation~\eqref{eq:lemma:restricted_trans_mat_system_complete_if_Q_closed:uniformdelta}---with $\Delta\coloneqq\Delta_m$ and $i\coloneqq i_m$---to infer that there is some $Q_m\in\rateset$ such that
\begin{equation*}
\norm{Q_*-Q_m}
\leq
\norm{Q_*-\frac{\presuper{*}T_r^{r+\Delta_m} - I}{\Delta_m}}
+
\norm{\frac{\presuper{*}T_r^{r+\Delta_m}-\presuper{i_m}T_r^{r+\Delta_m}}{\Delta_m}}
+
\norm{\frac{\presuper{i_m}T_{r}^{r+\Delta_m} - I}{\Delta_m}- Q_m}
\leq\frac{3}{m}.
\end{equation*}
By repeating this procedure for every $m\in\nats$, we obtain a sequence $\{Q_m\}_{n\in\nats}$ in $\rateset$ such that $\norm{Q_*-Q_m}\leq\nicefrac{3}{m}$ for all $m\in\nats$, which clearly implies that $\lim_{m\to+\infty}Q_m=Q_*$. Since the sequence $\{Q_m\}_{m\in\nats}$ belongs to the closed set $\rateset$, we must therefore have that $Q_*\in\rateset$.
\end{proof}

\begin{lemma}\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex}
Consider a non-empty, bounded and convex set of rate matrices $\rateset$ and any $t,s\in\realsnonneg$ such that $t\leq s$. Let $d$ be the metric that is defined in Equation~\eqref{eq:trans_mat_system_metric}. The metric space $\smash{(\mathbb{T}_\rateset^{[t,s]},d)}$ is then totally bounded.
\end{lemma}
\begin{proof}
In order to prove that $(\mathbb{T}_\rateset^{[t,s]},d)$ is totally bounded, it suffices to prove that, for all $\epsilon\in\realspos$, there is a finite collection of open $\epsilon$-balls centered on elements of $\smash{\mathbb{T}_\rateset^{[t,s]}}$, such that this collection covers $\smash{\mathbb{T}_\rateset^{[t,s]}}$, or equivalently, that there is a finite subset $\mathbb{C}_\epsilon$ of $\smash{\mathbb{T}_\rateset^{[t,s]}}$ such that
\begin{equation}\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:thereisacover}
(\forall \mathcal{T}^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]})\,(\exists \mathcal{S}^{[t,s]}\in \mathbb{C}_\epsilon)~d(\mathcal{T}^{[t,s]},\mathcal{S}^{[t,s]}) < \epsilon\,.
\end{equation}
Fix any $\epsilon>0$. We will now construct such a set $\mathbb{C}_\epsilon$ and prove that it satisfies Equation~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:thereisacover}.

%In other words: for any $\epsilon>0$, we need to construct a finite set $\mathbb{C}_\epsilon\subseteq\mathbb{T}_\rateset^{[t,s]}$ such that $\mathbb{T}_\rateset^{[t,s]}\subseteq\cup$

% Equivalently, we need to show that for all $\epsilon\in\realspos$, there is some set
% \begin{equation*}
% \mathbb{C}_\epsilon\coloneqq \left\{\mathcal{T}_i^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}\,:\,i\in\{1,\ldots,N\}\right\}\,,
% \end{equation*}
% with $N\in\nats$, such that
% \begin{equation*}
% (\forall \mathcal{T}^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]})(\exists \mathcal{S}^{[t,s]}\in \mathbb{C}_\epsilon)\,:\,d(\mathcal{T}^{[t,s]},\mathcal{S}^{[t,s]}) < \epsilon\,.
% \end{equation*}

Choose $\epsilon_1,\epsilon_2,\epsilon_3,\epsilon_4\in\reals_{>0}$ such that 
$(s-t)(\epsilon_1+\epsilon_2+\epsilon_3)+2\epsilon_4<\nicefrac{\epsilon}{2}$ and choose $\delta_3,\delta_4\in\reals_{>0}$ such that $\delta_3\norm{\rateset}^2<\epsilon_3$ and $2\delta_4\norm{\rateset}<\epsilon_4$ [this is clearly always possible].


Because $\rateset$ is bounded and finite-dimensional, it is totally bounded under the metric induced by our usual norm $\norm{\cdot}$. Therefore, $\rateset$ admits a finite cover of open balls with radius $\epsilon_2$. In other words, there is some finite set $\rateset_{\epsilon_2}\subseteq\rateset$ such that
\begin{equation}\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:epsilon2}
(\forall Q\in\rateset)(\exists \tilde{Q}\in\rateset_{\epsilon_2})\,:\,\norm{Q - \tilde{Q}} < \epsilon_2.
\end{equation}
Furthermore, because $\rateset$ is non-empty, bounded and convex, we know from Lemma~\ref{lemma:uniform_delta_for_convex_markov_set} that there is some $\delta_1\in\realspos$ such that for all $P\in\wmprocesses_\rateset$, all $t\in\realsnonneg$, and all $\Delta\in\realsnonneg$ such that $\Delta<\delta_1$:
\begin{equation}\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:epsilon1}
(\exists Q\in\rateset)\,\norm{T_t^{t+\Delta} - (I+\Delta Q)} \leq \Delta\epsilon_1.
\end{equation}

Now let $\delta\coloneqq\min\{\delta_1,\delta_3,\delta_4\}$ and consider any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then, without loss of generality, there is some $n\in\nats$ such that $u=t_0,\ldots,t_n$. 
%Then, let $\mathbb{T}_i$ be the set of restricted transition matrix systems defined by
% \begin{equation*}
% \mathbb{T}_i \coloneqq \left\{ \mathcal{T}_{Q_j}^{[t_{i-1},t_i]} \,:\,Q_j\in\rateset_{\epsilon'} \right\}\,.
% \end{equation*}
%Because $\rateset_{\epsilon'}$ is finite, $\mathbb{T}_i$ is also clearly finite.
%We now use the following claim, which is proved below.
%Because each $\mathbb{T}_i$ is finite, there are only finitely many ways in which the product in Claim~\ref{claim:composition_restricted_trans_mat_sys_in_set} can be constructed. Therefore, if we now let
Let
\begin{equation*}
\mathbb{C}_\epsilon \coloneqq \left\{\mathcal{T}_{\tilde{Q}_1}^{[t_{0},t_1]} \otimes \mathcal{T}_{\tilde{Q}_2}^{[t_{1},t_2]}\otimes~ \cdots ~\otimes \mathcal{T}_{\tilde{Q}_n}^{[t_{n-1},t_n]}\,:\,\left(\forall i\in\{1,\ldots,n\}\right)\,\tilde{Q}_i\in\rateset_{\epsilon_2}\right\},
\end{equation*}
where, for any $\tilde{Q}\in\rateset_{\epsilon_2}$, we let $\mathcal{T}_{\tilde{Q}}^{[t_{i-1},t_i]}$ be the restriction of $\mathcal{T}_{\tilde{Q}}$ to the interval $[t_{i-1},t_i]$, with $\mathcal{T}_{\tilde{Q}}$ as in Definition~\ref{def:systemfromQ}.
Since $n$ and $\abs{\rateset_{\epsilon_2}}$ are finite, $\mathbb{C}_\epsilon$ is clearly finite and, due to Lemma~\ref{lemma:nonhomogeneous_in_process_set}, $\mathbb{C}_\epsilon$ is a subset of $\smash{\mathbb{T}_\rateset^{[t,s]}}$. The only thing that we still need to prove is that $\mathbb{C}_\epsilon$ satisfies Equation~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:thereisacover}.

So fix any $\mathcal{T}^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}$. For any $q,r\in[t,s]$ such that $q\leq r$, we will use $T_q^r$ to denote the corresponding transition matrix. %We will show that there is some $\mathcal{S}^{[t,s]}\in \mathbb{C}_{\epsilon}$ such that $d(\mathcal{T}^{[t,s]},\mathcal{S}^{[t,s]})<\epsilon$. We start by constructing this $\mathcal{S}^{[t,s]}$.

For all $i\in\{1,\dots,n\}$, since $\Delta_i\leq\sigma(u)<\delta\leq\delta_1$, 
%Because $\mathcal{T}^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}$, it follows that there is some corresponding $P\in\wmprocesses_\rateset$ which agrees with $\mathcal{T}^{[t,s]}$ on all transition matrices $T_q^r\in\mathcal{T}^{[t,s]}$. 
it follows from Equations~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:epsilon1} and~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:epsilon2}---in that order---and Lemma~\ref{lemma:linearpartofexponential} that there are $Q_i\in\rateset$ and $\tilde{Q}_i^*\in\rateset_{\epsilon_2}$ such that 
\begin{align}
\norm{T_{t_{i-1}}^{t_i}-e^{\tilde{Q}_i^*\Delta_i}}
&\leq
\norm{T_{t_{i-1}}^{t_i}-(I+\Delta_i Q_i)}
+\norm{\Delta_i(Q_i-\tilde{Q}_i^*)}
+
\big\lVert I+\Delta_i\tilde{Q}_i^*-e^{\tilde{Q}_i^*\Delta_i}
\big\lVert\notag\\
&\leq
\Delta_i\epsilon_1
+\Delta_i\epsilon_2
+\Delta_i^2\norm{\rateset}^2
\leq\Delta_i(\epsilon_1+\epsilon_2+\epsilon_3),\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:epsilon123}
\end{align}
where the final inequality holds because $\Delta_i\norm{\rateset}^2\leq\delta\norm{\rateset}^2\leq\delta_3\norm{\rateset}^2<\epsilon_3$. We now use these $\tilde{Q}_i^*\in\rateset_{\epsilon_2}$, $i\in\{1,\dots,n\}$, to define
\begin{equation*}
\mathcal{S}^{[t,s]}\coloneqq \mathcal{T}_{\tilde{Q}_1^*}^{[t_{0},t_1]} \otimes\mathcal{T}_{\tilde{Q}_1^*}^{[t_{0},t_1]} \otimes ~\cdots~ \otimes \mathcal{T}_{\tilde{Q}_n^*}^{[t_{n-1},t_n]} \in \mathbb{C}_\epsilon.
\end{equation*}
For any $q,r\in[t,s]$ such that $q\leq r$, we will use $S_q^r$ to denote transition matrix that corresponds to the restricted transition matrix system $\smash{\mathcal{S}^{[t,s]}}$.

For any $i\in\{1,\dots,n\}$, it follows from Definition~\ref{def:systemfromQ} and Equation~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:epsilon123} that
\begin{equation}\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:firstlocal}
\norm{T_{t_{i-1}}^{t_i} - S_{t_{i-1}}^{t_i}} 
=\norm{T_{t_{i-1}}^{t_i} - e^{\tilde{Q}_i^*\Delta_i}} 
\leq \Delta_i(\epsilon_1+\epsilon_2+\epsilon_3)
\end{equation}
and from Lemma~\ref{lemma:uniformelywellbehaved} that, for all $q,r\in[t_{i-1},t_i]$ such that $q\leq r$,
\begin{equation}\label{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:secondlocal}
\norm{T_q^r - S_q^r} 
\leq
\norm{T_q^r - I}+\norm{S_q^r-I} 
\leq 2(r-q)\norm{\rateset}
%\leq 2(t_i-t_{i-1})\norm{\rateset}
%\leq 2\Delta_i\norm{\rateset}
<\epsilon_4,
\end{equation}
where the final inequality holds because $r-q\leq t_i-t_{i-1}\leq\Delta_i\leq\sigma(u)\leq\delta\leq\delta_4$ and because $2\delta_4\norm{\rateset}<\epsilon_4$.

Consider now any $q,r\in[t,s]$ such that $q\leq r$. We will prove that $\norm{T_q^r - S_q^r} < \nicefrac{\epsilon}{2}$. If $q,r\in[t_{i-1},t_i]$ for some $i\in\{1,\ldots,n\}$, this follows trivially from Equation~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:secondlocal}. In any other case, there must be some $k,\ell\in\{1,\ldots,n\}$ such that $k\leq\ell$, $q\in[t_{k-1},t_k]$ and $r\in[t_{\ell},t_{\ell+1}]$, and we then find that, again,
\begin{align*}
\norm{T_q^r - S_q^r} &= \norm{T_q^{t_k}\left(\prod_{i=k+1}^{\ell}T_{t_{i-1}}^{t_i}\right)T_{t_{\ell}}^r - S_q^{t_k}\left(\prod_{i=k+1}^{\ell}S_{t_{i-1}}^{t_i}\right)S_{t_{\ell}}^r} \\
&\leq
\norm{T_q^{t_k}-S_q^{t_k}}
+\sum_{i=k+1}^\ell\norm{T_{t_{i-1}}^{t_i}-S_{t_{i-1}}^{t_i}}
+\norm{T_{t_{\ell}}^r-S_{t_{\ell}}^r}\\
&\leq
2\epsilon_4+\sum_{i=k+1}^\ell\Delta_i(\epsilon_1+\epsilon_2+\epsilon_3)<\frac{\epsilon}{2},
\end{align*}
where the equality follows from Definition~\ref{def:trans_mat_system}, the first inequality follows from Definition~\ref{def:trans_mat_system} and Lemma~\ref{lemma:recursive}, the second inequality follows from Equations~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:firstlocal} and~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:secondlocal}, and the final inequality holds because $\sum_{i=k+1}^\ell\Delta_i=t_\ell-t_k\leq s-t$.
Hence, in all cases: $\norm{T_q^r - S_q^r} < \nicefrac{\epsilon}{2}$.

Since this is true for all $q,r\in[t,s]$ such that $q\leq r$, we immediately find that
\begin{equation*}
d(\mathcal{T}^{[t,s]},\mathcal{S}^{[t,s]}) = \sup\{\norm{T_q^r - S_q^r}\,:\,q,r\in[t,s], q\leq r\} \leq \frac{\epsilon}{2}<\epsilon.
\end{equation*}
Since $\mathcal{T}^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}$ is arbitrary, it follows that $\mathbb{C}_\epsilon$ satisfies Equation~\eqref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex:thereisacover}.
\end{proof}

% \begin{claim}\label{claim:composition_restricted_trans_mat_sys_in_set}
% For all $i\in\{1,\ldots,n\}$, choose some $\mathcal{T}_i^{[t_{i-1},t_i]}\in\mathbb{T}_i$. Then,
% \begin{equation*}
% \left(\mathcal{T}_1^{[t_{0},t_1]} \otimes \mathcal{T}_2^{[t_{1},t_2]} \cdots \otimes \mathcal{T}_n^{[t_{n-1},t_n]}\right) \in \mathbb{T}_\rateset^{[t,s]}\,.
% \end{equation*}
% \end{claim}
% \begin{proof}[Proof of Claim~\ref{claim:composition_restricted_trans_mat_sys_in_set}]
% For all $i\in\{1,\ldots,n\}$, choose some $\mathcal{T}_i^{[t_{i-1},t_i]}\in\mathbb{T}_i$. Let
% \begin{equation*}
% \mathcal{T}^{[t,s]}\coloneqq \mathcal{T}_1^{[t_{0},t_1]} \otimes \mathcal{T}_2^{[t_{1},t_2]} \cdots \otimes \mathcal{T}_n^{[t_{n-1},t_n]}\,.
% \end{equation*}
% Because for all $i\in\{1,\ldots,n\}$ we have that $\mathcal{T}_i^{[t_{i-1},t_i]}$ is a well-behaved restricted transition matrix system, it follows from Proposition~\ref{prop:concat_restr_trans_mat_systems_is_system} that $\mathcal{T}^{[t,s]}$ is a well-behaved restricted transition matrix system defined on $[t,s]$.

% We now have to show that $\mathcal{T}^{[t,s]} \in \mathbb{T}_\rateset^{[t,s]}$. To this end, consider any $Q\in\rateset$, and let $\mathcal{T}_Q$ be the transition matrix system corresponding to $Q$, defined as in Definition~\ref{def:systemfromQ}. Let
% \begin{align*}
% \mathcal{T} &\coloneqq \mathcal{T}_Q^{[0,t]}\otimes \mathcal{T}^{[t,s]} \otimes \mathcal{T}_Q^{[s,\infty)} \\
%  &= \mathcal{T}_Q^{[0,t]}\otimes \mathcal{T}_1^{[t_{0},t_1]} \otimes \mathcal{T}_2^{[t_{1},t_2]} \cdots \otimes \mathcal{T}_n^{[t_{n-1},t_n]} \otimes \mathcal{T}_Q^{[s,\infty)} \\
%  &= \mathcal{T}_Q^{[0,t]}\otimes \mathcal{T}_{Q_1}^{[t_{0},t_1]} \otimes \mathcal{T}_{Q_2}^{[t_{1},t_2]} \cdots \otimes \mathcal{T}_{Q_n}^{[t_{n-1},t_n]} \otimes \mathcal{T}_Q^{[s,\infty)} \,,
% \end{align*}
% for the corresponding rate matrices $Q_1,\ldots,Q_n\in\rateset_{\epsilon'}\subseteq\rateset$. Then, it follows from Lemma~\ref{lemma:nonhomogeneous_in_process_set} that there is some $P\in\wmprocesses_\rateset$ such that $\mathcal{T}_P=\mathcal{T}$. Because $P\in\wmprocesses_\rateset$, it follows that the restricted transition matrix system satisfies $\mathcal{T}^{[t,s]}=\mathcal{T}_P^{[t,s]}\in\mathbb{T}_\rateset^{[t,s]}$.
% \end{proof}

% \begin{proof}[Proof of Claim~\ref{claim:lemma_total_bounded_full_interval_error}]
% Consider any $i\in\{1,\ldots,n\}$. Then, with $T_{t_{i-1}}^{t_i}\in\mathcal{T}^{[t,s]}$ and $S_{t_{i-1}}^{t_i}\in\mathcal{S}^{[t,s]}$, it follows from Equation~\eqref{eq:lemma_total_bounded_full_interval_error_linear} that there is some $Q_i\in\rateset$ such that
% \begin{align*}
% \norm{T_{t_{i-1}}^{t_i} - S_{t_{i-1}}^{t_i}} &\leq \norm{T_{t_{i-1}}^{t_i} - (I+\Delta_iQ_i)} + \norm{(I+\Delta_iQ_i) - S_{t_{i-1}}^{t_i}} \\
%  &\leq \norm{T_{t_{i-1}}^{t_i} - (I+\Delta_iQ_i)} + \norm{(I+\Delta_iQ_i) - (I+\Delta_i\widetilde{Q}_i)} + \norm{(I+\Delta_i\widetilde{Q}_i) - S_{t_{i-1}}^{t_i}} \\
%  &< \frac{\Delta_i\epsilon^*}{6C} + \Delta_i\epsilon' + \norm{(I+\Delta_i\widetilde{Q}_i) - S_{t_{i-1}}^{t_i}} \\
%  &= \frac{\Delta_i\epsilon^*}{6C} + \Delta_i\epsilon' + \norm{(I+\Delta_i\widetilde{Q}_i) - e^{\widetilde{Q}_i\cdot\Delta_i}} \\
%  &\leq \frac{\Delta_i\epsilon^*}{6C} + \Delta_i\epsilon' + \Delta_i^2\norm{\rateset}^2\,,
% \end{align*}
% where the strict equality follows from the definition of $\mathcal{S}^{[t,s]}$, and the final inequality follows from Lemma~\ref{lemma:linearpartofexponential}. Furthermore, because $\Delta_i\leq\sigma(u)<\delta^*\leq\nicefrac{\epsilon^*}{(6C\norm{\rateset}^2)}$, we have that
% \begin{equation*}
% \norm{T_{t_{i-1}}^{t_i} - S_{t_{i-1}}^{t_i}} < \frac{\Delta_i\epsilon^*}{6C} + \Delta_i\epsilon' + \Delta_i^2\norm{\rateset}^2 < 2\frac{\Delta_i\epsilon^*}{6C} + \Delta_i\epsilon' = \frac{\Delta_i\epsilon^*}{3C} + \Delta_i\epsilon'\,.
% \end{equation*}
% \end{proof}

% \begin{proof}[Proof of Claim~\ref{claim:lemma_total_bounded_sub_interval_error}]
% Consider any $i\in\{1,\ldots,n\}$, and any $q,r\in[t_{i-1},t_i]$ such that $q\leq r$. Consider the transition matrices $T_q^r\in\mathcal{T}^{[t,s]}$ and $S_q^r\in\mathcal{S}^{[t,s]}$. Then, because $(r-q)\leq\Delta_i\leq\sigma(u)<\delta^*\leq\delta$, it follows from Equation~\ref{eq:lemma_total_bounded_uniform_delta} that there is some $Q\in\rateset$ such that
% \begin{align*}
% \norm{T_q^r - S_q^r} &< \frac{(r-q)\epsilon^*}{6C} + \norm{(I+(r-q)Q) - S_q^r} \\
%  &\leq \frac{(r-q)\epsilon^*}{6C} + \norm{(I+(r-q)Q) - (I+(r-q)\widetilde{Q}_i)} + \norm{(I+(r-q)\widetilde{Q}_i) - S_q^r} \\
%  &\leq \frac{(r-q)\epsilon^*}{6C} + 2(r-q)\norm{\rateset} + \norm{(I+(r-q)\widetilde{Q}_i) - S_q^r} \\
%  &= \frac{(r-q)\epsilon^*}{6C} + 2(r-q)\norm{\rateset} + \norm{(I+(r-q)\widetilde{Q}_i) - e^{\widetilde{Q}_i\cdot(r-q)}} \\
%  &\leq \frac{(r-q)\epsilon^*}{6C} + 2(r-q)\norm{\rateset} + (r-q)^2\norm{\rateset}^2 \\
%  &\leq \frac{\Delta_i\epsilon^*}{6C} + 2(r-q)\norm{\rateset} + \Delta_i^2\norm{\rateset}^2 \\
%  &\leq \frac{\Delta_i\epsilon^*}{6C} + 2(r-q)\norm{\rateset} + \frac{\Delta_i\epsilon^*}{6C}\,,
% \end{align*}
% where the last inequality follows from the fact that $\Delta_i\leq\sigma(u)<\delta^*\leq\nicefrac{\epsilon^*}{(6C\norm{\rateset}^2)}$. Hence, and because $(r-q)\leq\Delta_i\leq\sigma(u)<\delta^*\leq\nicefrac{\epsilon^*}{12\norm{\rateset}}$, we have
% \begin{equation*}
% \norm{T_q^r - S_q^r} < \frac{\Delta_i\epsilon^*}{3C} + 2\Delta_i\norm{\rateset} < \frac{\Delta_i\epsilon^*}{3C} + \frac{\epsilon^*}{6}\,.
% \end{equation*}
% \end{proof}

\begin{corollary}\label{cor:restricted_trans_mat_system_totally_bounded_if_Q_bounded}
Consider a non-empty bounded set of rate matrices $\rateset$ and any $t,s\in\realsnonneg$ such that $t\leq s$. Let $d$ be the metric that is defined in Equation~\eqref{eq:trans_mat_system_metric}. The metric space $\smash{(\mathbb{T}_\rateset^{[t,s]},d)}$ is then totally bounded.
\end{corollary}
\begin{proof}
Let $\rateset'$ denote the convex hull of $\rateset$. Then, clearly, $\rateset'$ is a non-empty, bounded, and convex set of rate matrices. Furthermore, since $\rateset\subseteq\rateset'$, we know that $\mathbb{T}_\rateset^{[t,s]}\subseteq \mathbb{T}_{\rateset'}^{[t,s]}$. Because any subspace of a totally bounded metric space is itself totally bounded \cite[top of p.175]{book:metricspaces}, the result now follows trivially from Lemma~\ref{lemma:restricted_trans_mat_system_totally_bounded_if_Q_bounded_convex}.
% $\mathbb{T}_{\widetilde{\rateset}}^{[t,s]}$ is totally bounded. Therefore, and because any subset of a totally bounded set is itself totally bounded~*** {\bf CITE} ***, it follows that $\mathbb{T}_\rateset^{[t,s]}$ is totally bounded as well.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theorem:restricted_transmatsystem_space_compact_if_Q_closed}]
Since a metric space is compact if and only if it is complete and totally bounded~\cite[Theorem 5.1.7]{book:metricspaces}, this result is an immediate consequence of Lemma~\ref{lemma:restricted_trans_mat_system_complete_if_Q_closed} and Corollary~\ref{cor:restricted_trans_mat_system_totally_bounded_if_Q_bounded}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:lower_expectation_reached_if_Q_closed}]
Since $\underline{\mathbb{E}}^{\mathrm{WM}}_\rateset$ computes an infimum, there is a sequence $\{P_i\}_{i\in\nats}$ in $\wmprocesses_\rateset$ such that
\begin{equation}\label{eq:prop:lower_expectation_reached_if_Q_closed}
\underline{\mathbb{E}}^{\mathrm{WM}}_\rateset[f(X_s)\vert\,X_t=x]
=\lim_{i\to+\infty}\mathbb{E}_i[f(X_s)\vert\,X_t=x]
=\lim_{i\to+\infty}\left[\presuper{i}T_t^sf\right](x),
\end{equation}
where we use $\mathbb{E}_i$ and $\presuper{i}T_t^s$ to denote the expectation operator and transition matrix that corresponds to $P_i$, respectively, and where the last equality follows from Remark~\ref{remark:expectationT}.

For every $i\in\nats$, we now let $\mathcal{T}_i^{[t,s]}$ be the corresponding restricted transition matrix system, which, since $P_i$ belongs to $\wmprocesses_\rateset$, is clearly an element of $\mathbb{T}_\rateset^{[t,s]}$.
Hence, since we know from Theorem~\ref{theorem:restricted_transmatsystem_space_compact_if_Q_closed} that the metric space $(\mathbb{T}_\rateset^{[t,s]},d)$ is compact, the sequence $\{\mathcal{T}_i^{[t,s]}\}_{i\in\nats}$ contains a convergent subsequence $\{\mathcal{T}_{i_k}^{[t,s]}\}_{k\in\nats}$ whose limit $\mathcal{T}^{[t,s]}$ belongs to $\mathbb{T}_\rateset^{[t,s]}$, which in turn implies that there is some $P\in\wmprocesses_\rateset$ such that $\mathcal{T}_{P}^{[t,s]}=\mathcal{T}^{[t,s]}$. Let $\mathbb{E}$ and $T_t^s$ be the expectation operator and transition matrix that corresponds to $P$, respectively. It then follows from Remark~\ref{remark:expectationT} and the fact that $\{\mathcal{T}_{i_k}^{[t,s]}\}_{k\in\nats}$ converges to $\mathcal{T}^{[t,s]}$ that
\begin{equation*}
\mathbb{E}[f(X_s)\,\vert\,X_t=x]
=\left[T_t^sf\right](x)
=\lim_{k\to+\infty}\left[\presuper{i_k}T_t^sf\right](x)
\end{equation*}
By combining this expression with Equation~\eqref{eq:prop:lower_expectation_reached_if_Q_closed}, the result is now immediate.
\end{proof}


\section{Proofs and Lemmas for Section~\ref{sec:lowertrans}}

\begin{proof}[Proof of Proposition~\ref{lemma:completemetricspace}]
Consider any sequence $\{\lt_i\}_{i\in\nats}$ of lower transition operators that is Cauchy with respect to the operator norm $\norm{\cdot}$. We will prove that $\{\lt_i\}_{i\in\nats}$ converges to a limit $\lt\colon\gamblesX\to\gamblesX$ that is itself a lower transition operator.

Consider any $f\in\gamblesX$ and $x\in\states$. For any $k,\ell\in\nats$, \eqref{N:normAf} then implies that
\begin{equation*}
\abs{[\lt_k f](x)-[\lt_\ell f](x)}
\leq\norm{\lt_k f-\lt_\ell f}
=\norm{(\lt_k-\lt_\ell)f}
\leq\norm{\lt_k-\lt_\ell}\norm{f}.
\end{equation*}
Therefore, and because $\{\lt_i\}_{i\in\nats}$ is Cauchy with respect to the norm $\norm{\cdot}$, it follows that $\{[\lt_i f](x)\}_{i\in\nats}$ is Cauchy with respect to the norm $\abs{\cdot}$. Hence, since $\reals$ is (well known to be) complete with respect to the topology that is induced by $\abs{\cdot}$, we find that $\{[\lt_i f](x)\}_{i\in\nats}$ converges to a limit in $\reals$, which we will denote by $[\lt f](x)$. Let $\lt f$ be the unique function in $\gamblesX$ that has $[\lt f](x)$, $x\in\states$, as its components. Then clearly, $\lt f=\lim_{i\to+\infty}\lt_if$. 

Let $\lt\colon\gamblesX\to\gamblesX$ be the unique operator that maps any $f\in\gamblesX$ to $\lt f$. We will prove that $\lt$ is a lower transition operator, or equivalently, that $\lt$ satisfies \ref{LT:bounded_min}--\ref{LT:homo}. For all $f,g\in\gamblesX$, we know that, for all $i\in\nats$, $\lt_i(f+g)\geq\lt_if+\lt_ig$ because $\lt_i$ is a lower transition operator, and therefore, we find that
\begin{equation*}
\lt(f+g)
=\lim_{i\to+\infty}\lt_i(f+g)
\geq\lim_{i\to+\infty}\lt_if
+
\lim_{i\to+\infty}\lt_ig=\lt f+\lt g.
\end{equation*}
Since $f$ and $g$ are arbitrary elements of $\gamblesX$, this means that $\lt$ satisfies \ref{LT:super_additive}. The proof for \ref{LT:bounded_min} and~\ref{LT:homo} is completely analogous. Hence, $\lt$ is a lower transition operator. Since $\lt f=\lim_{i\to+\infty}\lt_i f$ for all $f\in\gamblesX$, and because, for all $i\in\nats$, $\lt_i$ is a lower transition operator, it now follows from \cite[Proposition~1]{DeBock:2016} that $\lim_{i\to+\infty}\lt_i=\lt$.
% Since $\{\lt_i\}_{i\in\nats}$ is cauchy with respect to $\norm{\cdot}$, we know that
% \begin{equation}\label{eq:lemma:completemetricspace:cauchy}
% (\forall\epsilon\in\realspos)\,(\exists n_\epsilon\in\nats)\,(\forall k,\ell > n_\epsilon)
% ~\norm{\lt_k-\lt_\ell} < \epsilon.
% \end{equation}
\end{proof}

%THIS IS AN OLDER VERSION OF THE PROOF ABOVE
% \begin{proof}[Proof of Proposition~\ref{lemma:completemetricspace}]
% For all $\lt\in\underline{\mathcal{T}}$ and $x\in\states$, define $\lt_x:\gamblesX\rightarrow\reals$ by
% \begin{equation*}
% \lt_x(f) \coloneqq \lt(f)(x)\text{~~for all $f\in\gamblesX$}
% \end{equation*}
% and let $\underline{\mathcal{T}}_x\coloneqq\left\{\lt_x\,:\,\lt\in\underline{\mathcal{T}}\right\}$ for all $x\in\states$. Then clearly, any $\lt\in\underline{\mathcal{T}}$ is a finite vector of elements $\lt_1\in\underline{\mathcal{T}}_1,\ldots,\lt_m\in\underline{\mathcal{T}}_m$, and $\underline{\mathcal{T}}=\underline{\mathcal{T}}_1\times\cdots\times \underline{\mathcal{T}}_m$.

% Furthermore, for all $x\in\states$ and all $\lt_x\in\underline{\mathcal{T}}_x$, because of Definition~\ref{def:coh_low_trans}, $\lt_x$ is a map from $\gamblesX$ to $\reals$ that is super-additive, positively homogeneous, and bounded below by the minimum operator. Hence, by definition~\cite[Definition~2.3.3]{Walley:1991vk}, $\lt_x$ is a coherent lower prevision on $\gamblesX$, and $\underline{\mathcal{T}}_x$ corresponds to the space of all coherent lower previsions on $\gamblesX$.

% Let now $\gambles_{\leq1}(\states)\coloneqq\left\{f\in\gamblesX\,:\,\forall x\in\states, 0\leq f(x)\leq 1\right\}$ be the set of non-negative functions $f\in\gamblesX$ with $\norm{f}\leq 1$. It was previously shown~\cite{DeBock:2015ck} that the metric space $(\underline{\mathcal{T}}_x,d_x)$ is compact under the topology generated by the metric $d_x$, defined for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$ by
% \begin{equation*}
% d_x(\lt_x,\underline{S}_x) \coloneqq \sup\bigl\{ \abs{\lt_xf - \underline{S}_xf} \,:\,f\in\gambles_{\leq1}(\states) \bigr\}\,.
% \end{equation*}
% Now consider the metric $d_x^*$, which we define for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$ by
% \begin{equation*}
% d_x^*(\lt_x,\underline{S}_x) \coloneqq \sup\left\{ \abs{\lt_xf - \underline{S}_xf}\,:\, f\in\gamblesX, \norm{f}=1\right\}\,.
% \end{equation*}
% It is fairly easy to see that for all $\lt_x,\underline{S}_x\in\underline{\mathcal{T}}_x$, we have that
% \begin{equation*}
% d_x(\lt_x,\underline{S}_x) \leq d_x^*(\lt_x,\underline{S}_x) \leq 2d_x(\lt_x,\underline{S}_x)\,,
% \end{equation*}
% from which it follows that any subset of $\underline{\mathcal{T}}_x$ that is open with respect to $d_x$ is also open with respect to $d_x^*$. Hence, $d_x$ and $d_x^*$ generate the same topology on $\underline{\mathcal{T}}_x$, and therefore because $(\underline{\mathcal{T}}_x, d_x)$ is compact, so is $(\underline{\mathcal{T}}_x,d_x^*)$. Because $(\underline{\mathcal{T}}_x,d_x^*)$ is a metric space, compactness now implies that $(\underline{\mathcal{T}}_x,d_x^*)$ is complete.

% Therefore~\cite[Theorem 10.5.1]{OSearcoid:2006}, and because $\underline{\mathcal{T}}$ is a finite product of $\underline{\mathcal{T}}_1,\ldots,\underline{\mathcal{T}}_m$, the metric space $(\underline{\mathcal{T}},D)$ is complete under any metric $D$ that is \emph{conserving}~\cite[Definition 1.6.2]{OSearcoid:2006} with respect to $d_1^*,\ldots,d_m^*$. For $D$ to be conserving, it suffices to show that for all $\lt,\underline{S}\in\underline{\mathcal{T}}$,
% \begin{equation*}
% D(\lt, \underline{S}) = \max_{x\in\states}\bigl\{d_x^*(\lt_x, \underline{S}_x)\bigr\}\,.
% \end{equation*}
% Consider the metric $d$ on $\underline{\mathcal{T}}$ that is induced by $\norm{\cdot}$. We have, for any $\lt,\underline{S}\in\underline{\mathcal{T}}$, 
% \begin{align*}
% d(\lt,\underline{S}) &= \norm{\lt - \underline{S}} \\
% % &= \sup\left\{ \norm{\lt f - \underline{S}f}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
% % &= \sup\left\{ \max_{x\in\states}\{\abs{\lt(f)(x) - \underline{S}(f)(x)}\}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
%  &= \sup\left\{ \max_{x\in\states}\{\abs{\lt_xf - \underline{S}_xf}\}\,:\,f\in\gamblesX, \norm{f}=1 \right\} \\
%  &=  \max_{x\in\states}\bigl\{\sup\left\{\abs{\lt_xf - \underline{S}_xf}\,:\,f\in\gamblesX, \norm{f}=1 \bigr\}\right\} \\
%  &=  \max_{x\in\states}\left\{d_x^*(\lt_x, \underline{S}_x)\right\}\,.
% \end{align*}
% Thus, $d$ is conserving with respect to $d_1^*,\ldots,d_m^*$, and hence $(\underline{\mathcal{T}},d)$ is complete.
% \end{proof}

% \begin{lemma}\label{lemma:justtheindicator}
% Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
% \begin{equation*}
% \norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
% \end{equation*}
% \end{lemma}
% \begin{proof}
% \begin{align*}
% \norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
% &=\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)+\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
% &\leq\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
% &\leq\norm{(I+\Delta_n\lrate)-I}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
% &=\Delta_n\norm{\lrate}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}
% \end{align*}
% where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttrans}. By repeating this argument over and over again (actually, by induction), we find that
% \begin{align*}
% \norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
% \leq \Delta_n\norm{\lrate} +\Delta_{n-1}\norm{\lrate}+\cdots
% +\Delta_1\norm{\lrate}
% =\Delta\norm{\lrate}.
% \end{align*}
% \end{proof}

\begin{lemma}\label{lemma:recursive_lower_trans}
Consider two finite sequences $\underline{T}_1,\ldots,\underline{T}_n$ and $\underline{S}_1,\ldots,\underline{S}_n$ of lower transition operators. Then
\begin{equation}\label{eq:lemma_recursive_inequality}
\norm{\prod_{i=1}^n\underline{T}_i - \prod_{i=1}^n\underline{S}_i} \leq \sum_{i=1}^n \norm{\underline{T}_i - \underline{S}_i}.
\end{equation}
\end{lemma}
\begin{proof}
We provide a proof by induction. Clearly, Equation~\eqref{eq:lemma_recursive_inequality} holds for $n=1$. Suppose that it holds for $n-1$. We show that it then also holds for $n$.
\begin{align*}
\norm{\prod_{i=1}^n\underline{T}_i - \prod_{i=1}^n\underline{S}_i} &= \norm{\prod_{i=1}^n\underline{T}_i - \left(\prod_{i=1}^{n-1}\underline{T}_i\right)\underline{S}_n + \left(\prod_{i=1}^{n-1}\underline{T}_i\right)\underline{S}_n - \prod_{i=1}^n\underline{S}_i} \\
 &\leq \norm{\prod_{i=1}^n\underline{T}_i - \left(\prod_{i=1}^{n-1}\underline{T}_i\right)\underline{S}_n} + \norm{\left(\prod_{i=1}^{n-1}\underline{T}_i\right)\underline{S}_n - \prod_{i=1}^n\underline{S}_i} \\
 &= \norm{\left(\prod_{i=1}^{n-1}\underline{T}_i\right)\underline{T}_n - \left(\prod_{i=1}^{n-1}\underline{T}_i\right)\underline{S}_n} + \norm{\left(\prod_{i=1}^{n-1}\underline{T}_i - \prod_{i=1}^{n-1}\underline{S}_i\right)\underline{S}_n} \\
 &\leq \norm{\underline{T}_n - \underline{S}_n} + \norm{\prod_{i=1}^{n-1}\underline{T}_i - \prod_{i=1}^{n-1}\underline{S}_i}\norm{\underline{S}_n} \\
 &\leq \norm{\underline{T}_n - \underline{S}_n} + \norm{\prod_{i=1}^{n-1}\underline{T}_i - \prod_{i=1}^{n-1}\underline{S}_i} \\
 &\leq \norm{\underline{T}_n - \underline{S}_n} + \sum_{i=1}^{n-1}\norm{\underline{T}_i - \underline{S}_i} = \sum_{i=1}^{n}\norm{\underline{T}_i - \underline{S}_i}\,.
\end{align*}
Here, in the second inequality, we applied Proposition~\ref{lemma:compositioncoherence} and properties \ref{LT:differencenorm} and~\ref{N:normAB}. In the third inequality, we used property~\ref{LT:norm_at_most_one}. In the final inequality, we used the induction hypothesis.
\end{proof}

\begin{lemma}\label{lemma:justthelinearpart}
Consider a lower transition rate operator $\lrate$ and, for all $i\in\{1,\ldots,n\}$, some $\Delta_i\geq0$ such that $\Delta_i\norm{\lrate}\leq1$. Let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
%To avoid trivialities, we will assume that $\norm{\lrate}\neq 0$.
We provide a proof by induction. For $n=1$, the result is trivial. So consider the case $n\geq2$ and assume that the result is true for $n-1$. 

For all $i\in\{2,\dots,n\}$, since $\Delta_i\norm{\lrate}\leq1$, it follows from Proposition~\ref{lemma:normQsmallenough} that $(I+\Delta_i\lrate)$ and $I$ are lower transition operators. Therefore,
\begin{multline*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
\begin{aligned}
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\prod_{i=2}^n(I-\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\prod_{i=2}^n(I-\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq(\sum_{i=2}^n\Delta_i)^2\norm{\lrate}^2
+2\Delta_1\norm{\lrate}
\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I}\\
&\leq(\sum_{i=2}^n\Delta_i)^2\norm{\lrate}^2
+2\Delta_1\norm{\lrate}
\sum_{i=2}^n
\norm{(I+\Delta_i\lrate)-I}\\
% &=(\sum_{i=2}^n\Delta_i)^2\norm{\lrate}^2
% +2\Delta_1\norm{\lrate}
% \sum_{i=2}^n
% \Delta_i\norm{\lrate}\\
&=(\sum_{i=2}^n\Delta_i)^2\norm{\lrate}^2
+\Big(2\Delta_1
\sum_{i=2}^n
\Delta_i\Big)\norm{\lrate}^2
\leq\Big(
\Delta_1+\sum_{i=2}^n\Delta_i
\Big)^2\norm{\lrate}^2
=\Delta^2\norm{\lrate}^2,
\end{aligned}
\end{multline*}\\[3pt]
where the second inequality follows from the induction hypothesis and property~\ref{LR:differenceofnorm}, and the third inequality follows from Lemma~\ref{lemma:recursive_lower_trans}.
\end{proof}


% \begin{proof}
% \begin{align*}
% &\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
% &=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
% &\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\Delta_1\lrate}\\
% &\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1\norm{\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\lrate}\\
% &\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I},
% \end{align*}
% where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttransrate}. Due to Lemma~\ref{lemma:justtheindicator}, this implies that
% \begin{align*}
% &\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
% &\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
% &=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right).
% \end{align*}
% By continuing in this way (applying induction) we find that
% \begin{align*}
% &\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
% &\leq
% \norm{\prod_{i=n}^n(I+\Delta_i\lrate)-(I+\sum_{i=n}^n\Delta_i\lrate)}
% +2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
% +\cdots
% +2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
% &=2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
% +\cdots
% +2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
% &=2\norm{\lrate}^2\sum_{k=1}^n\Delta_k\sum_{i=k+1}^n\Delta_i\\
% &\leq2\norm{\lrate}^2\frac{1}{2}\left(\sum_{k=1}^n\Delta_k\right)^2=\Delta^2\norm{\lrate}^2
% \end{align*}
% \end{proof}

\begin{lemma}\label{lemma:differencebetweennested}
For any $k\in\{1,\dots,n\}$, consider a sequence $\Delta_{k,i}\geq 0$, $i\in\{1,\dots,n_k\}$, and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{k,i}$. Let $C\coloneqq\sum_{k=1}^n\Delta_k$ and let $\delta\coloneqq\max_{k=1}^n\Delta_k$. Then for any lower transition rate operator $\lrate$ such that $\delta\norm{\lrate}\leq1$:
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\delta C\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
For any $k\in\{1,\dots,n\}$, we know that $\Delta_{k,i}\norm{\lrate}\leq\Delta_k\norm{\lrate}\leq\delta\norm{\lrate}\leq1$ for all $i\in\{1,\dots,n_k\}$, and therefore, it follows from Propositions~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence} that $\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)$ and $(I+\Delta_k\lrate)$ are lower transition operators. Hence,
\begin{align*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
&\leq
\sum_{k=1}^n
\norm{
	\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
	-
	(I+\Delta_k\lrate)
}\\
&\leq
\sum_{k=1}^n
\Delta_k^2\norm{\lrate}^2
\leq
\sum_{k=1}^n
\delta\Delta_k\norm{\lrate}^2
=\delta C\norm{\lrate}^2,
\end{align*}
where the first inequality follows from Lemma~\ref{lemma:recursive_lower_trans} and the second inequality follows from Lemma~\ref{lemma:justthelinearpart}.
\end{proof}

\begin{corollary}\label{corol:differencebetweennestedintermsofu}
Let $\lrate$ be a lower transition rate operator. Consider any $t,s\in\realsnonneg$ such that $t\leq s$ and any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)\norm{\lrate}\leq 1$. Then for all $u'\in\mathcal{U}_{[t,s]}$ such that $u\subseteq u'$:
\begin{equation*}
\norm{\Phi_u-\Phi_{u'}}\leq\sigma(u)(s-t)\norm{\lrate}^2.\vspace{5pt}
\end{equation*}
\end{corollary}
\begin{proof}
This result is trivial if $s=t$, because then $u=u'=\{t\}=\{s\}$. Hence, without loss of generality, we assume that $s>t$, which implies that $u=(t_0,\dots,t_n)$, with $n\in\nats$, $t_0=t$ and $t_n=s$.
Since $u\subseteq u'$, we know that, for all $k\in\{1,\dots,n\}$, there is some sequence $\Delta_{k,i}> 0$, $i\in\{1,\dots,n_k\}$, with $n_k\in\nats$, such that $\Delta_k=\sum_{i=1}^{n_k}\Delta_{k,i}\leq\sigma(u)$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\text{~~and~~}
\Phi_{u}\coloneqq\prod_{k=1}^n(I+\Delta_{k}\lrate).
\end{equation*}
Because of Lemma~\ref{lemma:differencebetweennested}, this implies that $\norm{\Phi_{u'}-\Phi_u}\leq\sigma(u)(s-t)\norm{\lrate}^2$. 
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:differencebetweenu}]
Consider any $u'\in\mathcal{U}_{[t,s]}$ that is finer than $u$ and $u^*$, meaning that the timepoints it consists of contain the timepoints in $u$ and the timepoints in $u^*$. For example, let $u'$ be the ordered union of the timepoints in $u$ and $u^*$. Lemma~\ref{lemma:differencebetweennested} then implies that $\norm{\Phi_{u'}-\Phi_u}\leq\delta C\norm{\lrate}^2$ and $\norm{\Phi_{u'}-\Phi_{u^*}}\leq\delta C\norm{\lrate}^2$, and therefore, it follows that
\begin{equation*}
\norm{\Phi_{u}-\Phi_{u^*}}
%=
%\norm{\Phi_{u}-\Phi_{u'}+\Phi_{u'}-\Phi_{u^*}}
\leq
\norm{\Phi_{u}-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u^*}}
\leq2\delta C\norm{\lrate}^2.\vspace{-10pt}
\end{equation*}
\end{proof}


\begin{proof}[Proof of Corollory~\ref{corol:cauchy}]
By definition of a Cauchy sequence, we need to show that
\begin{equation*}
(\forall \epsilon>0)(\exists n\in\nats)(\forall i,j\geq n)
\norm{\Phi_{u_i}-\Phi_{u_j}}<\epsilon.
\end{equation*}
This follows directly from Proposition~\ref{prop:differencebetweenu} by noting that because $\lim_{i\to\infty}\sigma(u_i)=0$, we can decrease the $\delta$ in Proposition~\ref{prop:differencebetweenu} as we increase $n$.
\end{proof}

\begin{proof}[Proof of Corollary~\ref{corol:limitexistsandiscoherent}]
Since $\lim_{i\to\infty}\sigma(u_i)=0$, and because of Propositions~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence} and~\ref{LR:normlratefinite}, there is some index $n$ such that the sequence $\Phi_{u_n},\Phi_{u_{n+1}},\ldots$ consists of lower transition operators. Due to Corollary~\ref{corol:cauchy}, this sequence is Cauchy and therefore, because of Proposition~\ref{lemma:completemetricspace}, this sequence has a limit that is also a lower transition operator. Since the limit starting from $n$ and the limit starting from $1$ are identical (initial elements do not influence the limit), we find that the sequence $\left\{\Phi_{u_i}\right\}_{i\in\nats}$ has a limit, and that this limit is a lower transition operator.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:convergencelowerbound}]
Consider any sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$. Because of Corollary~\ref{corol:limitexistsandiscoherent}, the sequence $\{\Phi_{u_i}\}_{i\in\nats}$ then converges to a lower transition operator, which we denote by $\lt$. 

Fix any $\epsilon>0$, let $C\coloneqq s-t$ and choose any $\delta>0$ such that $\smash{4\delta C\norm{\lrate}^2<\epsilon}$ and $\delta\norm{\lrate}\leq1$ [this is clearly always possible].
% \begin{equation*}
% \delta\coloneqq\min\left\{\frac{\epsilon}{4C\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
% \end{equation*}
Since $\lim_{i\to+\infty}\Phi_{u_i}=\lt$ and $\lim_{i\to\infty}\sigma(u_i)=0$, there is some $i^*\in\nats$ such that
\begin{equation}\label{eq:theo:convergencelowerbound:proof}
\sigma(u_{i^*})<\delta\text{ and }\norm{\lt - \Phi_{u_{i^*}}}<\frac{\epsilon}{2}.
\end{equation}
Consider now any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then
\begin{equation*}
\norm{\lt - \Phi_u}\leq\norm{\lt-\Phi_{u_{i^*}}}
+\norm{\Phi_{u_{i^*}}-\Phi_u}
<\frac{\epsilon}{2}+2\delta C\norm{\lrate}^2\leq\epsilon,
\end{equation*}
where the strict inequality follows from Equation~\eqref{eq:theo:convergencelowerbound:proof} and Proposition~\ref{prop:differencebetweenu}.

Now assume \emph{ex absurdo} that $\lt$ is not unique, and let $\lt'$ be any other lower transition operator that satisfies Equation~\eqref{eq:theo:convergencelowerbound}. Then clearly, for any $\epsilon>0$, there is some $u\in\mathcal{U}_{[t,s]}$ such that $\norm{\lt-\Phi_u}<\epsilon$ and $\norm{\lt'-\Phi_u}<\epsilon$, and therefore also $\norm{\lt-\lt'}<2\epsilon$. Since $\epsilon>0$ is arbitrary, this implies that $\norm{\lt-\lt'}=0$, or equivalently, that $\lt=\lt'$, a contradiction.
% In summary, we have shown that there is lower transition operator $\lt$ such that
% \begin{equation*}
% (\forall\epsilon>0)\,
% (\exists\delta>0)\,
% (\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lt - \Phi_u}<\epsilon\,.
% \end{equation*}
\end{proof}

\begin{lemma}\label{lemma:limitboundonL}
Let $\lrate$ be a lower transition rate operator. Then for any $t,s\in\realsnonneg$ such that $t\leq s$ and any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)\norm{\lrate}\leq 1$:
\begin{equation*}
\norm{L_t^s-\Phi_{u}}\leq\sigma(u)(s-t)\norm{\lrate}^2.\vspace{5pt}
\end{equation*}
\end{lemma}
\begin{proof}
Fix any $\epsilon>0$. Because of Definition~\ref{def:low_trans}, there is some $u'\in\mathcal{U}_{[t,s]}$ such that $u\subseteq u'$ and $\norm{L_t^s-\Phi_{u'}}<\epsilon$. By combining this with Corollary~\ref{corol:differencebetweennestedintermsofu}, it follows that
\begin{equation*}
\norm{L_t^s-\Phi_{u}}
\leq
\norm{L_t^s-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u}}
<\epsilon+
\sigma(u)(s-t)\norm{\lrate}^2.
\end{equation*}
Since $\epsilon>0$ is arbitrary, the result is now immediate.
\end{proof}

\begin{lemma}\label{lemma:quadraticboundonL}
Let $\lrate$ be a lower transition rate operator. Then for any $t,\Delta\in\realsnonneg$:
\begin{equation*}
\norm{L_t^{t+\Delta}-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.\vspace{5pt}
\end{equation*}
\end{lemma}
\begin{proof}
Fix any $\epsilon>0$. Because of Definition~\ref{def:low_trans}, there is some $\smash{u\in\mathcal{U}_{[t,t+\Delta]}}$ such that $\sigma(u)\norm{\lrate}\leq1$ and $\norm{L_t^s-\Phi_{u}}<\epsilon$. By combining this with Lemma~\ref{lemma:justthelinearpart}, it follows that
\begin{equation*}
\norm{L_t^s-(I+\Delta\lrate)}
\leq
\norm{L_t^s-\Phi_{u}}
+
\norm{\Phi_{u}-(I+\Delta\lrate)}
<\epsilon+
\Delta^2\norm{\lrate}^2.
\end{equation*}
Since $\epsilon>0$ is arbitrary, the result is now immediate.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:lower_trans_system_is_system}]
Consider any $t\in\realsnonneg$. Then $\mathcal{U}_{[t,t]}$ contains only a single degenerate sequence of time points $u$, which consists of the single time point $t$, and for this sequence $u$, we trivially have that $\Phi_u=I$ and $\sigma(u)=0$. Therefore, it follows from Definition~\ref{def:low_trans} that $L_t^t=I$.

% *** STILL NEED TO CHECK THIS ***
% We start by showing that for all $t\in\realsnonneg$, it holds that $L_t^t=I$. To this end, consider any $t\in\realsnonneg$. Then, the set $\mathcal{U}_{[t,t]}$ of sequences of time points that partition the interval $[t,t]$ only contains degenerate sequences. That is, for all $u\in\mathcal{U}_{[t,t]}$, it holds that $t_0=t_1=\cdots=t_n=t$. Therefore, the corresponding differences $\Delta_1,\ldots,\Delta_n$ satisfy $\Delta_j=0$ for all $j\in\{1,\ldots,n\}$. Hence, $\lim_{i\to\infty}\sigma(u_i)=0$ trivially holds for any sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,t]}$. Consider now any such sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,t]}$. Then, for all $i\in\nats$, we have
% \begin{equation*}
% \Phi_{u_i} = \prod_{k=1}^n(I+\Delta_k\lrate) = \prod_{k=1}^n I = I\,.
% \end{equation*}
% Hence, $\lim_{i\to\infty}\Phi_{u_i}=I$, and by Theorem~\ref{theo:convergencelowerbound}, it now follows that $L_t^t=\lim_{i\to\infty}\Phi_{u_i}=I$.

Consider now any $t,r,s,\in\realsnonneg$ such that $t\leq r\leq s$. It remains to show that $L_t^s=L_t^rL_r^s$. If $t=r$ or $r=s$, this follows trivially from the first part of this proof. Hence, without loss of generality, we may assume that $t<r<s$.

Fix any $\delta>0$ such that $\delta\norm{\lrate}\leq1$. Since $r>t$, there is some $0<\Delta_1<\delta$ and $n_1\in\nats$ such that $\Delta_1 n_1=r-t$. Similarly, since $s>r$, there is some $0<\Delta_2<\delta$ and $n_2\in\nats$ such that $\Delta_2 n_2=s-r$. Furthermore, because of Propositions~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}, $(I+\Delta_1\lrate)^{n_1}$ and $(I+\Delta_1\lrate)^{n_1}$ are lower transition operators. Hence, we find that
\begin{align*}
&\norm{L_t^s-L_t^rL_r^s}
\leq
\norm{L_t^s-(I+\Delta_1\lrate)^{n_1}(I+\Delta_2\lrate)^{n_2}}
+
\norm{(I+\Delta_1\lrate)^{n_1}(I+\Delta_2\lrate)^{n_2}-L_t^rL_r^s}\\
&~~~~~~~~\leq
\norm{L_t^s-(I+\Delta_1\lrate)^{n_1}(I+\Delta_2\lrate)^{n_2}}
+
\norm{(I+\Delta_1\lrate)^{n_1}-L_t^r}
+
\norm{(I+\Delta_2\lrate)^{n_2}-L_r^s}\\
&~~~~~~~~\leq
\max\{\Delta_1,\Delta_2\}(s-t)\norm{\lrate}^2
+
\Delta_1(r-t)\norm{\lrate}^2
+
\Delta_2(s-r)\norm{\lrate}^2\\
&~~~~~~~~\leq
\delta(s-t)\norm{\lrate}^2
+
\delta(r-t)\norm{\lrate}^2
+
\delta(s-r)\norm{\lrate}^2
\leq
3\delta(s-t)\norm{\lrate}^2,
\end{align*}
where the second inequality follows from Lemma~\ref{lemma:recursive_lower_trans} and the third inequality follows from Lemma~\ref{lemma:limitboundonL}. Since $\delta>0$ can be chosen arbitrarily small, this implies that $\norm{L_t^s-L_t^rL_r^s}=0$, or equivalently, that $L_t^s=L_t^rL_r^s$.
% To this end, consider any $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,r]}$ and any $\{v_i\}_{i\in\nats}$ in $\mathcal{U}_{[r,s]}$, such that $\lim_{i\to\infty}\sigma(u_i)=0$ and $\lim_{i\to\infty}\sigma(v_i)=0$. Then, by Theorem~\ref{theo:convergencelowerbound}, $\lim_{i\to\infty}\Phi_{u_i}=L_t^r$ and $\lim_{i\to\infty}\Phi_{v_i}=L_r^s$. Denote $u_i=t_0^i,\ldots,t_{n_i}^i$, and $v_i=s_0^i,\ldots,s_{m_i}^i$. Let the differences be denoted as $\Delta_1^{u_i},\ldots,\Delta_{n_i}^{u_i}$ and $\Delta_1^{v_i},\ldots,\Delta_{m_i}^{v_i}$ for $u_i$ and $v_i$, respectively.
% For all $i\in\nats$, let now $w_i\in\mathcal{U}_{[t,s]}$ be such that $w_i=u_i\cup v_i$. Then, clearly, $w_i=t_0^i,\ldots,t_{n_i}^i,s_0^i,\ldots,s_{m_i}^i$, and because $t_{n_i}^i=r=s_0^i$, it holds that $\sigma(w_i)=\max\{\sigma(u_i),\sigma(v_i)\}$. Therefore, we find that $\lim_{i\to\infty}\sigma(w_i)=0$. Consider now the corresponding sequence $\{\Phi_{w_i}\}_{i\in\nats}$. For all $i\in\nats$, this satisfies
% \begin{equation*}
% \Phi_{w_i} = \prod_{k=1}^{n_i}(I+\Delta_{k}^{u_i}\lrate)\prod_{k=1}^{m_i}(I+\Delta_{k}^{v_i}\lrate) = \Phi_{u_i}\Phi_{v_i}\,.
% \end{equation*}
%We will now show that
%\begin{equation*}
%\lim_{i\to\infty}\Phi_{w_i} = \lim_{i\to\infty}\{\Phi_{u_i}\Phi_{v_i}\} = \lim_{i\to\infty}\{\Phi_{u_i}\}\lim_{i\to\infty}\{\Phi_{v_i}\}\,.
%\lim_{i\to\infty}\{\Phi_{u_i}\Phi_{v_i}\} = \lim_{i\to\infty}\{\Phi_{u_i}\}\lim_{i\to\infty}\{\Phi_{v_i}\}\,.
%\end{equation*}
%Due to Theorem~\ref{theo:convergencelowerbound}, this now implies that
%\begin{equation*}
%L_t^s = \lim_{i\to\infty}\Phi_{w_i} = \lim_{i\to\infty}\{\Phi_{u_i}\}\lim_{i\to\infty}\{\Phi_{v_i}\} = L_t^rL_r^s\,.
%\end{equation*}
% Furthermore, due to Theorem~\ref{theo:convergencelowerbound}, we have that $L_t^s = \lim_{i\to\infty}\Phi_{w_i}$.
% It remains to show that
% \begin{equation*}
% L_t^s = \lim_{i\to\infty}\Phi_{w_i} = L_t^rL_r^s\,.
% \end{equation*}
% Suppose \emph{ex absurdo} that this does not hold. Then,
% \begin{equation*}
% (\exists \epsilon\in\realspos)(\forall n\in\nats)(\exists i>n)\,:\,\norm{\Phi_{w_i} - L_t^rL_r^s} \geq \epsilon\,.
% \end{equation*}
% Consider this $\epsilon$. Because $\lim_{i\to\infty}\Phi_{u_i}=L_t^r$ and $\lim_{i\to\infty}\Phi_{v_i}=L_r^s$, we have that
% \begin{equation*}
% (\exists n_u\in\nats)(\forall i>n_u)\,:\,\norm{L_t^r - \Phi_{u_i}}<\frac{\epsilon}{2}\,,\quad\text{and,}\quad(\exists n_v\in\nats)(\forall i>n_v)\,:\,\norm{L_r^s - \Phi_{v_i}}<\frac{\epsilon}{2}\,.
% \end{equation*}
% Let now $n\coloneqq \max\{n_u,n_v\}$. Then, there is some $i>n$ such that
% \begin{equation*}
% \norm{\Phi_{w_i} - L_t^rL_r^s} \geq \epsilon\,.
% \end{equation*}
% Because $\Phi_{w_i}=\Phi_{u_i}\Phi_{v_i}$, and due to Lemma~\ref{lemma:recursive_lower_trans}, we find that
% \begin{align*}
% \epsilon &\leq \norm{\Phi_{u_i}\Phi_{v_i} - L_t^rL_r^s} \leq \norm{L_t^r - \Phi_{u_i}} + \norm{L_r^s - \Phi_{v_i}} < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\,,
% \end{align*}
% a contradiction. Hence, we find that indeed $L_t^s=L_t^rL_r^s$.
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:lower_transition_is_homogeneous}]
Consider any $t,s\in\realsnonneg$ such that $t\leq s$, any $\Delta\in\realsnonneg$, and any sequence $\{u_i\}_{i\in\nats}$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{i\to\infty}\sigma(u_i)=0$. %Then, by Theorem~\ref{theo:convergencelowerbound}, $\lim_{i\to\infty}\Phi_{u_i}=L_t^s$.
For any $i\in\nats$, with $u_i=t_0,\ldots,t_n$, we now define the sequence $u_i^*=(t_0+\Delta),(t_1+\Delta),\ldots,(t_n+\Delta)$. Then clearly, $\lim_{i\to\infty}\sigma(u_i^*)=0$ and, for all $i\in\nats$, $u_i^*\in\mathcal{U}_{[t+\Delta,s+\Delta]}$ and $\Phi_{u_i}=\Phi_{u_i^*}$.
% Then, for all $i\in\nats$, we have that $u_i^*\in\mathcal{U}_{[t+\Delta,s+\Delta]}$. Furthermore, because $\lim_{i\to\infty}\sigma(u_i)=0$, we have $\lim_{i\to\infty}\sigma(u_i^*)=0$. Therefore, and by Theorem~\ref{theo:convergencelowerbound},  it holds that $\lim_{i\to\infty}\Phi_{u_i^*}=L_{t+\Delta}^{s+\Delta}$.
% Consider now any $u_i=t_0,\ldots,t_n$, $i\in\nats$, with differences $\Delta_1,\ldots,\Delta_n$. Then, the corresponding sequence $u_i^*$ has differences $\Delta^*_j=(t_j+\Delta)-(t_{j-1}+\Delta)=\Delta_j$, for all $j\in\{1,\ldots,n\}$. Therefore, and by the definition of $\Phi_{u_i}$ and $\Phi_{u_i^*}$, we have that $\Phi_{u_i}=\Phi_{u_i^*}$. Hence, and because the $i\in\nats$ was arbitrary, we find that $\{\Phi_{u_i}\}_{i\in\nats}=\{\Phi_{u_i^*}\}_{i\in\nats}$. Because these sequences are identical, they have the same limit, and therefore
We now have that
\begin{equation*}
L_t^s=\lim_{i\to\infty}\Phi_{u_i}=\lim_{i\to\infty}\Phi_{u_i^*}=L_{t+\Delta}^{s+\Delta},
\end{equation*}
where the first and last equality follow from Theorem~\ref{theo:convergencelowerbound}.
\end{proof}

\begin{lemma}\label{lemma:linearboundonL}
Let $\lrate$ be a lower transition rate operator. Then for any $t,\Delta\in\realsnonneg$:
\begin{equation*}
\norm{L_t^{t+\Delta}-I}\leq\Delta\norm{\lrate}.\vspace{5pt}
\end{equation*}
\end{lemma}
\begin{proof}
Fix any $n\in\nats$ and let $\Delta_n\coloneqq\nicefrac{\Delta}{n}$. Propositions~\ref{prop:lower_trans_system_is_system} and~\ref{prop:lower_transition_is_homogeneous} then imply that
\begin{equation*}
L_t^{t+\Delta}
=L_0^{\Delta}
=L_0^{n\Delta_n}
=L_0^{\Delta_n}
L_{\Delta_n}^{2\Delta_n}
\cdots
L_{(n-2)\Delta_n}^{(n-1)\Delta_n}
L_{(n-1)\Delta_n}^{n\Delta_n}
=L_0^{\Delta_n}
L_0^{\Delta_n}
\cdots
L_0^{\Delta_n}
L_0^{\Delta_n}
=(L_0^{\Delta_n})^n,
\end{equation*}
and therefore, it follows from Lemma~\ref{lemma:recursive_lower_trans} that
\begin{equation*}
\norm{L_t^{t+\Delta}-I}
=
\norm{(L_0^{\Delta_n})^n-I^n}
%\leq\sum_{i=1}^n\norm{L_0^{\Delta_n}-I}
\leq n\norm{L_0^{\Delta_n}-I}
\leq n\Delta_n\norm{\lrate}+n\norm{L_0^{\Delta_n}-(I+\Delta_n\lrate)},
\end{equation*}
which, when combined with Lemma~\ref{lemma:quadraticboundonL}, implies that
\begin{equation*}
\norm{L_t^{t+\Delta}-I}
\leq
n\Delta_n\norm{\lrate}+n\Delta_n^2\norm{\lrate}^2
=\Delta\norm{\lrate}+\frac{1}{n}\Delta^2\norm{\lrate}^2.
\end{equation*}
Since $n\in\nats$ is arbitrary, the result is now immediate.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:lower_transition_has_deriv}]
We first prove that $\frac{\partial}{\partial t}L_t^s=-\lrate L_t^s$, or equivalently, that
\begin{equation}\label{eq:lower_deriv_backward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert <\delta,\,0\leq t+\Delta\leq s)~
\norm{\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s}<\epsilon.
\end{equation}
%We start by proving Equation~\eqref{eq:lower_deriv_backward}.
Fix any $\epsilon\in\realspos$, and choose any $\delta>0$ such that $3\delta\norm{\lrate}^2<\epsilon$. Consider now any $\Delta\in\reals$ such that $0<\abs{\Delta}<\delta$ and $0\leq t+\Delta\leq s$.
% \begin{equation}\label{eq:derivative_max_delta}
% \delta \coloneqq \min\left\{\frac{\epsilon}{5\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
% \end{equation}
% Then, consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
% \begin{equation*}
% \norm{\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s}<\epsilon\,.
% \end{equation*}
We will show that the inequality in Equation~\eqref{eq:lower_deriv_backward} holds.
Let $t'\coloneqq\max\{t,t+\Delta\}$. We then find that
\begin{align*}
\norm{L_{t+\Delta}^s-L_t^s+\Delta\lrate L_t^s}
=\norm{L_{t'}^s-L_{t'-\abs{\Delta}}^s+\abs{\Delta}\lrate L_{t}^s}
=\norm{L_{t'}^s-L_{t'-\abs{\Delta}}^{t'}L_{t'}^s+\abs{\Delta}\lrate L_{t}^{t'}L_{t'}^s},
\end{align*}
where the last equality follows from Proposition~\ref{prop:lower_trans_system_is_system} and because $t\leq t'\leq s$. Therefore, and because of \ref{N:normAB} and~\ref{LT:norm_at_most_one}, we find that
\begin{align*}
\norm{L_{t+\Delta}^s-L_t^s+\Delta\lrate L_t^s}
\leq\norm{I-L_{t'-\abs{\Delta}}^{t'}+\abs{\Delta}\lrate L_{t}^{t'}}\norm{L_{t'}^s}
\leq\norm{I-L_{t'-\abs{\Delta}}^{t'}+\abs{\Delta}\lrate L_{t}^{t'}},
\end{align*}
which implies that
\begin{align*}
\norm{L_{t+\Delta}^s-L_t^s+\Delta\lrate L_t^s}
&\leq\norm{I+\abs{\Delta}\lrate-L_{t'-\abs{\Delta}}^{t'}}
+
\abs{\Delta}\norm{\lrate L_{t}^{t'}-\lrate}\\
&\leq
\Delta^2\norm{\lrate}^2+\abs{\Delta}2\norm{\lrate}\norm{L_{t}^{t'}-I}\leq\Delta^2\norm{\lrate}^2+2\abs{\Delta}(t'-t)\norm{\lrate}^2,
\end{align*}
using Lemma~\ref{lemma:quadraticboundonL} for the second inequality and Lemma~\ref{lemma:linearboundonL} for the third inequality. Hence, since $0\leq(t'-t)\leq\abs{\Delta}$, we find that $\norm{L_{t+\Delta}^s-L_t^s+\Delta\lrate L_t^s}\leq3\Delta^2\norm{\lrate}^2$, which implies that indeed, as required,
\begin{equation*}
\norm{\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s}
=\frac{1}{\abs{\Delta}}\norm{L_{t+\Delta}^s-L_t^s+\Delta\lrate L_t^s}
\leq3\abs{\Delta}\norm{\lrate}^2
\leq3\delta\norm{\lrate}^2
<\epsilon.
\end{equation*}
% \begin{equation*}
% \norm{\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s} = \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s}\,.
% \end{equation*}
% Now, using Propositions~\ref{prop:lower_trans_system_is_system} and~\ref{prop:lower_transition_is_homogeneous}, we find that
% \begin{align*}
% \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &= \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^{t'}L_{t'}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}L_{t'}^s} \\
%  &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}}\norm{L_{t'}^s} \\
%  &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}} \\
%  &= \norm{\frac{I - L_{0}^{\lvert\Delta\rvert}}{\lvert\Delta\rvert}+\lrate L_{0}^{\Delta'}} \\
%  &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate L_{0}^{\Delta'}} \\
%  &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate} + \norm{\lrate - \lrate L_{0}^{\Delta'}} \\
%  &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + 2\norm{\lrate}\norm{I - L_{0}^{\Delta'}}\,.
% % &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2.
% \end{align*}
% Next, using Lemma~\ref{lemma:justthelinearpart} together with Theorem~\ref{theo:convergencelowerbound}, we find that
% \begin{equation*}
% \norm{L_0^{\abs{\Delta}}-(I+\abs{\Delta}\lrate)} \leq \abs{\Delta}^2\norm{\lrate}^2\,.
% \end{equation*}
% Furthermore, again using Lemma~\ref{lemma:justthelinearpart} together with Theorem~\ref{theo:convergencelowerbound}, and also the fact that $\Delta'\leq\abs{\Delta}<\delta\leq\nicefrac{1}{\norm{\lrate}}$, we find
% \begin{align*}
% \norm{I - L_0^{\Delta'}} &\leq \norm{I - (I+\Delta'\lrate)} + \norm{(I+\Delta'\lrate) - L_0^{\Delta'}} \\
%  &\leq \Delta'\norm{\lrate} + (\Delta')^2\norm{\lrate}^2 \\
%  &< \Delta'\norm{\lrate} + \Delta'\delta\norm{\lrate}^2 \\
%  &\leq \Delta'\norm{\lrate} + \Delta'\norm{\lrate} = 2\Delta'\norm{\lrate}\,.
% \end{align*}
% Again using $\Delta'\leq\abs{\Delta}$, we therefore find that
% \begin{align*}
% \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &\leq 
% \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + 2\norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
%  &< \frac{1}{\abs{\Delta}}\abs{\Delta}^2\norm{\lrate}^2 + 2\norm{\lrate}(2\Delta'\norm{\lrate}) \\
%  &= \abs{\Delta}\norm{\lrate}^2 + 4\Delta'\norm{\lrate}^2 \\
%  &\leq \abs{\Delta}\norm{\lrate}^2 + 4\abs{\Delta}\norm{\lrate}^2 \\
%  &= 5\abs{\Delta}\norm{\lrate}^2 \\
%  &< 5\delta\norm{\lrate}^2 \leq \epsilon\,,
% %\lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2 \\
% % &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
% % &= 3\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
% % &< 3\delta\norm{\lrate}^2 = \epsilon\,,
% \end{align*}
% where the second-to-last inequality used $\abs{\Delta}<\delta$, and the final step used Equation~\eqref{eq:derivative_max_delta}. This concludes the proof of Equation~\eqref{eq:lower_deriv_backward}. 

Next, we prove that $\frac{\partial}{\partial s}L_t^s=\lrate L_t^s$, or equivalently, that
\begin{equation}\label{eq:lower_deriv_forward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert<\delta,\,t\leq s+\Delta)~
\norm{\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lrate\lbound_t^s }<\epsilon.
\end{equation}
Fix any $\epsilon>0$ and $\alpha>0$ and let $t^*\coloneqq t+\alpha$ and $s^*\coloneqq s+\alpha$. It then follows from Equation~\eqref{eq:lower_deriv_backward} that there is some $\delta^*>0$ such that
\begin{equation}\label{eq:lower_deriv_backward:star}
(\forall\Delta^*\,:\,0<\lvert\Delta^*\rvert <\delta^*,\,0\leq t^*+\Delta^*\leq s^*)~
\norm{\frac{L_{t^*+\Delta^*}^{s^*}-L_{t^*}^{s^*}}{\Delta^*}+\lrate L_{t^*}^{s^*}}<\epsilon.
\end{equation}
Let $\delta\coloneqq\min\{\delta^*,\alpha\}$. Consider now any $\Delta\in\reals$ such that $0<\abs{\Delta}<\delta$ and $t\leq s+\Delta$. We will prove that the inequality in Equation~\eqref{eq:lower_deriv_forward} holds. Let $\Delta^*\coloneqq-\Delta$. We then have that $0<\abs{\Delta^*}<\delta\leq\delta^*$ and that $t^*+\Delta^*=t+\alpha-\Delta\geq t+\alpha-\delta\geq t\geq 0$ and $t^*+\Delta^*=t+\alpha-\Delta\leq s+\Delta+\alpha-\Delta=s^*$, and therefore, we find that indeed, as required,
\begin{align*}
\norm{\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lrate\lbound_t^s}
= \norm{\frac{L_{t^*+\Delta^*}^{s^*}-L_{t^*}^{s^*}}{-\Delta^*}-\lrate\lbound_{t^*}^{s^*}}
= \norm{\frac{L_{t^*+\Delta^*}^{s^*}-L_{t^*}^{s^*}}{\Delta^*}+\lrate\lbound_{t^*}^{s^*}}<\epsilon,
\end{align*}
where the first equality follows from Proposition~\ref{prop:lower_transition_is_homogeneous} and the final inequality follows from Equation~\eqref{eq:lower_deriv_backward:star}.
%Again choose any $\epsilon\in\realspos$, and let $\delta$ be given by
%\begin{equation*}
%\delta \coloneqq \frac{\epsilon}{2\norm{\lrate}^2}\,.
%\end{equation*}
%Consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
%\begin{equation*}
%\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon\,.
%\end{equation*}
%We again first rewrite the statement to prevent having to perform case-work in the sign of $\Delta$. Let $s'\coloneqq\min\{s,s+\Delta\}$, and let $\Delta'\coloneqq s-s'$. Then,
%\begin{equation*}
%\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert = \norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate}\,.
%\end{equation*}
%Now,
%\begin{align*}
%\norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate} &= \norm{\frac{L_t^{s'}L_{s'}^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'}L_{s'}^{s'+\Delta'}\lrate} \\
% &\leq \norm{L_t^{s'}}\norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
% &\leq \norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
% &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{0}^{\Delta'}\lrate} \\
% &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - \lrate} + \norm{\lrate - L_{0}^{\Delta'}\lrate} \\
% &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + \norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
% &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \Delta'\norm{\lrate}^2 \\
% &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
% &= 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
% &< 2\delta\norm{\lrate}^2 \\
% &= \epsilon\,.
%\end{align*}
%
%It remains to show that $L_t^s\lrate=\lrate L_t^s$. To this end, consider any $\epsilon\in\realspos$. Then, it follows from Equations~\eqref{eq:lower_deriv_backward} and~\eqref{eq:lower_deriv_forward} that there exist $\delta_-,\delta_+\in\realspos$, such that
%\begin{align*}
%(\forall\Delta:0<\abs{\Delta}<\delta_-)&\,\norm{\frac{L_{t+\Delta}^s - L_t^s}{\Delta} + \lrate L_t^s} < \frac{\epsilon}{2}\,, \\
%&\text{and,}\\
%(\forall\Delta:0<\abs{\Delta}<\delta_+)&\,\norm{\frac{L_{t}^{s+\Delta} - L_t^s}{\Delta} - L_t^s\lrate} < \frac{\epsilon}{2}\,.
%\end{align*}
%Consider now any $\Delta\in\realspos$ such that $\Delta<\min\{\delta_-,\delta_+\}$. Then,
%\begin{align*}
% &\quad \norm{\lrate L_t^s - L_t^s\lrate} \\
% &\leq \norm{\lrate L_t^s - \frac{L_{t-\Delta}^s - L_t^s}{\Delta}} + \norm{\frac{L_{t-\Delta}^s - L_t^s}{\Delta} - \frac{L_{t}^{s+\Delta} - L_t^s}{\Delta}} + \norm{\frac{L_{t}^{s+\Delta} - L_t^s}{\Delta} - L_t^s\lrate} \\
% &= \norm{\lrate L_t^s + \frac{L_{t-\Delta}^s - L_t^s}{-\Delta}} + \norm{\frac{L_{t-\Delta}^s - L_t^s}{\Delta} - \frac{L_{t}^{s+\Delta} - L_t^s}{\Delta}} + \norm{\frac{L_{t}^{s+\Delta} - L_t^s}{\Delta} - L_t^s\lrate} \\
% &< \frac{\epsilon}{2} + \norm{\frac{L_{t-\Delta}^s - L_t^s}{\Delta} - \frac{L_{t}^{s+\Delta} - L_t^s}{\Delta}} + \frac{\epsilon}{2} \\
% &= \epsilon\,,
%\end{align*}
%where in the last step we applied $L_{t-\Delta}^s=L_t^{s+\Delta}$, by Proposition~\ref{prop:lower_transition_is_homogeneous}. Hence, we have found that $\norm{\lrate L_t^s - L_t^s\lrate}<\epsilon$. Because this holds for any $\epsilon\in\realspos$, we conclude that indeed $\lrate L_t^s=L_t^s\lrate$.
\end{proof}

\section{Proofs and Lemmas for Section~\ref{sec:connections}}

\begin{proof}[Proof of Proposition~\ref{prop:lowerenvelopeislowertrans}]
Consider any $Q\in\rateset$. It then follows from Definition~\ref{def:rate_matrix} that the matrix $Q$, when regarded as a map from $\gamblesX$ to $\gamblesX$, satisfies \ref{LR:constantzero}--\ref{LR:homo}. Since each of these properties is preserved under taking lower envelopes, it follows that $\lrate$ satisfies \ref{LR:constantzero}--\ref{LR:homo}, which means that $\lrate$ is a lower transition rate operator.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:dominating_nonempty_bounded}]
Fix any $f\in\gamblesX$. Choose $\Delta>0$ small enough such that $0\leq\Delta\norm{\lrate}\leq 1$ (this always possible because of property~\ref{LR:normlratefinite}). Define $\lt\coloneqq I+\Delta\lrate$. Since $\lrate$ is a lower transition rate operator, it follows from Proposition~\ref{lemma:normQsmallenough} that $\lt$ is a lower transition operator. For any $x\in\states$, we now consider the operator $\lt_x\colon\gamblesX\to\reals$, as defined by Equation~\eqref{eq:lowerprevisionfromlt}, which is a coherent lower prevision. Because of \cite[Theorem~3.3.3(b)]{Walley:1991vk}, this implies the existence of an expectation operator $\mathbb{E}_x$ on $\gamblesX$---Reference~\cite{Walley:1991vk} calls this a linear prevision on $\gamblesX$---such that $\mathbb{E}_xg\geq\lt_xg$ for all $g\in\gamblesX$ and $\mathbb{E}_xf=\lt_xf$. Let $P_x$ be the unique probability mass function that corresponds to $\mathbb{E}_x$. For all $x,y\in\states$, we now let $T(x,y)\coloneqq P_x(y)\coloneqq \mathbb{E}_x(\ind{y})$. Then $T$ is clearly a transition matrix. Furthermore, for every $x\in\states$ and $g\in\gamblesX$, we have that $(Tg)(x)=\mathbb{E}_xg$. Hence, it follows that $Tg\geq\lt g$ for all $g\in\gamblesX$ and that $Tf=\lt f$. Now let $Q\coloneqq\nicefrac{1}{\Delta}(T-I)$, which, because of Proposition~\ref{prop:rate_from_stochastic_matrix}, is a rate matrix. Since $Tf=\lt f$, it then follows that
\begin{equation*}
Qf=\frac{1}{\Delta}(Tf-f)\geq\frac{1}{\Delta}(\lt f-f)=\lrate f.
\end{equation*}
Similarly, since $Tg\geq\lt g$ for all $g\in\gamblesX$, it follows that $Qg\geq\lrate g$, or equivalently, since $Q$ is a rate matrix, that $Q\in\rateset_{\lrate}$. Since $f$ was arbitrary, this proves that, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $Qf=\lrate f$. Since $\gamblesX$ is non-empty, this clearly implies that $\rateset_{\lrate}$ is non-empty.

We end this proof by showing that $\rateset_{\lrate}$ is bounded. Consider any $x\in\states$. Then for all $Q\in\rateset_{\lrate}$, we have that $Q(x,x)=(Q\ind{x})(x)\geq(\lrate\ind{x})(x)$, which implies that
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset_{\lrate}\right\}\geq(\lrate\ind{x})(x)>-\infty.
\end{equation*}
Since $x\in\states$ is arbitary, Proposition~\ref{prop:alternativedefforbounded} now guarantees that $\rateset_{\lrate}$ is bounded. 
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:dominatingproperties}]
We start by showing that $\rateset_{\lrate}$ is closed, or equivalently, that for any converging sequence $\{Q_i\}_{i\in\nats}$ in $\rateset_{\lrate}$, the limit $Q\coloneqq\lim_{i\to+\infty}Q_i$ is again an element of $\rateset_{\lrate}$. Since $\{Q_i\}_{i\in\nats}$ belongs to the bounded set of rate matrices $\rateset_{\lrate}$---see Proposition~\ref{prop:dominating_nonempty_bounded}---we know that the limit $Q$ is again a rate matrix. Now, assume \emph{ex absurdo} that $Q\notin\rateset_{\lrate}$. Then, by Equation~\eqref{eq:dominatingratematrices}, there is some $f\in\gamblesX$ and some $x\in\states$ such that $\smash{\left[Qf\right](x) < \left[\lrate f\right](x)}$, and therefore some $\epsilon\in\realspos$ such that $[Qf](x) + \epsilon < [\lrate f](x)$. Hence, since $\lim_{i\to+\infty}Q_i=Q$, there is some $i^*\in\nats$ such that $[Q_{i^*}f](x) <[Qf](x) + \epsilon<[\lrate f](x)$. Since $Q_{i^*}\in\rateset_{\lrate}$, this is a contradiction. Therefore, $Q\in\rateset_{\lrate}$, and because the converging sequence $\{Q_i\}_{i\in\nats}$ was arbitrary, this proves that $\rateset_{\lrate}$ is closed.

Next, we show that $\rateset_{\lrate}$ is convex, or equivalently, that for any two rate matrices $Q_1,Q_2\in\rateset_{\lrate}$, and any $\lambda\in[0,1]$, the matrix $Q_\lambda\coloneqq\lambda Q_1 + (1-\lambda)Q_2$ is again an element of $\rateset_{\lrate}$. It is easily verified from Definition~\ref{def:rate_matrix} that $Q_\lambda$ is a rate matrix. Furthermore, for any $f\in\gamblesX$, we find that
\begin{equation*}
Q_\lambda f=\lambda Q_1f+(1-\lambda)Q_2f\geq\lambda \lrate f+(1-\lambda)\lrate f=\lrate f,
\end{equation*}
where the inequality holds because $Q_1$ and $Q_2$ belong to $\rateset_{\lrate}$. Hence, it follows from Equation~\ref{eq:dominatingratematrices} that $Q_\lambda\in\rateset_{\lrate}$.

We finally show that $\rateset_{\lrate}$ has separately specified rows. For all $x\in\states$, let $\rateset_x\coloneqq\{Q(x,\cdot)\,:\,Q\in\rateset_{\lrate}\}$. Consider now any rate matrix $Q$ such that, for all $x\in\states$, $Q(x,\cdot)\in\rateset_x$ and assume \emph{ex absurdo} that $\smash{Q\notin\rateset_{\lrate}}$. Equation~\eqref{eq:dominatingratematrices} then implies the existence of some $f\in\gamblesX$ and $x\in\states$ such that $\smash{\left[Qf\right](x) < \left[\lrate f\right](x)}$. Since $Q(x,\cdot)\in\rateset_x$, this in turn implies that there is some $Q'\in\rateset_{\lrate}$ such that $\left[Q'f\right](x) < \left[\lrate f\right](x)$, a contradiction. Hence we find that $Q\in\rateset_{\lrate}$.
\end{proof}

\begin{lemma}\label{lemma:rows_nonempty_bounded_closed_convex}
Let $\rateset$ be a non-empty, bounded, closed and convex set of rate matrices. Then, for all $x\in\states$, $\rateset_x\coloneqq\{Q(x,\cdot)\,:\,Q\in\rateset\}$ is a non-empty, bounded, closed, and convex subset of $\gamblesX$.
\end{lemma}
\begin{proof}
Fix any $x\in\states$. The non-emptiness, boundedness and convexity of $\rateset_x$ then follows trivially from the fact that $\rateset$ is non-empty, bounded and convex. It remains to show that $\rateset_x$ is closed. Consider therefore any sequence $\{Q_i\}_{i\in\nats}$ in $\rateset$ such that $\{Q_i(x,\cdot)\}_{i\in\nats}$ converges to a limit $Q_{\infty}(x,\cdot)$. We need to prove that $Q_{\infty}(x,\cdot)\in\rateset_x$.

Since $\{Q_i\}_{i\in\nats}$ belongs to the---closed and bounded and therefore---compact set $\rateset$, it has a convergent subsequence $\{Q_{i_k}\}_{k\in\nats}$ whose limit $Q^*$ belongs to $\rateset$. Since $\{Q_i(x,\cdot)\}_{i\in\nats}$ converges to $Q_{\infty}(x,\cdot)$, it follows that $Q^*(x,\cdot)=Q_{\infty}(x,\cdot)$, which, since $Q^*\in\rateset$, implies that $Q_{\infty}(x,\cdot)\in\rateset_x$.
% First, for any $y\in\states$ such that $y\neq x$, choose some $Q(y,\cdot)\in\rateset_{y}$. Next, for any $i\in\nats$, let $\hat{Q}_i$ be the unique rate matrix such that $\hat{Q}_i(x,\cdot)=Q_i(x,\cdot)$ and, for all $y\in\states$ such that $y\neq x$, $\hat{Q}_i(y,\cdot)=Q(y,\cdot)$. Since $\rateset$ has separately specified rows, the sequence $\{\hat{Q}_i\}_{i\in\nats}$ belongs to $\rateset$, and therefore, since $\rateset$ is closed, 
%  Since $\rateset$ has separately specified rows, and because $\hat{Q}_i(y,\cdot)\in\rateset_y$ for all $y\in\states$ and all $i\in\nats$, we then have that $\hat{Q}_i\in\rateset$, for all $i\in\nats$.
% Furthermore, in the sequence $\{\hat{Q}_i\}_{i\in\nats}$, only the $x$-th row is changing. Therefore, because $\{Q_i(x,\cdot)\}_{i\in\nats}$ is converging, so is $\{\hat{Q}_i\}_{i\in\nats}$. In particular, $\{\hat{Q}_i(x,\cdot)\}_{i\in\nats}\to Q^*(x,\cdot)$. Let $Q^*\coloneqq\lim_{i\to+\infty}\hat{Q}_i$. Because $\rateset$ is closed, and $\{\hat{Q}_i\}_{i\in\nats}$ is in $\rateset$, it follows that $Q^*\in\rateset$. Therefore, we find that $Q^*(x,\cdot)\in\rateset_x$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:dominating_unique_characterization}]
Since $\rateset$ has $\lrate$ as its lower envelope, it follows from Equation~\eqref{eq:dominatingratematrices} that $\rateset\subseteq\rateset_{\lrate}$. Assume \emph{ex absurdo} that $\rateset\subset\rateset_{\lrate}$, or equivalently, that there is some $Q\in\rateset_{\lrate}$ such that $Q\notin\rateset$. Consider any such $Q$.

Because $\rateset$ has separately specified rows, it follows from $Q\notin\rateset$ that there is some $x\in\states$ such that $Q(x,\cdot)\notin\rateset_x\coloneqq\{Q'(x,\cdot)\,:\,Q'\in\rateset\}$. Since $\rateset$ is non-empty, bounded, closed and convex, it then follows from Lemma~\ref{lemma:rows_nonempty_bounded_closed_convex} that $\rateset_x$ is a non-empty, bounded, closed and convex subset of $\gamblesX$, and therefore, since $\gamblesX$ is a normed linear space, it follows from the separating hyperplane theorem~\cite[Chapter 14, Corollary 25]{Royden:2010vn} that there is a linear operator $\psi\colon\gamblesX\to\reals$ such that
\begin{equation}\label{eq:prop:dominating_unique_characterization:C}
\psi(Q(x,\cdot))<\inf\{\psi(q)\colon q\in\rateset_x\}=\colon C.
\end{equation}
Now let $f\in\gamblesX$ be defined by $f(y)\coloneqq\psi(\ind{y})$ for all $y\in\states$. Then for any $\epsilon>0$, since $\rateset$ has $\lrate$ as its lower envelope, and because of Equation~\eqref{eq:correspondinglowertrans}, there is some $Q^*\in\rateset$ such that $[Q^*f](x)\leq[\lrate f](x)+\epsilon$, and therefore also
\begin{equation*}
C
\leq\psi(Q^*(x,\cdot))
%=\psi\left(\sum_{y\in\states}Q_q(x,y)\ind{y}\right)
=\sum_{y\in\states}Q^*(x,y)\psi(\ind{y})
=\sum_{y\in\states}Q^*(x,y)f(y)
=[Q^* f](x)\leq[\lrate f](x)+\epsilon.
\end{equation*}
Since $\epsilon>0$ is arbitrary, this implies that $C\leq[\lrate f](x)$ and, by combining this with Equation~\eqref{eq:prop:dominating_unique_characterization:C}, it follows that
\begin{equation*}
[Qf](x)
=\sum_{y\in\states}Q(x,y)f(y)
=\sum_{y\in\states}Q(x,y)\psi(\ind{y})
=\psi(Q(x,\cdot))
<C\leq[\lrate f](x).
\end{equation*}
This implies that $Qf\not\geq\lrate f$, and therefore, that $Q\notin\rateset_{\lrate}$, a contradiction.
% Note that we can interpret $\rateset_x\subset\reals^m$ as a subset of the vector space $\reals^m$, where $m$ is the size of $\states$. Furthermore, because $\rateset_x$ is closed and bounded, we have that $\rateset_x$ is compact. Because $Q(x,\cdot)\notin\rateset_x$, the (non-empty, closed, convex) singleton set $\{Q(x,\cdot)\}$ is clearly disjoint from $\rateset_x$.
% Therefore, by the hyperplane separation theorem\footnote{That is, by one of them.} ({\bf REF}), there must be some $f\in\gamblesX$ and some $c_1,c_2\in\reals$, with $c_1<c_2$, such that $Q(x,\cdot)f = [Qf](x) \leq c_1$, and such that, for all $Q'(x,\cdot)\in\rateset_x$, $Q'(x,\cdot)f = [Q'f](x) \geq c_2$.
% This clearly implies that, for all $Q'\in\rateset$, it holds that $[Qf](x) \leq c_1 < c_2 \leq [Q'f](x)$. In turn, this implies the existence of some $\epsilon\in\realspos$ for which $[Qf](x) + \epsilon < [Q'f](x)$, for all $Q'\in\rateset$. Because $\lrate$ is the lower envelope of $\rateset$, by Equation~\eqref{eq:correspondinglowertrans}, we therefore find that $[Qf](x) < [\lrate f](x)$. Hence, we have found a $f\in\gamblesX$ and a $x\in\states$, such that for some $Q\in\rateset_{\lrate}$, it holds that $[Qf](x) < [\lrate f](x)$. By Equation~\eqref{eq:dominatingratematrices}, this is a contradiction, and hence $\rateset=\rateset_{\lrate}$.
\end{proof}

\begin{lemma}\label{lemma:rateset_has_arginf}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$ and $\epsilon\in\realspos$, there exists a $Q\in\rateset$ such that
\begin{equation*}
\norm{\lrate f - Qf} < \epsilon\,.
\end{equation*}
\end{lemma}
\begin{proof}
This is immediate from the definition of the lower envelope of $\rateset$, as given by Equation~\eqref{eq:correspondinglowertrans}, and the fact that $\rateset$ has separately specified rows.
\end{proof}

\begin{lemma}\label{lemma:weirddecomposition}
Consider any $P\in\wprocesses_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{<t}$ and $x_u\in\states^u$ and any sequence $t=t_0<t_1<\cdots<t_n=s$, with $n\in\nats$.
Then for any $f\in\gamblesX$ and $x_t\in\states$:
\begin{equation*}
[T_{t,\,x_u}^sf](x_t)
=\Big[T_{t_0,\,x_u}^{t_1}\Bigg(\prod_{i=2}^{n}T_{t_{i-1},\,x_{u\cup\{t\}}}^{t_{i}}\Bigg)f\Big](x_t)
\end{equation*}
\end{lemma}
\begin{proof}
We provide a proof by induction. For $n=1$, the result holds trivially. So consider now any $n>1$ and assume that the result is true for $n-1$.
For any $g\in\gamblesX$, we then find that
\begin{align*}
[T_{t_0,\,x_u}^{t_2}g](x_t)
%=[T_{t,\,x_u}^{t_2}g](x_t)
&=\mathbb{E}(g(X_{t_2})\vert\,X_{t}=x_t,X_u=x_u)\\
&=\sum_{x_{t_1}\in\states}\mathbb{E}(g(X_{t_2})\vert\,X_{t_1}=x_{t_1},X_{t}=x_t,X_u=x_u)P(X_{t_1}=x_{t_1}\vert\,X_{t}=x_t,X_u=x_u)\\[-8pt]
&\quad\quad\quad\quad\quad=\sum_{x_{t_1}\in\states}
[T_{t_1,\,x_{u\cup\{t\}}}^{t_{2}}g](x_{t_1})P(X_{t_1}=x_{t_1}\vert\,X_{t}=x_t,X_u=x_u)\\[-6pt]
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad=[T_{t_0,\,x_u}^{t_1}
T_{t_1,\,x_{u\cup\{t\}}}^{t_{2}}g](x_t),
\end{align*}
and therefore, it follows that
\begin{equation*}
[T_{t,\,x_u}^sf](x_t)
=\Big[T_{t_0,\,x_u}^{t_2}\Bigg(\prod_{i=3}^{n}T_{t_{i-1},\,x_{u\cup\{t\}}}^{t_{i}}\Bigg)f\Big](x_t)
=\Big[T_{t_0,\,x_u}^{t_1}
T_{t_1,\,x_{u\cup\{t\}}}^{t_{2}}
\Bigg(\prod_{i=3}^{n}T_{t_{i-1},\,x_{u\cup\{t\}}}^{t_{i}}\Bigg)f\Big](x_t),
\end{equation*}
using the induction hypothesis for the first equality.
\end{proof}

\begin{lemma}\label{lemma:productofQsdominatesproductoflrates}
Consider a non-empty bounded set $\rateset$ of rate matrices and let $\lrate$ be the corresponding lower transition rate operator. Fix any $n\in\nats$ and, for all $i\in\{1,\dots,n\}$, consider some $\Delta_i\geq0$ and $Q_i\in\rateset$. Then for any $f\in\gamblesX$:
\begin{equation*}
\prod_{i=1}^n(I+\Delta_iQ_i)f
\geq
\prod_{i=1}^n(I+\Delta_i\lrate)f.
\end{equation*}
\end{lemma}
\begin{proof}
We provide a proof by induction. For $n=1$, the result follows trivially from Equation~\eqref{eq:correspondinglowertrans}. Consider now any $n>1$ and assume that the result is true for $n-1$. We then find that
\begin{equation*}
\prod_{i=1}^n(I+\Delta_iQ_i)f
% =
% (I+\Delta_1Q_1)\prod_{i=2}^n(I+\Delta_iQ_i)f
\geq
(I+\Delta_1Q_1)\prod_{i=2}^n(I+\Delta_i\lrate)f
\geq
\prod_{i=1}^n(I+\Delta_i\lrate)f,
\end{equation*}
where the first inequality follows from the induction hypothesis and the second inequality follows from Equation~\eqref{eq:correspondinglowertrans}.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded}]
%Consider any $P\in\wprocesses_\rateset$, any $t,s\in\realsnonneg$ such that $t\leq s$, any $u\in\mathcal{U}_{<t}$, any $x\in\states$ and $x_u\in\states^u$, and any $f\in\gamblesX$. 
This result is trivial if $t=s$. Hence, without loss of generality, we may assume that $t<s$. Fix any $\epsilon>0$ and let $C\coloneqq(s-t)$. Choose any $\epsilon_1>0$ such that $\epsilon_1\norm{f}<\nicefrac{\epsilon}{2}$ and any $\epsilon_2>0$ such that $\epsilon_2 C\norm{f}<\nicefrac{\epsilon}{2}$.

Due to Theorem~\ref{theo:convergencelowerbound}, there is some $\delta\in\realspos$ such that $\delta\norm{\lrate}\leq1$ and
\begin{equation}\label{eq:theorem:nonmarkov_single_var_lower_bounded}
(\forall v\in\mathcal{U}_{[t,s]}\,:\,\sigma(v)<\delta) \norm{L_{t}^s - \Phi_v} < \epsilon_1,
\end{equation}
with $\Phi_v$ as in Equation~\eqref{eq:aux_lower_trans}.
%, which implies that for all $v\in\mathcal{U}_{[t,s]}$ with $\sigma(v)<\delta$,
% \begin{equation}\label{eq:lowerbound_proof_linear_approx_lbound}
% \left[L_{t}^sf\right](x) - \frac{\epsilon}{2} < \left[\Phi_vf\right](x)\,.
% \end{equation}
Since $\smash{P\in\wprocesses_\rateset}$, it now follows from Proposition~\ref{prop:outerderivativebehaveslikelimit} that there is some $0<\Delta_1<\min\{\delta,C\}$ such that
\begin{equation*}
(\exists Q_1\in\overline{\partial}_+T_{t,\,x_u}^{t+\Delta_1}\subseteq\rateset)~
\norm{T_{t_0,\,x_u}^{t_1} - (I+\Delta_1 Q_1)}
=
\norm{T_{t,\,x_u}^{t+\Delta_1} - (I+\Delta_1 Q_1)} < \Delta_1\epsilon_2,
\end{equation*}
with $t_0\coloneqq t$ and $t_1\coloneqq t+\Delta_1$.
% Choose any $\Delta_0$ such that $\Delta_0<\min\{\delta,\delta'\}$. This implies that for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$, we have that
% \begin{equation*}
% \norm{T_{t,x_u}^{t+\Delta_0}g - (I+\Delta_0 Q_0)g} \leq \norm{T_{t,x_u}^{t+\Delta_0} - (I+\Delta_0 Q_0)}\norm{g} < \frac{\Delta_0\epsilon}{2C\norm{f}}\norm{g} \leq \frac{\Delta_0\epsilon}{2C}\,,
% \end{equation*}
% which in turn implies that for all $y\in\states$,
% \begin{equation*}
% \left[(I+\Delta_0 Q_0)g\right](y) - \frac{\Delta_0\epsilon}{2C} < \left[T_{t,x_u}^{t+\Delta_0}g\right](y)\,.
% \end{equation*}
% Because $Q_0\in\rateset$, we find using Equation~\eqref{eq:correspondinglowertrans} that $\lrate g\leq Q_0g$, and hence that for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$, and all $y\in\states$,
% \begin{equation}\label{eq:lowerbound_proof_linear_approx_last_part}
% \left[(I+\Delta_0 \lrate)g\right](y) - \frac{\Delta_0\epsilon}{2C} < \left[T_{t,x_u}^{t+\Delta_0}g\right](y)\,.
% \end{equation}
%Next, we focus on the remaining interval $[t+\Delta_0,s]$. To this end, we fix the history $(X_t=x,X_u=x_u)$ by defining $x_{u\cup\{t\}}\in\states^{u\cup\{t\}}$ as $x_{u\cup\{t\}}\coloneqq (x_u,x)$.
Furthermore, since $P\in\wprocesses_\rateset$, and because $\Delta_1<C$ implies that $t+\Delta_1<s$, it follows from Lemma~\ref{lemma:bound_on_linear_approx_partition} that there is some $v\in\mathcal{U}_{[t+\Delta_1,s]}$ such that $\sigma(v)<\delta$, with $v=t_1,\ldots,t_n$ and $t_n=s$, and such that for all $i\in\{2,\ldots,n\}$, with $\Delta_i\coloneqq t_i-t_{i-1}$:
\begin{equation*}
(\exists Q_i\in\rateset)\norm{T_{t_{i-1},x_{u\cup \{t\}}}^{t_{i}} - (I+\Delta_{i}Q_i)} < \Delta_{i}\epsilon_2.\vspace{5pt}
\end{equation*}
Since $\Delta_1<\delta$ and, for all $i\in\{2,\dots,n\}$, $\Delta_i\leq\sigma(v)<\delta$, we know that, for all $i\in\{1,\dots,n\}$, $\Delta_i<\delta$ and therefore also $\Delta_i\norm{Q_i}\leq\delta\norm{\rateset}\leq1$.
% which implies that there exist rate matrices $Q_{1},\ldots,Q_{m}\in\rateset$ such that, for all functions $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$ and all $i\in\{1,\ldots,m\}$,
% \begin{equation*}
% \norm{T_{\tau_{i-1},x_{u\cup\{t\}}}^{\tau_{i}}g - (I+\Delta_{i}Q_i)g} \leq \norm{T_{\tau_{i-1},x_{u\cup\{t\}}}^{\tau_{i}} - (I+\Delta_{i}Q_i)} \norm{g} < \frac{\Delta_{i}\epsilon}{2C\norm{f}}\norm{g} \leq \frac{\Delta_{i}\epsilon}{2C}\,.
% \end{equation*}
% This in turn implies that for all $i\in\{1,\ldots,m\}$, all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$, and all $y\in\states$,
% \begin{equation*}
% \left[(I+\Delta_{i}Q_{i})g\right](y) - \frac{\Delta_{i}\epsilon}{2C} < \left[T_{\tau_{i-1},x_{u\cup\{t\}}}^{\tau_{i}}g\right](y)\,.
% \end{equation*}
% Because for all $i\in\{1,\ldots,m\}$ it holds that $Q_i\in\rateset$, by Equation~\eqref{eq:correspondinglowertrans} we have $\lrate g\leq Q_ig$, and therefore, for all $g\in\gamblesX$ with $\norm{g}\leq\norm{f}$, and all $y\in\states$,
% \begin{equation}\label{eq:lowerbound_proof_linear_approx_lrate}
% \left[(I+\Delta_{i}\lrate)g\right](y) - \frac{\Delta_{i}\epsilon}{2C} < \left[T_{\tau_{i-1},x_{u\cup\{t\}}}^{\tau_{i}}g\right](y)\,.
% \end{equation}
Therefore, we find that
\begin{multline*}
\abs{[T_{t,\,x_u}^sf](x_t)
-\left[\left(\prod_{i=1}^n(I+\Delta_iQ_i)\right)f\right](x_t)}\\
\begin{aligned}
&=\abs{\Big[T_{t_0,\,x_u}^{t_1}\Bigg(\prod_{i=2}^{n}T_{t_{i-1},\,x_{u\cup\{t\}}}^{t_{i}}\Bigg)f\Big](x_t)
-\left[\left(\prod_{i=1}^n(I+\Delta_iQ_i)\right)f\right](x_t)}\\
&\leq\norm{T_{t_0,\,x_u}^{t_1}\Bigg(\prod_{i=2}^{n}T_{t_{i-1},\,x_{u\cup\{t\}}}^{t_{i}}\Bigg)
-\prod_{i=1}^n(I+\Delta_iQ_i)}\norm{f}\\
&\leq
\norm{T_{t_0,\,x_u}^{t_1}-(I+\Delta_1Q_1)}\norm{f}
+\sum_{i=2}^{n}
\norm{T_{t_{i-1},\,x_{u\cup\{t\}}}^{t_{i}}-(I+\Delta_iQ_i)}\norm{f}\\
&<\sum_{i=1}^n\Delta_i\epsilon_2\norm{f}=C\epsilon_2\norm{f}<\frac{\epsilon}{2},
\end{aligned}
\end{multline*}
where the equality follows from Lemma~\ref{lemma:weirddecomposition}, the first inequality follows from the properties of $\norm{\cdot}$, and the second inequality follows from Lemma~\ref{lemma:recursive} and Proposition~\ref{prop:stochastic_from_rate_matrix} and, we also find that
\begin{align*}
\abs{[L_t^sf](x_t)-\left[\left(\prod_{i=1}^n(I+\Delta_i\lrate)\right)f\right](x_t)}
&\leq
\norm{L_t^s-\prod_{i=1}^n(I+\Delta_i\lrate)}\norm{f}\leq\epsilon_1\norm{f}<\frac{\epsilon}{2},
\end{align*}
using Equation~\eqref{eq:theorem:nonmarkov_single_var_lower_bounded} to establish the second inequality. Hence, Lemma~\ref{lemma:productofQsdominatesproductoflrates} implies that
\begin{multline*}
[L_{t}^s f](x_t)
<\left[\left(\prod_{i=1}^n(I+\Delta_i\lrate)\right)f\right](x_t)+\frac{\epsilon}{2}\\
\leq
\left[\left(\prod_{i=1}^n(I+\Delta_i Q_i)\right)f\right](x_t)+\frac{\epsilon}{2}
<[T_{t,\,x_u}^sf](x_t)+\epsilon.
\end{multline*}\\[-0pt]
Since $\epsilon>0$ was arbitrary, this allows us to infer that $[L_{t}^s f](x_t)\leq [T_{t,\,x_u}^sf](x_t)$. The result now follows from Remark~\ref{remark:expectationT}.
% Now, observe that because $\tau_m=s$, it holds for all $y\in\states$ that
% \begin{align*}
% \left[T_{\tau_{m-1},x_{u\cup\{t\}}}^{\tau_{m}}f\right](y) &= \mathbb{E}[f(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}=y,X_t=x,X_u=x_u]\\
%  &= \mathbb{E}[f(X_{s})\,\vert\,X_{s-\Delta_m}=y,X_t=x,X_u=x_u]\,.
% \end{align*}
% Furthermore, by the basic properties of expectation, we have that
% \begin{equation*}
% \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u] = \mathbb{E}\bigl[\mathbb{E}[f(X_s)\,\vert\,X_{s-\Delta_m},X_t=x,X_u=x_u]\,\vert\,X_t=x,X_u=x_u\bigr]\,,
% \end{equation*}
% and because this (outer) expectation computes a convex combination of values, we find by substitution of Equation~\eqref{eq:lowerbound_proof_linear_approx_lrate}, that
% \begin{equation*}
% \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u] > \mathbb{E}\bigl[[(I+\Delta_m\lrate)f](X_{\tau_{m-1}})\,\vert\,X_t,X_u=x_u\bigr] - \frac{\Delta_{m}\epsilon}{2C}\,.
% \end{equation*}
% Because $\Delta_m\leq\sigma(v)<\delta^*\leq\nicefrac{1}{\norm{\lrate}}$, we find by Proposition~\ref{lemma:normQsmallenough} that $(I+\Delta_m\lrate)$ is a lower transition operator. Therefore, $[(I+\Delta_m\lrate)f]\in\gamblesX$, and by property~\ref{LT:norm_at_most_one}, we have that $\norm{(I+\Delta_m\lrate)f}\leq\norm{f}$. Hence, Equation~\eqref{eq:lowerbound_proof_linear_approx_lrate} applies, and we can use backward induction on $m$ to find
% \begin{align*}
% \mathbb{E}[&f(X_s)\,\vert\,X_t=x,X_u=x_u] \\
% & > \mathbb{E}\bigl[[(I+\Delta_m\lrate)f](X_{\tau_{m-1}})\,\vert\,X_t=x,X_u=x_u\bigr] - \frac{\Delta_{m}\epsilon}{2C} \\
%  &> \mathbb{E}\bigl[[(I+\Delta_{m-1}\lrate)(I+\Delta_m\lrate)f](X_{\tau_{m-2}})\,\vert\,X_t=x,X_u=x_u\bigr] - \frac{\Delta_{m-1}\epsilon}{2C} - \frac{\Delta_{m}\epsilon}{2C} \\
% &\vdots \\
% %& > \mathbb{E}\left[\left[\left(\prod_{i=2}^m(I+\Delta_i\lrate)\right)f\right](X_{\tau_{1}})\,\Bigg\vert\,X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n}\right] - \sum_{i=2}^m\frac{\Delta_{i}\epsilon}{2C} \\
% & > \mathbb{E}\left[\left[\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](X_{\tau_{0}})\,\Bigg\vert\,X_t=x,X_u=x_u\right] - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C} \\
% & = \mathbb{E}\left[\left[\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](X_{t+\Delta_0})\,\Bigg\vert\,X_t=x,X_u=x_u\right] - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C} \\
% &= \left[T_{t,x_u}^{t+\Delta_0}\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](x) - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C}\,.
% %& = \left[\left(\prod_{i=1}^m(I+\Delta_i\lrate)\right)f\right](x_{t_n}) - \sum_{i=1}^m\frac{\Delta_{i}\epsilon}{2C}\\
% %& = \left[\Phi_v f\right](x_{t_n}) - \frac{\epsilon}{2}\,,
% \end{align*}
% Because $\prod_{i=1}^m(I+\Delta_i\lrate)$ is a lower transition operator, we have that $[\prod_{i=1}^m(I+\Delta_i\lrate)f]\in\gamblesX$, and by property~\ref{LT:norm_at_most_one}, that $\norm{\prod_{i=1}^m(I+\Delta_i\lrate)f}\leq\norm{f}$. Therefore, Equation~\eqref{eq:lowerbound_proof_linear_approx_last_part} applies, and hence we find that
% \begin{equation*}
% \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u] > \left[\prod_{i=0}^m(I+\Delta_i\lrate)f\right](x) - \sum_{i=0}^m\frac{\Delta_{i}\epsilon}{2C}\,.
% \end{equation*}
% Let now $v^*\coloneqq (\{t\}\cup v)=t,\tau_0,\ldots,\tau_m$. Then clearly, $v^*\in\mathcal{U}_{[t,s]}$. Then, with $\Phi_{v^*}$ as in Equation~\eqref{eq:aux_lower_trans}, we find using $\sum_{i=0}^m\Delta_i=C$ that
% \begin{equation*}
% \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u] > \left[\Phi_{v^*}f\right](x) - \frac{\epsilon}{2}\,,
% \end{equation*}
% and because $\sigma(v^*)=\max\{\Delta_0,\sigma(v)\}<\delta$, we now find from Equation~\eqref{eq:lowerbound_proof_linear_approx_lbound} that
% \begin{equation*}
% \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u] > \left[L_{t}^sf\right](x) - \epsilon\,.
% \end{equation*}
% Because the $\epsilon\in\realspos$ was arbitrary, this completes the proof.
\end{proof}

%
%**** {\bf the one below is deprecated}
%\begin{proof}[Proof of Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded}]
%Consider any $P\in\processes_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, and any $g\in\gambles(\states^{\{s\}})$. We will show that for all $\epsilon\in\realspos$,
%\begin{equation*}
%[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,,
%\end{equation*}
%which then implies $[L_t^s g](x_{t_n}) \leq \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]$. Start by choosing any $\epsilon\in\realspos$, and let $C\coloneqq (s-t)$.
%
%Because $P\in\processes_\rateset$, it follows from Definition~\ref{def:set_non_markov_process} that there is some $\delta\in\realspos$ such that
%\begin{align}\label{eq:nonmarkov_bound_proof_deriv_bounded}
%\begin{split}
% &(\forall \tau\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall v\in\mathcal{U}_{[0,\tau]})\,(\forall(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}})\,(\exists Q\in\rateset)\,: \\
% &(\forall f\in\gambles(\states^{v\cup\{\tau+\Delta\}}))\,(\forall x_{\tau_m}\in\states^{\{\tau_m\}}): \\
% &\left\lvert \frac{\mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}] 
% - f(x_{\tau_0},\ldots,x_{\tau_m},x_{\tau_m})}{\Delta} \right. \\
% &\quad\quad\quad\quad\quad\quad - \left[Q f(x_{\tau_0},\ldots,x_{\tau_{m}},X_{\tau+\Delta})\right](x_{\tau_m})\biggr\rvert \quad < \quad\frac{\epsilon}{2C\norm{g}}\cdot\norm{f}\,.
%\end{split}
%\end{align}
%Furthermore, it follows from Theorem~\ref{theo:convergencelowerbound} that there is some $\delta'\in\realspos$ such that
%\begin{equation}\label{eq:nonmarkov_bound_proof_lbound_approx}
%(\forall v\in\mathcal{U}_{[t,s]}\,:\,\sigma(v)<\delta')\,(\forall x_t\in\states)\abs{\left[L_t^s g\right](x_t) - \left[\prod_{k=1}^n(I+\Delta_i\lrate)g\right](x_t)} < \frac{\epsilon}{2}\,.
%\end{equation}
%
%Let $\delta^*\coloneqq\min\{\delta,\delta'\}$, and choose any $n>\nicefrac{C}{\delta^*}$. Then, for $\Delta\coloneqq\nicefrac{C}{n}$, we have $\Delta<\delta$ and $\Delta<\delta'$.
%
%Equation \eqref{eq:nonmarkov_bound_proof_deriv_bounded} now implies that for all $\tau\in\realspos$, all $v\in\mathcal{U}_{[0,\tau]}$ such that $v=\tau_0,\ldots,\tau_m$, and all $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, there is some $Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}}\in\rateset$ such that, for all $f\in\gambles(\states^{v\cup\{\tau+\Delta\}})$ and all $x_{\tau_m}\in\states^{\{\tau_m\}}$,
%\begin{align}\label{eq:nonmarkov_bound_proof_deriv_inequal}
%\begin{split}
% &\left[(I + \Delta Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}})f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\right](x_{\tau_m}) - \frac{\Delta\epsilon\norm{f}}{2C\norm{g}} \\
% &\quad< \mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}]\,.
%\end{split}
%\end{align}
%Now, note that by the basic properties of probability,
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
%\end{align*}
%By choosing $\tau=(s-\Delta)$, setting $v=t_0,\ldots,t_n,(s-\Delta)$, and noting that $g\in\gambles(\states^{\{s\}})=\gambles(\states^{\{\tau+\Delta\}})\subset\gambles(\states^{v\cup\{\tau+\Delta\}})$, we find from Equation \eqref{eq:nonmarkov_bound_proof_deriv_inequal} that for all $(x_{t_0},\ldots,x_{t_n})\in\states^{u}$ there is some $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$ such that, for all $x_{s-\Delta}\in\states^{\{s-\Delta\}}$,
%\begin{equation*}
%\left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}=x_{s-\Delta}]\,.
%\end{equation*}
%Furthermore, we find using Equation~\eqref{eq:correspondinglowertrans} and the fact that $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$, that
%\begin{equation*}
%\left[(I + \Delta \lrate)g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} \leq \left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C}\,.
%\end{equation*}
%Noting that an expectation takes a convex combination of values, we find by substitution that
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] \\
%&> \mathbb{E}\bigl[[(I+\Delta\lrate)g](X_{s-\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] - \frac{\Delta\epsilon}{2C}\,.
%\end{align*}
%
%Note also that
%\begin{equation*}
%[(I+\Delta\lrate)g](X_{s-\Delta})\in\gambles(\states^{\{s-\Delta\}})\subset\gambles(\states^{u\cup\{s-\Delta\}})\,.
%\end{equation*}
%Hence, we can repeat this argument, factoring the expectation at time points $(s-2\Delta),\ldots,(s-(n-1)\Delta)$, to obtain
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - n\cdot\frac{\Delta\epsilon}{2C} \\
% &= \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2}\,.
%\end{align*}
%
%It follows from Equation \eqref{eq:nonmarkov_bound_proof_lbound_approx} and the fact that $\Delta<\delta'$,
%\begin{equation*}
%\left[L_t^s g\right](x_{t_n}) - \frac{\epsilon}{2} < \left[(I+\Delta\lrate)^n g\right](x_{t_n})\,,
%\end{equation*}
%so that by substitution,
%\begin{align*}
%\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2} \\
% &> [L_t^s g](x_{t_n}) - \frac{\epsilon}{2} - \frac{\epsilon}{2} \\
% &= [L_t^s g](x_{t_n}) - \epsilon\,.
%\end{align*}
%Thus, we have found that
%\begin{equation*}
%[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,.
%\end{equation*}
%Because the $\epsilon\in\realspos$ was arbitrary, this concludes the proof.
%\end{proof}


\begin{lemma}\label{lemma:nonhomogeneous_in_process_set}
Let $\rateset$ be a non-empty bounded set of rate matrices. Then for any finite sequence of time points $u=t_0,\ldots,t_n$ and corresponding collection of rate matrices $Q_0,\ldots,Q_{n+1}\in\rateset$, there is a process $P\in\wmprocesses_\rateset$ such that
\begin{equation}\label{eq:nonhomogen_in_process_set_system_composition}
\mathcal{T}_P = \mathcal{T}_{Q_0}^{[0,t_0]}\otimes \mathcal{T}_{Q_1}^{[t_0,t_1]} \otimes \cdots \otimes \mathcal{T}_{Q_n}^{[t_{n-1},t_n]} \otimes \mathcal{T}_{Q_{n+1}}^{[t_n,\infty)}\,.
\end{equation}
\end{lemma}
\begin{proof}
Proposition~\ref{prop:finite_different_rate_matrix_has_process} implies the existence of a process $P\in\wmprocesses$ that satisfies Equation~\eqref{eq:nonhomogen_in_process_set_system_composition}. It remains to show that $P\in\wmprocesses_\rateset$, or equivalently, that $\smash{\overline{\partial}}T_t^t\subseteq\rateset$ for all $t\in\realsnonneg$. To this end, consider any $t\in\realsnonneg$.

We consider several cases. If $t<t_0$, then $\smash{\overline{\partial}}T_t^t$ corresponds to $\mathcal{T}_{Q_0}^{[0,t_0]}$, and hence $\smash{\overline{\partial}}T_t^t=\{Q_0\}\subseteq\rateset$. If $t>t_n$, then $\smash{\overline{\partial}}T_t^t$ corresponds to $\smash{\mathcal{T}_{Q_{n+1}}^{[t_n,\infty)}}$, in which case $\smash{\overline{\partial}}T_t^t=\{Q_{n+1}\}\subseteq\rateset$. Similarly, if $t\in(t_{i-1},t_i)$ for some $i\in\{1,\ldots,n\}$, then $\smash{\overline{\partial}}T_t^t$ corresponds to $\mathcal{T}_{Q_i}^{[t_{i-1},t_i]}$, and therefore $\smash{\overline{\partial}}T_t^t=\{Q_i\}\subseteq\rateset$. The only remaining case is when $t=t_i$ for some $i\in\{0,\ldots,n\}$. In this case, we have that $\smash{\overline{\partial}}_-T_t^t=\{Q_i\}$ and $\smash{\overline{\partial}}_+T_t^t=\{Q_{i+1}\}$, and therefore
\begin{equation*}
\smash{\overline{\partial}}T_t^t = \smash{\overline{\partial}}_-T_t^t\cup\smash{\overline{\partial}}_+T_t^t=\{Q_{i},Q_{i+1}\}\subseteq\rateset.
\end{equation*}
% Because this covers all the possible cases, in summary, we have found that $\smash{\overline{\partial}}T_t^t\subseteq\rateset$ for all $t\in\realsnonneg$, and hence $P\in\wmprocesses_\rateset$.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{theorem:lower_markov_bound_is_tight}]
Fix any $t,s\in\realsnonneg$ such that $t\leq s$, any $f\in\gamblesX$ and any $\epsilon\in\realspos$. If $t=s$, the result is trivial. Hence, without loss of generality, we may assume that $t<s$. Let $C\coloneqq (s-t)$, choose any $\epsilon^*>0$ such that $\epsilon^*C<\epsilon$, choose any $\epsilon_1,\epsilon_2,\epsilon_3>0$ such that $\epsilon_1+\epsilon_2+\epsilon_3<\epsilon^*$, choose any $\delta>0$ such that $\delta\norm{\rateset}^2\norm{f}<\epsilon_3$ and $\smash{\delta\norm{\lrate}^2\norm{f}<\epsilon_1}$ (this is always possible because of~\ref{LR:normlratefinite}), and consider any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$, with $u=t_0,\ldots,t_n$ and $n\in\nats$. 

Fix any $i\in\{1,\dots,n\}$ and let $g_i\coloneqq L_{t_i}^{t_n}f$. It then follows from Lemma~\ref{lemma:rateset_has_arginf} that there is some $Q_i\in\rateset$ such that $\norm{\lrate g_i-Q_i g_i}<\epsilon_2$ and, due to~\ref{N:normAf} and~\ref{LT:norm_at_most_one}, we also know that $\norm{g_i}=\norm{L_{t_i}^{t_n}f}\leq\norm{L_{t_i}^{t_n}}\norm{f}\leq\norm{f}$.
% \begin{equation}\label{eq:prop_lower_markov_tight_rate_error}
% \norm{\lrate L_{t_i}^sf - Q_iL_{t_i}^sf} < \frac{\epsilon}{2C}\,.
% \end{equation}
Hence, we find that 
\begin{align}
&\norm{L_{t_{i-1}}^{t_i}g_i - e^{Q_i\Delta_i}g_i}\notag\\
&\quad\quad\quad\leq \norm{L_{t_{i-1}}^{t_i}g_i - \left[I+\Delta_i\lrate\right]g_i} 
+\norm{\Delta_i\lrate g_i-\Delta_iQ_ig_i}
+ \norm{\left[I+\Delta_iQ_i\right]g_i - e^{Q_i\Delta_i}g_i} \notag\\
&\quad\quad\quad\leq \norm{L_{t_{i-1}}^{t_i} - \left[I+\Delta_i\lrate\right]}\norm{g_i} 
+\Delta_i\norm{\lrate g_i-Q_ig_i}
+ \norm{I+\Delta_iQ_i - e^{Q_i\Delta_i}}\norm{g_i} \notag\\
&\quad\quad\quad< \norm{L_{t_{i-1}}^{t_i} - \left[I+\Delta_i\lrate\right]}\norm{f} 
+\Delta_i\epsilon_2
+ \norm{I+\Delta_iQ_i - e^{Q_i\Delta_i}}\norm{f} \notag\\
&\quad\quad\quad\leq
\Delta_i^2\norm{\lrate}^2\norm{f}
+\Delta_i\epsilon_2
+
\Delta_i^2\norm{Q_i}^2\norm{f}\notag\\
&\quad\quad\quad\leq
\Delta_i(
\delta\norm{\lrate}^2\norm{f}
+\epsilon_2
+
\delta\norm{\rateset}^2\norm{f})
<\Delta_i(\epsilon_1+\epsilon_2+\epsilon_3)<\Delta_i\epsilon^*,\label{eq:theorem:lower_markov_bound_is_tight:localbound}
\end{align}\\[-8pt]
where the second inequality holds because of~\ref{N:normAf}, the third inequality holds because $\norm{g_i}\leq\norm{f}$ and $\norm{\lrate g_i-Q_i g_i}<\epsilon_2$, the fourth inequality holds because of Lemmas~\ref{lemma:quadraticboundonL} and~\ref{lemma:linearpartofexponential} and where the fifth inequality holds because $\Delta_i\leq\sigma(u)<\delta$.



For all $i\in\{0,\ldots,n\}$, let $\mathcal{T}_{Q_i}$ denote the transition matrix system corresponding to $Q_i$, as in Definition~\ref{def:systemfromQ}. Then, by Lemma~\ref{lemma:nonhomogeneous_in_process_set}, there is some $P\in\wmprocesses_\rateset$ with transition matrix system $\mathcal{T}_P$, such that
\begin{equation*}
\mathcal{T}_P = \mathcal{T}_{Q_0}^{[0,t_0]}\otimes \mathcal{T}_{Q_1}^{[t_0,t_1]}\otimes \cdots \otimes \mathcal{T}_{Q_n}^{[t_{n-1},t_n]} \otimes \mathcal{T}_{Q_n}^{[t_{n},\infty)}\,.
\end{equation*}
Due to Equation~\eqref{eq:theorem:lower_markov_bound_is_tight:localbound}, we know that the transition matrices of this process $P$ satisfy
\begin{equation}\label{eq:theorem:lower_markov_bound_is_tight:localbounds}
(\forall i\in\{1,\ldots,n\})~
\norm{L_{t_{i-1}}^{t_i}g_i - T_{t_{i-1}}^{t_i}g_i}= \norm{L_{t_{i-1}}^{t_i}g_i - e^{Q_i\Delta_i}g_i}<\Delta_i\epsilon^*.
\end{equation}
Furthermore, we also know that
\begin{align*}
\norm{L_{t_0}^{t_n}g_n - T_{t_0}^{t_n}g_n} 
 &\leq \norm{L_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}g_n - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}g_n} + \norm{T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}g_n - T_{t_0}^{t_{n-1}}T_{t_{n-1}}^{t_n}g_n} \\
 %&\leq \norm{L_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{{T_{t_0}^{t_{n-1}}}}\cdot\norm{L_{t_{n-1}}^{t_n}g_n - T_{t_{n-1}}^{t_n}g_n} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}g_{n-1} - T_{t_0}^{t_{n-1}}g_{n-1}} + \norm{L_{t_{n-1}}^{t_n}g_n - T_{t_{n-1}}^{t_n}g_n},
\end{align*}
using Proposition~\ref{prop:lower_trans_system_is_system} and Equation~\eqref{eq:markovintermsofmatrices} for the first equality, and Proposition~\ref{prop:lower_trans_system_is_system} and \ref{LT:differencenorm} for the second one.
Similarly, we also find that
\begin{equation*}
\norm{L_{t_0}^{t_{n-1}}g_{n-1} - T_{t_0}^{t_{n-1}}g_{n-1}}
\leq \norm{L_{t_0}^{t_{n-2}}g_{n-2} - T_{t_0}^{t_{n-2}}g_{n-2}} + \norm{L_{t_{n-2}}^{t_{n-1}}g_{n-1} - T_{t_{n-2}}^{t_{n-1}}g_{n-1}}.
\end{equation*}
By continuing in this way---applying induction---we eventually find that
\begin{equation*}
\norm{L_t^sf - T_t^sf}=\norm{L_{t_0}^{t_n}g_n - T_{t_0}^{t_n}g_n} 
\leq \sum_{i=1}^{n} \norm{L_{t_{i-1}}^{t_i}g_i - T_{t_{i-1}}^{t_i}g_i}\leq\sum_{i=1}^n\Delta_i\epsilon^*=C\epsilon^*<\epsilon,
\end{equation*}
using Equation~\eqref{eq:theorem:lower_markov_bound_is_tight:localbounds} to establish the second inequality. The result now follows directly from Remark~\ref{remark:expectationT}.
% Consider now any $i\in\{1,\ldots,n\}$. Then, we find from Lemma~\ref{lemma:justthelinearpart} that
% \begin{align*}
% &\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - \left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f} + \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \norm{L_{t_{i-1}}^{t_i} - \left[I+\Delta_i\lrate\right]}\cdot\norm{f} + \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \Delta_i^2\norm{\lrate}^2\norm{f} + \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \Delta_i^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,,
% \end{align*}
% where in the last step we used the fact that $\norm{\lrate}\leq\norm{\rateset}$. We now focus on the remaining second summand:
% \begin{align*}
%  &\quad \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
%  &\leq \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - \left[I+\Delta_iQ_i\right]L_{t_i}^{t_n}f} + \norm{\left[I+\Delta_iQ_i\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
%  &\leq \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - \left[I+\Delta_iQ_i\right]L_{t_i}^{t_n}f} + \norm{\left[I+\Delta_iQ_i\right] - T_{t_{i-1}}^{t_i}}\cdot\norm{f} \\
%  &= \Delta_i\norm{\lrate L_{t_i}^{t_n}f - Q_iL_{t_i}^{t_n}f} + \norm{\left[I+\Delta_iQ_i\right] - e^{Q_i\cdot\Delta_i}}\cdot\norm{f} \\
%  &< \frac{\Delta_i\epsilon}{2C} + \Delta_i^2\norm{Q_i}^2\norm{f}\,,
% \end{align*}
% where in the last step we applied Equation~\eqref{eq:prop_lower_markov_tight_rate_error} and Lemma~\ref{lemma:linearpartofexponential}. Hence, we have found that for any $i\in\{1,\ldots,n\}$, it holds that
% \begin{align*}
% &\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
%  &< \Delta_i^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta_i\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
%  &< \Delta_i^2\norm{\mathcal{Q}}^2\norm{f} + \Delta_i^2\norm{Q_i}^2\norm{f} + \frac{\Delta_i\epsilon}{2C} \\
%  &\leq 2\Delta_i^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta_i\epsilon}{2C} \\
%  &< 2\Delta_i\delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta_i\epsilon}{2C}\,,
% \end{align*}
% where we used $\norm{Q_i}\leq\norm{\rateset}$, and in the final step applied $\Delta_i\leq\sigma(u)<\delta$.
% \begin{align*}
% \norm{L_t^sf - T_t^sf} 
% &\leq \sum_{i=1}^n \norm{L_{t_{i-1}}^{t_i}g_i - T_{t_{i-1}}^{t_i}g_i}= \sum_{i=1}^n \norm{L_{t_{i-1}}^{t_i}g_i - e^{Q_i\Delta_i}g_i}<\sum_{i=1}^n\Delta_i\epsilon^*
% =C\epsilon^*<\epsilon.
% \end{align*}
%***
%
%By Equation~\eqref{eq:lower_char_rate_matrix}, we have $\norm{Q_{t_i}L_{t_i}^{t_n}f - \lrate L_{t_i}^{t_n}f} < \nicefrac{\epsilon}{2C}$. Furthermore, by Equation~\eqref{eq:lower_char_matrix_linear_approx}, we have $Q_{t_i}^u=Q_{t_i}$, so that
%\begin{align*}
% &\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \norm{\left[I+\Delta \lrate\right]L_{t_i}^{t_n}f - \left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f} \\
% &= \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \Delta\norm{\lrate L_{t_i}^{t_n}f - Q_{t_i}L_{t_i}^{t_n}f} \\
% &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \frac{\Delta\epsilon}{2C} \\
% &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right] - T_{t_{i-1}}^{t_i}}\cdot\norm{f} + \frac{\Delta\epsilon}{2C} \\
% &= \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}\right] - e^{Q_{t_i}\cdot(t_i-t_{i-1})}}\cdot\norm{f} + \frac{\Delta\epsilon}{2C} \\
% &\leq 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C}\,,
%\end{align*}
%where we used the definition of $T_{t_{i-1}}^{t_i}$ from Lemma~\ref{lemma:nonhomogen_trans_mat_system}, and where the last step used Lemma~\ref{lemma:linearpartofexponential} and the fact that $\norm{Q_{t_i}}\leq\norm{\rateset}$, since $Q_{t_i}\in\rateset$.
%
%Thus, using the fact that $n\Delta=C$, we find
%\begin{align*}
%\norm{L_t^sf - T_t^sf} &\leq \sum_{i=1}^n \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
% &\leq \sum_{i=1}^n 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C} \\
% &= 2n\Delta^2\norm{\mathcal{Q}}^2\norm{f} + n\frac{\Delta\epsilon}{2C}\\
% &= 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2}\,,
%\end{align*}
%so that by Equation~\eqref{eq:delta_required_for_tight_bound} and the fact that $\Delta<\delta$, we have
%\begin{equation*}
%\norm{L_t^sf - T_t^sf} \leq 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} < 2C\delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\,.
%\end{equation*}
\end{proof}

\begin{proof}[Proof of Corollary~\ref{cor:lower_operator_is_infimum}]
%We start by proving the first statement. By Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded} and Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, we have that for all $P\in\wmprocesses_\rateset$ it holds that $\left[L_t^sf\right](x) \leq \mathbb{E}[f(X_s)\,\vert\,X_t=x]$. Furthermore,
Fix any $\epsilon>0$.
It then follows from Proposition~\ref{theorem:lower_markov_bound_is_tight} that there is some $P\in\wmprocesses_\rateset$ such that $\mathbb{E}[f(X_s)\vert\,X_t=x_t] < [L_t^sf](x_t)+\epsilon$, which implies that
\begin{align*}
\underline{\mathbb{E}}^{\mathrm{WM}}_\rateset\left[f(X_s)\,\vert\,X_t=x_t,X_u=x_u\right]
&\leq
\mathbb{E}[f(X_s)\vert\,X_t=x_t,X_u=x_u]\\
&=\mathbb{E}[f(X_s)\vert\,X_t=x_t]
< [L_t^sf](x_t)+\epsilon,
\end{align*}
using Definition~\ref{def:process_sets} for the first inequality and the Markov property of $P$ for the equality. Hence, since $\epsilon>0$ is arbitrary, we find that  $\underline{\mathbb{E}}^{\mathrm{WM}}_\rateset\left[f(X_s)\,\vert\,X_t=x_t,X_u=x_u\right]\leq[L_t^sf](x_t)$.
% Together this implies, using Definition~\ref{def:lower_exp}, that
% \begin{equation*}
% \left[L_t^sf\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\wmprocesses_\rateset\right\} = \underline{\mathbb{E}}^{\mathrm{WM}}_\rateset\left[f(X_s)\,\vert\,X_t=x\right].
% \end{equation*}
% We now move on to the second statement. By Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded} we have that for all $P\in\wprocesses_\rateset$ it holds that $\left[L_t^sf\right](x) \leq \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u]$.
% Furthermore, by Proposition~\ref{theorem:lower_markov_bound_is_tight} and Proposition~\ref{prop:markov_set_subset_of_nonmarkov_set}, for all $\epsilon\in\realspos$ there is some $P\in\wprocesses_\rateset$, for which $T_{t,x_u}^s=T_t^s$, such that
% $\norm{L_t^sf - T_{t,x_u}^sf} < \epsilon$.
% Together this implies, using Definition~\ref{def:lower_exp}, that
% \begin{equation*}
% \left[L_t^sf\right](x) = \inf\left\{\left[T_{t,x_u}^sf\right](x)\,:\,P\in\wprocesses_\rateset\right\} = \underline{\mathbb{E}}^\mathrm{W}_\rateset\left[f(X_s)\,\vert\,X_t=x,X_u=x_u\right]\,.
% \end{equation*}
This implies the result because we also have that
\begin{align*}
\left[L_t^sf\right](x_t)
&\leq\inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u]\colon P\in\wprocesses_{\rateset}\right\}\\
&=\underline{\mathbb{E}}^{\mathrm{W}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u] 
\leq
\underline{\mathbb{E}}^{\mathrm{WM}}_{\,\rateset}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u],
\end{align*}
where the first inequality follows from Proposition~\ref{theorem:nonmarkov_single_var_lower_bounded} and the last inequality follows from Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{theo:dominating_rate_processes_max_set}]
% *** still need to check this ***
% By Propositions~\ref{prop:dominating_nonempty_bounded} and \ref{prop:dominatingproperties}, the set $\rateset_{\lrate}$ is non-empty, bounded, has separately specified rows, and has $\lrate$ as its corresponding lower transition rate operator. Therefore, the fact that $L_t^s$ computes the lower expectation for $\wprocesses_{\rateset_{\lrate}}$ follows directly from Corollary~\ref{cor:lower_operator_is_infimum}.

% To prove that this is the largest set for which $L_t^s$ does this, we need to show that, for every $P\notin \wprocesses_{\rateset_{\lrate}}$, there is some $t,s\in\realsnonneg$ with $t\leq s$, some $u\in\mathcal{U}_{<t}$, some $x\in\states$ and $x_u\in\states^u$, and some $f\in\gamblesX$, such that
% \begin{equation}\label{theo:largest_set_outside_process_lower}
% \mathbb{E}[f(X_s)\,\vert\,X_t=x,X_u=x_u] < [L_t^sf](x)\,.
% \end{equation}

% To this end, consider any $P\in\processes$ such that $P\notin\wprocesses_{\rateset_{\lrate}}$. Clearly, we must have that either $P\in\wprocesses$ or $P\notin\wprocesses$. We will start by assuming the former, and show that the statement then follows.

% Because $P\notin\wprocesses_{\rateset_{\lrate}}$, by Definition~\ref{def:consistent_process}, there must be some $t\in\realsnonneg$, some $u\in\mathcal{U}_{<t}$, and some $x_u\in\states^u$, such that $\smash{\overline{\partial}}T_{t,x_u}^{t}\nsubseteq\rateset_{\lrate}$. Furthermore, by Proposition~\ref{prop:boundednon-emptyandclosed}, we have that $\smash{\overline{\partial}}T_{t,x_u}^{t}\neq\emptyset$. Hence, there must be some rate matrix $Q\in\smash{\overline{\partial}}T_{t,x_u}^{t}$ such that $Q\notin\rateset_{\lrate}$, which by Equation~\eqref{eq:dominatingratematrices} implies that there is some $f\in\gamblesX$ and some $x\in\states$ such that $\left[Qf\right](x)<\left[\lrate f\right](x)$.

% Now, assume \emph{ex absurdo} that Equation~\eqref{theo:largest_set_outside_process_lower} is false. Because $Q\in\smash{\overline{\partial}}T_{t,x_u}^{t}$, we must have that either $Q\in\smash{\overline{\partial}}_+T_{t,x_u}^{t}$ or $Q\in\smash{\overline{\partial}}_-T_{t,x_u}^{t}$. Assume it is the former. Then, by Equation~\ref{eq:rightouterderivative}, there is some $\{\Delta_i\}_{i\in\nats}\to0^+$ such that $\lim_{i\to+\infty}\nicefrac{1}{\Delta_i}(T_{t,x_u}^{t+\Delta_i}-I)=Q$. Because we assumed that Equation~\eqref{theo:largest_set_outside_process_lower} is false, we must have that for all $\Delta\in\realspos$, it holds that $\left[L_t^{t+\Delta}f\right](x) \leq \left[T_{t,x_u}^{t+\Delta}f\right](x)$,
% or equivalently, that
% \begin{equation*}
% \frac{1}{\Delta}\left[(L_t^{t+\Delta} - I)f\right](x) \leq \frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)f\right](x)\,.
% \end{equation*}
% Taking limits on both sides with respect to $\{\Delta_i\}_{i\in\nats}$, we find that
% \begin{align*}
% \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(L_t^{t+\Delta_i} - I)f\right](x) &\leq \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(T_{t,x_u}^{t+\Delta_i} - I)f\right](x) \\
% \left[\lrate f\right](x) &\leq \left[Qf\right](x)\,,
% \end{align*}
% where the left-hand limit follows from Proposition~\ref{prop:lower_transition_has_deriv}. This contradicts the earlier observation that $\left[Qf\right](x)<\left[\lrate f\right](x)$. Hence, if the statement is indeed false, we must instead have that $Q\in\smash{\overline{\partial}}_-T_{t,x_u}^{t}$. In that case, there is some other $\{\Delta_i\}_{i\in\nats}\to0^+$, such that $\lim_{i\to+\infty}\nicefrac{1}{\Delta_i}(T_{t-\Delta_i,x_u}^{t}-I)=Q$, and similarly, we should have for all $\Delta\in\realspos$ that
% \begin{equation*}
% \frac{1}{\Delta}\left[(L_{t-\Delta}^{t} - I)f\right](x) \leq \frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^{t} - I)f\right](x)\,.
% \end{equation*}
% By Proposition~\ref{prop:lower_transition_is_homogeneous}, we have that $L_{t-\Delta}^{t}=L_{t}^{t+\Delta}$, and so after taking limits on both sides,
% \begin{align*}
% \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(L_t^{t+\Delta_i} - I)f\right](x) &\leq \lim_{i\to+\infty}\frac{1}{\Delta_i}\left[(T_{t-\Delta_i,x_u}^{t} - I)f\right](x) \\
% \left[\lrate f\right](x) &\leq \left[Qf\right](x)\,,
% \end{align*}
% we again find a contradiction. Therefore, if $P\in\wprocesses$, Equation~\eqref{theo:largest_set_outside_process_lower} follows.

% Now, suppose that instead $P\notin\wprocesses$. Assume \emph{ex absurdo} that Equation~\eqref{theo:largest_set_outside_process_lower} is false. Because $P\notin\wprocesses$, by Definition~\ref{def:well-behaved} there must be some $t\in\realsnonneg$, some $u\in\mathcal{U}_{<t}$, some $x_u\in\states^u$, and some $x,y\in\states$, such that either 
% \begin{equation*}
% \limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t+\Delta}=y\,\vert\,X_t=x,X_u=x_u)-\delta_{xy}}\nless+\infty\,,
% \end{equation*}
% or,
% \begin{equation*}
% \limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t}=y\,\vert\,X_{t-\Delta}=x,X_u=x_u)-\delta_{xy}}\nless+\infty\,.
% \end{equation*}
% Start by assuming that it is the former. Clearly, we have for all $\Delta\in\realspos$ that $\left[T_{t,x_u}^{t+\Delta}\ind{y}\right](x)=P(X_{t+\Delta}=y\,\vert\,X_t=x,X_u=x_u)$, and furthermore, that $\left[I\ind{y}\right](x)=\delta_{xy}$. Hence, we have that
% \begin{equation*}
% \left[(T_{t,x_u}^{t+\Delta}-I)\ind{y}\right](x) = P(X_{t+\Delta}=y\,\vert\,X_t=x,X_u=x_u) - \delta_{xy}\,.
% \end{equation*}
% By Proposition~\ref{prop:rate_from_stochastic_matrix}, we have that $\nicefrac{1}{\Delta}(T_{t,x_u}^{t+\Delta}-I)$ is a transition rate matrix, and hence by Definition~\ref{def:rate_matrix} we must have that $\nicefrac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta}-I)\ind{x}\right](x)\leq 0$, and $\nicefrac{1}{\Delta}\abs{\left[(T_{t,x_u}^{t+\Delta}-I)\ind{y}\right](x)}\leq\nicefrac{1}{\Delta}\abs{\left[(T_{t,x_u}^{t+\Delta}-I)\ind{x}\right](x)}$. It therefore follows that also
% \begin{equation}\label{eq:nonwellbehaved_limit_diagonal}
% \limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{\left[(T_{t,x_u}^{t+\Delta}-I)\ind{x}\right](x)} \nless +\infty\,.
% \end{equation}
% Similarly, by Lemma~\ref{lemma:lower_trans_to_lower_rate}, we have that $\nicefrac{1}{\Delta}(L_t^{t+\Delta}-I)$ is a lower transition rate operator, and hence by Definition~\ref{def:coh_low_trans_rate}, we must have that $\nicefrac{1}{\Delta}\left[(L_t^{t+\Delta}-I)\ind{x}\right](x)\leq 0$. 

% Because we assumed that the statement is false, and because $\ind{x}\in\gamblesX$, it must hold for all $\Delta\in\realspos$ that $\left[L_{t}^{t+\Delta}\ind{x}\right](x)\leq\left[T_{t,x_u}^{t+\Delta}\ind{x}\right](x)$, or equivalently, that
% \begin{equation*}
% \frac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{x}\right](x) \leq \frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)\,.
% \end{equation*}
% From our earlier observations, we know that both sides of this inequality are non-positive. Hence, we must have that
% \begin{equation*}
% \abs{\frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)} \leq \abs{\frac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{x}\right](x)}\,.
% \end{equation*}
% By the definition of the norm, we have that $\abs{\nicefrac{1}{\Delta}\left[(L_{t}^{t+\Delta} - I)\ind{x}\right](x)}\leq\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)\ind{x}}$, and because $\ind{x}\in\gamblesX$ and $\norm{\ind{x}}=1$, we have $\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)\ind{x}}\leq \norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)}$. Hence, we must have that
% \begin{equation*}
% \abs{\frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)} \leq \norm{\frac{1}{\Delta}(L_{t}^{t+\Delta} - I)}\,.
% \end{equation*}

% Now, by Proposition~\ref{prop:lower_transition_has_deriv}, we have that $\lim_{\Delta\to0^+}\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I) = \lrate$, and hence it follows that $\lim_{\Delta\to0^+}\norm{\nicefrac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate}$. By property~\ref{LR:normlratefinite}, we furthermore know that $\norm{\lrate}<+\infty$. Taking limits on both sides of our earlier inequality, we therefore find that
% \begin{equation*}
% \limsup_{\Delta\to0^+}\abs{\frac{1}{\Delta}\left[(T_{t,x_u}^{t+\Delta} - I)\ind{x}\right](x)} \leq \limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate} < +\infty\,,
% \end{equation*}
% which, by Equation~\eqref{eq:nonwellbehaved_limit_diagonal}, is a contradiction. Therefore, if Equation~\eqref{theo:largest_set_outside_process_lower} is indeed false, we must instead have that for some (other) $t\in\realspos$, some (other) $u\in\mathcal{U}_{<t}$, some (other) $x_u\in\states^u$, and some (other) $x,y\in\states$,
% \begin{equation}\label{eq:nonwellbehaved_limit_diagonal_left}
% \limsup_{\Delta\to0^+}\frac{1}{\Delta}\abs{P(X_{t}=y\,\vert\,X_{t-\Delta}=x,X_u=x_u)-\delta_{xy}}\nless+\infty\,.
% \end{equation}
% Using the same argument as before, we must then have for all $\Delta\in\realspos$ that
% \begin{equation*}
% \abs{\frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^t - I)\ind{x}\right](x)} \leq \norm{\frac{1}{\Delta}(L_{t-\Delta}^t - I)}\,.
% \end{equation*}
% Because by Proposition~\ref{prop:lower_transition_is_homogeneous} we have that $L_{t-\Delta}^t=L_t^{t+\Delta}$, we again find after taking limits on both sides that
% \begin{equation*}
% \limsup_{\Delta\to0^+}\abs{\frac{1}{\Delta}\left[(T_{t-\Delta,x_u}^t - I)\ind{x}\right](x)} \leq \limsup_{\Delta\to0^+}\norm{\frac{1}{\Delta}(L_{t}^{t+\Delta} - I)} = \norm{\lrate} < +\infty\,,
% \end{equation*}
% a contradiction with Equation~\eqref{eq:nonwellbehaved_limit_diagonal_left}. Therefore, if $P\notin\wprocesses$, Equation~\eqref{theo:largest_set_outside_process_lower} follows.

% *** first attempt at shorter version  ***
Clearly, it suffices to prove that for any $P\in\processes$ that is not well-behaved or not consistent with $\rateset_{\lrate}$, there are $t,s\in\realsnonneg$ with $t< s$, $u\in\mathcal{U}_{<t}$, $x_u\in\states^u$, $x_t\in\states$ and $f\in\gamblesX$ such that
\begin{equation}\label{eq:theo:dominating_rate_processes_max_set:toprove}
% (\exists t,s\in\realsnonneg\colon t\leq s)\,
% (\exists u\in\mathcal{U}_{<t})\,
% (\exists x_u\in\states^u)\,
% (\exists x_t\in\states)\,
% (\exists f\in\gamblesX)
\mathbb{E}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u] < [L_t^sf](x_t).\vspace{5pt}
\end{equation}
% We consider two cases: $P\in\processes\setminus\wprocesses$. and $P\in\wprocesses\setminus\wprocesses_{\rateset_{\lrate}}$

We start with the case that $P$ is not well-behaved. Fix any $\epsilon>0$ and let $C\coloneqq\norm{\lrate}+\epsilon$. It then follows from Proposition~\ref{prop:stochasticprocess:simpleproperties} that there are $t,s\in\realsnonneg$ with $t< s$, $u\in\mathcal{U}_{<t}$ and $x_u\in\states^u$ such that $\nicefrac{1}{\Delta}\norm{T_{t,\,x_u}^s-I}>C$ and $\smash{\Delta\norm{\lrate}^2<\epsilon}$, with $\Delta\coloneqq s-t>0$. Let $Q\coloneqq\nicefrac{1}{\Delta}(T_{t,\,x_u}^s-I)$. Since $\norm{Q}>C$, it then follows that there is some $f'\in\gamblesX$ such that $\norm{Qf'}>C$, which in turn implies that there is some $x_t\in\states$ such that $\abs{[Qf'](x_t)}>C$. If $[Qf'](x_t)<0$, we let $f\coloneqq f'$, and if $[Qf'](x_t)>0$, we let $f\coloneqq -f'$. Clearly, this implies that $[Qf](x_t)<-C$, which, since $[\lrate f](x_t)\geq-\norm{\lrate}$, implies that $[Qf](x_t)\leq[\lrate f](x_t)-\epsilon$ and therefore that $[(I+\Delta Q)f](x_t)+\Delta\epsilon\leq [(I+\Delta\lrate)f](x_t)$. Hence, since we also know that
\begin{align*}
\abs{[L_t^sf](x_t)-[(I+\Delta\lrate)f](x_t)}
\leq
\norm{L_t^s-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2<\Delta\epsilon,
\end{align*}
using Lemma~\ref{lemma:quadraticboundonL} to establish the first inequality, it follows that
\begin{align*}
%\mathbb{E}[f(X_s)\,\vert\,X_t=x_t,X_u=x_u] 
%=
[T_{t,\,x_u}^sf](x_t)
=[(I+\Delta Q)f](x_t)
=[(I+\Delta \lrate)f](x_t)-\Delta\epsilon
<[L_t^sf](x_t),
\end{align*}
which, because of Remark~\ref{remark:expectationT}, implies that Equation~\eqref{eq:theo:dominating_rate_processes_max_set:toprove} holds.

Next, we consider the case that $P$ is well behaved, but not consistent with $\rateset_{\lrate}$. In that case, it follows from Definition~\ref{def:consistent_process} that there are $t^*\in\realsnonneg$, $u\in\mathcal{U}_{<t}$ and $x_u\in\states^u$ such that $\smash{\overline{\partial}T_{t^*,\,x_u}^{t^*} \not\subseteq \rateset_{\lrate}}$. Since we know that $\smash{\overline{\partial}T_{t^*,\,x_u}^{t^*}}$ is a non-empty set of rate matrices because of Proposition~\ref{prop:boundednon-emptyandclosed}, this implies the existence of a rate matrix $Q^*\in\smash{\overline{\partial}}T_{t^*,x_u}^{t^*}$ such that $Q^*\notin\rateset_{\lrate}$. Furthermore, since $Q^*\notin\rateset_{\lrate}$, Equation~\eqref{eq:dominatingratematrices} implies that there are $f\in\gamblesX$ and $x_t\in\states$ such that $[Q^*f](x_t)<[\lrate f](x_t)$. Consider now any $\epsilon>0$ such that $[Q^*f](x_t)\leq[\lrate f](x_t)-2\epsilon$ (this is clearly possible). Since $Q^*\in\smash{\overline{\partial}}T_{t^*,x_u}^{t^*}$, it then follows from Definition~\ref{def:outerpartialderivatives} that thare are $t,s\in\realsnonneg$ such that $u<t<s$, $\norm{\nicefrac{1}{\Delta}(T_{t,\,x_u}^s-I)-Q^*}\leq\epsilon$ and $\smash{\Delta\norm{\lrate}^2<\epsilon}$, with $\Delta\coloneqq s-t>0$. Let $Q\coloneqq\nicefrac{1}{\Delta}(T_{t,\,x_u}^s-I)$. Since $\norm{Q-Q^*}\leq\epsilon$, it then follows that $[Qf](x_t)\leq[Q^*f](x_t)+\epsilon\leq[\lrate f](x_t)-\epsilon$. Hence, using the exact same argument as in the first part of this proof, we find that Equation~\eqref{eq:theo:dominating_rate_processes_max_set:toprove} holds.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:approximation_error_bound}]
Let $u\in\mathcal{U}_{[t,s]}$ be such that $u=t_0,t_1,\ldots,t_n$, where, for all $i\in\{0,1,\ldots,n\}$, $t_i\coloneqq t + i\cdot\Delta$. Then, clearly, $t_0=t$, $t_n=s$, and $\sigma(u)=\Delta$. Let
\begin{equation*}
\Phi_u\coloneqq \prod_{i=1}^n (I + \Delta_i^u\lrate) = \prod_{i=1}^n(I+\Delta\lrate).
\end{equation*}
We have that
\begin{align*}
\Delta &= \frac{(s-t)}{n} \leq (s-t)\frac{\epsilon}{(s-t)^2\norm{\lrate}^2\norm{f}}  = \frac{\epsilon}{(s-t)\norm{\lrate}^2\norm{f}}.
\end{align*}
It now follows from Property~\ref{N:normAf} and Lemma~\ref{lemma:limitboundonL} that
\begin{align*}
\norm{L_t^sf - \prod_{i=1}^n(I+\Delta\lrate)f} &= \norm{L_t^sf - \Phi_uf} \leq \norm{L_t^s - \Phi_u}\norm{f} \\
 &\leq \sigma(u)(s-t)\norm{\lrate}^2\norm{f} = \Delta(s-t)\norm{\lrate}^2\norm{f} \\
 &\leq \frac{\epsilon}{(s-t)\norm{\lrate}^2\norm{f}} (s-t)\norm{\lrate}^2\norm{f} = \epsilon\,.
\end{align*}
\end{proof}

%\section{Proofs and Lemmas for Section~\ref{sec:funcs_multi_time_points}}

%\begin{proof}[Proof of Proposition~\ref{prop:multi_var_single_future_bounded}]
%Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we therefore have to show that $\left[L_t^sg\right](x_{t_n}) \leq \mathbb{E}\left[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]$. Because $g\in\gambles(\states^{\{s\}})$, this inequality holds by Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
%\end{proof}
%
%\begin{proof}[Proof of Proposition~\ref{prop:multi_var_single_future_tight}]
%Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we now have to show that there is a $P\in\wmprocesses_\rateset$ such that
%\begin{equation*}
%\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} < \epsilon\,. 
%\end{equation*}
%
%Because $g\in\gambles(\states^{\{s\}})$, by Theorem~\ref{theorem:lower_markov_bound_is_tight} there must be some $P\in\wmprocesses_\rateset$ such that $\norm{L_t^sg - \mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon$. Consider this $P$. By the Markov property, its expectation must satisfy $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} = \abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]}$, and by the definition of the norm, we have 
%\begin{equation*}
%\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]} \leq \norm{L_t^sg-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon\,.
%\end{equation*}
%\end{proof}
%
%\begin{proof}[Proof of Corollary~\ref{cor:inf_works_for_single_future_var}]
%This is a direct consequence of Propositions \ref{prop:markov_set_subset_of_nonmarkov_set}, \ref{prop:lower_exp_markov_bounded_by_nonmarkov}, \ref{prop:multi_var_single_future_bounded}, and \ref{prop:multi_var_single_future_tight}. The proof is similar to that of Corollary~\ref{cor:lower_operator_is_infimum}.
%\end{proof}

%\begin{proof}[Proof of Proposition~\ref{prop:multivar_bounded}]
%By the basic properties of expectation, we can decompose the expectation functional as
%\begin{align*}
% &\quad \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
% &= \mathbb{E}\bigl[\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s_0,\ldots,s_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
%\end{align*}
%Note that the nested expectation is conditioned on the time points $t_0,\ldots,t_n,s_0,\ldots,s_{m-1}$; it therefore only computes an expectation on a single point $s_m$ in the future. Hence, by Proposition~\ref{prop:multi_var_single_future_bounded} and the fact that the (outer) expectation computes a convex combination of values, we have
%\begin{align*}
%&\quad\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
% &\geq \mathbb{E}\bigl[[L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_{m-1}})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
%\end{align*}
%The proof is then finished by backward induction on $m$.
%\end{proof}
%
%\begin{proof}[Proof of Proposition~\ref{prop:multivar_bound_tight}]
%{\bf TODO} *** deze is nog een beetje ingewikkeld, we moeten weer een process bouwen en laten zien dat die in $\wprocesses_\rateset$ zit ***
%\end{proof}
%
%\begin{proof}[Proof of Corollary~\ref{cor:inf_works_for_multivar}]
%This is a direct consequence of Propositions~\ref{prop:multivar_bounded} and \ref{prop:multivar_bound_tight}.
%\end{proof}


\end{document}
