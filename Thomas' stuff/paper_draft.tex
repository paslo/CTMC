\documentclass[10pt]{paper}
%\documentclass[a4paper,reqno]{amsart}
\usepackage[british]{babel}
%\usepackage[garamond]{mathdesign}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{courier}
\usepackage{amssymb}
%\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{nicefrac}
\usepackage{bm}
%\usepackage{pdfsync}
%\usepackage{authblk}

\theoremstyle{definition}
\newtheorem{exmp}{Example}%[section]
 
\renewcommand{\ttdefault}{cmtt}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem*{remark*}{Remark}

% - macros

\newcommand{\nats}{\mathbb{N}}
\newcommand{\natswith}{\nats_{0}}
\newcommand{\reals}{\mathbb{R}}

\newcommand{\realspos}{\reals_{>0}}
\newcommand{\realsnonneg}{\reals_{\geq 0}}

\newcommand{\states}{\mathcal{X}}

\newcommand{\paths}{\Omega}
\newcommand{\path}{\omega}

\newcommand{\power}{\mathcal{P}(\paths)}
\newcommand{\nonemptypower}{\power_{\emptyset}}
\newcommand{\events}{\mathcal{E}}
%\newcommand{\nonemptyevents}{\events^{\emptyset}}
\newcommand{\filter}[1][t]{\mathcal{F}_{#1}}
\newcommand{\eventst}[1][t]{\events_{#1}}

\newcommand{\processes}{\mathbb{P}}
\newcommand{\mprocesses}{\processes^{\mathrm{M}}}

\newcommand{\hmprocesses}{\processes^{\mathrm{HM}}}

\newcommand{\wprocesses}{\processes^{\mathrm{W}}}
\newcommand{\wmprocesses}{\processes^{\mathrm{WM}}}

\newcommand{\whmprocesses}{\processes^{\mathrm{WHM}}}


\newcommand{\lt}{\underline{T}}
\newcommand{\lbound}{L}

\newcommand{\gambles}{\mathcal{L}}
\newcommand{\gamblesX}{\gambles(\states)} 

\newcommand{\ind}[1]{\mathbb{I}_{#1}}

\newcommand{\rateset}{\mathcal{Q}}
\newcommand{\lrate}{\underline{Q}}

\newcommand{\asa}{\Leftrightarrow}
\newcommand{\then}{\Rightarrow}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\abs}[1]{\left\vert #1 \right\vert}

\newcommand{\coloneqq}{:\!=}

\newcommand{\opinset}{\,\,\widetilde{\in}\,\,}

\newcommand{\argmin}{\arg\min}

\newcommand{\exampleend}{\hfill$\Diamond$}

\title{Imprecise Continuous-Time Discrete-Space Stochastic Processes}

%\author[1]{Thomas E. Krak\thanks{t.e.krak@uu.nl}}
%\author[2]{Jasper de Bock\thanks{jasper.debock@ugent.be}}
%\affil[1]{Universiteit Utrecht}
%\affil[2]{Ghent University}

\author{Thomas Krak \and Jasper de Bock}

\begin{document}

%\author{{\bf Thomas E. Krak} \\ Utrecht}
%\address{Utrecht University}
%\curraddr{}
%\email{t.e.krak@uu.nl}
%\thanks{}

%\author{{\bf Jasper de Bock} \\ Ghent}
%\address{Ghent University}

%\author{
	%{\bf Thomas E. Krak} \quad\quad {\bf Jasper de Bock} \\
%	Utrecht University \quad Ghent University \\
	%Department of Information and Computing Sciences \\
	%Princetonplein 5, De Uithof \\
	%3584 CC Utrecht \\
	%The Netherlands \\
%	\texttt{\quad\quad t.e.krak@uu.nl} \quad\quad \texttt{jasper.debock@ugent.be}
%\and
	%{\bf Jasper de Bock} \\
%	Ghent University \\
	%SYSTeMS Research Group \\
	%Technologiepark -- Zwijnaarde 914 \\
	%9052 Zwijnaarde \\ 
	%Belgium \\
%	\texttt{jasper.debock@ugent.be}
%}
\date{}
\maketitle

\begin{abstract}
Lorem ipsum.
\end{abstract}

\section{Introduction}\label{sec:introduction}

This paper is organized as follows. In Section~\ref{sec:prelim} we introduce notation and basic definitions used throughout this work. Section~\ref{sec:lower_operator} contains our definition for the lower transition operator, along with existence proofs and the connection to previous work from the literature. 

In Sections~\ref{sec:imp_markov} and~\ref{sec:imp_non_markov} we show that this lower transition operator correctly gives the lower envelope of expectation functionals with respect to sets of Markov and non-Markov processes, respectively. In Section~\ref{sec:decomp}, we extend these results to expectation functionals on multiple time points, and show that certain decomposition properties hold when the lower envelope is taken with respect to sets of non-Markov models. 

Section~\ref{sec:tractability} contains some notes on tractability aspects of  computing such lower expectations, and we describe different function classes with varying degrees of tractability. We finally close with some conclusions and an outlook to further work in Section~\ref{sec:conclusions}.

\section{Preliminaries}\label{sec:prelim}

Consider some finite \emph{state space} $\states=\{1,\dots,m\}$.

*** introduce state space, notation for naturals, reals, ... ***

\subsection{Stochastic matrices and rate matrices}
\begin{definition}[Rate Matrix]\label{def:rate_matrix}
A real-valued $m\times m$ matrix $Q$ is said to be a \emph{rate matrix} if

\vspace{5pt}
\begin{enumerate}[label=R\arabic*:]
\item
$\sum_{y\in\states}Q(x,y)=0$ for all $x\in\states$;
\item
$Q(x,y)\geq0$ for all $x,y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\noindent
We use $\mathcal{R}$ to denote the set of all rate matrices. 
\end{definition}

Clearly, $\mathcal{R}$ is closed under finite sums and multiplication with non-negative scalars. 

\begin{definition}[Stochastic Matrix]\label{def:stoch_matrix}
A real-valued $m\times m$ matrix $A$ is said to be \emph{stochastic} if
\vspace{5pt}
\begin{enumerate}[label=S\arabic*:]
\item
$\sum_{y\in\states}A(x,y)=1$ for all $x\in\states$;
\item
$A(x,y)\geq0$ for all $x,y\in\states$.
\end{enumerate}
\vspace{5pt}
\noindent
\end{definition}

\begin{proposition}\label{prop:stochastic_from_rate_matrix}
Consider any $m\times m$ rate matrix $Q\in\mathcal{R}$, and any $0\leq \Delta \leq \nicefrac{1}{\norm{Q}}$. Let $I$ denote the $m\times m$ identity matrix. Then, the matrix $[I+\Delta Q]$ is stochastic.
\end{proposition}
\begin{proof}
Let $A=[I+\Delta Q]$. We will verify the properties from Definition~\ref{def:stoch_matrix}.

We start with property S1. Consider any $x\in\states$. Then
\begin{equation*}
\sum_{y\in\states} A(x,y) = \sum_{y\in\states} [I + \Delta Q](x,y) = \sum_{y\in\states}I(x,y) + \Delta \sum_{y\in\states}Q(x,y) = 1\,,
\end{equation*}
where we used property R1 from Definition~\ref{def:rate_matrix}.

For property S2, note that $0\leq \Delta \leq \nicefrac{1}{\norm{Q}}$. Whence, for all $x\in\states$, we have $-1\leq \Delta Q(x,x) \leq 0$, so that $[I+\Delta Q](x,x) \geq 0$. Furthermore, for all $x,y\in\states$ s.t. $x\neq y$, we have $0\leq \Delta Q(x,y) \leq 1$, so that $[I+\Delta Q](x,y)\geq 0$.
\end{proof}

\begin{proposition}\label{prop:rate_from_stochastic_matrix}
Consider any stochastic $m\times m$ matrix $A$, and any $\Delta\in\realspos$. Let $I$ denote the $m\times m$ identity matrix. Then, $[\nicefrac{1}{\Delta}\cdot(A-I)]$ is a rate matrix.
\end{proposition}
\begin{proof}
This follows from a similar argument as the proof of Proposition~\ref{prop:stochastic_from_rate_matrix}; simply verify the properties from Definition~\ref{def:rate_matrix}.
\end{proof}

\subsection{Functions, Operators, and Norms}\label{sec:func_oper_norm}
Let $\gamblesX$ denote the set of all real-valued functions on $\states$. Because $\states$ is finite, we can also interpret any function $f\in\gamblesX$ as a vector in $\reals^m$. Hence, we will in the sequel use the terms `function' and `vector' interchangeably when referring to elements of $\gamblesX$.

For any vector $f\in\gamblesX$, we let
\begin{equation*}
\norm{f}\coloneqq\norm{f}_{\infty}\coloneqq\max\{\abs {f(x)}\colon x\in\states\}
\end{equation*}
be the maximum norm. For any operator $A$ from $\gamblesX$ to $\gamblesX$ that is non-negatively homogeneous, meaning that
\begin{equation*}
A(\lambda f)=\lambda A(f)\text{ for all $f\in\gamblesX$ and all $\lambda\geq0$,}
\end{equation*}
we consider the induced operator norm
\begin{equation*}
\norm{A}\coloneqq\sup\left\{\norm{Af}\colon f\in\gamblesX,\norm{f}=1\right\}.
\end{equation*}
If $A$ is an $m\times m$ matrix, we have that
\begin{equation*}
\norm{A}
=
\max\left\{\sum_{y\in\states}\abs{A(x,y)}\colon x\in\states\right\}.
\end{equation*}

\noindent
These norms satisfy the following properties. 

\begin{proposition}
For all $f,g\in\gamblesX$, all $A,B$ from $\gamblesX$ to $\gamblesX$ that are non-negatively homogeneous, all $\lambda\in\reals$ and all $x\in\states$, we have that
\vspace{5pt}

\begin{enumerate}[label=N\arabic*:,ref=N\arabic*]
\item
$\abs{f(x)}\leq\norm{f}$
\item
$\norm{f}\geq0$
\item
$\norm{f}=0\asa f=0$
\item
$\norm{f+g}\leq\norm{f}+\norm{g}$
\item
$\norm{\lambda f}=\abs{\lambda}\norm{f}$
\item
$\norm{A}\geq0$
\item
$\norm{A}=0\asa A=0$
\item
$\norm{A+B}\leq\norm{A}+\norm{B}$
\item
$\norm{AB}\leq\norm{A}\norm{B}$
\item
$\norm{\lambda A}=\abs{\lambda}\norm{A}$
\item\label{N:normAf}
$\norm{Af}\leq\norm{A}\norm{f}$
\item
$\norm{A}=1$ if $A$ is a stochastic matrix.
\end{enumerate}
\vspace{5pt}
\end{proposition}

\noindent
Finally, for any set $\mathcal{A}$ of square matrices, we define

\begin{equation*}
\norm{\mathcal{A}}\coloneqq\sup\{\norm{A}\colon A\in\mathcal{A}\}.
\end{equation*}

\subsubsection{wat notatie voor meerdere variabelen}\label{sec:multivar_notation}

*** dit kan opzich wel korter denk ik, maar kunnen we bij oppoetsen straks nog wel even checken ***

We will also find it convenient to have notation for functions defined on the state space at multiple time points.
To this end, consider any $t,s\in\realsnonneg$ such that $t<s$, and any $u\in\mathcal{U}_{[t,s]}$. We will now define
\begin{equation*}
\states^u\coloneqq \prod_{i=0}^n\states
\end{equation*}
to be the joint state space at times $t_0,\ldots,t_n$. Let $\gambles(\states^u)$ denote the set of functions on $(X_{t_0},\ldots,X_{t_n})$.

For any $t,t',s\in\realsnonneg$ such that $t<t'<s$ and any $u\in\mathcal{U}_{[t,t']}$, let $\states^{\{s\}}$ denote the state space at time $s$, and let
\begin{equation*}
\states^{u\cup\{s\}} \coloneqq \states^u\times\states^{\{s\}}
\end{equation*}
denote the joint state space at times $t_0,\ldots,t_n,s$. Similarly, for any $t,t',s,s'\in\realsnonneg$ such that $t<t'<s<s'$, any $u\in\mathcal{U}_{[t,t']}$ and $v\in\mathcal{U}_{[s,s']}$, let
\begin{equation*}
\states^{u\cup v}\coloneqq\states^u\times\states^v\,.
\end{equation*}
Let the sets of functions $\gambles(\states^{u\cup\{s\}})$ and $\gambles(\states^{u\cup v})$ be defined in the obvious manner.

For any function $f\in\gambles(\states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}})$, let the norm $\norm{f}$ be defined as
\begin{equation*}
\norm{f} \coloneqq \max\left\{ \abs{f(x_{t_0},\ldots,x_{t_n})}\,:\,(x_{t_0},\ldots,x_{t_n})\in \states^{\{t_0\}}\times\cdots\times\states^{\{t_n\}}\right\}\,.
\end{equation*}

For the sake of brevity, we will also write a joint state assignment as
\begin{equation*}
\left(X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right)\equiv (X_{t_0}=x_{t_0},\ldots,X_{t_n}=x_{t_n})\,.
\end{equation*}

For a function $f\in\gambles(\states^{u\cup\{s\}})$, we will write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ specific to the joint state assignment $(x_{t_0},\ldots,x_{t_n})$. Thus, $f(x_{t_0},\ldots,x_{t_n},X_s)$ corresponds to a $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$.

Finally, we will want to use operators as defined in Section~\ref{sec:func_oper_norm} for functions defined on (multiple) explicit time points. It is clear that an operator $A$ defined on $\gamblesX$ can be applied to any element of a set of functions $\gambles(\states^{\{s\}})$ that depend on the state at just a single point in time. It remains to determine how to cope with functions defined on multiple time points. 

To avoid introducing overly complex notation, we will stipulate the following convention. If $f\in\gambles(\states^u)$ is a function defined on an ordered sequence $u$ of time points $t_0<t_1<\cdots<t_n$, and if $A$ is a non-negatively homogeneous operator from $\gamblesX$ to $\gamblesX$, we allow $A$ to be applied to $f$ by applying it to the restriction of $f$ to the state $\states^{\{t_n\}}$ at the \emph{latest} time point $t_n$ in $u$. Because such a restriction is dependent on the specific state assignment $(x_{t_0},\ldots,x_{t_{n-1}})$, the result is a function $[Af]\in\gambles(\states^{\{t_0,\ldots,t_{n-1}\}})$. In other words, we then have
\begin{equation*}
\left[Af\right](x_{t_0},\ldots,x_{t_{n-1}}) \equiv \left[A f(x_{t_0},\ldots,x_{t_{n-1}},X_{t_n})\right](x_{t_{n-1}})\,.
\end{equation*}

\subsection{Outer limits}

*** introduce some definitions and properties here ***


\section{Continuous-Time Stochastic Processes}

*** add nice introtext here ***

\subsection{Full conditional probabilities}

A cadlag function from $\realsnonneg$ to $\states$ is called a \emph{path}, where cadlag means that it is right continuous for all $t\in\realsnonneg$ and that the left limit exists for all $t\in\realspos$. Let $\paths$ be the set of all paths. For any path $\path\in\paths$ and any time point $t\in\realsnonneg$, the value of $\path$ in $t$ is denoted by $\path(t)$.

A subset $E$ of $\paths$ is called an \emph{event}. The set of all events is denoted by $\power$ and we let $\nonemptypower\coloneqq\power\setminus\{\emptyset\}$. For any $t\in\realsnonneg$ and $x\in\states$, we define the elementary event
\begin{equation*}
(X_t=x)\coloneqq\{\path\in\paths\colon\path(t)=x\}.
\end{equation*}
%We use $\events^{\mathrm{f}}$ to denote the set of all these elementary events. 
Similarly, for any finite sequence of time points $0\leq t_1<\dots<t_n$ and any $x_{i}\in\states$, $i\in\{1,\dots,n\}$, we define the event
\begin{align*}
\left(X_{t_1}=x_{1}, \dots, X_{t_n}=x_{n}\right)
\coloneqq%&
%\bigcap_{i\in\{0,\dots,n\}}(X_{t_i}=x_{t_i})\\
%=&
\left\{\path\in\paths\colon(\forall i\in\{1,\dots,n\})~\path(t_i)=x_{i}\right\}.%,
\end{align*}
%which we will sometimes also denote by $(X_{t_i}=x_{t_i}, i\in\{0,\dots,n\})$.
%We use $\events^{\mathrm{s}}$ to denote the set of all these events.

Consider now any $t\in\realsnonneg$. We then let $\mathcal{A}_{>t}$ be the algebra that is generated by all the elementary events $X_s=x$, for $s> t$ and $x\in\states$,\footnote{This is the smallest subset of $\power$ that contains all these elementary events and that is furthermore closed under complements, finite unions and hence also finite intersections.} and we let $\mathcal{F}_{\leq t}$ be the set of all events $\left(X_{t_1}=x_{1}, \dots, X_{t_n}=x_{n}\right)$, with $0\leq t_1\leq\dots\leq t_n\leq t$ and $x_1$.%, and we define $\filter\coloneqq\mathcal{C}_{\leq t}\setminus\{\emptyset\}$.

\begin{definition}[Full conditional probability]\label{def:cond_prob}
A full conditional probability $P$ is a real-valued map from $\power\times\nonemptypower$ to $\reals$ that satisfies the following axioms. For all $A,B\in\power$ and all \mbox{$C,D\in\nonemptypower$}:
\vspace{5pt}

\begin{enumerate}[label=F\arabic*:]
\item
$P(C\vert C)=1$;
\item
$0\leq P(A\vert C)\leq 1$;
\item
$P(A\cup B\vert C)=P(A\vert C)+P(B\vert C)$ if $A\cap B=\emptyset$;
\item
$P(A\vert C)=P(A\vert D)P(D\vert C)$ if $A\subseteq D\subseteq C$.
\end{enumerate}
\vspace{5pt}

\noindent
For any $A\in\power$ and $C\in\nonemptypower$, we call $P(A\vert C)$ the probability of $A$ conditional on $C$. Also, for any $A\in\power$, we use the shorthand notation $P(A)\coloneqq P(A\vert\paths)$ and then call $P(A)$ the probability of $A$.
\end{definition}

\begin{definition}[Coherent conditional probability]\label{def:coherence}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is said to be a coherent conditional probability if, for all $n\in\mathbb{N}$ and every choice of $(A_i,C_i)\in\mathcal{C}$ and $\lambda_i\in\reals$, $i\in\{1,\dots,n\}$,
\begin{equation*}
\sup\left\{\sum_{i=1}^n\lambda_i\ind{C_i}(\omega)\bigl(P(A_i\vert C_i)-\ind{A_i}(\omega)\bigr)~\Bigg\vert~\omega\in C_0\right\}\geq0,
\end{equation*}
with $C_0\coloneqq\cup_{i=1}^nC_i$.
\end{definition}

\begin{theorem}\label{theo:coherentextendable}
Let $P$ be a real-valued map from $\mathcal{C}\subseteq\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it can be extended to a full conditional probability.
\end{theorem}
\begin{proof}
*** REFERENCE ***
\end{proof}

\begin{corollary}
Let $P$ be a real-valued map from $\power\times\nonemptypower$ to $\reals$. Then $P$ is a coherent conditional probability if and only if it is a full conditional probability.
\end{corollary}
\begin{proof}
Trivial consequence of Theorem~\ref{theo:coherentextendable}.
\end{proof}

\subsection{Finitely additive stochastic processes}


\begin{definition}[Stochastic Process]\label{def:stoch_process}
A \emph{stochastic process} is the restriction of a full conditional probability to
\begin{equation*}
\mathcal{C}^\mathrm{SP}\coloneqq\big\{
(A,C)
\colon
A\in\mathcal{A}_{> t},~C\in\filter,~t\in\realsnonneg\big\}\subset\power\times\nonemptypower.
\end{equation*}
We denote the set of all such stochastic processes by $\processes$.
\end{definition}

\begin{corollary}
Let $P$ be a real-valued map from $\mathcal{C}^\mathrm{SP}$ to $\reals$. Then $P$ is a stochastic process if and only if it is a coherent conditional probability.
\end{corollary}
\begin{proof}
Trivial consequence of Theorem~\ref{theo:coherentextendable}.
\end{proof}

*** explain that $\sigma$-additivity is not required ***

*** discuss that a unique $\sigma$-additive extension always exists, but that for our purposes, it is not necessary to consider it ***

\subsection{Well-behaved stochastic processes}

*** explain that stochastic process can behave in rather extreme ways. For example, the transition probabilities can jump instantaneously. In order to avoid this behavior, we restrict our attention to a subclass of stochastic processes, which we call well-behaved. ***

\begin{definition}[Well-behaved stochastic process]
A stochastic process $P\in\processes$ is said to be well-behaved if, for any time sequence $0\leq t_0<\dots<t_{n}$ and any set of states $x_{0},\dots,x_{n},x,y\in\states$:
\begin{equation*}
\lim_{s\to t^{+}}P(X_s=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})=\delta_{xy}
\text{~~for all $t>t_n$}
\end{equation*}
and
\begin{equation*}
\lim_{t\to s^{-}}P(X_s=y\vert X_t=x, X_{t_0}=x_{0}, \dots, X_{t_n}=x_{n})=\delta_{xy}
\text{~~for all $s>t_n$}
\end{equation*}
The set of all well-behaved Markov processes is denoted by $\wprocesses$.
\end{definition}

*** introduce (history-dependent) transition matrices and explain that we require them to be continous ***

*** prove that this implies (and even is equivalent to, I think) that the outer limit exists and is bounded. ***

\begin{definition}[Transition matrix]\label{def:trans_matrix}
Consider a stochastic process $P\in\processes$. Then, for any $0\leq t\leq s$, the \emph{transition matrix} $T_t^s$ is a $m\times m$ matrix that is defined by
\begin{equation*}
T_t^s(x_t, x_s) \coloneqq P(X_s=x_s\,\vert X_t=x_t)\quad\text{for all $x_s,x_t\in\states$}\,.
\end{equation*}
We denote this family of matrices by $\mathcal{T}_P$.%, and call it the \emph{system of transition matrices} that corresponds to $P$.
\end{definition}

\begin{proposition}\label{prop:stochasticprocess:simpleproperties}
Let $P\in\processes$ be a stochastic process. Then for any $0\leq t\leq s$, $T_t^s$ is stochastic and $T_t^t=I$. If $P$ is well-behaved, then also
\begin{equation}\label{eq:wellbehavedtransitionmatrix}
\lim_{s\to t^{+}}T_t^s=\lim_{t\to s^{-}}T_t^s=I.
\end{equation}
\end{proposition}
\begin{proof}
*** TRIVIAL ***
\end{proof}

Obviously, a transition matrix $T_t^s$ can also be regarded as a map from $\gamblesX$ to $\gamblesX$, defined for all $f\in\gamblesX$ by $T_t^s(f)\coloneqq T_t^sf$. Note that for all $x_t\in\states$, we have that
\begin{align*}
\left[T_t^sf\right](x_t) &= \sum_{x_s\in\states}f(x_s)P(X_s=x_s\,\vert\,X_t=x_t)
= \mathbb{E}_{X_s}\left[f(X_s)\,\vert\,X_t=x_t\right]\,.
\end{align*}

%In order to re-use the lower transition operator $L_t^s$ from Definition~\ref{def:low_trans}, we will finally have to introduce some conventions. Note that $L_t^s$ is a map from $\gamblesX$ to $\gamblesX$, so it is straightforward to apply to functions $g\in\gambles(\states^{\{s\}})$. This is because these functions only depend on a single variable. However, $L_t^s$ is not yet properly defined for, e.g., functions $f\in\gambles(\states^{u\cup\{s\}})$. To this end, let first 
%\begin{equation*}
%f(x_{t_0},\ldots,x_{t_n},X_{s})\in\gambles(\states^{\{s\}})
%\end{equation*}
%denote the restriction of any $f\in\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{s\}})$, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$. Similarly, for all $(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}}$, let
%\begin{equation*} f(x_{t_0},\ldots,x_{t_{n-1}},X_{t_n},X_s)\in\gambles(\states^{\{t_n,s\}})\,,
%\end{equation*}
%and so forth.

\section{Continuous-Time Markov Chains}

*** tralala ***



%\begin{proposition}\label{prop:transitionmatrixhasprocess}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and any stochastic $m\times m$ matrix $A$. Then, there is at least one stochastic process $P\in\mprocesses$ that satisfies the Markov property, and that has a corresponding transition matrix $T_t^s$ such that
%\begin{equation*}
%T_t^s = A\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%This is immediate from Definition~\ref{def:trans_matrix} and the fact that $\mprocesses$ contains all Markov processes, according to Definition~\ref{def:markov_property}.\newline
%{\bf ATTN:} Not sure how to give an in-depth argument for this, but I don't see how it could be false.
%\end{proof}



% \begin{lemma}\label{lemma:transitionmatrixfactorises}
% Consider any $P\in\processes$. Then
% \begin{equation*}
% T_t^s=\prod_{k=1}^n T_{t_{k-1}}^{t_k} \coloneqq T_{t_0}^{t_1}T_{t_1}^{t_2}\cdots T_{t_{n-1}}^{t_n}
% \end{equation*}
% for every sequence $0\leq t=t_0<t_1<t_2,\dots,t_{n}=s$.
% \end{lemma}
% \begin{proof}
% This property is well known and therefore stated without proof. {\bf TODO:} Ref.
% \end{proof}



%\begin{proposition}\label{prop:transitionmatrix_factorized_has_process}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, any sequence of time points $u\in\mathcal{U}_{[t,s]}$, and any sequence of stochastic matrices $A_1,\ldots,A_n$. Then, there is at least one stochastic process $P\in\mprocesses$ such that, for all $i\in\{1,\ldots,n\}$, $P$ has corresponding transition matrix $T_{t_{i-1}}^{t_i}$ on the interval $[t_{i-1},t_i]$, and
%\begin{equation*}
%T_{t_{i-1}}^{t_i} = A_i\,.
%\end{equation*}
%Furthermore, $P$ has a corresponding transition matrix $T_t^s$ on the interval $[t,s]$ such that
%\begin{equation*}
%T_t^s = \prod_{i=1}^n A_i\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%Should be true. Final statement is trivial assuming the first is true. First statement should be true, given that $\mprocesses$ contains \emph{all} Markov processes, and a finite sequence of well-defined stochastic matrices should be non-degenerate.
%
%{\bf ATTN:} Does need a proof though.
%\end{proof}

%\begin{lemma}\label{lemma:linear_factorization_has_process}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, any bounded set of rate matrices $\mathcal{Q}\subset\mathcal{R}$, any sequence of time points $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\nicefrac{1}{\norm{\mathcal{Q}}}$, and any sequence of rate matrices $Q_1,\ldots,Q_n$ such that $Q_i\in\mathcal{Q}$ for all $i\in\{1,\ldots,n\}$. 
%
%Consider the induced sequence of matrices $[I + \Delta_1 Q_1],\ldots,[I+\Delta_n Q_n]$. Then,
%\begin{enumerate}
%\item The matrix $[I+\Delta_i Q_i]$ is stochastic for all $i\in\{1,\ldots,n\}$.
%\item There is at least one stochastic process $P\in\mprocesses$ such that $T_{t_{i-1}}^{t_i}=[I+\Delta_i Q_i]$ for all $i\in\{1,\ldots,n\}$, and $T_t^s = \prod_{i=1}^n[I+\Delta_i Q_i]$.
%\end{enumerate}
%\end{lemma}
%\begin{proof}
%The first claim is immediate from Proposition~\ref{prop:stochastic_from_rate_matrix}. The second claim is immediate from Proposition~\ref{prop:transitionmatrix_factorized_has_process}.
%\end{proof}
%
%\begin{lemma}\label{lemma:exponential_matrix_has_process}
%Consider any rate matrix $Q$, any $t,s\in\realsnonneg$ such that $t<s$, and any sequence of sequences of time points $u_1,u_2,\ldots,u_n,\ldots$ such that $u_i\in\mathcal{U}_{[t,s]}$ and $\lim_{n\rightarrow\infty}\sigma(u_n)= 0$. For each $u_i$, define the stochastic matrix
%\begin{equation*}
%\Phi_i \coloneqq \prod_{k=1}^{n_i} [I+\Delta^i_k Q]\,.
%\end{equation*}
%Then, the corresponding sequence $\Phi_1,\Phi_2,\ldots,\Phi_n,\ldots$ converges, with solution
%\begin{equation*}
%\lim_{n\rightarrow\infty}\Phi_n = \exp\{(s-t)Q\}\,,
%\end{equation*}
%where the right hand side denotes the matrix exponential.
%
%Furthermore, there is at least one stochastic process $P\in\mprocesses$ such that, for all $\tau,\tau'\in[t,s]$ such that $\tau<\tau'$, this process has corresponding transition matrix
%\begin{equation*}
%T_\tau^{\tau'} = \exp\{(\tau'-\tau)Q\}\,.
%\end{equation*}
%\end{lemma}
%\begin{proof}
%The claim of convergence is well-known. The last claim is then immediate from Lemma~\ref{lemma:linear_factorization_has_process}.
%\end{proof}
%
%\begin{corollary}\label{corollary:exponential_process_approximates_everywhere_and_has_derivative}
%Consider any rate matrix $Q$ and any $t,s\in\realsnonneg$ such that $t<s$. Then, there is at least one stochastic process $P\in\mprocesses$ such that, for all $\tau\in[t,s]$, this process has corresponding transition matrix $T_t^\tau$, and
%\begin{equation*}
%(\forall \epsilon\in\realspos)(\exists \delta\in\realspos)(\forall u\in\mathcal{U}_{[t,\tau]}\,:\,\sigma(u)<\delta) \norm{T_t^\tau - \prod_{k=1}^n[I+\Delta_i Q]} < \epsilon\,.
%\end{equation*}
%Furthermore, this process' transition matrix satisfies
%\begin{equation*}
%(\forall \epsilon>0)(\exists \delta>0)(\forall \Delta\in(0,\delta))(\forall \tau\in[t,s))\norm{\frac{T_\tau^{\tau+\Delta} - I}{\Delta} - Q} < \epsilon\,.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%This is immediate from Lemma~\ref{lemma:exponential_matrix_has_process}.
%\end{proof}

\begin{definition}[Markov chain, Markov property]\label{def:markov_property}
A stochastic process $P\in\processes$ satisfies the \emph{Markov property} if, for any time sequence $0\leq t_0<\dots<t_{n}<t<s$ and any set of states $x_{t_0},\dots,x_{t_n},x_t,x_s\in\states$:
\begin{equation*}
P(X_s=x_s\vert X_{t_n}=x_{t})=P(X_s=x_s\vert X_{t_0}=x_{t_0}, \dots, X_{t_n}=x_{t_n}, X_t=x_t).
\end{equation*}
A stochastic process that satisfies this property is called a \emph{Markov chain}. We denote the set of all Markov chains by $\mprocesses$ and use $\wmprocesses$ to refer to the subset that only contains the well-behaved Markov chains.
\end{definition}

\subsection{Transition matrix systems}

We know from Proposition~\ref{prop:stochasticprocess:simpleproperties} that the transition matrices of a stochastic process---and therefore also, in particular, of a Markov chain---satisfy a number of simple properties. For the specific case of a Markov chain $P\in\mprocesses$, the family of transition matrices $\mathcal{T}_P$ also satisfies and additional property---see Equation~\eqref{eq:transmatrixproduct}. Whenever this is the case, we will call such a family of stochastic matrices a transition matrix system.

\begin{definition}[Transition matrix system]
A \emph{transition matrix system} $\mathcal{T}$ is a family of stochastic matrices $T_t^s$, defined for all $0\leq t\leq s$, such that
\begin{equation}\label{eq:transmatrixproduct}
T_t^s=T_t^r T_r^s
\text{~~for all $0\leq t\leq r\leq s$}
\end{equation}
and $T_t^t=I$ for all $t\geq0$. A transition matrix system that also satisfies Equation~\eqref{eq:wellbehavedtransitionmatrix} is called a \emph{well-behaved transition matrix system}.
\end{definition}

\begin{proposition}
Consider a Markov chain $P\in\mprocesses$ and let $\mathcal{T}_P$ be the corresponding family of transition matrices. Then $\mathcal{T}_P$ is a transition matrix system and, if $P$ is well behaved, then $\mathcal{T}_P$ is also well-behaved.
\end{proposition}
\begin{proof}
*** fairly easy to prove ***
\end{proof}

At this point, we already know that every (well-behaved) Markov chain has a corresponding (well-behaved) transition matrix system. Our next result establishes that the converse is true as well: every (well-behaved) transition matrix system has a corresponding (well-behaved) Markov chain, and for a given initial distribution, this Markov chain is even unique.

\begin{theorem}\label{theo:uniqueMarkovchain}
 Let $p$ be an arbitrary mass function on $\states$ and let $\mathcal{T}$ be a transition matrix system. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}$ and $P(X_0)=p(X_0)$ and, if $\mathcal{T}$ is well-behaved, then $P$ is also well-behaved.
\end{theorem}
\begin{proof}
*** Not trivial; I have a coherence-based proof that I'll add here someday ***
\end{proof}

Hence, Markov chains---and well-behaved Markov chains in particular---are completely characterised by their transition matrices and their initial distribution. 
We now focus on a number of special cases.

\subsection{Homogeneous Markov chains}

\begin{definition}[Homogeneous Markov chain]\label{def:homogeneousMarkov}
A Markov chain $P\in\mprocesses$ is called \emph{homogeneous} if its transition matrices $T_t^s$ do not depend on the absolute value of $t$ and $s$, but only on the time-difference $s-t$:
\begin{equation}\label{eq:homogeneousMarkov}
T_t^s=T_{t+\Delta}^{s+\Delta}
\text{~~for all $0\leq t\leq s$ and $\Delta\geq0$.}
\end{equation}
We denote the set of all homogeneous Markov chains by $\hmprocesses$ and use $\whmprocesses$ to refer to the subset that consists of the well-behaved homogeneous Markov chains.
\end{definition}


\begin{definition}\label{def:systemfromQ}For any rate matrix $Q\in\mathcal{R}$, we use $\mathcal{T}_Q$ to denote the family of stochastic matrices that is defined by
\begin{equation*}
T_t^s=e^{Q(s-t)}
\text{~~for all $0\leq t\leq s$.}
\end{equation*}
\end{definition}

\begin{proposition}
\label{prop:systemQ}
For any $Q\in\mathcal{R}$, $\mathcal{T}_Q$ is a well-behaved transition matrix system.
\end{proposition}
\begin{proof}
*** trivial ***
\end{proof}

\begin{corollary}
 Consider any rate matrix $Q\in\mathcal{R}$ and let $p$ be an arbitrary mass function on $\states$. Then there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$ and, furthermore, this unique Markoch chain is well-behaved and homogeneous.
\end{corollary}
\begin{proof}
Since we know from Proposition~\ref{prop:systemQ} that $\mathcal{T}_Q$ is a well-behaved transition matrix system, it follows from Theorem~\ref{theo:uniqueMarkovchain} that there is a unique Markov chain $P\in\mprocesses$ such that $\mathcal{T}_P=\mathcal{T}_Q$ and $P(X_0)=p(X_0)$, and that this Markov chain is furthermore well-behaved. Since it---trivially---follows from Definition~\ref{def:systemfromQ} that $\mathcal{T}_Q$ satisfies Equation~\eqref{eq:homogeneousMarkov}, Definition~\ref{def:homogeneousMarkov} implies that $P$ is homogeneous.
\end{proof}



%As our next result shows, the converse is true as well: every well-behaved transition matrix system of a well-behaved homogeneous Markov chain can be uniquely characterised by a single transition rate matrix.

\begin{theorem}
For any well-behaved homogeneous Markov chain $P\in\whmprocesses$, there is a unique rate matrix $Q\in\mathcal{R}$ such that $\mathcal{T}_P=\mathcal{T}_Q$.
\end{theorem}
\begin{proof}
*** Not trivial; I have a fairly complex proof that uses outer limits. If you can think of an easier one, be my guest... :-) ***
\end{proof}

Hence, well-behaved homogenous Markov chains are completely characterised by their initial distribution and a rate matrix $Q\in\mathcal{R}$.


\subsection{Non-homogeneous Markov chains}

*** I would explicitely discuss the special cases of continous $Q_t$ and piecewice constant $Q_t$ here, as examples, and refer to some other papers for more general cases, since we don't need them.

% \begin{definition}[Markov Process]\label{def:markov_process}
% A stochastic process $P\in\mprocesses$ is said to be a \emph{continuous-time Markov process} if the transition matrix corresponding to $P$ satisfies
% \begin{equation*}
% (\forall \epsilon\in\realspos)(\exists \delta\in\realspos)(\forall \Delta\in(0,\delta))(\forall t\in\realsnonneg)(\exists Q\in\mathcal{R}) \norm{\frac{T_t^{t+\Delta} - I}{\Delta} - Q} < \epsilon\,.
% \end{equation*}
% \end{definition}

% \begin{definition}[Characterizing Rate Matrix]\label{def:markov_process_char_matrix}
% Consider any continuous-time Markov process $P$, and let $Q_t$ be a function that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$. We say that $Q_t$ \emph{characterizes} $P$ if it satisfies
% \begin{equation*}
% (\forall \epsilon\in\realspos)(\exists \delta\in\realspos)(\forall \Delta\in(0,\delta))(\forall t\in\realsnonneg)\norm{\frac{T_t^{t+\Delta} - I}{\Delta} - Q_t} < \epsilon\,.
% \end{equation*}
% \end{definition}

\begin{theorem}\label{theorem:continuous_rate_matrix_has_process}
For any continuous map $Q_t$ from $\reals_{\geq0}$ to $\mathcal{R}$, there is a unique transition matrix system $\mathcal{T}$ such that
\begin{equation*}
test
\end{equation*}
\end{theorem}
\begin{proof}
*** I guess its best to provide a reference here, although I think it should also be possible to prove this ourselves. ***
\end{proof}

\begin{theorem}\label{theorem:continuous_rate_matrix_has_process}
Consider any right-continuous function $Q_t$ that gives for each time point $t\in\realsnonneg$ a rate matrix $Q_t\in\mathcal{R}$, where by right-continuity we mean that for all $t\in\realsnonneg$,
\begin{equation*}
\lim_{s\downarrow t} Q_s = Q_t\,.
\end{equation*}
Then, there exists a stochastic process $P\in\mprocesses$ such that $Q_t$ characterizes $P$.
\end{theorem}
\begin{proof}
{\bf TODO.}
\end{proof}

"Exact methods for the transient analysis of nonhomogeneous continuous time Markov chains"


"an empirical transition matrix for non-homogeneous markov chains based on censored observations"



% \subsubsection{*** non-markov version (?) ***}
% TODO

%\subsection{*** Ik denk dat dat brol is ***}
%
%\begin{lemma}\label{lemma:differenceproductoftransition}
%Consider two sequences $A_1,\dots,A_n$ and $B_1,\dots,B_n$ of stochastic matrixes such that, for all $i\in\{1,\dots,n\}$, $\norm{A_i-B_i}\leq c$. Then
%\begin{equation*}
%\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}\leq nc
%\end{equation*}
%\end{lemma}
%\begin{proof}
%We provide a proof by induction. For $n=1$, the result is trivially true. Assume that the result holds for $n=k-1$. The following derivation then shows that it also holds for $n=k$: 
%\begin{align*}
%\norm{\prod_{i=1}^nA_i-\prod_{i=1}^nB_i}
%&=
%\norm{\prod_{i=1}^{n}A_i-\left(\prod_{i=1}^{n-1}A_i\right)B_n+\left(\prod_{i=1}^{n-1}A_i\right)B_n-\prod_{i=1}^{n}B_i}\\
%&\leq
%\left(\prod_{i=1}^{n-1}\norm{A_i}\right)\norm{A_n-B_n}+\norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\norm{B_n}\\
%&\leq c + \norm{\prod_{i=1}^{n-1}A_i-\prod_{i=1}^{n-1}B_i}\leq c+(n-1)c= nc.
%\end{align*}
%\end{proof}

%\section{The Lower Transition Operator}\label{sec:lower_operator}

\subsection{Stuff that needs to find a place}




*** Throughout this work, we will make extensive use of the notion of sequences of time points. For any $t,s\in\realsnonneg$ such that $t<s$, let $u$ denote any finite sequence of time points $t_0,t_1,\ldots,t_n$ such that $t=t_0 < t_1 <\ldots < t_n = s$. We use $\mathcal{U}_{[t,s]}$ to denote the set of all such sequences. 
Given any $u\in\mathcal{U}_{[t,s]}$, we define for all $i\in\{1,\ldots,n\}$ the terms $\Delta_i\coloneqq t_i-t_{i-1}$.  Finally, define a function $\sigma(u)$, as
\begin{equation*}
\sigma(u) \coloneqq \max\bigl\{\Delta_i\,:\,i\in\{1,\ldots,n\}\bigr\}\,.
\end{equation*}
***

*** deze moeten we ergens kwijt, ik heb hem maar even hier neergezet. beetje onhandig alleen aangezien we boundedness van $\rateset$ hier nog niet gedefinieerd hebben ***

\begin{lemma}\label{lemma:bound_precise_linear_approx}
Let $Q_\tau$ be any piecewise-constant function that gives for each time point $\tau\in\realsnonneg$ a rate matrix in some non-empty bounded set of rate matrices $\rateset$. By Theorem~\ref{theorem:continuous_rate_matrix_has_process}, there is then some Markov process $P\in\mprocesses_\rateset$ that is characterized by $Q_\tau$. Consider any $t\in\realsnonneg$ and any $\Delta\in\realsnonneg$ such that $Q_\tau$ is constant on the interval $[t,t+\Delta]$. Then, the transition matrix $T_t^{t+\Delta}$ corresponding to $P$ satisfies
\begin{equation*}
\norm{\left[I + \Delta Q_t\right] - T_t^{t+\Delta}} \leq \Delta^2\norm{\rateset}^2\,.
\end{equation*}
\end{lemma}
\begin{proof}
{\bf TODO}
\end{proof}

\section{Imprecise Continuous-Time Markov chains}
\label{sec:iCTMC}

*** starten van een set van Q's (bounded), en dan drie manieren beschouwen om imprecies te maken:

Consider any set $\rateset\subseteq\mathcal{R}$ of rate matrices. Then $\rateset$ is said to be \emph{non-empty} if $\rateset\neq\emptyset$ and $\rateset$ is said to be \emph{bounded} if $\norm{\rateset}<+\infty$. The following proposition provides a simple alternative characterisation of boundedness.

\begin{proposition}\label{prop:alternativedefforbounded}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ is bounded if and only if
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset\right\}>-\infty\text{~~for all $x\in\states$.}
\end{equation*}
\end{proposition}
\begin{proof}
*** moet dit nog invullen ***
\end{proof}

\subsection{*** onder van homogene ***}

uitleggen dat het dan NIET is

\subsection{*** onder van inhomogene Markov ***}


\begin{definition}[Set of Markov Processes]\label{def:markov_process_set_new}
For any bounded set of rate matrices $\rateset$, we define the set $\mprocesses_{\rateset}$ of all continuous-time Markov processes \emph{consistent} with $\rateset$. Formally, we let $\mprocesses_{\rateset}$ be the set of all $P\in\mprocesses$ such that
\begin{align*}\label{eq:conditionforMarkov_new}
(\forall\epsilon\in\realspos)&\,
(\exists\delta\in\realspos)\,
(\forall t\in\realsnonneg)\,
(\forall\Delta\in(0,\delta))\,
(\exists Q\in\rateset)\,:\\
 &\,(\forall f\in\gamblesX)(\forall x\in\states)~
\left\lvert\frac{\mathbb{E}_{X_{t+\Delta}}[f(X_{t+\Delta})\,\vert\,X_t=x]-f(x)}{\Delta}-\left[Qf\right](x)\right\rvert<\epsilon\cdot\norm{f}\,.
\end{align*}
\end{definition}

\begin{definition}[Lower Expectation for Set of Markov Processes]\label{def:lower_markov} Consider any $t,s\in\realsnonneg$ such that $t<s$, any bounded set of rate matrices $\rateset$, and the set of corresponding continuous-time Markov processes $\mprocesses_\rateset$. Then, the \emph{lower expectation with respect to $\mprocesses_\rateset$} is defined for all $f\in\gamblesX$ and $x_t\in\states$, as
\begin{equation*}
\underline{\mathbb{E}}^\mathrm{M}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\mprocesses_\rateset\right\}\,.
\end{equation*}
\end{definition}

\subsection{*** onder van niet-Markov}

\begin{definition}[Set of Non-Markov Processes]\label{def:set_non_markov_process}
For any bounded set of rate matrices $\rateset$, we consider the set $\processes_\rateset$ of all stochastic processes \emph{consistent} with $\rateset$. Formally, we let $\processes_\rateset$ be the set of all $P\in\processes$ such that
\begin{align*}
&(\forall\epsilon\in\realspos)\,(\exists\delta\in\realspos)\,: \\
 &(\forall t\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall u\in\mathcal{U}_{[0,t]})\,(\forall(x_{t_0},\ldots,x_{t_{n-1}})\in\states^{\{t_0,\ldots,t_{n-1}\}})\,(\exists Q\in\rateset)\,: \\
 &(\forall f\in\gambles(\states^{u\cup\{t+\Delta\}}))\,(\forall x_{t_n}\in\states^{\{t_n\}}): \\
 &\abs{\frac{\mathbb{E}_{X_{t+\Delta}}[f(x_{t_0},\ldots,x_{t_n},X_{t+\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] - f(x_{t_0},\ldots,x_{t_n},x_{t_n})}{\Delta} - \left[Q f(x_{t_0},\ldots,x_{t_{n}},X_{t+\Delta})\right](x_{t_n})} \\ 
 &\quad\quad < \epsilon\cdot\norm{f}\,.
\end{align*}
\end{definition}

\begin{definition}[Lower Expectation for Set of Non-Markov Processes]\label{def:lower_non_markov} Consider any $t,s\in\realsnonneg$ such that $t<s$, any bounded set of rate matrices $\rateset$, and the set of corresponding continuous-time non-Markov processes $\processes_\rateset$. Then, the \emph{lower expectation with respect to $\processes_\rateset$} is defined for all $f\in\gamblesX$ and $x_t\in\states$, as
\begin{equation*}
\underline{\mathbb{E}}[f(X_s)\,\vert\,X_t=x_t] \coloneqq \inf\left\{\mathbb{E}[f(X_s)\,\vert\,X_t=x_t]\,:\,P\in\processes_\rateset\right\}\,.
\end{equation*}
\end{definition}

\begin{proposition}\label{prop:markov_set_subset_of_nonmarkov_set}
Consider any bounded set of rate matrices $\rateset$, and the corresponding sets $\mprocesses_\rateset$ and $\processes_\rateset$ of continuous-time Markov processes and non-Markov processes, respectively. Then,
\begin{equation*}
\mprocesses_\rateset \subseteq \processes_\rateset\,.
\end{equation*}
\end{proposition}
\begin{proof}
This is immediate from Definitions \ref{def:markov_process_set_new} and \ref{def:set_non_markov_process}.
\end{proof}

\begin{proposition}\label{prop:lower_exp_markov_bounded_by_nonmarkov}
Consider any $t,s\in\realsnonneg$ such that $t<s$, any bounded set of rate matrices $\rateset$, any $f\in\gamblesX$, and any $x_t\in\states$. Then,
\begin{equation*}
\underline{\mathbb{E}}^\mathrm{M}[f(X_s)\,\vert\,X_t=x_t] \geq \underline{\mathbb{E}}[f(X_s)\,\vert\,X_t=x_t]\,.
\end{equation*}
\end{proposition}
\begin{proof}
This is immediate from Definitions \ref{def:lower_markov} and \ref{def:lower_non_markov}, and Proposition \ref{prop:markov_set_subset_of_nonmarkov_set}.
\end{proof}

\section{An Important Lower Transition Operator}
\label{sec:lowertrans}

Having introduced the notion of lower expectations with respect to sets of (non-)Markov processes, one might wonder how to compute these quantities, either numerically or for analytical purposes. 

Obviously, one way to go about doing this is to work directly with the definitions. That is, explicitly generate the entire set $\mprocesses_\rateset$ (or $\processes_\rateset$) for a given $\rateset$, compute expectations of a function $f$ for each element of this set, and then find the infimum of these expectations. It should be clear that this approach is fairly unwieldy, not in the least because for arbitrary $\rateset$ the corresponding set of processes may be infinite.

Therefore, we will instead provide an alternative characterization of these lower expectations. In particular, we will in this section introduce a specific \emph{lower transition operator}, which is a map from $\gamblesX$ to $\gamblesX$ that generalizes the notion of a transition matrix. We will here focus on introducing the relevant concepts, and showing that the operator of interest is well-defined. We end this section by relating this operator to existing work from the literature. In Sections~\ref{sec:connections} and~\ref{sec:funcs_multi_time_points} we will then establish the relation between this operator and lower expectations, and show that we can indeed use it to compute the quantities of interest.

\subsection{Lower Transition (Rate) Operators}

It is clear from Section {\bf SEC REF CTMC} that there is a strong connection between rate matrices and transition matrices corresponding to Markov chains. We here generalize these two concepts to \emph{lower transition rate operators} and \emph{lower transition operators}, respectively.

\begin{definition}[Lower Transition Rate Operator]\label{def:coh_low_trans_rate}
We will call a map $\lrate$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition rate operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, all constant functions $\mu\in\gamblesX$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=LR\arabic*:,ref=LR\arabic*]
\item\label{LR:constantzero}
$\lrate(\mu)(x)=0$;
\item\label{LR:subadditive}
$\lrate(f+g)(x)\geq\lrate(f)(x)+\lrate(g)(x)$;
\item\label{LR:homo}
$\lrate(\lambda f)(x)=\lambda\lrate(f)(x)$;
\item\label{LR:nondiagpos}
$\lrate(\ind{y})(x)\geq0$ for all $y\in\states$ such that $x\neq y$.
\end{enumerate}
\vspace{5pt}
\end{definition}


\begin{definition}[Lower Transition Operator]\label{def:coh_low_trans}
We will call a map $\lt$ from $\gamblesX$ to $\gamblesX$ a \emph{lower transition operator} if, for all $f,g\in\gamblesX$, all $\lambda\in\realsnonneg$, and all $x\in\states$:

%\vspace{5pt}
\begin{enumerate}[label=C\arabic*:]
\item
$\lt(f)(x)\geq\min\left\{f(y)\,\vert\,y\in\states\right\}$
\item
$\lt(f+g)(x)\geq\lt(f)(x)+\lt(g)(x)$;
\item
$\lt(\lambda f)(x)=\lambda\lt(f)(x)$.
\end{enumerate}
\vspace{5pt}
\end{definition}

\noindent We start by giving some useful properties of the norm of these operators.

\begin{lemma}\label{lem:normlratefinite}
For any lower transition rate operator $\lrate$, we have that $0\leq\norm{\lrate}<+\infty$.
\end{lemma}
\begin{proof}
*** {\bf TODO } ***
\end{proof}

\begin{lemma}\label{lemma:normofcoherenttrans}
For any lower transition operator $\lt$, we have that $0\leq \norm{\lt}\leq 1$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from coherence.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttransrate}
Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lrate$ be an arbitrary lower transition rate operator. Then, it holds that $\norm{\lrate A-\lrate B}\leq 2\norm{\lrate}\norm{A-B}$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from the definition of the norm and the properties of $\lrate$.
\end{proof}

\begin{lemma}\label{lemma:differencenormofcoherenttrans}
Consider any two non-negatively homogeneous operators $A$, $B$ from $\gamblesX$ to $\gamblesX$, and let $\lt$ be an arbitrary lower transition operator. Then, it holds that $\norm{\lt A-\lt B}\leq \norm{A-B}$.
\end{lemma}
\begin{proof}
*** {\bf TODO } *** This can be shown to follow from coherence.
\end{proof}

We next have two results about the set of all lower transition operators. As the first result shows, this set is closed under composition.
\begin{lemma}\label{lemma:compositioncoherence}
If $\lt_1,\lt_2,\dots,\lt_n$ are lower transition operators, then  $\lt_1\lt_2\cdots\lt_n$ is also a lower transition operator.
\end{lemma}
\begin{proof}
Simply check each of the properties.
\end{proof}

\noindent Furthermore, this set is a complete metric space.

\begin{lemma}\label{lemma:completemetricspace}
The set of all lower transition operators is a complete metric space.
\end{lemma}
\begin{proof}
Since a coherent lower transition operator is just a finite vector of coherent lower previsions, this result should follow fairly easily for the (known) fact that the set of all coherent lower previsions (on a given fixed space) is a complete metric space.

*** {\bf TODO} ***
\end{proof}

We conclude this section by establishing that there is a correspondence between lower transition rate operators and lower transition operators that is analogous to the one found in Section {\bf SEC REF CTMC}. 

\begin{lemma}\label{lemma:normQsmallenough}
Consider any lower transition rate operator $\lrate$, and any $0\leq\Delta\leq\nicefrac{1}{\norm{\lrate}}$. Then, $(I+\Delta\lrate)$ is a lower transition operator.
\end{lemma}
\begin{proof}
Just check each of the three defining properties. C2 and C3 are trivial. C1 requires a bit more work. *** {\bf TODO?} ***
\end{proof}

\noindent We therefore have the following result.
\begin{lemma}\label{lemma:productiscoherent}
Consider any $t,s\in\realsnonneg$ such that $t<s$, any lower transition rate operator $\lrate$, and any sequence $u\in\mathcal{U}_{[t,s]}$ of time points such that $\sigma(u)\leq\nicefrac{1}{\norm{\lrate}}$. Then
\begin{equation*}
\prod_{k=1}^n(I+\Delta_i\lrate)
\end{equation*}
is a lower transition operator.
\end{lemma}
\begin{proof}
Trivial consequence of Lemma~\ref{lemma:normQsmallenough} and~\ref{lemma:compositioncoherence}.
\end{proof}

\subsection{*** The operator of interest ***}

\begin{definition}[Lower Transition Operator]\label{def:low_trans}

Consider any $t,s\in\realsnonneg$ such that $t<s$ and let $\lrate$ be an arbitrary coherent lower transition rate operator . The corresponding \emph{lower transition operator} $\lbound_t^s$ is then a map from $\gamblesX$ to $\gamblesX$, defined by
\begin{equation}\label{eq:lowerbound}
\lbound_t^s\coloneqq\lim_{\sigma(u)\to0}\prod_{k=1}^n(I+\Delta_k\lrate),
\end{equation}
where the limit is taken with respect to the set $\mathcal{U}_{[t,s]}$ of all finite sequences of time points that partition the interval $[t,s]$.
\end{definition}

\noindent The remainder of this section establishes that the limit in Equation~\eqref{eq:lowerbound} exists, and that it is a coherent lower transition operator.

The proof basically works as follows. In Lemmas~\ref{lemma:normofcoherenttrans}-\ref{lemma:differencebetweennested}, we first establish bounds on the norm of the difference between various coherent lower transition operators.

For any $u\in\mathcal{U}_{[t,s]}$, we then define the operator
\begin{equation*}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate)\,,
\end{equation*}
so that for any sequence $u_1,u_2,\ldots,u_n,\ldots$, we can obtain a corresponding sequence $\Phi_{u_1},\Phi_{u_2},\ldots,\Phi_{u_n},\ldots$ of operators. Using the previously established bounds, Corollary~\ref{corol:limitexistsandiscoherent} establishes that if we choose $u_1,u_2,\ldots,u_n,\ldots$ such that $\lim_{n\rightarrow\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\ldots,\Phi_{u_n},\ldots$ converges, and that its limit point is a coherent lower transition operator. Theorem~\ref{theo:convergencelowerbound} finally establishes that this limit is furthermore unique. Thus, we show that the limit in Equation~\eqref{eq:lowerbound} exists, and that $L_t^s$ is a coherent lower transition operator. 

We begin with some bounds on norms.

\begin{lemma}\label{lemma:justtheindicator}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
\end{equation*}
\end{lemma}
\begin{proof}
See Lemma~\ref{lemma:justtheindicator_appendix} in the appendix.
\end{proof}

\begin{lemma}\label{lemma:justthelinearpart}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
See Lemma~\ref{lemma:justthelinearpart_appendix} in the appendix.
\end{proof}

\begin{lemma}\label{lemma:differencebetweennested}
For any $k\in\{1,\dots,n\}$, consider a sequence of $\Delta_{k,i}>0$, $i=1,\dots,n_k$ and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{n,k}$. Let $\Delta\coloneqq\sum_{k=1}^n\Delta_k$ and let $\alpha\coloneqq\max\{\Delta_k\colon k\in\{1,\dots,n\}\}$. If $\alpha\leq\nicefrac{1}{\norm{\lrate}}$, then
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
See Lemma~\ref{lemma:differencebetweennested_appendix} in the appendix.
\end{proof}

\noindent For any $u\in\mathcal{U}_{[t,s]}$, we now let
\begin{equation*}
\Phi_u\coloneqq\prod_{k=1}^n(I+\Delta_k\lrate)\,.
\end{equation*}

\begin{proposition}\label{prop:differencebetweenu}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and any $u,u^*\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\alpha$ and $\sigma(u^*)<\alpha$, with $0<\alpha\leq\nicefrac{1}{\norm{\lrate}}$. Let $\Delta\coloneqq s-t$. Then,
\begin{equation*}
\norm{\Phi_u-\Phi_{u^*}}\leq 2\alpha\Delta\norm{\lrate}^2
\end{equation*}
\end{proposition}
\begin{proof}
Consider any $u'\in\mathcal{U}_{[t,s]}$ that is finer than $u$ and $u^*$, meaning that the timepoints it consists of contain the timepoints in $u$ and the timepoints in $u^*$. For example, let $u'$ be the ordered union of the timepoints in $u$ and $u^*$.

This implies that, for all $k\in\{1,\dots,n\}$, there is some sequence $\Delta_{k,i}>0$, $i\in\{1,\dots,n_k\}$, such that $\Delta_k=\sum_{i=1}^{n_k}\Delta_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_u}\leq\alpha\Delta\norm{\lrate}^2$. 

Similarly, for all $k\in\{1,\dots,n^*\}$, there is some sequence $\Delta^*_{k,i}>0$, $i\in\{1,\dots,n^*_k\}$, such that $\Delta^*_k=\sum_{i=1}^{n^*_k}\Delta^*_{k,i}$ and
\begin{equation*}
\Phi_{u'}\coloneqq\prod_{k=1}^{n^*}\left(\prod_{i=1}^{n^*_k}(I+\Delta^*_{k,i}\lrate)\right).
\end{equation*}
It then follows from Lemma~\ref{lemma:differencebetweennested} that $\norm{\Phi_{u'}-\Phi_{u^*}}\leq\alpha\Delta\norm{\lrate}^2$.

Hence, we find that
\begin{equation*}
\norm{\Phi_{u}-\Phi_{u^*}}
=
\norm{\Phi_{u}-\Phi_{u'}+\Phi_{u'}-\Phi_{u^*}}
\leq
\norm{\Phi_{u}-\Phi_{u'}}
+
\norm{\Phi_{u'}-\Phi_{u^*}}
\leq2\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{proof}

\begin{corollary}\label{corol:cauchy}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ is a \emph{cauchy sequence}, meaning that
\begin{equation*}
(\forall \epsilon>0)(\exists N\in\nats)(\forall n,m\geq N)
\norm{\Phi_{u_n}-\Phi_{u_m}}<\epsilon.
\end{equation*}
\end{corollary}
\begin{proof}
This follows almost directly from Proposition~\ref{prop:differencebetweenu}.
\end{proof}

\begin{corollary}\label{corol:limitexistsandiscoherent}
For every sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$, the corresponding sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a coherent lower transition operator.
\end{corollary}
\begin{proof}
Since $\lim_{n\to\infty}\sigma(u_n)=0$, and because of Lemma~\ref{lemma:productiscoherent}, there is some index $i$ such that the sequence $\Phi_{u_i},\Phi_{u_{i+1}},\dots,\Phi_{u_n},\dots$ consists of coherent lower transition operators. Due to Corollary~\ref{corol:cauchy}, this sequence is cauchy and therefore, because of Lemma~\ref{lemma:completemetricspace}, this sequence has a limit that is also a coherent lower transition operator. Since the limit starting from $i$ and the limit starting from $1$ are identical (initial elements do not influence the limit), we find that the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ has a limit, and that this limit is a coherent lower transition operator.
\end{proof}

\begin{theorem}\label{theo:convergencelowerbound}
For any $t,s\in\realsnonneg$ such that $t<s$ and any coherent lower transition rate operator $\lrate$, there is a coherent lower transition operator $\lbound_t^s$ such that 
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \prod_{k=1}^n(I+\Delta_k\lrate)}<\epsilon.
\end{equation*}
\end{theorem}
\begin{proof}
Consider any sequence $u_1,u_2,\dots,u_n,\dots$ in $\mathcal{U}_{[t,s]}$ such that $\lim_{n\to\infty}\sigma(u_n)=0$. Due to Corollary~\ref{corol:limitexistsandiscoherent}, the sequence $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to a coherent lower transition operator, which we denote by $\lbound_t^s$. 

Consider now any $\epsilon>0$ and let $\Delta\coloneqq s-t$ and
\begin{equation*}
\delta\coloneqq\min\left\{\frac{\epsilon}{4\Delta\norm{\lrate}^2},\frac{1}{\norm{\lrate}}\right\}.
\end{equation*}

\noindent Since $\Phi_{u_1},\Phi_{u_2},\dots,\Phi_{u_n},\dots$ converges to $\lbound_t^s$, there is some $N\in\nats$ such that
\begin{equation*}
(\forall n\geq N)~\norm{\lbound_t^s - \Phi_{u_n}}<\frac{\epsilon}{2}.
\end{equation*}
Therefore, since $\lim_{n\to\infty}\sigma(u_n)=0$, there is some $N^*\geq N$ such that
\begin{equation*}
\sigma(u_{N^*})<\delta\text{ and }\norm{\lbound_t^s - \Phi_{u_{N^*}}}<\frac{\epsilon}{2}
\end{equation*}

\noindent Consider now any $u\in\mathcal{U}_{[t,s]}$ such that $\sigma(u)<\delta$. Then

\begin{equation*}
\norm{\lbound_t^s - \Phi_u}\leq\norm{\lbound_t^s-\Phi_{u_{N^*}}}
+\norm{\Phi_{u_{N^*}}-\Phi_u}
<\frac{\epsilon}{2}+2\delta\Delta\norm{\lrate}^2\leq\epsilon,
\end{equation*}
where the strict inequality follows from Proposition~\ref{prop:differencebetweenu}.
In summary, we have shown that there is some coherent lower transition operator $\lbound_t^s$ such that
\begin{equation*}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall u\in\mathcal{U}_{[t,s]}\colon\sigma(u)<\delta)~\norm{\lbound_t^s - \Phi_u}<\epsilon\,.
\end{equation*}
\end{proof}

\subsection{Derivatives, Relation to previous work, computations}

The derivative of $\lbound_t^s$ with respect to $t$ satisfies differential equations in the style of Damjan.

\begin{proposition}\label{prop:lower_transition_has_deriv}
Consider any $t,s\in\realsnonneg$ such that $t<s$, let $\lrate$ be an arbitrary coherent lower transition rate operator and let $\lbound_t^s$ be the corresponding lower transition operator. Then $\frac{d}{dt}\lbound_t^s=-\lrate\lbound_t^s$ and $\frac{d}{ds}\lbound_t^s=\lbound_t^s\lrate$, meaning that
\begin{equation}\label{eq:lower_deriv_backward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert <\delta)~
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation}
and
\begin{equation}\label{eq:lower_deriv_forward}
(\forall\epsilon>0)\,
(\exists\delta>0)\,
(\forall\Delta\,:\,0<\lvert\Delta\rvert<\delta)~
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon.
\end{equation}
\end{proposition}
\noindent The proof for the above proposition can be found in Appendix~\ref{sec:proof_appendix}.

\section{Connections Between $L_t^s$ and Imprecise Continuous-Time Markov Chains}\label{sec:connections}

One of the objectives of this paper is to establish a connection between the operator $\lbound_t^s$ that we have just introduced, and the different types of imprecise continous-time Markov chains that were discussed in Section~\ref{sec:iCTMC}. Since the former is derived from a lower transition rate operator $\lrate$ and the latter are derived from a non-empty bounded set of rate matrices $\rateset$, an obvious first step is to investigate the connection between lower transition rate operators and non-empty bounded sets of rate matrices.

\subsection{Connections Between $\lrate$ and Sets of Rate Matrices $\rateset$}

% $\rateset$ and $\lrate$. 
%In order to do that, we start by discussing some properties of sets of rate matrices.

We start by considering a non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices. For any $f\in\gamblesX$,
\begin{equation}\label{eq:correspondinglowertrans}
\lrate f\coloneqq\inf\{Qf\colon Q\in\rateset\}\\[2mm]
\end{equation}
is then again an element of $\gamblesX$.\footnote{%Since $\rateset$ is non-empty, the components of $\lrate f$ cannot be $+\infty$.
Since $\rateset$ is bounded,~\ref{N:normAf} implies that $\norm{Qf}\leq\norm{Q}\norm{f}<+\infty$ for all $Q\in\rateset$. Therefore, and since $\rateset$ is non-empty, the components of $\lrate f$ cannot be infinite. Hence, $\lrate f$ is a real-valued function on $\states$.}
Therefore, $\lrate$ is a map from $\gamblesX$ to $\gamblesX$. We call this operator $\lrate$, as defined by Equation~\eqref{eq:correspondinglowertrans}, the \emph{lower envelope} of $\rateset$. It is a matter of straightforward verification to see that $\lrate$ is a lower transition operator.

\begin{proposition}\label{prop:lowerenvelopeislowertrans}
For any non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices, the corresponding operator $\lrate\colon\gamblesX\to\gamblesX$, as defined by Equation~\eqref{eq:correspondinglowertrans}, is a lower transition rate operator.
\end{proposition}
\begin{proof}
Consider any $Q\in\rateset$. It then follows from Definition~\ref{def:rate_matrix} that the matrix $Q$, when regarded as a map from $\gamblesX$ to $\gamblesX$, satisfies \ref{LR:constantzero}--\ref{LR:nondiagpos}. Since each of these properties is preserved under taking lower envelopes, it follows that $\lrate$ satisfies \ref{LR:constantzero}--\ref{LR:nondiagpos}, which means that $\lrate$ is a lower transition rate operator.
\end{proof}

\noindent
Inspired by this result, we will also refer to the lower envelope of $\rateset$ as the \emph{lower transition rate operator that corresponds to $\rateset$}. %As we have just seen, every non-empty bounded set $\rateset\subseteq\mathcal{R}$ of rate matrices has such a corresponding lower transition rate operator $\lrate$. 
However, this correspondence is not one-to-one. As the following example establishes, different non-empty bounded sets of rate matrices may have the same corresponding lower transition rate operator.

\begin{exmp}
*** TO BE COMPLETED ***
\exampleend
\end{exmp}

Next, we consider some fixed lower transition rate operator $\lrate$.
All the non-empty bounded sets $\rateset$ of rate matrices that have $\lrate$ as their lower envelope then share a common property: they consist of rate matrices $Q$ that dominate $\lrate$, in the sense that $Qf\geq\lrate f$ for all $f\in\gamblesX$. Therefore, each of these sets $\rateset$ is contained in the following set of dominating rate matrices:
\begin{equation}\label{eq:dominatingratematrices}
\rateset_{\lrate}\coloneqq
\left\{
Q\in\mathcal{R}
\colon
Qf\geq\lrate f\text{ for all $f\in\gamblesX$}
\right\}.
\end{equation}
As our next result shows, this set $\rateset_{\lrate}$ is non-empty and bounded, and has $\lrate$ as its lower envelope. Even stronger, the infimum in Equation~\eqref{eq:correspondinglowertrans} is reached---can be replaced by a minimum.

\begin{proposition}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is non-empty and bounded and, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $\lrate f=Qf$.
\end{proposition}
\begin{proof}
Fix any $f\in\gamblesX$. Choose $\Delta>0$ small enough such that $0\leq\Delta\norm{\lrate}\leq 1$ (this always possible because of Lemma~\ref{lem:normlratefinite}). Define $\lt\coloneqq I+\Delta\lrate$. Since $\lrate$ is a lower transition rate operator, it follows from Lemma~\ref{lemma:normQsmallenough} that $\lt$ is a lower transition operator. For any $x\in\states$, we now let
\begin{equation*}
\lt_xg\coloneqq(\lt g)(x)
\text{~~for all $g\in\gamblesX$.}
\end{equation*}
Since $\lt$ is a lower transition operator, it follows that $\lt_x\colon \gamblesX\to\reals$ is subadditive, positively homogeneous and bounded below by the minimum operator. Hence, by definition~\cite[Definition~2.3.3]{Walley:1991vk}, $\lt_x$ is a coherent lower prevision on $\gamblesX$. Because of \cite[Theorem~3.3.3(b)]{Walley:1991vk}, this implies the existence of an expectation operator $E_x$ on $\gamblesX$---Reference~\cite{Walley:1991vk} calls this a linear prevision on $\gamblesX$---such that $E_xg\geq\lt_xg$ for all $g\in\gamblesX$ and $E_xf=\lt_xf$. Let $P_x$ be the unique probability mass function that corresponds to $E_x$. For all $x,y\in\states$, we now let $T(x,y)\coloneqq P_x(y)\coloneqq E_x(\ind{x})$. Then $T$ is clearly a stochastic matrix. Furthermore, for every $x\in\states$ and $g\in\gamblesX$, we have that $(Tg)(x)=E_xg$. Hence, it follows that $Tg\geq\lt g$ for all $g\in\gamblesX$ and that $Tf=\lt f$. Now let $Q\coloneqq\nicefrac{1}{\Delta}(T-I)$, which, because of Proposition~\ref{prop:rate_from_stochastic_matrix}, is a rate matrix. Since $Tf=\lt f$, it then follows that
\begin{equation*}
Qf=\frac{1}{\Delta}(Tf-f)\geq\frac{1}{\Delta}{\lt f-f}=\lrate f.
\end{equation*}
Similarly, since $Tg\geq\lt g$ for all $g\in\gamblesX$, it follows that $Qg\geq\lrate g$, or equivalently, since $Q$ is a rate matrix, that $Q\in\rateset_{\lrate}$. Since $f$ was arbitrary, this proofs that, for all $f\in\gamblesX$, there is some $Q\in\rateset_{\lrate}$ such that $Qf=\lrate f$. Since $\gamblesX$ is non-empty, this clearly implies that $\rateset_{\lrate}$ is non-empty.

We end this proof by showing that $\rateset_{\lrate}$ is bounded. Consider any $x\in\states$. Then for all $Q\in\rateset_{\lrate}$, we have that $Q(x,x)=(Q\ind{x})(x)\geq(\lrate\ind{x})(x)$, which implies that
\begin{equation*}
\inf\left\{Q(x,x)\colon Q\in\rateset_{\lrate}\right\}\geq(\lrate\ind{x})(x)>-\infty.
\end{equation*}
Since $x\in\states$ is arbitary, Proposition~\ref{prop:alternativedefforbounded} now guarantees that $\rateset_{\lrate}$ is bounded. 
\end{proof}

\noindent
Because of this result, and since---as discussed above---every non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope is a subset of $\rateset_{\lrate}$, it follows that $\rateset_{\lrate}$ is the largest non-empty bounded set of rate matrices that has $\lrate$ as its lower envelope.
Furthermore, as we will show in Proposition~\ref{prop:dominatingproperties} below, this set $\rateset_{\lrate}$ is also closed and convex, and has \emph{separately specified rows}.

\begin{definition}
A set of rate matrices $\rateset\subseteq\mathcal{R}$ has separately specified rows if
\begin{equation*}
\rateset=\left\{
Q\in\mathcal{R}
\colon
(\forall x\in\states)~Q(x,\cdot)\in\rateset_x\right\},
\end{equation*}
where, for every $x\in\states$, $\rateset_x\coloneqq\{Q(x,\cdot)\colon Q\in\rateset\}$ is some set of rows from which the $x$-th row of the rate matrices in $\rateset$ are taken.
\end{definition}
Thus, a set of rate matrices has separately specified rows if it is closed under taking arbitrary combinations of rows from its elements. We will later need the following result.
\begin{lemma}\label{lemma:rateset_has_arginf}
Let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$ and $\epsilon\in\realspos$, there exists a $Q\in\rateset$ such that
\begin{equation*}
\norm{\lrate f - Qf} < \epsilon\,.
\end{equation*}
\end{lemma}
\begin{proof}
This is immediate from the definition of the lower envelope of $\rateset$, as given by Equation~\eqref{eq:correspondinglowertrans}, and the fact that $\rateset$ has separately specified rows.

{\bf ATTN:} *** werkt dit eigenlijk ook als $\rateset$ geen separately specified rows heeft? Equation~\eqref{eq:correspondinglowertrans} is een beetje ambigu over of hij component-wise is of globaal, maar geen idee wat globaal zou betekenen. Als component-wise (zoals we eerder hadden), lijkt me dat we s.s.r. sowieso nodig hebben voor deze eigenschap (en dus ook voor benaderbaarheid van $L_t^s$)? ***
\end{proof}


\begin{proposition}\label{prop:dominatingproperties}
Consider a lower transition rate operator $\lrate$ and let $\rateset_{\lrate}$ be the corresponding set of dominating rate matrices, as defined by Equation~\eqref{eq:dominatingratematrices}. Then $\rateset_{\lrate}$ is closed and convex, and has separately specified rows.
\end{proposition}
\begin{proof}
*** TO BE COMPLETED ***
\end{proof}


\begin{exmp}
*** TO BE COMPLETED ***
\exampleend
\end{exmp}

\noindent
These additional properties characterise $\rateset_{\lrate}$ completely, in the sense that no other set satisfies them.

\begin{proposition}
Consider any non-empty, bounded, closed and convex set of rate matrices $\rateset\subseteq\mathcal{R}$ with separately specified rows that has $\lrate$ as its lower envelope. Then $\rateset=\rateset_{\lrate}$.
\end{proposition}
\begin{proof}
*** TO BE COMPLETED ***
\end{proof}

We conclude from all of this that non-empty bounded sets of rate matrices are more informative than lower transition rate matrices. Different non-empty bounded sets of rate matrices $\rateset$ may have the same lower transition rate operator $\lrate$ and therefore, in general, knowledge of $\lrate$ does not suffice to reconstruct $\rateset$; we can only reconstruct an outer approximation $\rateset_{\lrate}$, which is guaranteed to include $\rateset$. This changes if, besides non-empty and bounded, $\rateset$ is also closed and convex and has separately specified rows. In that case, $\lrate$ serves as an alternative representation for $\rateset$ because, since $\rateset=\rateset_{\lrate}$, we can use $\lrate$ to reconstruct $\rateset$. In other words: there is a one-to-one correspondence between lower transition rate operators and non-empty, bounded, closed and convex sets of rate matrices that have separately specified rows.

%\section{Imprecise Continuous-Time Markov Chains}\label{sec:imp_markov}

%\subsection{New Version}

%This section contains the new, simplified proofs.

\subsection{Connections Between $L_t^s$ and Lower Expectations $\underline{\mathbb{E}}$}\label{sec:single_var_lower_exp}

Having established a strong connection between the operator $\lrate$ and non-empty bounded sets of rate matrices $\rateset$, we will now turn to the connection between the operator $L_t^s$ and lower expectations $\underline{\mathbb{E}}$ with respect to sets of (non-)Markov processes. Specifically, we will in this section focus on the lower expectation of functions defined on the state space at a single point in time. In Section~\ref{sec:funcs_multi_time_points} we will then use and generalize these results when we consider functions defined on the state space at multiple time points.

As the following result shows, the operator $L_t^s$ induced by a lower transition rate operator $\lrate$ computes a lower bound on the expectation of a function $g\in\gambles(\states^{\{s\}})$, with respect to the set $\processes_\rateset$ of non-Markov processes induced by any set $\rateset$ of which $\lrate$ is the lower envelope.

\begin{theorem}\label{theorem:nonmarkov_single_var_lower_bounded}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\processes_\rateset$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $g\in\gambles(\states^{\{s\}})$,
\begin{equation*}
[L_t^s g](x_{t_n}) \leq \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{theorem}
\begin{proof}
Consider any $P\in\processes_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, and any $g\in\gambles(\states^{\{s\}})$. We will show that for all $\epsilon\in\realspos$,
\begin{equation*}
[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,,
\end{equation*}
which then implies $[L_t^s g](x_{t_n}) \leq \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]$. Start by choosing any $\epsilon\in\realspos$, and let $C\coloneqq (s-t)$.

Because $P\in\processes_\rateset$, it follows from Definition~\ref{def:set_non_markov_process} that there is some $\delta\in\realspos$ such that
\begin{align}\label{eq:nonmarkov_bound_proof_deriv_bounded}
\begin{split}
 &(\forall \tau\in\realsnonneg)\,(\forall\Delta\in(0,\delta))\,(\forall v\in\mathcal{U}_{[0,\tau]})\,(\forall(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}})\,(\exists Q\in\rateset)\,: \\
 &(\forall f\in\gambles(\states^{v\cup\{\tau+\Delta\}}))\,(\forall x_{\tau_m}\in\states^{\{\tau_m\}}): \\
 &\left\lvert \frac{\mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}] 
 - f(x_{\tau_0},\ldots,x_{\tau_m},x_{\tau_m})}{\Delta} \right. \\
 &\quad\quad\quad\quad\quad\quad - \left[Q f(x_{\tau_0},\ldots,x_{\tau_{m}},X_{\tau+\Delta})\right](x_{\tau_m})\biggr\rvert \quad < \quad\frac{\epsilon}{2C\norm{g}}\cdot\norm{f}\,.
\end{split}
\end{align}
Furthermore, it follows from Theorem~\ref{theo:convergencelowerbound} that there is some $\delta'\in\realspos$ such that
\begin{equation}\label{eq:nonmarkov_bound_proof_lbound_approx}
(\forall v\in\mathcal{U}_{[t,s]}\,:\,\sigma(v)<\delta')\,(\forall x_t\in\states)\abs{\left[L_t^s g\right](x_t) - \left[\prod_{k=1}^n(I+\Delta_i\lrate)g\right](x_t)} < \frac{\epsilon}{2}\,.
\end{equation}

Let $\delta^*\coloneqq\min\{\delta,\delta'\}$, and choose any $n>\nicefrac{C}{\delta^*}$. Then, for $\Delta\coloneqq\nicefrac{C}{n}$, we have $\Delta<\delta$ and $\Delta<\delta'$.

Equation \eqref{eq:nonmarkov_bound_proof_deriv_bounded} now implies that for all $\tau\in\realspos$, all $v\in\mathcal{U}_{[0,\tau]}$ such that $v=\tau_0,\ldots,\tau_m$, and all $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, there is some $Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}}\in\rateset$ such that, for all $f\in\gambles(\states^{v\cup\{\tau+\Delta\}})$ and all $x_{\tau_m}\in\states^{\{\tau_m\}}$,
\begin{align}\label{eq:nonmarkov_bound_proof_deriv_inequal}
\begin{split}
 &\left[(I + \Delta Q^v_{x_{\tau_0},\ldots,x_{\tau_{m-1}}})f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\right](x_{\tau_m}) - \frac{\Delta\epsilon\norm{f}}{2C\norm{g}} \\
 &\quad< \mathbb{E}[f(x_{\tau_0},\ldots,x_{\tau_m},X_{\tau+\Delta})\,\vert\,X_{\tau_0,\ldots,\tau_m}=x_{\tau_0,\ldots,\tau_m}]\,.
\end{split}
\end{align}
Now, note that by the basic properties of probability,
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
By choosing $\tau=(s-\Delta)$, setting $v=t_0,\ldots,t_n,(s-\Delta)$, and noting that $g\in\gambles(\states^{\{s\}})=\gambles(\states^{\{\tau+\Delta\}})\subset\gambles(\states^{v\cup\{\tau+\Delta\}})$, we find from Equation \eqref{eq:nonmarkov_bound_proof_deriv_inequal} that for all $(x_{t_0},\ldots,x_{t_n})\in\states^{u}$ there is some $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$ such that, for all $x_{s-\Delta}\in\states^{\{s-\Delta\}}$,
\begin{equation*}
\left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}=x_{s-\Delta}]\,.
\end{equation*}
Furthermore, we find using Equation~\eqref{eq:correspondinglowertrans} and the fact that $Q^v_{x_{t_0},\ldots,x_{t_n}}\in\rateset$, that
\begin{equation*}
\left[(I + \Delta \lrate)g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C} \leq \left[(I + \Delta Q^v_{x_{t_0},\ldots,x_{t_n}})g\right](x_{s-\Delta}) - \frac{\Delta\epsilon}{2C}\,.
\end{equation*}
Noting that an expectation takes a convex combination of values, we find by substitution that
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &= \mathbb{E}\bigl[\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s-\Delta}]\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] \\
&> \mathbb{E}\bigl[[(I+\Delta\lrate)g](X_{s-\Delta})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr] - \frac{\Delta\epsilon}{2C}\,.
\end{align*}

Note also that
\begin{equation*}
[(I+\Delta\lrate)g](X_{s-\Delta})\in\gambles(\states^{\{s-\Delta\}})\subset\gambles(\states^{u\cup\{s-\Delta\}})\,.
\end{equation*}
Hence, we can repeat this argument, factoring the expectation at time points $(s-2\Delta),\ldots,(s-(n-1)\Delta)$, to obtain
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - n\cdot\frac{\Delta\epsilon}{2C} \\
 &= \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2}\,.
\end{align*}

It follows from Equation \eqref{eq:nonmarkov_bound_proof_lbound_approx} and the fact that $\Delta<\delta'$,
\begin{equation*}
\left[L_t^s g\right](x_{t_n}) - \frac{\epsilon}{2} < \left[(I+\Delta\lrate)^n g\right](x_{t_n})\,,
\end{equation*}
so that by substitution,
\begin{align*}
\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] &> \left[(I+\Delta\lrate)^ng\right](x_{t_n}) - \frac{\epsilon}{2} \\
 &> [L_t^s g](x_{t_n}) - \frac{\epsilon}{2} - \frac{\epsilon}{2} \\
 &= [L_t^s g](x_{t_n}) - \epsilon\,.
\end{align*}
Thus, we have found that
\begin{equation*}
[L_t^s g](x_{t_n}) < \mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] + \epsilon\,.
\end{equation*}
Because the $\epsilon\in\realspos$ was arbitrary, this concludes the proof.
\end{proof}

As this previous result showed, $L_t^sg$ is a lower bound on the expectation of a function $g\in\gambles(\states^{\{s\}})$, with respect to a set of non-Markov processes $\processes_\rateset$ induced by some non-empty bounded set of rate matrices $\rateset$. Our next result establishes that this bound is tight if $\rateset$ also has separately specified rows. Specifically, we show that $L_t^sg$ can then be approximated to arbitrary precision by carefully choosing a Markov process $P$ from the set $\mprocesses_\rateset$.

\begin{theorem}\label{theorem:lower_markov_bound_is_tight}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $f\in\gamblesX$ and $\epsilon\in\realspos$, there exists a $P\in\mprocesses_{\rateset}$ such that
\begin{equation*}
\norm{\lbound_t^sf-T_t^sf} < \epsilon\,.
\end{equation*}
\end{theorem}
\begin{proof}
Choose any $f\in\gamblesX$ and any $\epsilon\in\realspos$. Let $C\coloneqq (s-t)$. 

The proof works by selecting a rate matrix at each point in time, showing that there is a $P\in\mprocesses_\rateset$ that is characterized by this construction, and finally establishing that this $P$ satisfies the inequality of interest.

By Lemma~\ref{lemma:rateset_has_arginf}, for all $\tau\in[t,s]$ there is some rate matrix $Q_\tau\in\rateset$ such that
\begin{equation}\label{eq:lower_char_rate_matrix}
\norm{\lrate \lbound_\tau^sf - Q_\tau \lbound_\tau^sf} < \frac{\epsilon}{2C}\,.
\end{equation}
To fix $Q_\tau$'s values outside of the interval $[t,s]$, let $Q_\tau \coloneqq Q_t$ for all $\tau\in[0,t]$, and let $Q_\tau\coloneqq Q_s$ for all $\tau>s$.
%Define a \emph{lower-characterizing rate matrix} $Q_\tau$ as
%\begin{equation}\label{eq:lower_char_rate_matrix}
%Q_\tau(x_\tau,\cdot)\coloneqq \argmin\left\{ Q(x_\tau,\cdot)\bigl[\lbound_\tau^sf\bigr]\,:\,Q(x_\tau,\cdot)\in\mathcal{Q}_{x_\tau}\right\}\quad\text{for all $x_\tau\in\states$ and $\tau\in[t,s]$}\,.
%\end{equation}
%Note that the $\argmin\{\cdot\}$ may be set-valued, in which case take an arbitrary element.
%Furthermore, let $Q_\tau \coloneqq Q_t$ for all $\tau\in[0,t]$, and let $Q_\tau\coloneqq Q_s$ for all $\tau>s$.

Next, define
\begin{equation}\label{eq:delta_required_for_tight_bound}
\delta \coloneqq \min\left\{\frac{\epsilon}{4C\norm{\mathcal{Q}}^2\norm{f}},\frac{1}{\norm{\lrate}}\right\}\,,
\end{equation}
and choose any $n>\nicefrac{C}{\delta}$. Then, for $\Delta\coloneqq \nicefrac{C}{n}$, we have $\Delta<\delta$.

For all $k\in\{0,\ldots,n\}$, define $t_k=t+k\Delta$. Let $u\coloneqq t_0,t_1,\ldots,t_n$. Finally, define a piecewise-constant approximation $Q_\tau^u$ to $Q_\tau$ such that, for all $k\in\{1,\ldots,n\}$,
\begin{equation}\label{eq:lower_char_matrix_linear_approx}
Q_\tau^u \coloneqq Q_{t_k},\quad\text{for all $\tau\in (t_{k-1},t_k]$}\,.
\end{equation}
Let $Q_\tau^u\coloneqq Q_{t_0}$ for all $\tau\leq t_0$ and let $Q_\tau^u\coloneqq Q_{t_n}$ for all $\tau>t_n$.

{\bf ATTN:} *** $Q_\tau^u$ is left-continuous, so the statement below may need some work. ***

Then, by Theorem~\ref{theorem:continuous_rate_matrix_has_process}, there is some $P\in\mprocesses_\mathcal{Q}$ that is characterized by $Q_\tau^u$, with corresponding transition matrix $T_t^s$. It remains to show that $\norm{L_t^sf - T_t^sf}<\epsilon$. We have
\begin{align*}
\norm{L_t^sf - T_t^sf} &= \norm{L_{t_0}^{t_n}f - T_{t_0}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f - T_{t_0}^{t_{n-1}}L_{t_{n-1}}^{t_n}f} + \norm{{T_{t_0}^{t_{n-1}}}}\cdot\norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \,.
\end{align*}
Recursion on the first summand yields
\begin{align*}
\norm{L_t^sf - T_t^sf} &\leq \norm{L_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_0}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
 &\leq \norm{L_{t_0}^{t_{n-2}}\left[L_{t_{n-2}}^{t_n}f\right] - T_{t_0}^{t_{n-2}}\left[L_{t_{n-2}}^{t_n}f\right]} \\
 &\quad\quad\quad+ \norm{L_{t_{n-2}}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right] - T_{t_{n-2}}^{t_{n-1}}\left[L_{t_{n-1}}^{t_n}f\right]} + \norm{L_{t_{n-1}}^{t_n}f - T_{t_{n-1}}^{t_n}f} \\
&\vdots \\
 &\leq \sum_{i=1}^{n} \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,.
\end{align*}
For all $i\in\{1,\ldots,n\}$, we have using Lemma~\ref{lemma:justthelinearpart} and the facts that $\Delta<\delta\leq\nicefrac{1}{\norm{\lrate}}$ and $\norm{\lrate}\leq\norm{\rateset}$,
\begin{align*}
&\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - \left[I+\Delta\lrate\right]L_{t_i}^{t_n}f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \norm{L_{t_{i-1}}^{t_i} - \left[I+\Delta\lrate\right]}\cdot\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \Delta^2\norm{\lrate}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
&\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]}\,.
\end{align*}
By Equation~\eqref{eq:lower_char_rate_matrix}, we have $\norm{Q_{t_i}L_{t_i}^{t_n}f - \lrate L_{t_i}^{t_n}f} < \nicefrac{\epsilon}{2C}$. Furthermore, by Equation~\eqref{eq:lower_char_matrix_linear_approx}, we have $Q_{t_i}^u=Q_{t_i}$, so that
\begin{align*}
 &\quad \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta\lrate\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \norm{\left[I+\Delta \lrate\right]L_{t_i}^{t_n}f - \left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f} \\
 &= \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \Delta\norm{\lrate L_{t_i}^{t_n}f - Q_{t_i}L_{t_i}^{t_n}f} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right]L_{t_i}^{t_n}f - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} + \frac{\Delta\epsilon}{2C} \\
 &\leq \Delta^2\norm{\mathcal{Q}}^2\norm{f} + \norm{\left[I+\Delta Q_{t_i}^u\right] - T_{t_{i-1}}^{t_i}}\cdot\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &\leq 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C}\,,
\end{align*}
where the last step used Lemma~\ref{lemma:bound_precise_linear_approx}.

Thus, using the fact that $n\Delta=C$, we find
\begin{align*}
\norm{L_t^sf - T_t^sf} &\leq \sum_{i=1}^n \norm{L_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right] - T_{t_{i-1}}^{t_i}\left[L_{t_i}^{t_n}f\right]} \\
 &\leq \sum_{i=1}^n 2\Delta^2\norm{\mathcal{Q}}^2\norm{f} + \frac{\Delta\epsilon}{2C} \\
 &= 2n\Delta^2\norm{\mathcal{Q}}^2\norm{f} + n\frac{\Delta\epsilon}{2C}\\
 &= 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2}\,,
\end{align*}
so that by Equation~\eqref{eq:delta_required_for_tight_bound} and the fact that $\Delta<\delta$, we have
\begin{equation*}
\norm{L_t^sf - T_t^sf} \leq 2C\Delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} < 2C\delta\norm{\mathcal{Q}}^2\norm{f} + \frac{\epsilon}{2} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon\,.
\end{equation*}
\end{proof}

Together, Theorems~\ref{theorem:nonmarkov_single_var_lower_bounded} and~\ref{theorem:lower_markov_bound_is_tight} establish a strong connection between $L_t^s$ and lower expectations $\underline{\mathbb{E}}$. In particular, for functions defined on a single point in time, they turn out to be equivalent, as shown by the result below.

\begin{corollary}\label{cor:lower_operator_is_infimum}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $g\in\gambles(\states^{\{s\}})$ and $x\in\states$,
\begin{equation*}
\left[L_t^sg\right](x) = \underline{\mathbb{E}}^\mathrm{M}[g(X_s)\,\vert\,X_t=x]\,,%\inf\Bigl\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\Bigr\}\,.
\end{equation*}
and furthermore,
\begin{equation*}
\left[L_t^sg\right](x) = \underline{\mathbb{E}}[g(X_s)\,\vert\,X_t=x]\,.%\inf\Bigl\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\Bigr\}\,.
\end{equation*}
\end{corollary}
\begin{proof}
We start by proving the first statement. By Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} and Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, we have that for all $P\in\mprocesses_\rateset$ it holds that $\left[L_t^sg\right](x) \leq \mathbb{E}[g(X_s)\,\vert\,X_t=x]$. Furthermore, by Theorem~\ref{theorem:lower_markov_bound_is_tight}, for all $\epsilon\in\realspos$ there is some $P\in\mprocesses_\rateset$ such that $\norm{L_t^sg - T_t^sg} < \epsilon$. Together this implies, using Definition~\ref{def:lower_markov}, that
\begin{equation*}
\left[L_t^sg\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\mprocesses_\rateset\right\} = \underline{\mathbb{E}}^{\mathrm{M}}\left[g(X_s)\,\vert\,X_t=x\right]\,.
\end{equation*}

We now move on to the second statement. By Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded} we have that for all $P\in\processes_\rateset$ it holds that $\left[L_t^sg\right](x) \leq \mathbb{E}[g(X_s)\,\vert\,X_t=x]$.
Furthermore, by Theorem~\ref{theorem:lower_markov_bound_is_tight} and Proposition~\ref{prop:markov_set_subset_of_nonmarkov_set}, for all $\epsilon\in\realsnonneg$ there is some $P\in\processes_\rateset$ such that
$\norm{L_t^sg - T_t^sg} < \epsilon$.
Together this implies, using Definition~\ref{def:lower_non_markov}, that
\begin{equation*}
\left[L_t^sg\right](x) = \inf\left\{\left[T_t^sf\right](x)\,:\,P\in\processes_\rateset\right\} = \underline{\mathbb{E}}\left[g(X_s)\,\vert\,X_t=x\right]\,.
\end{equation*}
\end{proof}

%\begin{definition}\label{def:lower_expectation}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices with separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $f\in\gamblesX$, we define the \emph{lower expectation of $f$ at time $s$, given the state at time $t$, with respect to $\mprocesses_\rateset$}, as
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}\left[f(X_s)\,\vert\,X_t\right]\coloneqq L_t^sf\,.
%\end{equation*}
%\end{definition}

One somewhat unsurprising result is therefore that $L_t^s$ computes the lower expectation of functions $f\in\gamblesX$ with respect to sets of Markov processes $\mprocesses_\rateset$. The reason that this is to be expected is the previously established correspondence between $L_t^s$ and the solution of the differential equation introduced in {\bf DAMJANREF}, which was there shown to compute exactly this quantity.

A rather more surprising result, perhaps, is that this \emph{same} operator also computes lower expectations with respect to sets $\processes_\rateset$ of non-Markov processes.

*** misschien nog iets over dat de precieze non-Markov dingen normaal als ``moeilijk'' gezien worden? ***

*** lalala sterker nog, zoals we in de volgende sectie gaan zien, kunnen we met $L_t^s$ dingen uitrekenen voor niet-Markov sets die niet kunnen voor Markov sets ***

\section{Functions Defined on Multiple Time Points}\label{sec:funcs_multi_time_points}

%\subsection{wat notatie voor meerdere variabelen}\label{sec:imp_non_markov}

%We will now move on to consider non-Markovian stochastic processes. Due to the need to consider multiple time points in such a process' history, we will find it convenient to switch from a transition-matrix notation to using (conditional) expectation operators. 
%We will start by introducing some notation for functions which makes explicit the time points that we are considering.

%\begin{definition}
%Consider any stochastic process $P\in\processes$, any $t,t',s\in\realsnonneg$ such that $t<t'<s$, any $u\in\mathcal{U}_{[t,t']}$, and any $f\in\gambles(\states^{u\cup\{s\}})$. Then, the \emph{expectation of $f$ at time $s$ given the states at times $u$}, with respect to $P$, is a map $\mathbb{E}$ from $\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^u)$, defined for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, as
%\begin{equation*}
%\mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \coloneqq \sum_{x_s\in\states^{\{s\}}} f(x_{t_0},\ldots,x_{t_n},x_s)P(X_s=x_s\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n})\,.
%\end{equation*}
%\end{definition}
%\noindent Note that $\gambles(\states^{\{s\}})\subset\gambles(\states^{u\cup\{s\}})$, so that for any $g\in\gambles(\states^{\{s\}})$,
%\begin{equation*}
%\mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \sum_{x_s\in\states^{\{s\}}} g(x_s)P(X_s=x_s\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n})\,.
%\end{equation*}

% \begin{definition}
% Consider any $t,t',s\in\realsnonneg$ such that $t<t'<s$, any $u\in\mathcal{U}_{[t,t']}$, any $g\in\gambles(\states^{\{s\}})$ and any $f\in\gambles(\states^{u\cup\{s\}})$. Let $\rateset$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. We will define the \emph{lower expectation operator} $\underline{\mathbb{E}}$, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, as
% \begin{equation*}
% \underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \coloneqq \left[L_{t_n}^sg\right](x_{t_n})\,,
% \end{equation*}
% and,
% \begin{equation*}
% \underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \coloneqq \left[L_{t_n}^sf(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
% \end{equation*}
% \end{definition}

Having shown that the operator $L_t^s$ computes lower expectations for functions defined on a single point in time, we will now turn our attention to functions defined on multiple time points. Because we are considering \emph{conditional} expectations, where the conditioning is done with respect to states in a (non-)Markov chain's history, it makes sense to distinguish between two different classes of functions defined at multiple time points. 

First, in Section~\ref{sec:function_single_future_multiple_past}, we will consider functions defined on a single time point in a chain's future, and multiple time points in the chain's history. Thus, we will consider functions $f\in\gambles(\states^{u\cup\{s\}})$, and lower expectations of the form
\begin{equation*}
\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
We will see that it is a straightforward implication of our results from Section~\ref{sec:single_var_lower_exp} that such lower expectations are computable using $L_t^s$, both with respect to sets of Markov chains and with respect to sets of non-Markov chains.

In Section~\ref{sec:decomposition} we will generalize this to functions defined on multiple time points in a chain's future and history, considering functions $f\in\gambles(\states^{u\cup v})$ and lower expectations of the form
\begin{equation*}
\underline{\mathbb{E}}\left[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
\end{equation*}
As we will see, for functions of this kind the lower expectations with respect to sets of non-Markov chains no longer correspond to those taken with respect to sets of Markov chains. One of the main results of this paper, however, is that we can still use the operator $L_t^s$ to compute lower expectations of this form if taken with respect to sets of non-Markov chains. We will see that this is because the optimization problem involved in computing lower expectations in some sense becomes simpler when we drop the Markov assumption.

Finally, in Section~\ref{sec:tractability}, we will show that although $L_t^s$ provides us with a way to compute such lower expectations, doing so for general functions $f\in\gambles(\states^{u\cup v})$ is still computationally intractable. However, we then provide algorithms to tractably compute lower expectations for large and practically useful subclasses of $\gambles(\states^{u\cup v})$.

\subsection{Multi-Variable Functions on a Single Point in the Future}\label{sec:function_single_future_multiple_past}

We start by considering functions $f\in\gambles(\states^{u\cup\{s\}})$ defined on a single time point $s$ in a (non-)Markov chain's future, and multiple time points $u=t_0,\ldots,t_n$ in a chain's history. Recall our notation from Section~\ref{sec:multivar_notation}; we write $f(x_{t_0},\ldots,x_{t_n},X_s)$ for the restriction of $f$ to $\states^{\{s\}}$ for a specific state assignment $(x_{t_0},\ldots,x_{t_n})$, and have defined
\begin{equation*}
\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n}) \equiv \left[L_t^sf(x_{t_0},\ldots,x_{t_n},X_s)\right](x_{t_n})\,.
\end{equation*}
The following results are now direct implications of our results from Section~\ref{sec:single_var_lower_exp}.
\begin{proposition}\label{prop:multi_var_single_future_bounded}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for all $P\in\processes_\rateset$, all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]\,.
\end{equation*}
\end{proposition}
\begin{proof}
Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we therefore have to show that $\left[L_t^sg\right](x_{t_n}) \leq \mathbb{E}\left[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]$. Because $g\in\gambles(\states^{\{s\}})$, this inequality holds by Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
\end{proof}

\begin{proposition}\label{prop:multi_var_single_future_tight}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $\epsilon\in\realspos$, there is a $P\in\mprocesses_\rateset$ such that
\begin{equation*}
\abs{\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n}) - \mathbb{E}\left[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\right]} < \epsilon\,.
\end{equation*}
\end{proposition}
\begin{proof}
Because $f(x_{t_0},\ldots,x_{t_n},X_s)$ is a restriction of $f$ to $\states^{\{s\}}$, there is some $g\in\gambles(\states^{\{s\}})$ such that $g(x_s) = f(x_{t_0},\ldots,x_{t_n},x_s)$ for all $x_s\in\states^{\{s\}}$. By substitution, we now have to show that there is a $P\in\mprocesses_\rateset$ such that $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} < \epsilon$. 

Because $g\in\gambles(\states^{\{s\}})$, by Theorem~\ref{theorem:lower_markov_bound_is_tight} there must be some $P\in\mprocesses_\rateset$ such that $\norm{L_t^sg - \mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon$. Consider this $P$. By the Markov property, its expectation must satisfy $\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]} = \abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]}$, and by the definition of the norm, we have 
\begin{equation*}
\abs{\left[L_t^sg\right](x_{t_n})-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}=x_{t_n}]} \leq \norm{L_t^sg-\mathbb{E}[g(X_s)\,\vert\,X_{t_n}]} < \epsilon\,.
\end{equation*}
\end{proof}

Note that this result is weaker than the corresponding Theorem~\ref{theorem:lower_markov_bound_is_tight} in Section~\ref{sec:single_var_lower_exp}. Specifically, this says that for a given history $(x_{t_0},\ldots,x_{t_n})\in\states^u$, there is a $P\in\mprocesses_\rateset$ that approaches $\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n})$. This does not imply that there is a $P\in\mprocesses_\rateset$ that approaches $\left[L_t^sf\right](x_{t_0},\ldots,x_{t_n})$ for \emph{all} histories! 

We will see in Section~\ref{sec:decomposition} that this is exactly the reason that computing lower expectations with respect to sets of non-Markov processes is ``easy''; by dropping the Markov assumption, the corresponding optimization problems become solvable locally with respect to a given history. In contrast, the optimization over sets of Markov processes must there be done globally with respect to all possible histories, because they do not allow for minimizing selections specific to a given trajectory.

For our present purposes, Proposition~\ref{prop:multi_var_single_future_tight} is still strong enough to imply the following result.

%\begin{proposition}
%In essentie, voor alle $f\in\gambles(\states^{u\cup\{s\}})$ en alle $\epsilon\in\realspos$, is er een $P\in\mprocesses_\rateset$ zodat
%\begin{equation*}
%\norm{L_t^s f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%{\bf TODO} This is immediate.
%\end{proof}

\begin{corollary}
Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and all $f\in\gambles(\states^{u\cup\{s\}})$,
\begin{equation*}
\left[L_t^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}^{\mathrm{M}}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]
\end{equation*}
and furthermore,
\begin{equation*}
\left[L_t^s f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]
\end{equation*}
\end{corollary}
\begin{proof}
This is a direct consequence of Propositions \ref{prop:markov_set_subset_of_nonmarkov_set}, \ref{prop:lower_exp_markov_bounded_by_nonmarkov}, \ref{prop:multi_var_single_future_bounded}, and \ref{prop:multi_var_single_future_tight}. The proof is similar to that of Corollary~\ref{cor:lower_operator_is_infimum}.
\end{proof}
Thus, we see that the operator $L_t^s$ can also be used to compute lower expectations of functions $f\in\gambles(\states^{u\cup\{s\}})$, both with respect to sets of Markov processes and with respect to sets of non-Markov processes. We will now turn to functions defined on multiple time points in a process' future, where we will find this correspondence to no longer hold.

%\begin{theorem}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\processes_\rateset$, any $u\in\mathcal{U}_{[0,t]}$, any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, and any $f\in\gambles(\states^{u\cup\{s\}})$,
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \leq \mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Consider any $P\in\processes_\rateset$, any $t,s\in\realsnonneg$ such that $t<s$, any $u\in\mathcal{U}_{[0,t]}$, any $f\in\gambles(\states^{u\cup\{s\}})$, and any $(x_{t_0},\ldots,x_{t_n})\in\states^u$.
%
%Note that
%\begin{equation*}
%f(x_{t_0},\ldots,x_{t_n},X_s)\in\gambles(\states^{\{s\}})
%\end{equation*}
%is the restriction of $f\in\gambles(\states^{u\cup\{s\}})$ to $\gambles(\states^{\{s\}})$. Hence, we can define a function $g\in\gambles(\states^{\{s\}})$ as
%\begin{equation*}
%g(x_s) \coloneqq f(x_{t_0},\ldots,x_{t_n},x_s)\,,\quad\quad\text{for all $x_s\in\states^{\{s\}}$}\,.
%\end{equation*}
%Then,
%\begin{equation*}
%\mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,,
%\end{equation*}
%and
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%Thus, it remains to show that
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \leq \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
%\end{equation*}
%Because $g\in\gambles(\states^{\{s\}})$, this was shown to hold by Theorem~\ref{theorem:nonmarkov_single_var_lower_bounded}.
%\end{proof}

%\begin{theorem}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for any $u\in\mathcal{U}_{[0,t]}$, any $g\in\gambles(\states^{\{s\}})$, and any $\epsilon\in\realspos$, there is some $P\in\processes_\rateset$ such that $P$ is Markovian, i.e. also $P\in\mprocesses_\rateset$, and furthermore,
%\begin{equation*}
%\norm{\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Because $g\in\gambles(\states^{\{s\}})$, we have by Definition~{\bf REF} that
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] = \underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_n}] = L_{t_n}^s g\,.
%\end{equation*}
%Furthermore, by Theorem~{\bf REF} there is some $P\in\mprocesses_\rateset$ with
%\begin{equation*}
%\mathbb{E}_{X_{s}}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] =  \mathbb{E}_{X_{s}}[g(X_s)\,\vert\,X_{t_n}] = T_{t_n}^sf
%\end{equation*}
%by virtue of the Markov property, and
%\begin{equation*}
%\norm{\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_{s}}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}]} = \norm{L_{t_n}^sg - T_{t_n}^sg}< \epsilon\,.
%\end{equation*}
%Because $P\in\mprocesses_\rateset$ and $\mprocesses_\rateset\subseteq\processes_\rateset$, we have $P\in\processes_\rateset$, which concludes the proof.
%\end{proof}

\subsection{Multi-Variable Functions on Multiple Points in the Future}\label{sec:decomposition}


We now consider functions $f\in\gambles(\states^{u\cup v})$, where $u=t_0,\ldots,t_n$ is a sequence of time points in a process' history, and $v=s_0,\ldots,s_m$ is a sequence of time points in a process' future; hence, we assume $s_0>t_n$. As the next example shows, the lower expectation of such functions, when taken with respect to a set $\mprocesses_\rateset$ of Markov processes, no longer necessarily corresponds to the lower expectation taken with respect to a set $\processes_\rateset$ of non-Markov processes.

\begin{exmp}
{\bf TODO} Example that sometimes $\underline{\mathbb{E}}^\mathrm{M}\neq \underline{\mathbb{E}}$ for functions $f\in\gambles(\states^{u\cup v})$.
\exampleend
\end{exmp}

An obvious question is therefore what the operator $L_t^s$ computes for functions of this form, as it clearly cannot compute both $\underline{\mathbb{E}}^\mathrm{M}$ and $\underline{\mathbb{E}}$. As we will see below, it turns out that we can use $L_t^s$ to compute the lower expectation of such functions with respect to sets of non-Markov processes. We start by showing that, using a composition of these operators, we can compute a lower bound with respect to a set $\processes_\rateset$.
\begin{proposition}\label{prop:multivar_bounded}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for all $P\in\processes_\rateset$, all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) \leq \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{proposition}
\begin{proof}
By the basic properties of expectation, we can decompose the expectation functional as
\begin{align*}
 &\quad \mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
 &= \mathbb{E}\bigl[\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{s_0,\ldots,s_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
Note that the nested expectation is conditioned on the time points $t_0,\ldots,t_n,s_0,\ldots,s_{m-1}$; it therefore only computes an expectation on a single point $s_m$ in the future. Hence, by Proposition~\ref{prop:multi_var_single_future_bounded} and the fact that the (outer) expectation computes a convex combination of values, we have
\begin{align*}
&\quad\mathbb{E}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] \\
 &\geq \mathbb{E}\bigl[[L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_{m-1}})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}\bigr]\,.
\end{align*}
The proof is then finished by backward induction on $m$.
\end{proof}

Note that a direct implication of this, together with Proposition~\ref{prop:lower_exp_markov_bounded_by_nonmarkov}, is that $L_t^s$ can also be used to compute lower bounds on the expectation with respect to a set $\mprocesses_\rateset$ of Markov processes. However, this bound will then in general not be tight, and hence will not correspond to the lower expectation.

To see why, observe that in the term $[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f](x_{t_0},\ldots,x_{t_n})$, the operators $L_{s_{i-1}}^{s_i}$ can take on different values depending on the choice of $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$. Hence, to approach these values of $L_{s_{i-1}}^{s_i}$ from within a set $\mprocesses$ of Markov processes, we have to be able to pick the approximating values such that they depend on the specific trajectory $(x_{t_0},\ldots,x_{t_n},x_{s_0},\ldots,x_{s_{i-1}})$, and this is exactly what the Markov condition prevents us from doing. As the next result shows, we can however approach this quantity from within a set $\processes_\rateset$ of non-Markov processes.

\begin{proposition}\label{prop:multivar_bound_tight}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$, all $v\in\mathcal{U}_{[s,s']}$, all $f\in\gambles(\states^{u\cup v})$, and all $\epsilon\in\realspos$, there is a $P\in\processes_\rateset$ such that
\begin{equation*}
\norm{L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f - \mathbb{E}[f(X_{t_0},\ldots,X_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
\end{equation*}
\end{proposition}
\begin{proof}
{\bf TODO} *** deze is nog een beetje ingewikkeld, we moeten weer een process bouwen en laten zien dat die in $\processes_\rateset$ zit ***
\end{proof}
\begin{corollary}
Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\rateset$ be an arbitrary non-empty bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$ and $(x_{t_0},\ldots,x_{t_n})\in\states^u$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$,
\begin{equation*}
\left[L_{t_n}^{s_0}L_{s_0}^{s_1}\cdots L_{s_{m-1}}^{s_m}f\right](x_{t_0},\ldots,x_{t_n}) = \underline{\mathbb{E}}[f(x_{t_0},\ldots,x_{t_n},X_{s_0},\ldots,X_{s_m})\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,.
\end{equation*}
\end{corollary}
\begin{proof}
This is a direct consequence of Propositions~\ref{prop:multivar_bounded} and \ref{prop:multivar_bound_tight}.
\end{proof}

%\begin{theorem}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for any $u\in\mathcal{U}_{[0,t]}$, any $f\in\gambles(\states^{u\cup\{s\}})$, and any $\epsilon\in\realspos$, there is some $P\in\processes_\rateset$ such that
%\begin{equation*}
%\norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} < \epsilon\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Choose any $u\in\mathcal{U}_{[0,t]}$, any $f\in\gambles(\states^{u\cup\{s\}})$, and any $\epsilon\in\realspos$.
%
%Define a \emph{lower-characterizing rate matrix} $Q_{x_{t_0},\ldots,x_{t_n},\tau}$ as
%\begin{equation*}
%Q_{x_{t_0},\ldots,x_{t_n},\tau}(x_\tau,\cdot) \coloneqq \argmin\bigl\{Q(x_\tau,\cdot)\left[\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n},X_{\tau}]\right]\,:\,Q(x_\tau,\cdot)\in\rateset_{x_\tau}\bigr\}\,,
%\end{equation*}
%for all $x_\tau\in\states^{\{\tau\}}$, all $\tau\in[t,s]$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$. Note that the $\argmin\{\cdot\}$ may be set-valued, in which case take an arbitrary element. Furthermore, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, let $Q_{x_{t_0},\ldots,x_{t_n},\tau}\coloneqq Q_{x_{t_0},\ldots,x_{t_n},t}$ for all $\tau\in[0,t]$, and let $Q_{x_{t_0},\ldots,x_{t_n},\tau}\coloneqq Q_{x_{t_0},\ldots,x_{t_n},s}$ for all $\tau>s$.
%
%Let $C\coloneqq (s-t)$, define
%\begin{equation*}
%\delta\coloneqq \min\left\{\frac{\epsilon}{2C\norm{\rateset}^2\norm{f}}, \frac{1}{\norm{\lrate}}\right\}\,,
%\end{equation*}
%and choose any $m>\nicefrac{C}{\delta}$. Then, for $\Delta\coloneqq\nicefrac{C}{m}$, we have $\Delta<\delta$.
%
%For all $k\in\{0,\ldots,m\}$, define $\tau_k\coloneqq t+k\Delta$. Let $v\coloneqq \tau_0,\tau_1,\ldots,\tau_m$. Finally, define for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$ a piecewise-constant approximation $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v$ to $Q_{x_{t_0},\ldots,x_{t_n},\tau}$ such that, for all $k\in\{1,\ldots,m\}$,
%\begin{equation*}
%Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\coloneqq Q_{x_{t_0},\ldots,x_{t_n},\tau_k}\,,\quad\text{for all $\tau\in(\tau_{k-1},\tau_k]$}\,.
%\end{equation*}
%Let $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\coloneqq Q_{x_{t_0},\ldots,x_{t_n},\tau_0}$ for all $\tau\leq\tau_0$ and let $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\coloneqq Q_{x_{t_0},\ldots,x_{t_n},\tau_m}$ for all $\tau>\tau_m$.
%
%Then, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$, $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v$ is a piecewise-constant matrix which takes values $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v\in\rateset$ for all $\tau\in\realsnonneg$. Hence, by Proposition {\bf REF}, there is some $P\in\processes_\rateset$ that is characterized by the collection $Q_{X_{t_0},\ldots,X_{t_n},\tau}^v$ of all $Q_{x_{t_0},\ldots,x_{t_n},\tau}^v$. 
%
%Let $\mathbb{E}$ be the expectation operator associated with this $P$. Then,
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad + \norm{\mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]}\,, \end{align*}
%where the last step used convexity of the expectation operator and the definition of the norm. Recursion on the first summand yields
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-1}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}\bigl[\mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}\bigl[\mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\quad\quad\quad + \norm{\mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_{\tau_{m-2}}}\bigl[\mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\quad\quad\quad + \norm{ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] - \mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&= \norm{\underline{\mathbb{E}}_{X_{\tau_{m-2}}}\bigl[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}]\,\big\vert\,X_{t_0,\ldots,t_n}\bigr] - \mathbb{E}_{X_{\tau_{m-2}}}[ \underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-2}}] \,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\quad\quad\quad + \norm{ \underline{\mathbb{E}}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}] - \mathbb{E}_{X_{\tau_{m-1}}}[\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{\tau_{m-1}}]\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-2}}]} \\
%&\quad\quad\quad + \norm{\underline{\mathbb{E}}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}] - \mathbb{E}_{X_{\tau_m}}[g(X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_{m-1}}]} \\
%&\vdots \\
%&\leq \sum_{i=1}^m \norm{\underline{\mathbb{E}}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr] - \mathbb{E}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr]}\,.
%\end{align*}
%For all $i\in\{1,\ldots,m\}$, we have
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr] - \mathbb{E}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr]} \\
%&\leq \norm{\underline{\mathbb{E}}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr] - [I+\Delta \lrate]\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}]} \\
%&\quad\quad + \norm{[I+\Delta \lrate]\underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] - \mathbb{E}_{X_{\tau_i}}\bigl[ \underline{\mathbb{E}}_{X_{\tau_m}}[f(X_{t_0},\ldots,X_{t_n},X_{\tau_m})\,\vert\,X_{t_0,\ldots,t_n,\tau_i}] \,\big\vert\,X_{t_0,\ldots,t_n,\tau_{i-1}}\bigr]} \\
%&\leq 2\Delta^2\norm{\rateset}^2\norm{f}\,,
%\end{align*}
%where the last step needs a bit more detail. Thus, we find
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\leq \sum_{i=1}^m 2\Delta^2\norm{\rateset}^2\norm{f} \\
%&= 2n\Delta^2\norm{\rateset}^2\norm{f} \\
%&= 2C\Delta\norm{\rateset}^2\norm{f}\,,
%\end{align*}
%so that by Equation {\bf REF} and the fact that $\Delta<\delta$, we have
%\begin{align*}
%&\quad \norm{\underline{\mathbb{E}}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}] - \mathbb{E}_{X_s}[f(X_{t_0},\ldots,X_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}]} \\
%&\leq 2C\Delta\norm{\rateset}^2\norm{f} \\
%&< 2C\delta\norm{\rateset}^2\norm{f} \\
%&\leq \epsilon\,.
%\end{align*}
%\end{proof}
%
%*** kan dit weg? ***
%\begin{corollary}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $u\in\mathcal{U}_{[0,t]}$, all $g\in\gambles(\states^{\{s\}})$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \inf\Bigl\{ \mathbb{E}_{X_s}[g(X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,:\, P\in\processes_\rateset \Bigr\}\,.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%This is immediate.
%\end{proof}
%
%
%*** kan dit weg? ****
%
%\begin{corollary}
%Consider any $t,s\in\realsnonneg$ such that $t<s$, and let $\rateset$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then for all $u\in\mathcal{U}_{[0,t]}$, all $f\in\gambles(\states^{u\cup\{s\}})$, and all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\underline{\mathbb{E}}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}] = \inf\Bigl\{ \mathbb{E}_{X_s}[f(x_{t_0},\ldots,x_{t_n},X_s)\,\vert\,X_{t_0,\ldots,t_n}=x_{t_0,\ldots,t_n}]\,:\, P\in\processes_\rateset \Bigr\}\,.
%\end{equation*}
%\end{corollary}
%\begin{proof}
%This is immediate.
%\end{proof}

\subsection{Algoritmes}\label{sec:tractability}

In the sequel, for a given $f\in\gambles(\states^{u\cup v})$, we will refer to the transition operator corresponding to the distribution $P\in\mathbb{P}_\mathcal{Q}$ that satisfies Theorem~\ref{theorem:nonmarkov_multi_variable_lower_envelope} 
as $\lt_u^v$.

\begin{proposition}
For any $f\in\gambles(\states^{u\cup v})$ and given the corresponding $\lt_u^v$, computing $\left[\lt_u^v f\right](x_{t_0},\ldots,x_{t_n})$ is not more difficult than computing $\left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})$ for an arbitrary $P\in\mathbb{P}_{\mathcal{Q}}$. Specifically, this takes a number of operations that is exponential in $m$.
\end{proposition}
\begin{proof}
The first claim is immediate from the fact that $\lt_u^v$ corresponds to a $P\in\mathbb{P}_\mathcal{Q}$. The difficulty of the computation follows from the fact that it is an expectation on $m$ variables, which requires summing over all values $(x_{\tau_0},\ldots,x_{\tau_m})\in\states^v$.
\end{proof}

\begin{proposition}
Identifying the $\lt_u^v$ corresponding to a given $f\in\gambles(\states^{u\cup v})$ is intractable.
\end{proposition}
\begin{proof}
The problem is essentially that for all combinations $(x_{\tau_0},\ldots,x_{\tau_m})\in\states^v$, we need to perform an optimization to find the corresponding $\lrate$. Clearly, the number of optimizations required is exponential in $m$.
\end{proof}

\begin{proposition}
There is a subclass of gambles $\mathcal{C}(\states^{u\cup v})\subset\gambles(\states^{u\cup v})$, so that for $f\in\mathcal{C}(\states^{u\cup v})$, identifying the corresponding $\lt_u^v$ is tractable, i.e., the required number of times that $\lrate$ needs to be computed is linear in $m$.
\end{proposition}
\begin{proof}
As a trivial example, take $\mathcal{C}(\states^{u\cup v}) = \gambles(\states^{\{s'\}})$. {\bf Claim: } This class can be made a lot bigger.
\end{proof}




\section{Conclusions \& Future Work}\label{sec:conclusions}

*** I would discuss sigma-additivity things here: explain that this is possible with our approach by using Kolmogorovs extension theorem, say that this would allow us to consider for example the lower and upper expected time till absorbtion. ***

%
%\section{*** Dit mag volgens mij volledig weg ***}\label{sec:decomp}
%
%We now generalize the results from the previous sections to the lower bound of expectations on functions $f\in\gambles(\states^{u\cup v})$, where $v\in\mathcal{U}_{[s,s']}$.
%
%We again start with some notation. For any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, any $u\in\mathcal{U}_{[0,t]}$ and any $v\in\mathcal{U}_{[s,s']}$, let $A_u^v$ be a non-negatively homogeneous operator from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$. Define the norm as before, i.e.,
%\begin{equation*}
%\norm{A_u^v} \coloneqq \sup\left\{\norm{A_u^v}\,:\,f\in\gambles(\states^{u\cup v}), \norm{f}=1\right\}\,.
%\end{equation*}
%This norm satisfies all properties N6-N12 from the previous section. The default notation for sequences of time points will be $u=t_0,\ldots,t_n$ and $v=\tau_0,\ldots,\tau_m$, where we will assume that $t_n\leq \tau_0$.
%
%As a special case of such an operator, define for all $f\in\gambles(\states^{u\cup v})$ the operator
%\begin{equation*}
%T_u^vf \coloneqq \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
%\end{equation*}
%This also satisfies for all $g\in\gambles(\states^v)$ the equality
%\begin{equation*}
%T_u^vg = \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[g(X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right]\,.
%\end{equation*}
%
%\begin{proposition}\label{proposition:nonmarkov_multi_variable_decompose}
%Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[s,s']}$, and any $P\in\mathbb{P}$ that has, for all $i\in\{1,\ldots,m\}$, transition operators
%\begin{equation*}
%T_{u\cup\{\tau_0,\ldots,\tau_{i-1}\}}^{\{\tau_i\}}\,.
%\end{equation*}
%Then, the expectation operator $T_u^v$ corresponding to $P$ satisfies, for all $f\in\gambles(\states^{u\cup v})$,
%\begin{equation*}
%T_u^v f = T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\,.
%\end{equation*}
%\end{proposition}
%\begin{proof}
%We will use backward induction on $m$. Note that
%\begin{equation*}
%T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} g = \mathbb{E}_{X_{\tau_m}}\left[g(X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]
%\end{equation*}
%This is immediate from the decomposition properties of expectation, i.e.,
%\begin{align*}
%&\quad T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f \\
% &= \mathbb{E}_{X_{\tau_0}}\left[\cdots\mathbb{E}_{X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\cdots\,\vert\,X_{t_0},\ldots,X_{t_n}\right] \\
% &= \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_m}}\left[f(X_{t_0},\ldots,X_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X_{t_0},\ldots,X_{t_n}\right] \\
% &= T_u^v f\,.
%\end{align*}
%\end{proof}
%
%\begin{definition}[Lower Joint Expectation Operator]\label{def:low_joint_exp_op}
%Recall the extended operator $L_u^{\{s\}}$ from Definition~\ref{def:low_full_cond_exp_op}, and define a new operator $L_u^v$ from $\gambles(\states^{u\cup v})$ to $\gambles(\states^u)$, as
%\begin{equation*}
%L_u^v \coloneqq L_u^{\{\tau_0\}}L_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}.
%\end{equation*}
%\end{definition}
%
%\begin{theorem}\label{theorem:nonmarkov_multi_variable_lower_bounded}
%Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\mathcal{Q}$ be an arbitrary bounded set of rate matrices with corresponding lower transition rate operator $\lrate$. Then, for any $P\in\mathbb{P}_\mathcal{Q}$, any $u\in\mathcal{U}_{[0,t]}$ and $v\in\mathcal{U}_{[s,s']}$, any $f\in\gambles(\states^{u\cup v})$ and any $(x_{t_0},\ldots,x_{t_n})\in\states^u$, it holds that
%\begin{equation*}
%\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) \leq \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%Consider any $(x_{t_0},\ldots,x_{t_n})\in\states^u$. Using the decomposition from Proposition~\ref{proposition:nonmarkov_multi_variable_decompose}, we find that
%\begin{align*}
%&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
%&= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
%&= \mathbb{E}_{X_{\tau_0}}\left[\cdots\mathbb{E}_{X_{\tau_m}}\left[f(x_{t_0},\ldots,x_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\cdots\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
%&= \mathbb{E}_{X_{\tau_0},\ldots,X_{\tau_{m-1}}}\left[\mathbb{E}_{X_{\tau_m}}\left[f(x_{t_0},\ldots,x_{t_n},X_{\tau_0},\ldots,X_{\tau_m})\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n}),X_{\tau_0},\ldots,X_{\tau_{m-1}}\right]\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right] \\
%&= \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{\tau_0},\ldots,x_{\tau_{m-1}})\,,
%\end{align*}
%where
%\begin{equation*}
%g(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \coloneqq \left[T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,.
%\end{equation*}
%Define a new function $g'(\cdot)$ as
%\begin{equation*}
%g'(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \coloneqq \left[L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_{m-1}})\,,
%\end{equation*}
%and note that by Theorem~\ref{theorem:nonmarkov_historic_variable_lower_bounded} we have for all $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$ that
%\begin{equation}\label{equation:nonmarkov_multiple_variable_theorem_eq}
%g'(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \leq g(x_{\tau_0},\ldots,x_{\tau_{m-1}})\,.
%\end{equation}
%Now, because
%\begin{equation*}
%\sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{\tau_0},\ldots,x_{\tau_{m-1}})
%\end{equation*}
%is a convex combination of values $g(x_{\tau_0},\ldots,x_{\tau_{m-1}})$ over all combinations $(x_{\tau_0},\ldots,x_{\tau_{m-1}})\in\states^{\{\tau_0,\ldots,\tau_{m-1}\}}$, we have by Equation~\ref{equation:nonmarkov_multiple_variable_theorem_eq} that
%\begin{align*}
%&\quad \left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) \\
%&= \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \\
%&\geq \sum_{x_{\tau_0}}\cdots\sum_{x_{\tau_{m-1}}} P\left(X_{\tau_0}=x_{\tau_0},\ldots,X_{\tau_{m-1}}=x_{\tau_{m-1}}\,\vert\,X^u=(x_{t_0},\ldots,x_{t_n})\right)g'(x_{\tau_0},\ldots,x_{\tau_{m-1}}) \\
%&= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-2}\}}^{\{\tau_{m-1}\}}L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{align*}
%Repeatedly applying this argument, i.e. using backward induction on $m$, finally reveals
%\begin{align*}
%\left[T_u^vf\right](x_{t_0},\ldots,x_{t_n}) &= \left[T_u^{\{\tau_0\}}T_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots T_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
% &\geq \left[L_u^{\{\tau_0\}}L_{u\cup\{\tau_0\}}^{\{\tau_1\}}\cdots L_{u\cup\{\tau_0,\ldots,\tau_{m-1}\}}^{\{\tau_m\}} f\right](x_{t_0},\ldots,x_{t_n}) \\
% &= \left[L_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{align*}
%\end{proof}
%
%\begin{theorem}\label{theorem:nonmarkov_multi_variable_lower_envelope}
%Consider any $t,s,s'\in\realsnonneg$ such that $t<s<s'$, and let $\mathcal{Q}$ be an arbitrary closed and bounded set of rate matrices that has separately specified rows, with corresponding lower transition rate operator $\lrate$. Then, for all $u\in\mathcal{U}_{[0,t]}$, all $v\in\mathcal{U}_{[s,s']}$, and all $f\in\gambles(\states^{u\cup v})$, there is some $P\in\mathbb{P}_\mathcal{Q}$ such that, for all $(x_{t_0},\ldots,x_{t_n})\in\states^u$,
%\begin{equation*}
%\left[L_u^v f\right](x_{t_0},\ldots,x_{t_n}) = \left[T_u^v f\right](x_{t_0},\ldots,x_{t_n})\,.
%\end{equation*}
%\end{theorem}
%\begin{proof}
%*** The non-Markovian stochastic process for which, for all $i\in\{0,\ldots,{m-1}\}$ and all $(x_{\tau_0},\ldots,x_{\tau_i})\in\states^{\{\tau_0,\ldots,\tau_i\}}$,
%\begin{equation*}
%Q_{u\cup\{\tau_0,\ldots,\tau_i,\mu\}}(x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_i}) \coloneqq \lrate L_\mu^{\tau_{i+1}} \left[L_{u\cup\{\tau_0,\ldots,\tau_{i+1}\}}^{\{\tau_{i+2}\}}f\right](x_{t_0},\ldots,x_{t_n},x_{\tau_0},\ldots,x_{\tau_i})
%\end{equation*}
%establishes the equality. ***
%\end{proof}


\bibliographystyle{plain} 
\bibliography{general}

\appendix

\section{Proofs of Lemmas from Section~\ref{sec:lower_operator}}\label{sec:proof_appendix}

\begin{lemma}\label{lemma:justtheindicator_appendix}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}\leq\Delta\norm{\lrate}.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
&=\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)+\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-\prod_{i=1}^{n-1}(I+\Delta_i\lrate)}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&\leq\norm{(I+\Delta_n\lrate)-I}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}\\
&=\Delta_n\norm{\lrate}+\norm{\prod_{i=1}^{n-1}(I+\Delta_i\lrate)-I}
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttrans}. By repeating this argument over and over again (actually, by induction), we find that
\begin{align*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-I}
\leq \Delta_n\norm{\lrate} +\Delta_{n-1}\norm{\lrate}+\cdots
+\Delta_1\norm{\lrate}
=\Delta\norm{\lrate}.
\end{align*}
\end{proof}


\begin{lemma}\label{lemma:justthelinearpart_appendix}
Consider any sequence $0<\Delta_i\leq\nicefrac{1}{\norm{\lrate}}$, $i=1,\dots,n$, and let $\Delta\coloneqq\sum_{i=1}^n\Delta_i$. Then
\begin{equation*}
\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\leq\Delta^2\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)+\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-(I+\sum_{i=2}^n\Delta_i\lrate)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\norm{\Delta_1\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\Delta_1\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1\norm{\lrate\left(\prod_{i=2}^n(I+\Delta_i\lrate)\right)-\lrate}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-I},
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:differencenormofcoherenttransrate}. Due to Lemma~\ref{lemma:justtheindicator}, this implies that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+\Delta_1 2\norm{\lrate}\left(\sum_{i=2}^n\Delta_i\right)\norm{\lrate}\\
&=\norm{\prod_{i=2}^n(I+\Delta_i\lrate)-(I+\sum_{i=2}^n\Delta_i\lrate)}+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right).
\end{align*}
By continuing in this way (applying induction) we find that
\begin{align*}
&\norm{\prod_{i=1}^n(I+\Delta_i\lrate)-(I+\Delta\lrate)}\\
&\leq
\norm{\prod_{i=n}^n(I+\Delta_i\lrate)-(I+\sum_{i=n}^n\Delta_i\lrate)}
+2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
&=2\norm{\lrate}^2\Delta_{n-1}\left(\sum_{i=n}^n\Delta_i\right)
+\cdots
+2\norm{\lrate}^2\Delta_1\left(\sum_{i=2}^n\Delta_i\right)\\
&=2\norm{\lrate}^2\sum_{k=1}^n\Delta_k\sum_{i=k+1}^n\Delta_i\\
&\leq2\norm{\lrate}^2\frac{1}{2}\left(\sum_{k=1}^n\Delta_k\right)^2=\Delta^2\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{lemma}\label{lemma:differencebetweennested_appendix}
For any $k\in\{1,\dots,n\}$, consider a sequence of $\Delta_{k,i}>0$, $i=1,\dots,n_k$ and let $\Delta_k\coloneqq\sum_{i=1}^{n_k}\Delta_{n,k}$. Let $\Delta\coloneqq\sum_{k=1}^n\Delta_k$ and let $\alpha\coloneqq\max\{\Delta_k\colon k\in\{1,\dots,n\}\}$. If $\alpha\leq\nicefrac{1}{\norm{\lrate}}$, then
\begin{equation*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
\leq\alpha\Delta\norm{\lrate}^2.
\end{equation*}
\end{lemma}
\begin{proof}
\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&=\norm{\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)\right)\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&=\norm{
\left(
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)(I+\Delta_n\lrate)
},
\end{align*}

\noindent
which, because of Lemma~\ref{lemma:productiscoherent}, \ref{lemma:normofcoherenttrans} and~\ref{lemma:differencenormofcoherenttrans}, implies that

\begin{align*}
&\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}\\
&\leq\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}\\
&~~~~+\norm{
\left(\prod_{i=1}^{n_n}(I+\Delta_{n,i}\lrate)\right)
-
(I+\Delta_n\lrate)
}\\
&\leq
\norm{
\left(
\prod_{k=1}^{n-1}\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
\right)
-
\left(\prod_{k=1}^{n-1}(I+\Delta_k\lrate)\right)
}
+
\Delta_n^2\norm{\lrate}^2,
\end{align*}
where the last inequality follows from Lemma~\ref{lemma:justthelinearpart}.

By continuing in this way (applying induction), we find that
\begin{align*}
\norm{\prod_{k=1}^n\left(\prod_{i=1}^{n_k}(I+\Delta_{k,i}\lrate)\right)
-
\prod_{k=1}^n(I+\Delta_k\lrate)
}
&\leq
\Delta_1^2\norm{\lrate}^2+\cdot+\Delta_k^2\norm{\lrate}^2+\cdot
+
\Delta_n^2\norm{\lrate}^2\\
&\leq
\alpha\Delta_1\norm{\lrate}^2+\cdot+\alpha\Delta_k\norm{\lrate}^2+\cdot
+
\alpha\Delta_n\norm{\lrate}^2\\
&=
\alpha\Delta\norm{\lrate}^2
\end{align*}
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:lower_transition_has_deriv}]
We start by proving Equation~\eqref{eq:lower_deriv_backward}. Choose any $\epsilon\in\realspos$, and let
\begin{equation}\label{eq:derivative_max_delta}
\delta \coloneqq \frac{\epsilon}{3\norm{\lrate}^2}.
\end{equation}
Then, consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
\begin{equation*}
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert<\epsilon
\end{equation*}
Start by rewriting the statement slightly to prevent having to consider different cases for positive and negative $\Delta$. Let $t'\coloneqq\max\{t,t+\Delta\}$, and let $\Delta'\coloneqq t'-t$. Then,
\begin{equation*}
\Big\lVert\frac{L_{t+\Delta}^s-L_t^s}{\Delta}+\lrate L_t^s\Big\rVert = \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s}\,.
\end{equation*}
Now,
\begin{align*}
\norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &= \norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^{t'}L_{t'}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}L_{t'}^s} \\
 &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}}\norm{L_{t'}^s} \\
 &\leq \norm{\frac{I - L_{t'-\lvert\Delta\rvert}^{t'}}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^{t'}} \\
 &= \norm{\frac{I - L_{0}^{\lvert\Delta\rvert}}{\lvert\Delta\rvert}+\lrate L_{0}^{\Delta'}} \\
 &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate L_{0}^{\Delta'}} \\
 &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert}-\lrate} + \norm{\lrate - \lrate L_{0}^{\Delta'}} \\
 &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + 2\norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2.
\end{align*}
Because $\Delta'$ satisfies either $\Delta'=0$ or $\Delta'=\lvert\Delta\rvert$, we have that $\Delta'\leq\lvert\Delta\rvert$. Thus,
\begin{align*}
\norm{\frac{L_{t'}^s - L_{t'-\lvert\Delta\rvert}^s}{\lvert\Delta\rvert}+\lrate L_{t'-\Delta'}^s} &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\Delta'\norm{\lrate}^2 \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &= 3\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &< 3\delta\norm{\lrate}^2 \\
 &= \epsilon\,,
\end{align*}
where the last step used Equation~\eqref{eq:derivative_max_delta}. This concludes the proof of Equation~\eqref{eq:lower_deriv_backward}. We will now prove Equation~\eqref{eq:lower_deriv_forward}.

Again choose any $\epsilon\in\realspos$, and let $\delta$ be given by
\begin{equation*}
\delta \coloneqq \frac{\epsilon}{2\norm{\lrate}^2}\,.
\end{equation*}
Consider any $\Delta\neq 0$ such that $\lvert\Delta\rvert<\delta$. We will show that
\begin{equation*}
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert<\epsilon\,.
\end{equation*}
We again first rewrite the statement to prevent having to perform case-work in the sign of $\Delta$. Let $s'\coloneqq\min\{s,s+\Delta\}$, and let $\Delta'\coloneqq s-s'$. Then,
\begin{equation*}
\Big\lVert\frac{L_{t}^{s+\Delta}-L_t^s}{\Delta}-\lbound_t^s\lrate \Big\rVert = \norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate}\,.
\end{equation*}
Now,
\begin{align*}
\norm{\frac{L_t^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'+\Delta'}\lrate} &= \norm{\frac{L_t^{s'}L_{s'}^{s'+\lvert\Delta\rvert} - L_t^{s'}}{\lvert\Delta\rvert} - L_t^{s'}L_{s'}^{s'+\Delta'}\lrate} \\
 &\leq \norm{L_t^{s'}}\norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
 &\leq \norm{\frac{L_{s'}^{s'+\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{s'}^{s'+\Delta'}\lrate} \\
 &= \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - L_{0}^{\Delta'}\lrate} \\
 &\leq \norm{\frac{L_{0}^{\lvert\Delta\rvert} - I}{\lvert\Delta\rvert} - \lrate} + \norm{\lrate - L_{0}^{\Delta'}\lrate} \\
 &\leq \frac{1}{\lvert\Delta\rvert}\cdot\norm{L_{0}^{\lvert\Delta\rvert} - (I+\lvert\Delta\rvert\lrate)} + \norm{\lrate}\norm{I - L_{0}^{\Delta'}} \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \Delta'\norm{\lrate}^2 \\
 &\leq \lvert\Delta\rvert\cdot\norm{\lrate}^2 + \lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &= 2\lvert\Delta\rvert\cdot\norm{\lrate}^2 \\
 &< 2\delta\norm{\lrate}^2 \\
 &= \epsilon\,.
\end{align*}
\end{proof}

\end{document}
