{\rtf1\ansi\ansicpg1252\cocoartf1504
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue233;}
{\*\expandedcolortbl;\csgray\c100000;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c93333;}
\paperw11900\paperh16840\margl1440\margr1440\vieww16580\viewh12400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf0 \expnd0\expndtw0\kerning0
Dear Thomas, \
\
Reviews about your paper Efficient Computation of Updated Lower Expectations for Imprecise Continuous-Time Hidden Markov Chains submitted to ISIPTA \'9217 have been received. Based on these reviews, we are happy to inform you that your submission is ACCEPTED for presentation at the symposium and inclusion in the proceedings. \
\
For your guidance, we append the reviewers' comments below. Please carefully revise your paper to address their comments. You will soon receive further instructions to submit the final version of your paper in the PMLR format. The resubmission deadline is {\field{\*\fldinst{HYPERLINK "http://airmail.calendar/2017-05-02%2012:00:00%20CEST"}}{\fldrslt \cf3 \ul \ulc3 May 2nd, 2017}}. \
\
Thank you for giving us the opportunity to consider your work. \
\
Yours sincerely, \
\
Alessandro Antonucci \
Giorgio Corani \
In\'e9s Couso \
S\'e9bastien Destercke \
ISIPTA \'9217 Program Committee Co-Chairs \
\
\
----------------------- REVIEW 1 --------------------- \
PAPER: 13 \
TITLE: Efficient Computation of Updated Lower Expectations for Imprecise Continuous-Time Hidden Markov Chains \
AUTHORS: Thomas Krak, Jasper De Bock and Arno Siebes \
\
Overall evaluation: 1 (weak accept) \
\
----------- Overall evaluation ----------- \
The authors put together work on imprecise markov chains and hidden markov chains by considering imprecise hidden markov chains in continuous time. They also discuss the problem of updating these chains and provide failry efficient algorithms to do so. \
\
The paper is nice in its attempt to cover as broad a model as possible, and the authors are also quite honest in the discussion of the strengths and weaknesses of their approach. My main concern is that I think that some of the choices made in the paper should be better justified: \
\
-For instance, when discussing imprecise continuous time markov chains they assume that any possible rate matrix can be combined with any possible initial model. What if this is not the case? I guess that since the approach is based on lower envelopes some loss of information is unavoidable; still, more precise models may be of interest in practice. \
\
-Similarly, the assumption of a bounded \\mathcal\{Q\} is not discussed, although this seems less problematic. \
\
-Some of the comments about the behaviour of the model are too vague (if M is 'nice enough', or that the converse 'often' holds. I'd suggest to be more specific here. \
\
-The authors (without mentioning its name) decide to work with the strong product in Definition 2, since they take the envelope of the products of precise models. This is fine of course, but since there are many different definitions of independent products for sets of probabilities I think you should mention why you take this approach here. One possible line would be that you try to be as informative as possible with the little information you have since this seems to be also behind the use of regular extension; but if so you should discuss it. \
\
-The measurability assumptions you make in section 3.1 were unclear to me, since in the end there are still a few problems with conditioning and also they are not made for other elements of the model. \
\
-Another point is that the well-behaviour of the regular extension is dependent on working on finite possibility spaces, since otherwise it may not produced an updated model that is coherent with the unconditional one. \
\
-Also, even in the first two cases you discuss in the last paragraph of page 10 there is a solution, they are not equivalent, since in one Generalised Bayes Rule has a unique solution and in the other it does not (and you must somewhat arbitrarily choose the most informative one). Also, for the third case you may mention solutions in the literature of imprecise probabilities such as full conditional measures or sets of desirable gambles. \
\
-Finally, I'd like the authors to be more specific on their approach to the generalised Bayes rule for density functions, showing that really in a number of interesting cases the method they propose works effectively...it may be the case, but after reading the paper I was not sure about whether the examples here were some lucky instances or not. Perhaps you can expand a bit this part. \
\
\
----------------------- REVIEW 2 --------------------- \
PAPER: 13 \
TITLE: Efficient Computation of Updated Lower Expectations for Imprecise Continuous-Time Hidden Markov Chains \
AUTHORS: Thomas Krak, Jasper De Bock and Arno Siebes \
\
Overall evaluation: 1 (weak accept) \
\
----------- Overall evaluation ----------- \
The authors develop imprecise continuous-time hidden Markov models, an extension of both continuous-time hidden Markov models and imprecise continuous-time Markov chains. They describe a few theoretical properties of such models, in particular, the characterization when output variables are continuous. They then present a dynamic programming approach to compute predictive inferences with the model. \
\
The paper is well-written, and the intuitive and less rigorous explanations allow one to navigate all the mathematics with not much effort. The results, while adapted from previous results in continuous-time Markov chains and imprecise (discrete-time) hidden Markov models, are interesting. \
\
My only "important" comment regards Definition 2, where an imprecise continuous-time hidden Markov model is _defined_ using a fixed/precise output model. IMHO it would be better to define the model allowing imprecise output, and then to assume a fixed output model (in this work). Otherwise, references in future work could become ambiguous w.r.t. to the definition of such models. \
\
I wonder whether the typical much simpler "trick" of transforming continuous (fintely-many) observed variables into binary random variables wouldn't be as general as the current approach (at least in what concerns the algorithm), while still mathematical precise. Could the authors please point out any advantage of allowing continuous variables in their model (given the inferences they entertain)? \
\
Another very useful inference in practice is to compute the probability/density of evidence. This can be used to classify sequences according to a collection of (imprecise) models. Perhaps the authors would like to include that possibility in the \
conclusion. \
\
\
----------------------- REVIEW 3 --------------------- \
PAPER: 13 \
TITLE: Efficient Computation of Updated Lower Expectations for Imprecise Continuous-Time Hidden Markov Chains \
AUTHORS: Thomas Krak, Jasper De Bock and Arno Siebes \
\
Overall evaluation: 1 (weak accept) \
\
----------- Overall evaluation ----------- \
Mathematics seems correct and English sounds fine. I think the authors could have made the paper more understandable with an example. Furthermore, practical utility should be also stressed: What can we say about prediction of future observations? What can we say about the evolution of the hidden process? Do its properties changes upon observations? Is it possible to make use of the algorithm (or a revised version) to simulate future behaviour (at least within some bands)? }